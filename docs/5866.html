<html>
<head>
<title>Decision Tree in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的决策树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-in-machine-learning-e380942a4c96?source=collection_archive---------2-----------------------#2018-11-13">https://towardsdatascience.com/decision-tree-in-machine-learning-e380942a4c96?source=collection_archive---------2-----------------------#2018-11-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/f7d07001046d68ca6a71fa9d7e8a182e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GnFhsaoOF6BvMJhvfFXzWw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="https://unsplash.com/search/photos/decision" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="fb85" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">决策树是一种类似流程图的结构，其中每个内部节点代表一个特征上的<code class="fe lb lc ld le b">test</code>(例如，硬币是正面还是反面)，每个叶节点代表一个<code class="fe lb lc ld le b">class label</code>(计算所有特征后做出的决定)，分支代表导致这些类别标签的特征的结合。从根到叶的路径代表<code class="fe lb lc ld le b">classification rules</code>。下图说明了用标签(下雨(是)，不下雨(否))进行决策的决策树的基本流程。</p><figure class="lg lh li lj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lf"><img src="../Images/e4671bb7880299fb257a5889ba80b3b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PB7MYQfzyaLaTp1n"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Decision Tree for Rain Forecasting</figcaption></figure><p id="de3c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">决策树是<code class="fe lb lc ld le b">statistics</code>、<code class="fe lb lc ld le b">data mining</code>和<code class="fe lb lc ld le b">machine learning</code>中使用的预测建模方法之一。</p><p id="feed" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">决策树是通过一种算法方法构建的，这种算法方法可以根据不同的条件识别分割数据集的方法。这是监督学习中最广泛使用和最实用的方法之一。决策树是一种<a class="ae kc" href="https://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/" rel="noopener ugc nofollow" target="_blank">非参数</a> <strong class="kf ir">监督学习</strong>方法，用于<strong class="kf ir">分类</strong>和<strong class="kf ir">回归</strong>任务。</p><p id="2290" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">目标变量可以取一组离散值的树模型称为<strong class="kf ir">分类树</strong>。目标变量可以取连续值(通常是实数)的决策树被称为<strong class="kf ir">回归树</strong>。分类和回归树(CART)是这个的通称。</p><p id="1503" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我将尝试用例子来解释。</p><h2 id="b553" class="lk ll iq bd lm ln lo dn lp lq lr dp ls ko lt lu lv ks lw lx ly kw lz ma mb mc bi translated">数据格式</h2><p id="77a3" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">数据以表格记录的形式出现。</p><pre class="lg lh li lj gt mi le mj mk aw ml bi"><span id="e448" class="lk ll iq le b gy mm mn l mo mp">(x,Y)=(x1,x2,x3,....,xk,Y)</span></pre><p id="f052" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因变量 Y 是我们试图理解、分类或概括的目标变量。向量 x 由特征 x1、x2、x3 等组成。，用于该任务。</p><p id="2749" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">示例</strong></p><pre class="lg lh li lj gt mi le mj mk aw ml bi"><span id="310d" class="lk ll iq le b gy mm mn l mo mp">training_data = [<br/>                  ['Green', 3, 'Apple'],<br/>                  ['Yellow', 3, 'Apple'],<br/>                  ['Red', 1, 'Grape'],<br/>                  ['Red', 1, 'Grape'],<br/>                  ['Yellow', 3, 'Lemon'],<br/>                  ]<br/> # Header = ["Color", "diameter", "Label"]<br/> # The last column is the label.<br/> # The first two columns are features.</span></pre><h1 id="6969" class="mq ll iq bd lm mr ms mt lp mu mv mw ls mx my mz lv na nb nc ly nd ne nf mb ng bi translated">决策树的生成方法</h1><p id="0d8a" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">在做决策树的时候，在树的每个节点我们会问不同类型的问题。基于所提出的问题，我们将计算与之对应的信息增益。</p><h2 id="f7a2" class="lk ll iq bd lm ln lo dn lp lq lr dp ls ko lt lu lv ks lw lx ly kw lz ma mb mc bi translated">信息增益</h2><p id="3039" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">信息增益用于决定在构建树的每一步中分割哪个特征。简单是最好的，所以我们想保持我们的树小。要做到这一点，在每一步我们都应该选择产生最纯净子节点的分裂。一种常用的纯度测量方法叫做信息。对于树的每个节点，信息值<strong class="kf ir">测量一个特征给我们多少关于类的信息。具有最高信息增益的分裂将被作为第一分裂，并且该过程将继续，直到所有子节点都是纯的，或者直到信息增益为 0。</strong></p><h1 id="196d" class="mq ll iq bd lm mr ms mt lp mu mv mw ls mx my mz lv na nb nc ly nd ne nf mb ng bi translated">提问</h1><pre class="lg lh li lj gt mi le mj mk aw ml bi"><span id="544c" class="lk ll iq le b gy mm mn l mo mp">class Question:<br/>  """A Question is used to partition a dataset.</span><span id="e703" class="lk ll iq le b gy nh mn l mo mp">  This class just records a 'column number' (e.g., 0 for Color) and a<br/>  'column value' (e.g., Green). The 'match' method is used to compare<br/>  the feature value in an example to the feature value stored in the<br/>  question. See the demo below.<br/>  """</span><span id="5c6a" class="lk ll iq le b gy nh mn l mo mp">  def __init__(self, column, value):<br/>      self.column = column<br/>      self.value = value</span><span id="61dc" class="lk ll iq le b gy nh mn l mo mp">  def match(self, example):<br/>      # Compare the feature value in an example to the<br/>      # feature value in this question.<br/>      val = example[self.column]<br/>      if is_numeric(val):<br/>          return val &gt;= self.value<br/>      else:<br/>          return val == self.value</span><span id="1dfb" class="lk ll iq le b gy nh mn l mo mp">  def __repr__(self):<br/>      # This is just a helper method to print<br/>      # the question in a readable format.<br/>      condition = "=="<br/>      if is_numeric(self.value):<br/>          condition = "&gt;="<br/>      return "Is %s %s %s?" % (<br/>          header[self.column], condition, str(self.value))</span></pre><p id="b955" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们尝试查询问题及其输出。</p><pre class="lg lh li lj gt mi le mj mk aw ml bi"><span id="3dde" class="lk ll iq le b gy mm mn l mo mp">Question(1, 3) ## Is diameter &gt;= 3?<br/>Question(0, "Green") ## Is color == Green?</span></pre><p id="410d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们将尝试根据提问的问题对数据集进行分区。在每个步骤中，数据将被分为两类。</p><pre class="lg lh li lj gt mi le mj mk aw ml bi"><span id="2184" class="lk ll iq le b gy mm mn l mo mp">def partition(rows, question):<br/>    """Partitions a dataset.</span><span id="0d89" class="lk ll iq le b gy nh mn l mo mp">    For each row in the dataset, check if it matches the question. If<br/>    so, add it to 'true rows', otherwise, add it to 'false rows'.<br/>    """<br/>    true_rows, false_rows = [], []<br/>    for row in rows:<br/>        if question.match(row):<br/>            true_rows.append(row)<br/>        else:<br/>            false_rows.append(row)<br/>    return true_rows, false_rows<br/>    <br/>   # Let's partition the training data based on whether rows are Red.<br/>   true_rows, false_rows = partition(training_data, Question(0, 'Red'))<br/>   # This will contain all the 'Red' rows.<br/>   true_rows ## [['Red', 1, 'Grape'], ['Red', 1, 'Grape']]<br/>   false_rows ## [['Green', 3, 'Apple'], ['Yellow', 3, 'Apple'], ['Yellow', 3, 'Lemon']]</span></pre><p id="138b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">构建决策树的算法通常是自上而下的，在<code class="fe lb lc ld le b">best</code>分割项目集的每一步选择一个变量。不同的算法使用不同的度量标准来测量<code class="fe lb lc ld le b">best</code>。</p><h1 id="958f" class="mq ll iq bd lm mr ms mt lp mu mv mw ls mx my mz lv na nb nc ly nd ne nf mb ng bi translated">基尼杂质</h1><p id="6ad4" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">首先让我们了解一下<strong class="kf ir">纯</strong>和<strong class="kf ir">不纯</strong>的含义。</p><h2 id="2855" class="lk ll iq bd lm ln lo dn lp lq lr dp ls ko lt lu lv ks lw lx ly kw lz ma mb mc bi translated">纯的</h2><p id="8040" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">纯意味着，在数据集的选定样本中，所有数据都属于同一类(纯)。</p><h2 id="0c23" class="lk ll iq bd lm ln lo dn lp lq lr dp ls ko lt lu lv ks lw lx ly kw lz ma mb mc bi translated">肮脏的</h2><p id="3d5d" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">不纯意味着，数据是不同类的混合物。</p><h2 id="e77b" class="lk ll iq bd lm ln lo dn lp lq lr dp ls ko lt lu lv ks lw lx ly kw lz ma mb mc bi translated">基尼系数的定义</h2><p id="f21c" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">基尼系数是对随机变量新实例不正确分类的可能性的一种度量，前提是该新实例是根据数据集中类别标签的分布随机分类的。</p><p id="0eac" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们的数据集是<code class="fe lb lc ld le b">Pure</code>，那么不正确分类的可能性是 0。如果我们的样本是不同类别的混合物，那么不正确分类的可能性将会很高。</p><p id="7f2e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">计算基尼杂质。</strong></p><pre class="lg lh li lj gt mi le mj mk aw ml bi"><span id="c0d0" class="lk ll iq le b gy mm mn l mo mp">def gini(rows):<br/>    """Calculate the Gini Impurity for a list of rows.<br/><br/>    There are a few different ways to do this, I thought this one was<br/>    the most concise. See:<br/>    https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity<br/>    """<br/>    counts = class_counts(rows)<br/>    impurity = 1<br/>    for lbl in counts:<br/>        prob_of_lbl = counts[lbl] / float(len(rows))<br/>        impurity -= prob_of_lbl**2<br/>    return impurity</span></pre><p id="d8b1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">示例</strong></p><pre class="lg lh li lj gt mi le mj mk aw ml bi"><span id="4ee2" class="lk ll iq le b gy mm mn l mo mp"># Demo 1:<br/>    # Let's look at some example to understand how Gini Impurity works.<br/>    #<br/>    # First, we'll look at a dataset with no mixing.<br/>    no_mixing = [['Apple'],<br/>                 ['Apple']]<br/>    # this will return 0<br/>    gini(no_mixing) ## output=0<br/>   <br/>   ## Demo 2:<br/>   # Now, we'll look at dataset with a 50:50 apples:oranges ratio<br/>    some_mixing = [['Apple'],<br/>                   ['Orange']]<br/>    # this will return 0.5 - meaning, there's a 50% chance of misclassifying<br/>    # a random example we draw from the dataset.<br/>    gini(some_mixing) ##output=0.5<br/>    <br/>    ## Demo 3:<br/>    # Now, we'll look at a dataset with many different labels<br/>    lots_of_mixing = [['Apple'],<br/>                      ['Orange'],<br/>                      ['Grape'],<br/>                      ['Grapefruit'],<br/>                      ['Blueberry']]<br/>    # This will return 0.8<br/>    gini(lots_of_mixing) ##output=0.8<br/>    #######</span></pre><h1 id="36ca" class="mq ll iq bd lm mr ms mt lp mu mv mw ls mx my mz lv na nb nc ly nd ne nf mb ng bi translated">制作决策树的步骤</h1><ul class=""><li id="b1c9" class="ni nj iq kf b kg md kk me ko nk ks nl kw nm la nn no np nq bi translated">获取行(数据集)的列表，这些行被考虑用于生成决策树(在每个节点递归)。</li><li id="e0fc" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated">计算我们数据集的<code class="fe lb lc ld le b">uncertanity</code>或<code class="fe lb lc ld le b">Gini impurity</code>或我们的<code class="fe lb lc ld le b">data is mixed up</code>有多少等等。</li><li id="13ab" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated">生成需要在该节点询问所有问题的列表。</li><li id="7e99" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated">根据每个问题将行划分为<code class="fe lb lc ld le b">True rows</code>和<code class="fe lb lc ld le b">False rows</code>。</li><li id="0321" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated">根据基尼系数和上一步数据划分计算信息增益。</li><li id="e67f" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated">根据每个问题更新最高信息增益。</li><li id="20cb" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated">基于信息增益(更高的信息增益)更新最佳问题。</li><li id="ac99" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated">在最佳问题上划分节点。从步骤 1 再次重复，直到我们得到纯节点(叶节点)。</li></ul><p id="991c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">上述步骤的代码</strong></p><pre class="lg lh li lj gt mi le mj mk aw ml bi"><span id="8d88" class="lk ll iq le b gy mm mn l mo mp">def find_best_split(rows):<br/>    """Find the best question to ask by iterating over every feature / value<br/>    and calculating the information gain."""<br/>    best_gain = 0  # keep track of the best information gain<br/>    best_question = None  # keep train of the feature / value that produced it<br/>    current_uncertainty = gini(rows)<br/>    n_features = len(rows[0]) - 1  # number of columns</span><span id="3b8e" class="lk ll iq le b gy nh mn l mo mp">    for col in range(n_features):  # for each feature</span><span id="cb3e" class="lk ll iq le b gy nh mn l mo mp">        values = set([row[col] for row in rows])  # unique values in the column</span><span id="e086" class="lk ll iq le b gy nh mn l mo mp">        for val in values:  # for each value</span><span id="8249" class="lk ll iq le b gy nh mn l mo mp">            question = Question(col, val)</span><span id="8ff4" class="lk ll iq le b gy nh mn l mo mp">            # try splitting the dataset<br/>            true_rows, false_rows = partition(rows, question)</span><span id="a433" class="lk ll iq le b gy nh mn l mo mp">            # Skip this split if it doesn't divide the<br/>            # dataset.<br/>            if len(true_rows) == 0 or len(false_rows) == 0:<br/>                continue</span><span id="5655" class="lk ll iq le b gy nh mn l mo mp">            # Calculate the information gain from this split<br/>            gain = info_gain(true_rows, false_rows, current_uncertainty)</span><span id="0421" class="lk ll iq le b gy nh mn l mo mp">            # You actually can use '&gt;' instead of '&gt;=' here<br/>            # but I wanted the tree to look a certain way for our<br/>            # toy dataset.<br/>            if gain &gt;= best_gain:<br/>                best_gain, best_question = gain, question</span><span id="3b25" class="lk ll iq le b gy nh mn l mo mp">    return best_gain, best_question<br/>    <br/>    #######<br/>    # Demo:<br/>    # Find the best question to ask first for our toy dataset.<br/>    best_gain, best_question = find_best_split(training_data)<br/>    best_question<br/>    ## output - Is diameter &gt;= 3?</span></pre><p id="ce8a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，根据上面讨论的步骤，在每个节点递归地构建决策树。</p><pre class="lg lh li lj gt mi le mj mk aw ml bi"><span id="a73f" class="lk ll iq le b gy mm mn l mo mp">def build_tree(rows):<br/>    """Builds the tree.</span><span id="02cb" class="lk ll iq le b gy nh mn l mo mp">    Rules of recursion: 1) Believe that it works. 2) Start by checking<br/>    for the base case (no further information gain). 3) Prepare for<br/>    giant stack traces.<br/>    """</span><span id="2ff2" class="lk ll iq le b gy nh mn l mo mp">    # Try <!-- -->partitioning <!-- -->the dataset on each of the unique attribute,<br/>    # calculate the information gain,<br/>    # and return the question that produces the highest gain.<br/>    gain, question = find_best_split(rows)</span><span id="327f" class="lk ll iq le b gy nh mn l mo mp">    # Base case: no further info gain<br/>    # Since we can ask no further questions,<br/>    # we'll return a leaf.<br/>    if gain == 0:<br/>        return Leaf(rows)</span><span id="86d3" class="lk ll iq le b gy nh mn l mo mp">    # If we reach here, we have found a useful feature / value<br/>    # to partition on.<br/>    true_rows, false_rows = partition(rows, question)</span><span id="80b2" class="lk ll iq le b gy nh mn l mo mp">    # Recursively build the true branch.<br/>    true_branch = build_tree(true_rows)</span><span id="7dfa" class="lk ll iq le b gy nh mn l mo mp">    # Recursively build the false branch.<br/>    false_branch = build_tree(false_rows)</span><span id="4c13" class="lk ll iq le b gy nh mn l mo mp">    # Return a Question node.<br/>    # This records the best feature / value to ask at this point,<br/>    # as well as the branches to follow<br/>    # dependingo on the answer.<br/>    return Decision_Node(question, true_branch, false_branch)</span></pre><h2 id="1d50" class="lk ll iq bd lm ln lo dn lp lq lr dp ls ko lt lu lv ks lw lx ly kw lz ma mb mc bi translated">构建决策树</h2><p id="cb0e" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">让我们根据训练数据建立决策树。</p><pre class="lg lh li lj gt mi le mj mk aw ml bi"><span id="901b" class="lk ll iq le b gy mm mn l mo mp">training_data = [<br/>                  ['Green', 3, 'Apple'],<br/>                  ['Yellow', 3, 'Apple'],<br/>                  ['Red', 1, 'Grape'],<br/>                  ['Red', 1, 'Grape'],<br/>                  ['Yellow', 3, 'Lemon'],<br/>                  ]<br/>  # Header = ["Color", "diameter", "Label"]<br/>  # The last column is the label.<br/>  # The first two columns are features.<br/>  <br/>  my_tree = build_tree(training_data)<br/>  <br/>  print_tree(my_tree)</span></pre><p id="d0b9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">输出</strong></p><pre class="lg lh li lj gt mi le mj mk aw ml bi"><span id="c687" class="lk ll iq le b gy mm mn l mo mp">Is diameter &gt;= 3?<br/>  --&gt; True:<br/>    Is color == Yellow?<br/>    --&gt; True:<br/>        Predict {'Lemon': 1, 'Apple': 1}<br/>    --&gt; False:<br/>        Predict {'Apple': 1}<br/> --&gt; False:<br/>    Predict {'Grape': 2}</span></pre><p id="780a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从上面的输出我们可以看到，在每一步，数据被分成<code class="fe lb lc ld le b">True</code>和<code class="fe lb lc ld le b">False</code>行。这个过程一直重复，直到我们到达信息增益为 0 的叶节点，并且由于节点是纯的，所以进一步的数据分割是不可能的。</p><p id="04de" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">决策树的优势</strong></p><ul class=""><li id="a501" class="ni nj iq kf b kg kh kk kl ko nw ks nx kw ny la nn no np nq bi translated">易于使用和理解。</li><li id="694b" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated">可以处理分类数据和数字数据。</li><li id="4001" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated">抵抗离群值，因此需要很少的数据预处理。</li></ul><p id="f193" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">决策树的缺点</strong></p><ul class=""><li id="e2ef" class="ni nj iq kf b kg kh kk kl ko nw ks nx kw ny la nn no np nq bi translated">容易过度拟合。</li><li id="42b7" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated">需要对他们的表现进行某种衡量。</li><li id="d5b8" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated">需要小心调整参数。</li><li id="7a18" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated">如果某些职业占优势，可能会创建有偏见的学习树。</li></ul><h2 id="5023" class="lk ll iq bd lm ln lo dn lp lq lr dp ls ko lt lu lv ks lw lx ly kw lz ma mb mc bi translated">如何避免决策树模型过拟合</h2><p id="715d" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">过拟合是机器学习中每个模型的主要问题之一。如果模型过度拟合，它将很难推广到新的样本。为了避免决策树过度拟合<strong class="kf ir">，我们移除了利用低重要性特征的分支。</strong>这种方法被称为<strong class="kf ir">修剪或后修剪。</strong>这样，我们将降低树的复杂性，并因此通过减少过拟合来提高预测准确性。</p><p id="39ad" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">修剪应该减少学习树的大小，而不会降低由<a class="ae kc" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" rel="noopener ugc nofollow" target="_blank">交叉验证</a>集测量的预测准确性。有两种主要的修剪技术。</p><ul class=""><li id="7f74" class="ni nj iq kf b kg kh kk kl ko nw ks nx kw ny la nn no np nq bi translated"><em class="nz">最小误差:</em>树被修剪回交叉验证误差最小的点。</li><li id="ad0a" class="ni nj iq kf b kg nr kk ns ko nt ks nu kw nv la nn no np nq bi translated"><em class="nz">最小的树:</em>树被修剪得比最小误差稍远。从技术上讲，修剪创建了交叉验证误差在最小误差的 1 个标准误差内的决策树。</li></ul><h2 id="deaf" class="lk ll iq bd lm ln lo dn lp lq lr dp ls ko lt lu lv ks lw lx ly kw lz ma mb mc bi translated"><strong class="ak">提前停止或预修剪</strong></h2><p id="3b1f" class="pw-post-body-paragraph kd ke iq kf b kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw mh ky kz la ij bi translated">防止过度拟合的另一种方法是，在生成样本非常少的叶子之前，尝试尽早停止树构建过程。这种试探法被称为<em class="nz">提前停止</em>，但有时也被称为预修剪决策树。</p><p id="320f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在分裂树的每个阶段，我们检查交叉验证错误。如果误差没有显著减小，那么我们停止。过早停止可能会因停止过早而不足。当前的拆分可能没什么好处，但在完成拆分后，后续的拆分会更显著地减少错误。</p><p id="d100" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">提前停止和修剪可以一起使用，单独使用，或者根本不使用。后期修剪决策树在数学上更加严格，找到一棵树至少和早期停止一样好。提前停止是一种快速解决的启发式方法。如果与修剪一起使用，尽早停止可以节省时间。毕竟，为什么建造一棵树只是为了再次修剪它？</p><h2 id="d4fd" class="lk ll iq bd lm ln lo dn lp lq lr dp ls ko lt lu lv ks lw lx ly kw lz ma mb mc bi translated">现实生活中的决策树</h2><ul class=""><li id="b03a" class="ni nj iq kf b kg md kk me ko nk ks nl kw nm la nn no np nq bi translated"><strong class="kf ir">选择要旅行的航班</strong></li></ul><p id="687b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设您需要为下一次旅行选择航班。我们该如何着手呢？我们先查一下那天的航班是否有空。如果没有，我们会寻找其他日期，但是如果有，我们会寻找航班的持续时间。如果我们只想要直达航班，那么我们会查看该航班的价格是否在您预先定义的预算内。如果太贵的话，我们看看其他的航班，或者我们预订它！</p><ul class=""><li id="77c4" class="ni nj iq kf b kg kh kk kl ko nw ks nx kw ny la nn no np nq bi translated"><strong class="kf ir">处理深夜的渴望</strong></li></ul><figure class="lg lh li lj gt jr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/025be47a8e0f25a24dc25ecab0cd1146.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/0*7wXqm23yUlcSddsP"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Source: Google</figcaption></figure><p id="5c50" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">决策树在现实生活中还有很多应用。更多决策树的应用可以查看<a class="ae kc" href="https://medium.com/@ailabs/5-machine-learning-algorithms-and-their-proper-use-cases-a8cfd0cedb51" rel="noopener">这个</a>和<a class="ae kc" href="http://what-when-how.com/artificial-intelligence/decision-tree-applications-for-data-modelling-artificial-intelligence/" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><p id="241e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从这篇文章中，我试图解释决策树的基础知识以及它是如何工作的。您可以在<a class="ae kc" href="https://github.com/java-byte/ML-Decision-Tree/blob/master/Decision_tree.ipynb" rel="noopener ugc nofollow" target="_blank"> github </a>找到本文使用的源代码。</p><p id="e374" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">希望你喜欢这篇文章。如果有任何修改或建议，请直接在这篇文章或 LinkedIn 上给我发消息。快乐学习—干杯:)</p></div></div>    
</body>
</html>