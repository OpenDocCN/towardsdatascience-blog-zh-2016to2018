<html>
<head>
<title>All you need to know about Regularization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于正规化，你需要知道的是</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-you-need-to-know-about-regularization-b04fc4300369?source=collection_archive---------3-----------------------#2018-12-23">https://towardsdatascience.com/all-you-need-to-know-about-regularization-b04fc4300369?source=collection_archive---------3-----------------------#2018-12-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="246c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">过度拟合的原因以及正则化如何改善它</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/fa35f5f5833e91d3a687e58df78887b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q2p1gkSePizfQ8_pS2-Xrw.png"/></div></div></figure><p id="458d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">爱丽丝</strong> <strong class="kt ir"> : </strong>嘿鲍勃！！！我已经对我的模型进行了 10 个小时的训练，但是我的模型产生了非常差的准确性，尽管它在训练数据上表现得非常好，这是什么问题？</p><p id="96a9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">鲍勃:哦！！看起来你的模型在训练数据上过度拟合了，你使用正则化了吗？</p><p id="d3d0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">艾丽斯:那是什么？</p><p id="9d2b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这种与过度拟合相关的问题在 ML 中很常见，有许多方法可以避免这种问题，但是<strong class="kt ir"> </strong>为什么会出现这种问题？</p><p id="e118" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">训练一个模型的过程似乎很简单:</p><ol class=""><li id="8be6" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">收集数据</li><li id="c67c" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">预处理数据</li><li id="b32f" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">设计一个模型</li><li id="9b7d" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">训练模型</li></ol><h2 id="3446" class="mb mc iq bd md me mf dn mg mh mi dp mj la mk ml mm le mn mo mp li mq mr ms mt bi translated">但是在这个过程中间发生了什么呢？什么导致过度拟合，为什么正则化给出了解决方案？</h2><p id="eb7b" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">过度拟合的主要原因可以追溯到开始的两个过程，数据的收集和预处理。具有不均匀特征分布、包含噪声、数据中的随机波动、非常高的方差的数据集合可能对模型训练产生相反的影响。这些随机误差和波动在训练时被模型很好地学习，以至于训练数据模型的精度变得非常高，导致数据的过度拟合。有时超过要求的训练会导致过度适应。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/a1fb779f2fb3e0364ba7566198aa0592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dG51OeX8jqUCXEMlLUZUzQ.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">source : <a class="ae ne" href="https://stats.stackexchange.com/" rel="noopener ugc nofollow" target="_blank">https://stats.stackexchange.com/</a></figcaption></figure><h2 id="bd1a" class="mb mc iq bd md me mf dn mg mh mi dp mj la mk ml mm le mn mo mp li mq mr ms mt bi translated">那么，怎样才能避免过度拟合的问题呢？</h2><p id="a689" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">可以容易地观察到，权重越高，非线性就越高，因此一种简单的方法是在更新权重时惩罚权重。在这一点上，我们有两个这样的技术使用这个想法。</p><ol class=""><li id="ff13" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated"><strong class="kt ir"> L1 定额:</strong></li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/2baaa9f6824b95883c08324275fd622a.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*aaTWtpj-EWT0Yu1rFCmPDg.jpeg"/></div></figure><p id="2909" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">其工作方式是在我们需要减少的误差函数中添加一个带有参数<strong class="kt ir"> λ </strong>的惩罚项。这里 w 不过是权重矩阵。</p><p id="f0af" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这里，<strong class="kt ir"> λ </strong>是一个超参数，其值由我们决定。如果<strong class="kt ir"> λ </strong>很高，它会对误差项增加很高的惩罚，使得学习到的超平面几乎是线性的，如果<strong class="kt ir"> λ </strong>接近 0，它对误差项几乎没有影响，不会导致正则化。</p><p id="014c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">L1 正则化也经常被视为一种特征选择技术，因为它将不需要的特征的相应权重清零。L1 在非稀疏情况下的计算效率也很低。L1 有时可能被称为<strong class="kt ir">套索回归</strong>。</p><p id="8895" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">2.<strong class="kt ir"> L2 常模:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/b778d7c93eadefb0a6374961d65bff7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y8bkQmtbYF5C40nsT3v4XA.png"/></div></div></figure><p id="d2f9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">L2 的正规化可能看起来与 L1 没有太大的不同，但它们有着几乎不相似的影响。这里重量“w”分别平方，然后相加。L1 的特征选择属性在这里丢失了，但是它在非稀疏情况下提供了更好的效率。有时 L2 被称为山脊回归。参数<strong class="kt ir"> λ </strong>的作用与 L1 相同。</p></div><div class="ab cl nh ni hu nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ij ik il im in"><p id="1773" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">提前停止:</strong>到目前为止，我们已经添加了惩罚来控制权重值，但也有其他方法来调整，如果在训练过程中，训练误差开始下降，测试误差开始上升，我们会停止训练。这将为我们提供所需的列车测试误差设置。这种技术通常被称为<strong class="kt ir">提前停止</strong>。这种技术可能会给我们想要的结果，但一般来说，有些人不建议这样做。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/2d48b2049953e7dc062e073d515cccd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yHKrjfpjbfpuohBcrQgAjg.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">source : deeplearning4j.org</figcaption></figure></div><div class="ab cl nh ni hu nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ij ik il im in"><p id="2a55" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">退出:</strong>这是 Srivastava 等人(2014 年)首次提出的一种有趣的正则化神经网络的方法，该论文提出，一个层中的一些节点必须随机选取，并且应该在训练期间被丢弃/忽略。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/cb2e3d57c63329bb919be3baf1aaacf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*O1m9boHrr59OpwH9rQQppQ.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">source : commonlounge.com</figcaption></figure><p id="62e1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">每个节点都有被丢弃的概率，假设一个节点的 drop_prob =0.4，那么它有 40%的机会被丢弃，60%的机会被拾起。每次这个概率都会导致神经网络改变形状，每次看起来都是一个新的网络。这种技术似乎很好地解决了正则化的问题。丢弃的概率不是很高，因为这将使网络稀疏，从而导致不匹配。</p></div><div class="ab cl nh ni hu nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ij ik il im in"><p id="06fb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">数据扩充:</strong>我们可能会看到计算机视觉中的过拟合问题，数据扩充是解决这一问题的更好方法。我们只需要自己放大图像，比如翻转、裁剪、旋转图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/030b28c230bfc9c043b7e01453b1bcd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*BQJUiaCMzRqhu5e_OV4o4Q.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">source: medium.com</figcaption></figure><p id="42f3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这种类型的增强似乎产生更好的结果，因为它提供了在一些变形图像和其他变化上进行训练的机会，并且当可用数据较少时，数据增强也有帮助。</p><p id="e3fc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这些是一些有助于解决过度拟合的正则化技术。有时调整超参数可能会得到想要的结果，但如果没有改善，那么上述技术肯定可以解决这个问题。</p></div></div>    
</body>
</html>