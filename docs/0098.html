<html>
<head>
<title>Adding Uncertainty to Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">给深度学习增加不确定性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adding-uncertainty-to-deep-learning-ecc2401f2013?source=collection_archive---------0-----------------------#2017-03-10">https://towardsdatascience.com/adding-uncertainty-to-deep-learning-ecc2401f2013?source=collection_archive---------0-----------------------#2017-03-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/be2aa0c0630a0bbfcf63e91c3f7990a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zotoRF1cnL19BqoADtHirw.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="f2e1" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">如何使用Edward和TensorFlow构建深度学习模型的预测区间</h2></div><p id="fd4b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">统计建模和机器学习之间的区别日益模糊。他们都从数据中学习并预测结果。主要的区别似乎来自于不确定性估计的存在。不确定性估计允许假设检验，尽管通常以可伸缩性为代价。</p><blockquote class="lm ln lo"><p id="fa3e" class="kq kr lp ks b kt ku kc kv kw kx kf ky lq la lb lc lr le lf lg ls li lj lk ll ij bi translated">机器学习=统计建模-不确定性+数据</p></blockquote><p id="4e30" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">理想情况下，我们通过向机器学习添加不确定性来融合两个世界的优点。最近在变分推理(<strong class="ks jc"> VI </strong>)和深度学习(<strong class="ks jc"> DL </strong>)方面的发展使这成为可能(也叫<a class="ae lt" href="http://dustintran.com/blog/a-quick-update-edward-and-some-motivations" rel="noopener ugc nofollow" target="_blank"> <strong class="ks jc">贝叶斯深度学习</strong> </a>)。VI的好处在于，它可以很好地随数据大小伸缩，并且很好地适应允许模型合成和随机优化的DL框架。</p><p id="b3b8" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">向模型添加不确定性的一个额外好处是，它促进了<a class="ae lt" href="https://blog.dominodatalab.com/an-introduction-to-model-based-machine-learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="ks jc">基于模型的机器学习</strong> </a>。在机器学习中，预测的结果是你的模型的基础。如果结果没有达到标准，策略就是“向问题扔数据”，或者“向问题扔模型”，直到满意为止。在基于模型(或贝叶斯)的机器学习中，您被迫指定数据和参数的概率分布。想法是首先明确地指定模型，然后检查结果(比点估计更丰富的分布)。</p><h2 id="a9b3" class="lu lv jb bd lw lx ly dn lz ma mb dp mc kz md me mf ld mg mh mi lh mj mk ml mm bi translated">贝叶斯线性回归</h2><p id="6de5" class="pw-post-body-paragraph kq kr jb ks b kt mn kc kv kw mo kf ky kz mp lb lc ld mq lf lg lh mr lj lk ll ij bi translated">这是一个给简单的线性回归模型增加不确定性的例子。简单的线性回归预测标签<em class="lp"> Y </em>给定数据<em class="lp"> X </em>权重<em class="lp"> w. </em></p><blockquote class="lm ln lo"><p id="bda5" class="kq kr lp ks b kt ku kc kv kw kx kf ky lq la lb lc lr le lf lg ls li lj lk ll ij bi translated">Y = w * X</p></blockquote><p id="391d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">目标是通过最小化损失函数找到未知参数<em class="lp"> w </em>的值。</p><blockquote class="lm ln lo"><p id="58a2" class="kq kr lp ks b kt ku kc kv kw kx kf ky lq la lb lc lr le lf lg ls li lj lk ll ij bi translated">(Y - w * X)</p></blockquote><p id="c840" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">让我们把这个转换成一个概率。如果假设<em class="lp"> Y </em>为高斯分布，则上述等价于最大化以下<strong class="ks jc">相对于<em class="lp"> w </em>的数据似然</strong>:</p><blockquote class="lm ln lo"><p id="45fb" class="kq kr lp ks b kt ku kc kv kw kx kf ky lq la lb lc lr le lf lg ls li lj lk ll ij bi translated">p(Y | X，w)</p></blockquote><p id="a1cf" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">到目前为止，这是传统的机器学习。要给你的体重估计增加不确定性，并将其转化为贝叶斯问题，只需在原始模型上附加一个<strong class="ks jc">先验分布</strong>即可。</p><blockquote class="lm ln lo"><p id="ebdb" class="kq kr lp ks b kt ku kc kv kw kx kf ky lq la lb lc lr le lf lg ls li lj lk ll ij bi translated">p(Y | X，w) * p(w)</p></blockquote><p id="dfac" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">请注意，这相当于通过贝叶斯规则反演原始机器学习问题的概率:</p><blockquote class="lm ln lo"><p id="1605" class="kq kr lp ks b kt ku kc kv kw kx kf ky lq la lb lc lr le lf lg ls li lj lk ll ij bi translated">p(w | X，Y) = p(Y | X，w) * p(w) /常数</p></blockquote><p id="458e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">给定数据的权重的概率(<em class="lp"> w </em>)就是我们对于不确定性区间所需要的。这是重量<em class="lp"> w </em>的<strong class="ks jc">后验分布</strong>。</p><p id="e8e9" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">尽管添加先验在概念上很简单，但计算通常很难；也就是说，<em class="lp">常数</em>是一个很大的、不好的积分。</p><h2 id="9e25" class="lu lv jb bd lw lx ly dn lz ma mb dp mc kz md me mf ld mg mh mi lh mj mk ml mm bi translated">蒙特卡罗积分</h2><p id="59f1" class="pw-post-body-paragraph kq kr jb ks b kt mn kc kv kw mo kf ky kz mp lb lc ld mq lf lg lh mr lj lk ll ij bi translated">概率分布的积分近似值通常是通过抽样来完成的。对分布进行采样和平均将得到期望值的近似值(也称为<strong class="ks jc">蒙特卡罗积分</strong>)。所以让我们把积分问题重新表述成一个期望问题。</p><p id="4d4e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">上面的<em class="lp">常数</em>从数据和权重之间的联合分布中整合出权重。</p><blockquote class="lm ln lo"><p id="581b" class="kq kr lp ks b kt ku kc kv kw kx kf ky lq la lb lc lr le lf lg ls li lj lk ll ij bi translated">常数= ∫ p(x，w) dw</p></blockquote><p id="4bbd" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">要将其重新表述为期望值，引入另一个分布，<em class="lp"> q </em>，并根据<em class="lp"> q </em>取期望值。</p><blockquote class="lm ln lo"><p id="9fd2" class="kq kr lp ks b kt ku kc kv kw kx kf ky lq la lb lc lr le lf lg ls li lj lk ll ij bi translated">∫ p(x，w) q(w) / q(w) dw = E[ p(x，w) / q(w) ]</p></blockquote><p id="8aad" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们选择一个<em class="lp"> q </em>分布，这样就很容易从中取样。从<em class="lp"> q </em>中抽取一串<em class="lp"> w </em>，取样本平均值得到期望值。</p><blockquote class="lm ln lo"><p id="4aca" class="kq kr lp ks b kt ku kc kv kw kx kf ky lq la lb lc lr le lf lg ls li lj lk ll ij bi translated">E[ p(x，w) / q(w) ] ≈样本均值[ p(x，w) / q(w) ]</p></blockquote><p id="ad61" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这个想法我们以后会用到变分推理。</p><h2 id="f6d1" class="lu lv jb bd lw lx ly dn lz ma mb dp mc kz md me mf ld mg mh mi lh mj mk ml mm bi translated">变分推理</h2><p id="49c7" class="pw-post-body-paragraph kq kr jb ks b kt mn kc kv kw mo kf ky kz mp lb lc ld mq lf lg lh mr lj lk ll ij bi translated">变分推理的思路是，你可以引入一个<strong class="ks jc">变分分布，<em class="lp"> q </em> </strong>，带<strong class="ks jc">变分参数，<em class="lp"> v </em> </strong>，把它变成一个优化问题。分布<em class="lp"> q </em>将近似于后验分布。</p><blockquote class="lm ln lo"><p id="cee5" class="kq kr lp ks b kt ku kc kv kw kx kf ky lq la lb lc lr le lf lg ls li lj lk ll ij bi translated">q(w | v) ≈ p(w | X，Y)</p></blockquote><p id="ac24" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这两个分布需要接近，一个自然的方法将最小化它们之间的差异。通常使用Kullback-Leibler散度(<strong class="ks jc"> KL散度</strong>)作为差分(或变分)函数。</p><blockquote class="lm ln lo"><p id="a6fd" class="kq kr lp ks b kt ku kc kv kw kx kf ky lq la lb lc lr le lf lg ls li lj lk ll ij bi translated">KL[q || p] = E[ log (q / p) ]</p></blockquote><p id="034f" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">KL散度可以分解为数据分布和证据下界(<strong class="ks jc"> ELBO </strong>)。</p><blockquote class="lm ln lo"><p id="947d" class="kq kr lp ks b kt ku kc kv kw kx kf ky lq la lb lc lr le lf lg ls li lj lk ll ij bi translated">KL[q || p] =常数<strong class="ks jc"> </strong> - <strong class="ks jc"> </strong> ELBO</p></blockquote><p id="cbef" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><em class="lp">常量</em>可以忽略，因为它不依赖于<em class="lp"> q </em>。直觉上，<em class="lp"> q </em>和<em class="lp"> p </em>的分母相互抵消，剩下ELBO。现在我们只需要在ELBO上进行优化。</p><p id="f7f2" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">ELBO只是具有变化分布的原始模型(<a class="ae lt" href="http://davmre.github.io/inference/2015/11/13/elbo-in-5min" rel="noopener ugc nofollow" target="_blank">在这里详细计算</a>)。</p><blockquote class="lm ln lo"><p id="05bd" class="kq kr lp ks b kt ku kc kv kw kx kf ky lq la lb lc lr le lf lg ls li lj lk ll ij bi translated">ELBO = E[ log p(Y | X，w)*p(w) - log q(w | v) ]</p></blockquote><p id="8c78" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">为了获得对<em class="lp"> q </em>的期望值，使用蒙特卡罗积分(取样并取平均值)。</p><p id="b22b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在深度学习中，通常使用随机优化来估计权重。对于每个小批，我们取损失函数的平均值来获得梯度的随机估计。类似地，任何具有自动微分的DL框架都可以将ELBO估计为损失函数。唯一的区别是你从<em class="lp"> q </em>中取样，平均值将是期望值的一个很好的估计，然后是梯度。</p><h2 id="470a" class="lu lv jb bd lw lx ly dn lz ma mb dp mc kz md me mf ld mg mh mi lh mj mk ml mm bi translated">构建预测间隔的代码示例</h2><p id="e981" class="pw-post-body-paragraph kq kr jb ks b kt mn kc kv kw mo kf ky kz mp lb lc ld mq lf lg lh mr lj lk ll ij bi translated">让我们对一些生成的数据运行贝叶斯简单线性回归示例。以下内容也适用于DL模型，因为它们只是简单的线性回归。我们使用一个名为Edward (基于<a class="ae lt" href="https://www.tensorflow.org" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>)的<a class="ae lt" href="http://edwardlib.org" rel="noopener ugc nofollow" target="_blank">贝叶斯深度学习库来构建模型。</a></p><p id="400e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><a class="ae lt" href="https://gist.github.com/tokestermw/a9de2ef498a09747bbf673ddf6ea4843" rel="noopener ugc nofollow" target="_blank">Gist上有完整的代码。</a></p><p id="3baf" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">第一步是定义权重的先验分布。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="c764" class="lu lv jb mx b gy nb nc l nd ne">weight = ed.models.Normal(mu=tf.zeros(1), sigma=tf.ones(1))</span></pre><p id="8b14" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在深度学习中，<code class="fe nf ng nh mx b">weight</code>将是点估计。在贝叶斯DL中，<code class="fe nf ng nh mx b">weight</code>是一个分布，它的后验概率由下面的变分分布来近似。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="a81d" class="lu lv jb mx b gy nb nc l nd ne">qw_mu = tf.get_variable(tf.random_normal([1]))<br/>qw_sigma = tf.nn.softplus(tf.get_variable(tf.random_normal([1]))<br/>qweight = ed.models.Normal(mu=qw_mu, sigma=qw_sigma)</span></pre><p id="b7e7" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">注意，变化参数<code class="fe nf ng nh mx b">qw_mu</code>和<code class="fe nf ng nh mx b">qw_sigma</code>是估计的，而<code class="fe nf ng nh mx b">weight</code>不是。从<code class="fe nf ng nh mx b">qweight</code>采样将给出<code class="fe nf ng nh mx b">weight</code>的后验不确定性区间(假设方差参数固定)。</p><p id="428e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们将线性回归定义为预测模型。任何DL型号都可以替代。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="9af2" class="lu lv jb mx b gy nb nc l nd ne">nn_mean = weight * x + bias</span></pre><p id="c201" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">数据似然性的构造类似于先验分布(假设方差参数固定)。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="41e2" class="lu lv jb mx b gy nb nc l nd ne">nn = ed.models.Normal(mu=nn_mean, sigma=tf.ones(1))</span></pre><p id="7af6" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">ELBO度量取决于数据似然性和先验。数据似然性由标签数据绑定。每个先验分布必须由变分分布约束(假设变分分布之间是独立的)。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="20ae" class="lu lv jb mx b gy nb nc l nd ne">latent_vars = {weight: qweight, bias: qbias}<br/>data = {nn: y}</span></pre><p id="c3fb" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">运行推理优化很容易，因为:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="4ce8" class="lu lv jb mx b gy nb nc l nd ne">inference = ed.KLqp(latent_vars, data)<br/>inference.run()</span></pre><p id="6ecd" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">为了提取不确定性区间，我们直接对变分分布<code class="fe nf ng nh mx b">qweight</code>进行采样。这里，前两个矩是在采样100次后提取的。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="8461" class="lu lv jb mx b gy nb nc l nd ne">mean_, var_ = tf.nn.moments(qweight.sample(100))</span></pre><p id="0c51" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们还可以将不确定性区间添加到预测中(也称为<strong class="ks jc">后验预测区间</strong>)。这可以通过复制模型并用所有后验概率替换先验来完成(本质上是在对数据进行训练之后更新模型)。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="92e7" class="lu lv jb mx b gy nb nc l nd ne">nn_post = ed.copy(nn, dict_swap={weight: qweight.mean(), bias: qbias.mean()})</span></pre><p id="3e49" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">再次运行图表100次，我们得到预测的不确定性区间。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="43c7" class="lu lv jb mx b gy nb nc l nd ne">mean_, var_ = tf.nn.moments(nn_post.sample(100), feed_dict={x: data})</span></pre><p id="c403" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">现在我们有了整个分布，假设检验就容易了。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="6b3c" class="lu lv jb mx b gy nb nc l nd ne">tf.reduce_mean(nn_post.sample(100) &gt; labels)</span></pre><h2 id="709a" class="lu lv jb bd lw lx ly dn lz ma mb dp mc kz md me mf ld mg mh mi lh mj mk ml mm bi translated">但是等等，还有更多，辍学间隔！</h2><p id="ef4c" class="pw-post-body-paragraph kq kr jb ks b kt mn kc kv kw mo kf ky kz mp lb lc ld mq lf lg lh mr lj lk ll ij bi translated">如果完全贝叶斯是一个太多的投资，不确定性区间可以计算从神经网络通过辍学层。</p><p id="bfcb" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">机器学习模型的正则化和贝叶斯模型中的先验分布之间有很强的联系。例如，经常使用的L2正则化本质上是高斯先验(<a class="ae lt" href="http://stats.stackexchange.com/a/163450" rel="noopener ugc nofollow" target="_blank">更多细节在此</a>)。</p><p id="8572" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">Dropout是一种根据伯努利分布随机清除神经元的技术。这可以通过KL散度标准下的高斯过程来近似(这里有<a class="ae lt" href="http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html#uncertainty-sense" rel="noopener ugc nofollow" target="_blank">更多细节，因为我不确定数学上的</a>)。</p><p id="98a0" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在实践中，这意味着我们在漏失层打开的情况下运行模型100次，并查看结果分布。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="64dc" class="lu lv jb mx b gy nb nc l nd ne"># keep the dropout during test time<br/>mc_post = [sess.run(nn, feed_dict={x: data}) for _ in range(100)]</span></pre><p id="fe3d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><code class="fe nf ng nh mx b">mc_post</code>的样本均值是估计值。对于不确定性区间，我们简单地计算样本方差加上以下反精度项:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="8f44" class="lu lv jb mx b gy nb nc l nd ne">def _tau_inv(keep_prob, N, l2=0.005, lambda_=0.00001):<br/>    tau = keep_prob * l2 / (2. * N * lambda_)<br/>    return 1. / tau</span></pre><p id="7837" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这给了我们原则上的不确定性区间，而不需要太多投资。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="a568" class="lu lv jb mx b gy nb nc l nd ne">np.var(mc_post) + _tau_inv(0.5, 100)</span></pre></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><h2 id="936c" class="lu lv jb bd lw lx ly dn lz ma mb dp mc kz md me mf ld mg mh mi lh mj mk ml mm bi translated">摘要</h2><p id="acd2" class="pw-post-body-paragraph kq kr jb ks b kt mn kc kv kw mo kf ky kz mp lb lc ld mq lf lg lh mr lj lk ll ij bi translated">要获得不确定性区间，您可以:</p><ol class=""><li id="04a4" class="np nq jb ks b kt ku kw kx kz nr ld ns lh nt ll nu nv nw nx bi translated">在模型中加入一个先验，通过变分推理逼近后验，然后从后验中取样</li><li id="c8fd" class="np nq jb ks b kt ny kw nz kz oa ld ob lh oc ll nu nv nw nx bi translated">运行现有模型很多次，并打开丢弃层</li></ol><p id="9cb5" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">混合解决方案是在感兴趣的特定参数上增加不确定性。在自然语言建模中，这可能只是将单词嵌入作为潜在变量。</p></div></div>    
</body>
</html>