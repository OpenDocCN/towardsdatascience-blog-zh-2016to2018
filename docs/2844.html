<html>
<head>
<title>CatBoost vs. Light GBM vs. XGBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CatBoost与轻型GBM和XGBoost</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db?source=collection_archive---------0-----------------------#2018-03-13">https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db?source=collection_archive---------0-----------------------#2018-03-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="f037" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">谁将赢得这场预言之战，代价是什么？我们来探索一下。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/a6000add7535f9e7fa4c16ccc79e64ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B9wDla365AI9HjZEcUFq6A.jpeg"/></div></div></figure><p id="d0d5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我最近参加了这个Kaggle竞赛(斯坦福大学的WIDS·数据通)，在那里我使用各种推进算法进入了前10名。从那以后，我一直对每个模型的精细工作非常好奇，包括参数调整、优点和缺点，因此决定写这篇博客。尽管最近神经网络重新出现并受到欢迎，但我还是专注于boosting算法，因为它们在有限的训练数据、很少的训练时间和很少的参数调整专业知识的情况下仍然更有用。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lb"><img src="../Images/412aa7a00a8cf90f7a59eb97ecc84457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i0CA9ho0WArOj-0UdpuKGQ.png"/></div></div></figure><p id="a0aa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于XGBoost(通常称为GBM黑仔)在机器学习领域已经有很长一段时间了，现在有很多文章专门讨论它，这篇文章将更多地关注CatBoost和LGBM。以下是我们将涉及的主题-</p><ul class=""><li id="b097" class="lc ld it js b jt ju jx jy kb le kf lf kj lg kn lh li lj lk bi translated">结构差异</li><li id="9e51" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn lh li lj lk bi translated">每个算法对分类变量的处理</li><li id="94da" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn lh li lj lk bi translated">了解参数</li><li id="5603" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn lh li lj lk bi translated">数据集上的实现</li><li id="70dc" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn lh li lj lk bi translated">每种算法的性能</li></ul><h1 id="d250" class="lq lr it bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">LightGBM和XGBoost的结构差异</h1><p id="b49e" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj ms kl km kn im bi translated">LightGBM使用一种新颖的基于梯度的单侧采样(GOSS)技术来过滤数据实例，以找到拆分值，而XGBoost使用预先排序的算法&amp;基于直方图的算法来计算最佳拆分。这里的实例指的是观察结果/样本。</p><p id="3626" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，让我们了解预排序拆分是如何工作的</p><ul class=""><li id="a2ca" class="lc ld it js b jt ju jx jy kb le kf lf kj lg kn lh li lj lk bi translated">对于每个节点，枚举所有功能</li><li id="b58b" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn lh li lj lk bi translated">对于每个特征，按特征值对实例进行排序</li><li id="c7ea" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn lh li lj lk bi translated">使用线性扫描来决定沿着该特征基础的最佳分割<a class="ae mt" href="https://en.wikipedia.org/wiki/Information_gain_ratio" rel="noopener ugc nofollow" target="_blank">信息增益</a></li><li id="be02" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn lh li lj lk bi translated">根据所有功能选择最佳分割解决方案</li></ul><p id="fc1a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">简而言之，基于直方图的算法将一个特征的所有数据点分割成离散的仓，并使用这些仓来找到直方图的分割值。虽然它在训练速度上比列举预排序特征值上所有可能的分裂点的预排序算法更有效，但在速度上仍落后于GOSS。</p><p id="543c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">那么是什么让这种戈斯方法如此有效呢？<br/> </strong>在AdaBoost中，样本权重作为样本重要性的一个很好的指标。然而，在梯度提升决策树(GBDT)中，没有本地样本权重，因此不能直接应用为AdaBoost提出的采样方法。接下来是基于梯度的采样。</p><blockquote class="mu mv mw"><p id="eebe" class="jq jr ko js b jt ju jv jw jx jy jz ka mx kc kd ke my kg kh ki mz kk kl km kn im bi translated">梯度表示损失函数的正切的斜率，因此在逻辑上，如果数据点的梯度在某种意义上很大，这些点对于找到最佳分裂点是重要的，因为它们具有较高的误差</p></blockquote><p id="378a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">GOSS保留所有具有大梯度的实例，并对具有小梯度的实例执行随机采样。例如，假设我有500，000行数据，其中10，000行具有更高的梯度。所以我的算法会选择(较高梯度的10k行+随机选择的剩余490k行的x%)。假设x是10%，根据找到的拆分值，所选的行总数是500K中的59k。</p><blockquote class="mu mv mw"><p id="a692" class="jq jr ko js b jt ju jv jw jx jy jz ka mx kc kd ke my kg kh ki mz kk kl km kn im bi translated">这里采用的基本假设是，具有小梯度的训练实例的样本具有较小的训练误差，并且它已经被很好地训练。<br/>为了保持相同的数据分布，在计算信息增益时，GOSS对梯度较小的数据实例引入了常数乘数。因此，GOSS在减少数据实例的数量和保持学习决策树的准确性之间取得了良好的平衡。</p></blockquote><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi na"><img src="../Images/5f96841fc1721c566e67ab7ead4cd5b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zo9K6RiHvBdjYxJKLpsyaA.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Leaf with higher gradient/error is used for growing further in LGBM</figcaption></figure><h1 id="15c8" class="lq lr it bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">每个模型如何对待分类变量？</h1><h2 id="5239" class="nf lr it bd ls ng nh dn lw ni nj dp ma kb nk nl me kf nm nn mi kj no np mm nq bi translated">CatBoost</h2><p id="62d9" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj ms kl km kn im bi translated">CatBoost可以灵活地给出分类列的索引，以便可以使用one_hot_max_size将其编码为一键编码(对于不同值的数量小于或等于给定参数值的所有要素使用一键编码)。</p><p id="1ff5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果您没有在cat_features参数中传递任何东西，CatBoost会将所有列视为数字变量。</p><blockquote class="mu mv mw"><p id="d8ab" class="jq jr ko js b jt ju jv jw jx jy jz ka mx kc kd ke my kg kh ki mz kk kl km kn im bi translated"><strong class="js iu">注意:如果cat_features中没有提供包含字符串值的列，CatBoost会抛出一个错误。此外，默认情况下，具有默认int类型的列将被视为数字，必须在cat_features中指定它，以使算法将其视为分类。</strong></p></blockquote><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nr"><img src="../Images/7ca2390eefa9155b8fcfc829aa1a6a1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*09uNKZvIG2rhSpjTTnrDvw.png"/></div></div></figure><p id="7cdc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于具有大于one_hot_max_size的唯一类别数的剩余分类列，CatBoost使用一种高效的编码方法，该方法类似于均值编码，但减少了过度拟合。过程是这样的— <br/> 1。以随机顺序排列该组输入观察值。产生多个随机排列<br/> 2。将标签值从浮点或类别转换为整数<br/> 3。使用以下公式将所有分类特征值转换为数值:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/75dda96f48f0fc42dea338f7206a4d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*gw_AZFeu-Q0C95W6QWFJ2g.png"/></div></figure><p id="970f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中，<strong class="js iu"> CountInClass </strong>是对象的标签值等于“1”的次数，当前分类特征值<br/> <strong class="js iu">在</strong>之前是分子的初始值。它由起始参数决定。<strong class="js iu"> TotalCount </strong>是具有与当前值匹配的分类特征值的对象总数(直到当前值)。<br/>在数学上，这可以用下面的等式来表示:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nt"><img src="../Images/7d1ceed94064c8cead16606400b33a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pDN6jzHnCYBqlW4_RoT0Gg.png"/></div></div></figure><h2 id="cca1" class="nf lr it bd ls ng nh dn lw ni nj dp ma kb nk nl me kf nm nn mi kj no np mm nq bi translated">LightGBM</h2><p id="8208" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj ms kl km kn im bi translated">与CatBoost类似，LightGBM也可以通过输入特性名称来处理分类特性。它不会转换为一键编码，并且比一键编码快得多。LGBM使用一种特殊的算法来寻找分类特征的分割值[ <a class="ae mt" href="http://www.csiss.org/SPACE/workshops/2004/SAC/files/fisher.pdf" rel="noopener ugc nofollow" target="_blank">链接</a> ]。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nu"><img src="../Images/1c833c8210d3eb68265749b1dda70b38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fR5nLi61SkS031Spb3qgLg.png"/></div></div></figure><blockquote class="mu mv mw"><p id="fc2c" class="jq jr ko js b jt ju jv jw jx jy jz ka mx kc kd ke my kg kh ki mz kk kl km kn im bi translated"><strong class="js iu">注意:在为LGBM构建数据集之前，应该将分类特征转换为int类型。即使通过categorical _ feature参数传递，它也不接受字符串值。</strong></p></blockquote><h2 id="3b12" class="nf lr it bd ls ng nh dn lw ni nj dp ma kb nk nl me kf nm nn mi kj no np mm nq bi translated">XGBoost</h2><p id="9a27" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj ms kl km kn im bi translated">与CatBoost或LGBM不同，XGBoost本身不能处理分类特征，它只接受类似随机森林的数值。因此，在向XGBoost提供分类数据之前，必须执行各种编码，如标签编码、均值编码或一键编码。</p><h1 id="5afc" class="lq lr it bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">超参数的相似性</h1><p id="99ee" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj ms kl km kn im bi translated">所有这些模型都有许多参数需要调整，但我们将只讨论重要的参数。下面是这些参数的列表，根据它们的功能以及它们在不同型号中的对应关系。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nv"><img src="../Images/8394400f791ef79f5be9ed6384937bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A0b_ahXOrrijazzJengwYw.png"/></div></div></figure><h1 id="b437" class="lq lr it bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">数据集上的实现</h1><p id="5c54" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj ms kl km kn im bi translated">我使用的是2015年航班延误的Kaggle <a class="ae mt" href="https://www.kaggle.com/usdot/flight-delays/data" rel="noopener ugc nofollow" target="_blank">数据集</a>，因为它同时具有分类和数字特征。该数据集大约有500万行，对于每种类型的提升，该数据集有助于判断优化模型在速度和准确性方面的性能。我将使用这个数据的10%的子集~ 50万行。<br/>以下是用于建模的特征:</p><ul class=""><li id="a9fa" class="lc ld it js b jt ju jx jy kb le kf lf kj lg kn lh li lj lk bi translated"><strong class="js iu">月、日、星期几</strong>:数据类型int</li><li id="18f7" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn lh li lj lk bi translated"><strong class="js iu">航空公司和航班号</strong>:数据类型int</li><li id="42aa" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn lh li lj lk bi translated"><strong class="js iu">出发地_机场</strong>和<strong class="js iu">目的地_机场:</strong>数据类型字符串</li><li id="2414" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn lh li lj lk bi translated"><strong class="js iu">出发时间:</strong>数据类型浮点型</li><li id="cdfd" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn lh li lj lk bi translated"><strong class="js iu"> ARRIVAL_DELAY </strong>:这将是目标，并被转换为布尔变量，指示超过10分钟的延迟</li><li id="808f" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn lh li lj lk bi translated"><strong class="js iu">距离和飞行时间</strong>:数据类型浮点</li></ul><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h2 id="b8b4" class="nf lr it bd ls ng nh dn lw ni nj dp ma kb nk nl me kf nm nn mi kj no np mm nq bi translated">XGBoost</h2><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h2 id="74b1" class="nf lr it bd ls ng nh dn lw ni nj dp ma kb nk nl me kf nm nn mi kj no np mm nq bi translated">轻型GBM</h2><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h2 id="7cd3" class="nf lr it bd ls ng nh dn lw ni nj dp ma kb nk nl me kf nm nn mi kj no np mm nq bi translated">CatBoost</h2><p id="9928" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj ms kl km kn im bi translated">在优化CatBoost的参数时，很难传递分类特征的索引。因此，我在没有传递分类特征的情况下调整了参数，并评估了两个模型——一个有分类特征，另一个没有分类特征。我单独调整了one_hot_max_size，因为它不会影响其他参数。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h1 id="07a5" class="lq lr it bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">结果</h1><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ny"><img src="../Images/c7d2a1afe8fa1466832b5a14f57a27c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w05Hg2QZ5ioDi2OXdCCMiw.png"/></div></div></figure><h1 id="74b0" class="lq lr it bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">结束注释</h1><p id="a493" class="pw-post-body-paragraph jq jr it js b jt mo jv jw jx mp jz ka kb mq kd ke kf mr kh ki kj ms kl km kn im bi translated">对于评估模型，我们应该从速度和准确性两个方面来考察模型的性能。</p><p id="abfb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">记住这一点，CatBoost在测试集上具有最高的准确性(0.816)，最小的过拟合(训练和测试准确性都很接近)以及最小的预测时间和调整时间。但这仅仅是因为我们考虑了分类变量并调优了one_hot_max_size。如果我们不利用CatBoost的这些特性，它的表现最差，只有0.752的精度。因此，我们了解到，只有当我们在数据中有分类变量并且我们适当地调整它们时，CatBoost才表现良好。</p><p id="40e5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的下一个表演者是XGBoost，它通常工作得很好。它的准确性非常接近CatBoost，即使忽略了我们在数据中有分类变量的事实，我们已经将其转换为消费的数值。然而，XGBoost唯一的问题是它太慢了。特别是调整它的参数真的令人沮丧(我花了6个小时来运行GridSearchCV——非常糟糕的主意！).更好的方法是单独调优参数，而不是使用GridSearchCV。查看这篇<a class="ae mt" href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" rel="noopener ugc nofollow" target="_blank">博客</a>文章，了解如何智能地调整参数。</p><p id="50ba" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，最后一名是Light GBM。这里需要注意的重要一点是，当使用cat_features时，它在速度和准确性方面的表现都很差。我认为它表现不佳的原因是因为它对分类数据使用了某种修改的均值编码，这导致了过度拟合(与测试精度相比，训练精度相当高，为0.999)。然而，如果我们像XGBoost一样正常使用它，它可以实现类似的(如果不是更高的)精度，但速度比XGBoost <br/> (LGBM — 0.785，XGBoost — 0.789)快得多。</p><p id="5d37" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，我不得不说，这些观察对于这个特定的数据集是正确的，对于其他数据集可能有效，也可能无效。然而，有一点是肯定的，XGBoost比其他两种算法要慢。</p><p id="7a93" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">那么你最喜欢哪一个呢？请评论原因。<br/>非常感谢您的任何反馈或改进建议！</p><p id="1795" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">查看我的其他博客<a class="ae mt" href="https://medium.com/@aswalin" rel="noopener">这里</a>！</p><p id="1f77" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">领英:</strong><a class="ae mt" href="http://www.linkedin.com/in/alvira-swalin" rel="noopener ugc nofollow" target="_blank"><strong class="js iu">www.linkedin.com/in/alvira-swalin</strong></a></p><h2 id="cb11" class="nf lr it bd ls ng nh dn lw ni nj dp ma kb nk nl me kf nm nn mi kj no np mm nq bi translated">资源</h2><ol class=""><li id="9ab2" class="lc ld it js b jt mo jx mp kb nz kf oa kj ob kn oc li lj lk bi translated"><a class="ae mt" href="http://learningsys.org/nips17/assets/papers/paper_11.pdf" rel="noopener ugc nofollow" target="_blank">http://learningsys.org/nips17/assets/papers/paper_11.pdf</a></li><li id="e672" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn oc li lj lk bi translated"><a class="ae mt" href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/6907-light GBM-a-highly-efficient-gradient-boosting-decision-tree . pdf</a></li><li id="9e46" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn oc li lj lk bi translated"><a class="ae mt" href="https://arxiv.org/pdf/1603.02754.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1603.02754.pdf</a></li><li id="1da7" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn oc li lj lk bi translated">https://github.com/Microsoft/LightGBM<a class="ae mt" href="https://github.com/Microsoft/LightGBM" rel="noopener ugc nofollow" target="_blank"/></li><li id="a08d" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn oc li lj lk bi translated"><a class="ae mt" href="https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2017/06/which-algorithm-take-the-crown-light-GBM-vs-xgboost/</a></li><li id="9648" class="lc ld it js b jt ll jx lm kb ln kf lo kj lp kn oc li lj lk bi translated"><a class="ae mt" href="https://stats.stackexchange.com/questions/307555/mathematical-differences-between-gbm-xgboost-lightgbm-catboost" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/307555/mathematical-differences-between-GBM-xgboost-light GBM-catboost</a></li></ol></div></div>    
</body>
</html>