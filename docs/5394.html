<html>
<head>
<title>Data Science for Startups: PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">创业数据科学:PySpark</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-science-for-startups-pyspark-1acf51e9d6ba?source=collection_archive---------12-----------------------#2018-10-15">https://towardsdatascience.com/data-science-for-startups-pyspark-1acf51e9d6ba?source=collection_archive---------12-----------------------#2018-10-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/41181afab8750b4e61ebd5f3da37912c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pFNARWlK_5snX5QQ.jpg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Source:Wikimedia Commons</figcaption></figure><div class=""/><p id="8a91" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><em class="la">我的创业数据科学第三部</em> <a class="ae lb" rel="noopener" target="_blank" href="/data-science-for-startups-introduction-80d022a18aec"> <em class="la">系列</em> </a> <em class="la"> s 现在重点介绍</em><a class="ae lb" rel="noopener" target="_blank" href="/data-science-for-startups-r-python-2ca2cd149c5c"><em class="la">Python</em></a><em class="la">。</em></p><p id="ea56" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">Spark 是一个很好的工具，可以让数据科学家将研究代码转化为生产代码，PySpark 使这种环境更容易访问。虽然我一直是谷歌用于产品化模型的云数据流的粉丝，但它缺乏一个易于原型化和部署数据科学模型的交互环境。Spark 对于初创公司来说是一个很好的工具，因为它既提供了执行分析的交互式环境，又提供了将模型投入生产的可伸缩性。这篇文章讨论了如何在 GCP 上启动集群并连接到 Jupyter，以便在笔记本环境中使用 Spark。</p><p id="0c68" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">有各种不同的生态系统可用于与 Spark 合作，从自托管到供应商选项。以下是我过去探索过的一些 Spark 选项:</p><ol class=""><li id="e7c8" class="lc ld jf ke b kf kg kj kk kn le kr lf kv lg kz lh li lj lk bi translated">阿帕奇安巴里+阿帕奇齐柏林</li><li id="f9fd" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">GCP DataProc + Jupyter</li><li id="6c9a" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">AWS EMR + SageMaker</li><li id="2448" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">供应商:DataBricks、Cloudera</li></ol><p id="c758" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">除了 SageMaker 之外，所有这些设置我都亲自操作过。在与大型企业合作时，我更喜欢供应商解决方案，但初创公司可能希望避免这种选择。最佳方法取决于您需要处理多少数据，您预计您的公司将增长多少，以及您需要用该基础架构支持多少数据科学家。</p><p id="26a8" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这篇文章的目标是展示如何尽快使用 PySpark 的云解决方案。我决定使用 GCP，因为该平台提供免费积分，并且进入笔记本环境是微不足道的。下面的帖子是 GCP 星火计划的一个很好的起点:</p><div class="ip iq gp gr ir lq"><a href="https://cloud.google.com/blog/products/gcp/google-cloud-platform-for-data-scientists-using-jupyter-notebooks-with-apache-spark-on-google-cloud" rel="noopener  ugc nofollow" target="_blank"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd jg gy z fp lv fr fs lw fu fw je bi translated">数据科学家的谷歌云平台:在谷歌上使用带有 Apache Spark 的 Jupyter 笔记本…</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">结合使用 Jupyter 笔记本电脑和 GCP，您将获得熟悉的数据科学体验，而不会有乏味的…</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">cloud.google.com</p></div></div><div class="lz l"><div class="ma l mb mc md lz me ix lq"/></div></div></a></div><p id="9b36" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这个例子的完整源代码可以在<a class="ae lb" href="https://github.com/bgweber/StartupDataScience/blob/master/Spark/PySpark_Natality.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><h2 id="ed5f" class="mf mg jf bd mh mi mj dn mk ml mm dp mn kn mo mp mq kr mr ms mt kv mu mv mw mx bi translated"><strong class="ak">设置</strong></h2><p id="6c78" class="pw-post-body-paragraph kc kd jf ke b kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz ij bi translated">首先，我们需要设置一个集群，我们将用 Jupyter 连接到这个集群。我们将使用大多数默认设置，即创建一个包含一个主节点和两个工作节点的集群。我们将指定的一个初始化步骤是运行位于 Google Storage 上的一个<a class="ae lb" href="https://console.cloud.google.com/storage/browser/dataproc-initialization-actions/jupyter?prefix=jupyter.sh&amp;project=gameanalytics-199018" rel="noopener ugc nofollow" target="_blank">脚本</a>，它为集群设置 Jupyter。</p><ol class=""><li id="7367" class="lc ld jf ke b kf kg kj kk kn le kr lf kv lg kz lh li lj lk bi translated">从 GCP 控制台，选择汉堡菜单，然后选择“DataProc”</li><li id="4257" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">从 DataProc 中，选择“创建集群”</li><li id="ffc7" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">分配一个集群名:“pyspark”</li><li id="d79b" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">单击“高级选项”，然后单击“添加初始化选项”</li><li id="8b53" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">添加如下命令(如下所示):<br/>GS://data proc-initial ization-actions/jupyter/jupyter . sh</li><li id="733b" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">点击“开始”</li></ol><figure class="ne nf ng nh gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/8e5a4a0c4cf15a73fe00467e739f03cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*UpHUXLOkPmdXWEHgQ7weUA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Adding the Jupyter initialization step.</figcaption></figure><p id="79ed" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">群集将需要几分钟的时间来启动。准备就绪后，状态将从“正在调配”变为“正在运行”。在开始使用 Jupyter 之前，我们需要为集群设置连接规则。推荐的方法是设置 SSH 隧道，如这里的<a class="ae lb" href="https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces" rel="noopener ugc nofollow" target="_blank"/>所述。为了快速启动并运行，我们将修改防火墙以接受来自特定 IP 的连接。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/44594ac1986011c73c251deb6792b304.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*CSh5W28Q46FULstpbBBsaQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">The initialized Spark cluster.</figcaption></figure><p id="e27b" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">要修改防火墙以接受来自您的计算机的连接:</p><ol class=""><li id="355b" class="lc ld jf ke b kf kg kj kk kn le kr lf kv lg kz lh li lj lk bi translated">点击集群“pyspark”</li><li id="dc49" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">单击“虚拟机实例”</li><li id="fbf7" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">对于“pyspark-m ”,单击 3 个垂直点，然后单击“查看网络详细信息”</li><li id="ce40" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">从左侧，选择“防火墙规则”</li><li id="ebb5" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">选择“创建防火墙规则”</li><li id="2f3f" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">对规则使用以下设置<br/> -名称:jupyter <br/> -目标标签:http-服务器<br/> -源 IP 范围:您的 v4 IP <br/> - tcp: 8123(脚本在此端口上设置 jupyter)</li><li id="0176" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">点击“创建”</li></ol><p id="b11c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在您应该能够测试到您的 DataProc 集群的连接了。浏览回“VM Instances”并单击“pyspark-m”以获取集群的外部 IP。然后点击“编辑”，启用“允许 HTTP 流量”，然后“保存”。将“:8123”附加到集群外部 IP 的末尾，并将结果粘贴到您的浏览器中。您应该会看到 Jupyter，如下所示。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/ddadbdcf99b588cfbbbe8b652fd945a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zLN4ZVBFDtdzpoLbb09o6g.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Jupyter set up for GCP DataProc.</figcaption></figure><p id="4743" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们现在已经为 PySpark 的云部署设置了 Jupyter 笔记本！</p><h2 id="4c02" class="mf mg jf bd mh mi mj dn mk ml mm dp mn kn mo mp mq kr mr ms mt kv mu mv mw mx bi translated"><strong class="ak"> PySpark </strong></h2><p id="b25d" class="pw-post-body-paragraph kc kd jf ke b kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz ij bi translated">设置好 Jupyter 环境后，您可以通过选择“新建”然后选择“PySpark”来创建新笔记本。虽然已经安装了一些常用库，如<em class="la"> matplotlib </em>和<em class="la"> numpy </em>，但是您可能希望通过 pip 添加额外的库。我们可以直接在笔记本上完成:</p><pre class="ne nf ng nh gt nk nl nm nn aw no bi"><span id="23b4" class="mf mg jf nl b gy np nq l nr ns">!pip install --upgrade  pandas<br/>!pip install --upgrade  pandas_gbq<br/>!pip install --upgrade  pyspark</span></pre><p id="2248" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">上面的代码片段在集群上安装 Pandas，以及 Pandas 的 BigQuery 连接器和 PySpark，我们将使用 py Spark 来获取对 Spark 上下文的引用。我们将遵循<a class="ae lb" href="https://cloud.google.com/blog/products/gcp/google-cloud-platform-for-data-scientists-using-jupyter-notebooks-with-apache-spark-on-google-cloud" rel="noopener ugc nofollow" target="_blank"> GCP 的例子</a>，从将数据从 BigQuery 拉入 Pandas 数据帧开始。此时，我们实际上并没有利用 Spark 的功能，因为使用 Pandas 需要将所有数据加载到驱动程序节点的内存中。但这是在担心 Spark 的一些复杂性之前快速启动和运行的一种方式。</p><p id="f3ae" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">下面的代码片段显示了如何在 BigQuery 上运行查询，并将结果拉入驱动程序节点上的 Pandas 数据帧。当处理较大的数据集时，应该使用<a class="ae lb" href="https://cloud.google.com/dataproc/docs/concepts/connectors/bigquery" rel="noopener ugc nofollow" target="_blank"> BigQuery 连接器</a>以 Spark 数据帧而不是 Pandas 数据帧的形式返回结果。</p><pre class="ne nf ng nh gt nk nl nm nn aw no bi"><span id="068e" class="mf mg jf nl b gy np nq l nr ns">from pandas.io import gbq</span><span id="ddea" class="mf mg jf nl b gy nt nq l nr ns">project_id = "your-project_id"<br/>query = """<br/>    SELECT * <br/>    FROM `bigquery-public-data.samples.natality` <br/>    limit 10000<br/>"""</span><span id="a37b" class="mf mg jf nl b gy nt nq l nr ns">births = gbq.read_gbq(query=query, dialect ='standard', project_id=project_id)<br/>births.head()</span></pre><p id="82f7" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因为我们将结果作为熊猫数据帧返回，所以我们可以使用<em class="la"> hist </em>函数来绘制不同属性的分布。下图显示了出生体重和母亲年龄的分布。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/8724bc7d645073b648675a83116c0c92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*JRrmO2HyxQqJTUJbJa4rOA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Histograms of different attributes in the data set.</figcaption></figure><p id="057a" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为了充分利用 Spark 的分布式计算能力，在使用 PySpark 时最好避免使用 Pandas。这意味着避免<em class="la"> toPandas() </em>，直接将数据加载到 Spark 而不是 Pandas 数据帧中。然而，<a class="ae lb" href="https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html" rel="noopener ugc nofollow" target="_blank">Pandas UDF</a>是在 Spark 环境中使用 Pandas 的一种很好的方式，但是调试起来很有挑战性。</p><p id="b3c3" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">首先，我们将展示如何从 Pandas 数据帧转换到 Spark 数据帧。关键的区别在于 Spark 数据帧是延迟评估的，并且分布在集群中，这意味着在需要计算结果之前不执行任何操作。当使用 Pandas 数据帧时，所有的操作都以一种急切的模式执行，并被立即拉入内存，即使结果在后面的步骤中不被使用。在使用 Spark 数据帧之前，我们首先需要获得对 Spark 上下文的引用，如下面的代码片段所示。一旦获得，我们可以用它来转换从熊猫到火花数据帧。</p><pre class="ne nf ng nh gt nk nl nm nn aw no bi"><span id="ef45" class="mf mg jf nl b gy np nq l nr ns">from pyspark.context import SparkContext<br/>from pyspark.sql.session import SparkSession</span><span id="dfee" class="mf mg jf nl b gy nt nq l nr ns"># Get a reference to the Spark Session <br/>sc = SparkContext()<br/>spark = SparkSession(sc)</span><span id="0842" class="mf mg jf nl b gy nt nq l nr ns"># convert from Pandas to Spark <br/>sparkDF = spark.createDataFrame(births)</span><span id="9990" class="mf mg jf nl b gy nt nq l nr ns"># perform an operation on the DataFrame<br/>print(sparkDF.count())</span><span id="d6cf" class="mf mg jf nl b gy nt nq l nr ns"># DataFrame head <br/>sparkDF.show(n=5)</span></pre><p id="5306" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">许多 Spark 操作类似于 Pandas 操作，但是执行流程有很大的不同。上例显示了如何使用<em class="la">显示</em>功能执行与<em class="la">头</em>操作类似的结果。Spark 的主要好处是执行是分布式的和懒惰的，从而产生可伸缩的管道。</p><h2 id="8240" class="mf mg jf bd mh mi mj dn mk ml mm dp mn kn mo mp mq kr mr ms mt kv mu mv mw mx bi translated">结论</h2><p id="8662" class="pw-post-body-paragraph kc kd jf ke b kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz ij bi translated">Spark 是构建数据管道的强大工具，PySpark 使这个生态系统更容易访问。虽然使用 spark 有多种选择，但本文主要关注如何使用 GCP 的 DataProc 和一个 Jupyter 初始化脚本来快速交互式访问 Spark 集群。有效地使用 PySpark 需要重新学习许多在 Python 中执行数据科学的标准方法，但结果是大规模可扩展的数据管道和分析。</p></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="1653" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">本·韦伯是 Zynga 的首席数据科学家。我们正在<a class="ae lb" href="https://www.zynga.com/careers/positions/categories" rel="noopener ugc nofollow" target="_blank">招聘</a>！</p></div></div>    
</body>
</html>