<html>
<head>
<title>Maximum Likelihood Estimation: How it Works and Implementing in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最大似然估计:工作原理及在 Python 中的实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/maximum-likelihood-estimation-how-it-works-and-implementing-in-python-b0eb2efb360f?source=collection_archive---------5-----------------------#2018-12-08">https://towardsdatascience.com/maximum-likelihood-estimation-how-it-works-and-implementing-in-python-b0eb2efb360f?source=collection_archive---------5-----------------------#2018-12-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/eff20325e503f4b465a7b31588f831d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FAJ3jWRrbe12vvsg-sAoEQ.jpeg"/></div></div></figure><p id="b3fc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">之前，我写了一篇关于使用<a class="ae kz" rel="noopener" target="_blank" href="/estimating-distributions-nonparametric-713ccf0647b">非参数估计量</a>来估计分布的文章，其中我讨论了估计从未知分布生成的数据的统计属性的各种方法。本文介绍了一种在给定数据的情况下估计概率分布参数的非常有效的方法，称为最大似然估计。</p><p id="0522" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这篇文章是研究投资组合优化的数学框架的系列文章的一部分，并解释了在<a class="ae kz" href="https://github.com/VivekPa/OptimalPortfolio" rel="noopener ugc nofollow" target="_blank"> OptimalPortfolio </a>中看到的它的实现。</p><h2 id="c481" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">极大似然估计量</h2><p id="cbb1" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">我们首先了解什么是最大似然估计量(MLE ),以及如何用它来估计数据的分布。当特定的分布被指定时，极大似然估计量被认为是参数估计量。</p><p id="3016" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">本质上，MLE 的目标是在给定一组概率分布参数的情况下，最大化每个数据点出现的概率。换句话说，找到使数据点的概率(可能性)最大化的概率分布的一组参数。形式上，这可以表示为</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/4aa84f420bf3b37a0d9b55e29c648741.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*naVe8aK2ASVwpLxCElr1bg.png"/></div></figure><p id="cc82" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">优化这个概率和的问题是，它通常涉及参数的非常讨厌的指数，这使得找到最优值更加困难。因此，引入了对数似然的概念。对数似然基本上是数据点出现的概率的对数。形式上，</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/026a39fc9d11fb129b20af8fc9dca445.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*pEu3DzTBafVbhHsA4EIO-g.png"/></div></div></figure><p id="a7d2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">使用对数似然的好处有两方面:</p><ol class=""><li id="2b5d" class="me mf it kd b ke kf ki kj km mg kq mh ku mi ky mj mk ml mm bi translated">概率密度函数中的指数变得更易于管理和优化。</li><li id="64ea" class="me mf it kd b ke mn ki mo km mp kq mq ku mr ky mj mk ml mm bi translated">概率的乘积变成一个和，这使得单个分量最大化，而不是使用 n 个概率密度函数的乘积。</li></ol><p id="1c59" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">MLE 的概念出奇的简单。困难在于如何有效地应用这种方法来估计给定数据的概率分布参数。在我们讨论实现之前，我们应该开发一些数学基础，看看 MLE 是否在所有情况下都有效。为此，请考虑以下因素:</p><p id="4edb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/a0cd1b95c9bd23d8d686851d4b3826f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*AzsGEPW4fk4ByWrth6iBGw.png"/></div></figure><p id="cd13" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">该函数将被最大化以找到参数。1/n 的附加因子显然不影响最大值，但对我们的证明是必要的。考虑:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/79b2c5933b23c24a51375a49738224d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*GLPaLRmB459kW2dRxQ6Z_g.png"/></div></figure><p id="941f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这是真实参数下对数似然的期望值。换句话说，在某种意义上这是我们的目标对数可能性。大数定律(LLN)指出，当数据点的数量趋于无穷大时，同独立(iid)随机变量的算术平均值收敛于随机变量的期望值。因此，我们可以证明:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/ea801dd729977d1c5ddf5768685f7cd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*UbRrsrCPD6CaMuYrer10fg.png"/></div></figure><p id="b5c1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这意味着在给定足够数据的情况下，MLE 是一致的，并且收敛于参数的真值。</p><h2 id="6ccf" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">学生的最大似然估计</h2><p id="d4b8" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">由于最大似然估计的通常介绍性例子总是高斯分布，我想用一个稍微复杂一点的分布来解释，即 Student-t 分布。这也是我的<a class="ae kz" href="https://github.com/VivekPa/OptimalPortfolio" rel="noopener ugc nofollow" target="_blank"> OptimalPortfolio </a>实现中使用的发行版。使用高斯分布和 Student-t 分布的区别在于，Student-t 分布不会产生 MLE 的解析解。因此，我们需要研究某种形式的优化算法来解决它。它为我们提供了一个学习期望最大化算法的机会。</p><h2 id="8ef1" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">EM 算法</h2><p id="96e8" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">EM 算法本质上是在给定数据和参数的先验分布的情况下计算对数似然的期望值，然后在给定这些参数的情况下计算对数似然函数的该期望值的最大值。一般来说，第一步是:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/7d8019238477b5d9aa9c6bef7862b061.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*qnfuGHaMgllMXLDK4Cab-g.png"/></div></figure><p id="24d1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/63e268e3fd902802751144ff19c325cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*y9Fofq5eil6MW6dDYbRzJg.png"/></div></figure><p id="832e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">重复这一过程，直到参数值收敛或达到给定的精度阈值。该算法可以相对容易地应用于 Student-t 分布。重要的事实是注意到 Student-t 分布的参数来自 Gamma 分布，因此，在第一步中计算的期望值如下:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/d5300da5b9d8dacf165ddd61eb5a4d16.png" data-original-src="https://miro.medium.com/v2/resize:fit:232/format:webp/1*d6Dg4CbSuk6Iw2jYHXz-DA.png"/></div></figure><p id="126b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">其中 d 为随机变量的维数，M 为马氏距离，定义如下:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi my"><img src="../Images/bdd6c3d9855167a879e16db0d7fb852b.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*V2_iyaVFrEtmIthQpXyJzQ.png"/></div></figure><p id="830a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一旦计算出来，我们就可以计算 Student-t 分布的对数似然的最大值，它有一个解析解，即:</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/81216ba5e51024d39b2ae1c95f39ccbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/format:webp/1*fKG2tOEpTrMyTdxCfoDkoQ.png"/></div></figure><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi na"><img src="../Images/87f4798882f83e1fb02ead5c0f5fbae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*kiSfP-s5-U0L3LoqD7LF7g.png"/></div></figure><p id="1c48" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这个估计和期望值的计算可以迭代直到收敛。在 python 中，它看起来像这样:</p><pre class="lz ma mb mc gt nb nc nd ne aw nf bi"><span id="1638" class="la lb it nc b gy ng nh l ni nj">import pandas as pd<br/>import numpy as np<br/><br/><br/>def expectation_max(data, max_iter=1000):<br/>    data = pd.DataFrame(data)<br/>    mu0 = data.mean()<br/>    c0 = data.cov()<br/><br/>    for j in range(max_iter):<br/>        w = []<br/>        # perform the E part of algorithm<br/>        for i in data:<br/>            wk = (5 + len(data))/(5 + np.dot(np.dot(np.transpose(i - mu0), np.linalg.inv(c0)), (i - mu0)))<br/>            w.append(wk)<br/>            w = np.array(w)<br/><br/>        # perform the M part of the algorithm<br/>        mu = (np.dot(w, data))/(np.sum(w))<br/><br/>        c = 0<br/>        for i in range(len(data)):<br/>            c += w[i] * np.dot((data[i] - mu0), (np.transpose(data[i] - mu0)))<br/>        cov = c/len(data)<br/><br/>        mu0 = mu<br/>        c0 = cov<br/><br/>    return mu0, c0</span></pre><h2 id="d0bb" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">结论</h2><p id="5536" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">分布参数的估计是数据统计建模的核心。这是任何数据科学家和定量分析师的必备技能。对于那些感兴趣的人来说，<a class="ae kz" href="https://github.com/VivekPa/OptimalPortfolio" rel="noopener ugc nofollow" target="_blank"> OptimalPortfolio </a>是对这些方法如何结合起来优化投资组合的阐述。</p></div></div>    
</body>
</html>