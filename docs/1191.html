<html>
<head>
<title>What are Hyperparameters ? and How to tune the Hyperparameters in a Deep Neural Network?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是超参数？以及如何调整深度神经网络中的超参数？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-are-hyperparameters-and-how-to-tune-the-hyperparameters-in-a-deep-neural-network-d0604917584a?source=collection_archive---------0-----------------------#2017-08-09">https://towardsdatascience.com/what-are-hyperparameters-and-how-to-tune-the-hyperparameters-in-a-deep-neural-network-d0604917584a?source=collection_archive---------0-----------------------#2017-08-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="88d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">什么是<strong class="jp ir">超参数</strong>？</p><p id="ded1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">超参数</strong>是决定网络结构的<strong class="jp ir">变量(例如:隐藏单元的数量)和决定网络如何训练的<strong class="jp ir">变量</strong>(例如:学习率)。</strong></p><p id="3ea8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">超参数</strong>是在训练之前<strong class="jp ir">设定的(在优化权重和偏差之前)。</strong></p><h1 id="94ab" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">与网络结构相关的超参数</strong></h1><h2 id="5ad2" class="lj km iq bd kn lk ll dn kr lm ln dp kv jy lo lp kz kc lq lr ld kg ls lt lh lu bi translated">隐藏层和单元的数量</h2><p id="6e9b" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">隐藏层是输入层和输出层之间的层。</p><p id="56fe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ma">“很简单。只要不断增加层，直到测试误差不再改善。”</em></p><p id="6d40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用正则化技术的图层中的许多隐藏单元可以提高精度。单元数量较少可能会导致<strong class="jp ir">装配不足</strong>。</p><h2 id="8d53" class="lj km iq bd kn lk ll dn kr lm ln dp kv jy lo lp kz kc lq lr ld kg ls lt lh lu bi translated">拒绝传统社会的人</h2><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/1d4de9fe0bfc911c4572f6e18e85f499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/0*dOZ6esAristenchF.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Random neurons are cancelled</figcaption></figure><p id="099d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Dropout是一种正则化技术，用于避免过拟合(提高验证精度)，从而提高泛化能力。</p><ul class=""><li id="eaa0" class="mn mo iq jp b jq jr ju jv jy mp kc mq kg mr kk ms mt mu mv bi translated">通常，使用20%-50%的神经元的小丢弃值，其中20%提供了一个好的起点。过低的概率影响最小，过高的值导致网络学习不足。</li><li id="467c" class="mn mo iq jp b jq mw ju mx jy my kc mz kg na kk ms mt mu mv bi translated">使用更大的网络。当在更大的网络上使用dropout时，您可能会获得更好的性能，从而为模型提供更多学习独立表示的机会。</li></ul><h2 id="3942" class="lj km iq bd kn lk ll dn kr lm ln dp kv jy lo lp kz kc lq lr ld kg ls lt lh lu bi translated">网络权重初始化</h2><p id="13ef" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">理想地，根据在每层上使用的激活函数，使用不同的权重初始化方案可能更好。</p><p id="1745" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大多使用<strong class="jp ir">均匀分布</strong>。</p><h2 id="3146" class="lj km iq bd kn lk ll dn kr lm ln dp kv jy lo lp kz kc lq lr ld kg ls lt lh lu bi translated"><strong class="ak">激活功能</strong></h2><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/684ac8a3ff4879a26b46978d90ec038e.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/0*7YK6yhluy4tLIj0M.gif"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Sigmoid activation function</figcaption></figure><p id="798f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">激活函数用于<strong class="jp ir">向模型引入非线性</strong>，这允许深度学习模型学习非线性预测边界。</p><p id="3667" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一般来说，<strong class="jp ir">整流器激活功能</strong>最受欢迎。</p><p id="6824" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">在进行<strong class="jp ir">二进制预测时，在输出层使用Sigmoid </strong>。</strong> <strong class="jp ir"> Softmax </strong>用于输出层，同时进行<strong class="jp ir">多类预测。</strong></p><h1 id="5a8c" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">与训练算法相关的超参数</strong></h1><h2 id="1833" class="lj km iq bd kn lk ll dn kr lm ln dp kv jy lo lp kz kc lq lr ld kg ls lt lh lu bi translated"><strong class="ak">学习率</strong></h2><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/d230b64e29d52e4e01b4a01055400142.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/0*FA9UmDXdzYzuOpeO.jpg"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Learning rate</figcaption></figure><p id="9d71" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">学习率定义了网络更新其参数的速度。</p><p id="82d5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">学习率低</strong>减缓学习过程但平滑收敛。<strong class="jp ir">较大的学习速率</strong>加速学习但可能不收敛。</p><p id="780d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通常一个<strong class="jp ir">衰减学习率</strong>是首选。</p><h2 id="37d7" class="lj km iq bd kn lk ll dn kr lm ln dp kv jy lo lp kz kc lq lr ld kg ls lt lh lu bi translated"><strong class="ak">气势</strong></h2><p id="444c" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">动量有助于用前几步的知识知道下一步的方向。它有助于防止振荡。动量的典型选择在0.5到0.9之间。</p><h2 id="2594" class="lj km iq bd kn lk ll dn kr lm ln dp kv jy lo lp kz kc lq lr ld kg ls lt lh lu bi translated">时代数</h2><p id="3a5a" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">epochs number是训练时整个训练数据显示给网络的次数。</p><p id="2920" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">增加历元数，直到验证精度开始下降，即使训练精度在增加(过拟合)。</p><h2 id="fcf5" class="lj km iq bd kn lk ll dn kr lm ln dp kv jy lo lp kz kc lq lr ld kg ls lt lh lu bi translated">批量</h2><p id="48ff" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">最小批量是参数更新发生后给网络的子样本数。</p><p id="e13b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">批量大小的一个好的默认值可能是32。也试试32，64，128，256等等。</p><h1 id="bf2b" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">找出超参数的方法</strong></h1><ol class=""><li id="2d2e" class="mn mo iq jp b jq lv ju lw jy nd kc ne kg nf kk ng mt mu mv bi translated"><em class="ma">手动搜索</em></li><li id="89d6" class="mn mo iq jp b jq mw ju mx jy my kc mz kg na kk ng mt mu mv bi translated"><em class="ma">网格搜索</em><a class="ae nh" href="http://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/" rel="noopener ugc nofollow" target="_blank">(http://machinelingmastery . com/Grid-Search-hyperparameters-deep-learning-models-python-keras/</a>)</li><li id="b838" class="mn mo iq jp b jq mw ju mx jy my kc mz kg na kk ng mt mu mv bi translated"><em class="ma">随机搜索</em></li><li id="d853" class="mn mo iq jp b jq mw ju mx jy my kc mz kg na kk ng mt mu mv bi translated"><em class="ma">贝叶斯优化</em></li></ol></div></div>    
</body>
</html>