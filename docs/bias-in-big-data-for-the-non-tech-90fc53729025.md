# 如果我们听之任之，大数据会有偏见

> 原文：<https://towardsdatascience.com/bias-in-big-data-for-the-non-tech-90fc53729025?source=collection_archive---------2----------------------->

如果我每次听到“数据不会说谎”都能得到一便士…

对于我们这些肩负着使用大数据来帮助解决组织中一些最大的低效、问题或难题的令人兴奋且日益增长的任务的人来说，永久化偏见是一种太容易犯的错误，我们现在都应该熟悉它了。

***对于其他人来说，这是怎么回事:***

# **快速回顾**对于那些当数据术语抛向你时感到困惑的人

*(* 如果*这不是您要找的信息，请沿着*移动到下一部分 *)*

*   **大数据**是大量的数据。大量用于识别模式、趋势和关系的定量和定性指标。
*   **算法**是 **'** 在计算或其他解决问题的操作中要遵循的过程或一组规则。例如，如果我决定早上穿什么，我会在心里使用一种算法，考虑天气、我的心情、我要去的地方，以及昨晚我不该吃的 Ben & Jerry's，这导致我选择我的服装。
*   **机器学习**为系统提供自动学习和根据经验改进的能力，而无需明确编程。。如果我是一个机器学习算法，我昨晚就不会吃那个本&杰里的，因为我会从上次做的事情中知道我会后悔。

# 偏见是如何被引入我们的“智能”世界的

我第一次接触数据驱动偏见的概念让我大吃一惊，让我想知道我以前怎么没见过。那是 ProPublica 的一篇题为《机器偏见》的文章。就在标题后面写着:

> 全国各地都有用来预测未来罪犯的软件。而且对黑人有偏见。

*TL；这里的故事是，美国几个州实施了一种算法来预测被告在法庭上再次犯罪的风险，并在量刑时将这一数值作为一个因素。有趣的是，种族或民族声称不是这个算法中的变量，但不知何故，它最让黑人失望。只有 20%的被告被认定在未来有实施暴力犯罪的高风险，但实际上却有；而且它误标黑人的比率几乎是白人的两倍。*

整篇文章和他们对数据的分析绝对值得一读，但最重要的是，这不是算法让少数民族和其他群体失败的唯一地方。

Credit: Ford Foundation

重要的是要注意，在我们进一步深入这个兔子洞之前，我不认为人们建造这些工具的意图是故意歧视或制造任何种类的偏见。我们可以提出完全相反的观点:像这样的工具旨在通过提供“不可否认的”、可量化的、值得信任的数据来限制任何进行风险评估的人的个人偏见。

很长一段时间以来，我的座右铭是“如果什么都不变，什么都不变”，在这种情况下它听起来最真实。偏见不是什么新鲜事，它需要在数据科学领域内外采取具体行动来克服。通过向我们的算法输入历史数据，我们含蓄地告诉它们歧视历史上被歧视过的每一个人。

这些例子中的一些已经融入了我们的文化，我们接受它们作为规范——尽管并不总是愉快的:你的性别、收入、教育水平和其他因素决定了你将支付多少医疗费用。一些与健康相关的指标也是如此，比如你是否吸烟。然而，不吸烟的健康女性比每天抽两包烟的久坐不动的男性支付更多的保险费并不罕见，尽管许多医生同意，根据该数据，后者更有可能生病。

性别偏见的另一面:出于类似的原因，男性确实会为汽车保险支付更多的钱。例如，一个住在内华达州的 18 岁男性，如果不幸在那里长大，平均每年要为他的轿车支付 6268 美元的保险费。这比他的双胞胎姐姐支付的费用高 51%(假设他们有相同的成绩和驾驶记录)，根据 CoverHound 的分析，他的双胞胎姐姐只需支付 4152 美元就可以购买一辆相同的汽车，[。既然我们谈到了汽车保险这个话题，在风险相似的社区里，](https://coverhound.com/press/coverhound-on-cbs-news-how-men-can-beat-gender-bias-in-car-insurance)[少数族裔比白人支付更多的汽车保险。](https://www.propublica.org/article/minority-neighborhoods-higher-car-insurance-premiums-white-areas-same-risk)

# **这比以往任何时候都重要**

这种算法已经存在了几十年，许多时间被组织用来通过使用适用于每个人的可重复模式来扩展他们的操作。

我们现在比以往任何时候都更需要关注这个问题的原因是，通过一个不断增长和繁荣的科技行业，这些模型正在几乎所有地方得到应用:从法院判决、[求职](https://hbr.org/2016/12/hiring-algorithms-are-not-neutral)、信用卡、大学和抵押贷款申请、[消费品](http://business.time.com/2012/05/18/when-consumers-pay-more-due-to-race-or-gender/)等，到[人工智能语音机器人](https://www.technologyreview.com/s/602950/how-to-fix-silicon-valleys-sexist-algorithms/)、[评估教师表现](https://qz.com/819245/data-scientist-cathy-oneil-on-the-cold-destructiveness-of-big-data/)、[有针对性的社交媒体广告](https://www.cmu.edu/news/stories/archives/2015/july/online-ads-research.html)等等。

这意味着，无论你是否对大数据、算法和技术感兴趣，你今天都是其中的一部分，它将越来越多地影响你。

如果我们不制定可靠、可操作和可访问的解决方案来处理数据科学中的偏见，这些类型的通常是无意的歧视将变得越来越正常，与人类方面正在尽最大努力进化过去的偏见的社会和机构对立，并作为一个全球社区在历史上前进。

# **今天做什么**

这个问题的解决方案不是停止围绕大数据算法和机器学习的创新。幸运的是，几个方面正在取得进展。

**算法英雄**

Joy Buolamwini(下面是她精彩的 TED 演讲)创立的算法正义联盟(Algorithmic Justice League)和算法观察(AlgorithmWatch)等组织正在努力通过提供教育和培训材料来帮助评估和识别现有算法中的偏见，并为人们提供一个协作和包容的空间来报告算法中的偏见，并作为一个社区来帮助解决这些问题。

有许多其他的个人、研究人员和组织正在以不同的方式处理这种情况。

**政策变化同**[](https://en.wikipedia.org/wiki/General_Data_Protection_Regulation)****欧洲****

**不幸的是，像 AJL 这样的组织不足以保证必要的改变。变革需要政策支持。在欧洲，GDPR(2018 年 5 月在欧盟生效的一般数据保护法规)将[监管涉及数据偏差的三个关键因素](https://blog.acolyer.org/2017/01/31/european-union-regulations-on-algorithmic-decision-making-and-a-right-to-explanation/)。**

**首先，**剖析**，他们将其定义为**

> ***任何形式的个人数据自动处理，包括使用个人数据评估与自然人相关的某些个人方面，特别是分析或预测与该自然人的工作表现、经济状况、健康状况、个人偏好、兴趣、可靠性、行为、位置或活动相关的方面。***

**与此同时，向消费者清楚地解释他们的数据将如何被使用，并为他们提供选择退出的选项。**

**其次，**解释权。**当公司使用自动化决策时，如果决策完全由算法和数据做出，用户将有权要求解释和争议决策。这一行动的范围尚未完全确定，但预计将适用于信贷申请、求职和其他关注领域。**

**最后但同样重要的是，有一个特定的**偏见和歧视**部分，防止组织使用可能会助长偏见的数据(如种族、性别、宗教或政治信仰、健康状况等)来做出自动决策(除了一些经核实的例外)。**

# **接下来需要发生什么**

**与人类偏见不同，我们可以通过将偏见作为另一个指标来快速教会算法考虑和避免偏见。我们还可以制定政策来防止数据驱动偏差的发生。在我看来，在不久的将来，我们需要在三个主要领域努力，以确保在数据空间中减少偏差。**

****教育****

**潜在的最重要的方面，也是短期内最容易实现的方面，是促进和要求对参与创建和维护自动化决策工具以及其他容易产生偏见的数据驱动工具的人员进行培训和教育。**

**在科技行业，我们看到了很多关于偏见的争议，并通过增加人力资源层面的教育和培训来应对这一问题；试图在个人层面传播多样性和平等的价值。现在是扩大培训范围的时候了，应该让所有相关人员了解他们在开发工具时所做的决定可能会对少数群体产生的影响，并提供相关的技术知识来防止这种情况发生。**

**不属于科技行业的人也应该意识到这一点，足以识别他们什么时候可能成为受害者并大声说出来。如果没有个人分享他们的故事，以及这些方法如何改变了他们的生活，信息就会变得冷漠和没有人情味，这正是我们试图避免的。**

****规定****

**我特别喜欢 GDPR 的例子(尽管它的实施效果如何还有待观察)，因为它来自一项命令。这不是一个建议或选项，它*需要*发生。欧盟议会认为，数据安全越来越与全体公民息息相关，并认为这也可能对部分公民不公平。这里面有难以置信的价值和验证。**

**这种数据监管，特别是关于偏见和歧视的监管，在我看来是大数据行业健康发展的关键。如果没有公共部门的领导，忽略对受歧视人群给予特别关注的必要性的机会太诱人了，也太实惠了。**

****透明度****

**最后，这是我个人的信念，我认为收集数据和开发这些工具的组织的某种程度的数据透明度将有助于识别和防止这种事情在未来发生。机器可以学习，但人类的洞察力需要成为他们的监督老师，通过开放和共享非个人数据来分析偏见，组织可以从寻求促进公平的多元化全球社区的力量中受益。**

***免责声明:这并不意味着对现有算法的科学分析或对前景的技术评估，而是对不经常参与这一领域的人们所发生的事情的一种谦卑的翻译。***