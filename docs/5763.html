<html>
<head>
<title>Data Science Skills: Web scraping javascript using python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学技能:使用 python 进行网页抓取 javascript</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-science-skills-web-scraping-javascript-using-python-97a29738353f?source=collection_archive---------0-----------------------#2018-11-08">https://towardsdatascience.com/data-science-skills-web-scraping-javascript-using-python-97a29738353f?source=collection_archive---------0-----------------------#2018-11-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/df43ca4dbf492f4f6182cca474ba2dfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8OO_rM6eZgNdDaNtzTEC3g.png"/></div></div></figure><div class=""/><p id="afb0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用 python 有不同的抓取网页的方法。在我的上一篇文章中，我通过使用库:<code class="fe kw kx ky kz b">requests</code>和<code class="fe kw kx ky kz b">BeautifulSoup</code>介绍了 web 抓取。然而，许多网页是动态的，并且使用 JavaScript 来加载它们的内容。这些网站通常需要不同的方法来收集数据。</p><div class="ip iq gp gr ir la"><a rel="noopener follow" target="_blank" href="/data-science-skills-web-scraping-using-python-d1a85ef607ed"><div class="lb ab fo"><div class="lc ab ld cl cj le"><h2 class="bd jc gy z fp lf fr fs lg fu fw ja bi translated">数据科学技能:使用 python 进行网络搜集</h2><div class="lh l"><h3 class="bd b gy z fp lf fr fs lg fu fw dk translated">作为一名数据科学家，我在工作中接受的第一批任务之一就是网络搜集。这完全是…</h3></div><div class="li l"><p class="bd b dl z fp lf fr fs lg fu fw dk translated">towardsdatascience.com</p></div></div><div class="lj l"><div class="lk l ll lm ln lj lo ix la"/></div></div></a></div><p id="6d86" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在本教程中，我将介绍几种收集包含 Javascript 的网页内容的不同方法。使用的技术如下:</p><ol class=""><li id="3aae" class="lp lq jb ka b kb kc kf kg kj lr kn ls kr lt kv lu lv lw lx bi translated">将<code class="fe kw kx ky kz b">selenium</code>与 Firefox 网络驱动一起使用</li><li id="10e1" class="lp lq jb ka b kb ly kf lz kj ma kn mb kr mc kv lu lv lw lx bi translated">使用带<code class="fe kw kx ky kz b">phantomJS</code>的无头浏览器</li><li id="7405" class="lp lq jb ka b kb ly kf lz kj ma kn mb kr mc kv lu lv lw lx bi translated">使用 REST 客户端或 python <code class="fe kw kx ky kz b">requests</code>库进行 API 调用</li></ol><h2 id="6eb2" class="md me jb bd mf mg mh dn mi mj mk dp ml kj mm mn mo kn mp mq mr kr ms mt mu mv bi translated">TL；DR 关于用 python 抓取 javascript 网页的例子，你可以在<a class="ae mw" href="https://github.com/kaparker/tutorials/blob/master/pythonscraper/websitescrapefasttrack.py" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到本教程涵盖的完整代码。</h2><p id="2a3b" class="pw-post-body-paragraph jy jz jb ka b kb mx kd ke kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv ij bi translated">2019 年 11 月 7 日更新:请注意，被抓取的网页的 html 结构可能会随着时间的推移而更新，本文最初反映的是 2018 年 11 月发布时的结构。这篇文章现在已经更新，可以在当前网页上运行，但将来可能会再次改变。</p><h1 id="dfa3" class="nc me jb bd mf nd ne nf mi ng nh ni ml nj nk nl mo nm nn no mr np nq nr mu ns bi translated">第一步</h1><p id="963a" class="pw-post-body-paragraph jy jz jb ka b kb mx kd ke kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv ij bi translated">开始教程，我首先需要找到一个网站刮。在继续使用 web scraper 之前，务必检查您计划抓取的网站上的条款和条件以及隐私政策，以确保您没有违反他们的任何使用条款。</p><h1 id="5e75" class="nc me jb bd mf nd ne nf mi ng nh ni ml nj nk nl mo nm nn no mr np nq nr mu ns bi translated">动机</h1><p id="92ba" class="pw-post-body-paragraph jy jz jb ka b kb mx kd ke kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv ij bi translated">当试图找到一个合适的网站来演示时，我首先看到的许多例子都明确表示禁止使用网络爬虫。直到看了一篇关于酸奶<a class="ae mw" href="https://www.bbc.co.uk/news/health-45565364" rel="noopener ugc nofollow" target="_blank">含糖量</a>的文章，想知道在哪里可以找到最新的营养信息，激发了我在哪里可以找到合适的网站的另一个思路；网上超市。</p><p id="3862" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在线零售商通常有使用 javascript 加载内容的动态网页，因此本教程的目的是从在线超市的网页中抓取酸奶的营养信息。</p><h1 id="dbbd" class="nc me jb bd mf nd ne nf mi ng nh ni ml nj nk nl mo nm nn no mr np nq nr mu ns bi translated">设置您的环境</h1><p id="4943" class="pw-post-body-paragraph jy jz jb ka b kb mx kd ke kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv ij bi translated">由于我们将使用一些新的 python 库来访问网页内容和处理数据，这些库将需要使用您常用的 python 包管理器<code class="fe kw kx ky kz b">pip</code>来安装。如果你还没有<code class="fe kw kx ky kz b">beautifulsoup</code>，那么你也需要在这里安装它。</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="f9b9" class="md me jb kz b gy ob oc l od oe">pip install selenium<br/>pip install pandas</span></pre><p id="805f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要使用<code class="fe kw kx ky kz b">selenium</code>作为网络驱动程序，还有一些额外的要求:</p><h2 id="f73b" class="md me jb bd mf mg mh dn mi mj mk dp ml kj mm mn mo kn mp mq mr kr ms mt mu mv bi translated">火狐浏览器</h2><p id="5753" class="pw-post-body-paragraph jy jz jb ka b kb mx kd ke kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv ij bi translated">我将使用 Firefox 作为我的网络驱动程序的浏览器，所以这意味着你要么需要安装 Firefox 来跟随这个教程，要么你可以使用 Chrome 和 Chrome。</p><h2 id="5f83" class="md me jb bd mf mg mh dn mi mj mk dp ml kj mm mn mo kn mp mq mr kr ms mt mu mv bi translated">壁虎</h2><p id="5a0e" class="pw-post-body-paragraph jy jz jb ka b kb mx kd ke kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv ij bi translated">要使用 web 驱动程序，我们需要安装一个 web 浏览器引擎 geckodriver。您需要为您的操作系统下载<a class="ae mw" href="https://github.com/mozilla/geckodriver/releases" rel="noopener ugc nofollow" target="_blank"> geckodriver </a>，提取文件并设置可执行路径位置。</p><p id="83d4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您可以通过几种方式来完成此操作:<br/> (i)将 geckodriver 移动到您选择的目录中，并在您的 python 代码中定义此可执行路径(参见后面的示例)，</p><p id="eadd" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(ii)将 geckodriver 移动到一个已经设置为可执行文件所在目录的目录中，这就是您的环境变量 path。<br/>您可以通过以下方式找到您的<code class="fe kw kx ky kz b">$PATH</code>中有哪些目录:</p><p id="da0c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">窗口<br/> </strong>转到:</p><p id="13f2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="of">控制面板&gt;环境变量&gt;系统变量&gt;路径</em></p><p id="ba7a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc"> Mac OSX / Linux </strong> <br/>在你的终端中使用命令:</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="90c0" class="md me jb kz b gy ob oc l od oe">echo $PATH</span></pre><p id="02a3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(iii)将壁虎驱动程序位置添加到您的<code class="fe kw kx ky kz b">PATH</code>环境变量中</p><p id="c459" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">窗口<br/>窗口</strong>转到:</p><p id="4c5f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="of">控制面板&gt;环境变量&gt;系统变量&gt;路径&gt;编辑<br/></em></p><p id="d8df" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc"> Mac OSX / Linux <br/> </strong>给你的<code class="fe kw kx ky kz b">.bash_profile</code> (Mac OSX)或者<code class="fe kw kx ky kz b">.bash_rc</code> (Linux)加一行</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="13e9" class="md me jb kz b gy ob oc l od oe"># add geckodriver to your PATH<br/>export PATH="<!-- -->$PATH:/path/to/your/directory"</span></pre><p id="5f28" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">重新启动您的终端，并使用(ii)中的命令检查您的新路径是否已经添加。</p><h2 id="ff69" class="md me jb bd mf mg mh dn mi mj mk dp ml kj mm mn mo kn mp mq mr kr ms mt mu mv bi translated">幻象</h2><p id="7ba6" class="pw-post-body-paragraph jy jz jb ka b kb mx kd ke kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv ij bi translated">类似于 geckodriver 的步骤，我们也需要下载<a class="ae mw" href="http://phantomjs.org/download.html" rel="noopener ugc nofollow" target="_blank"> PhantomJS </a>。下载完成后，按照上述相同的说明，解压文件并移动到选择的目录或添加到可执行文件路径。</p><h2 id="58b7" class="md me jb bd mf mg mh dn mi mj mk dp ml kj mm mn mo kn mp mq mr kr ms mt mu mv bi translated">REST 客户端</h2><p id="5be0" class="pw-post-body-paragraph jy jz jb ka b kb mx kd ke kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv ij bi translated">在这篇博客的最后一部分，我们将使用 REST 客户端向 API 发出请求。我将使用<a class="ae mw" href="https://insomnia.rest/" rel="noopener ugc nofollow" target="_blank">失眠</a>，但是你可以随意使用你喜欢的任何一个客户端！</p><h1 id="919e" class="nc me jb bd mf nd ne nf mi ng nh ni ml nj nk nl mo nm nn no mr np nq nr mu ns bi translated">使用 BeautifulSoup 抓取网页</h1><p id="519d" class="pw-post-body-paragraph jy jz jb ka b kb mx kd ke kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv ij bi translated">按照我的<a class="ae mw" href="https://medium.com/@_kaparker/data-science-skills-web-scraping-using-python-d1a85ef607ed" rel="noopener">web 抓取入门教程</a>中概述的标准步骤，我检查了<a class="ae mw" href="https://groceries.asda.com/search/yogurt" rel="noopener ugc nofollow" target="_blank">网页</a>，并想要提取重复的 HTML 元素:</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="b9e2" class="md me jb kz b gy ob oc l od oe">&lt;div data-cid="XXXX" class="listing category_templates clearfix productListing "&gt;...&lt;/div&gt;</span></pre><p id="0bf1" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">作为第一步，您可以尝试使用 BeautifulSoup 通过以下脚本提取这些信息。</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="4960" class="md me jb kz b gy ob oc l od oe"># import libraries<br/>import urllib.request<br/>from bs4 import BeautifulSoup</span><span id="5b69" class="md me jb kz b gy og oc l od oe"># specify the url<br/>urlpage = '<a class="ae mw" href="https://groceries.asda.com/search/yoghurt'" rel="noopener ugc nofollow" target="_blank">https://groceries.asda.com/search/yogurt'</a> <br/>print(urlpage)<br/># query the website and return the html to the variable 'page'<br/>page = urllib.request.urlopen(urlpage)<br/># parse the html using beautiful soup and store in variable 'soup'<br/>soup = BeautifulSoup(page, 'html.parser')<br/># find product items<br/># at time of publication, Nov 2018:<br/># results = soup.find_all('div', attrs={'class': 'listing category_templates clearfix productListing'})</span><span id="ac89" class="md me jb kz b gy og oc l od oe"># updated Nov 2019:<br/>results = soup.find_all('div', attrs={'class': 'co-product'})<br/>print('Number of results', len(results))</span></pre><p id="b0af" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">没想到的是，在运行 python 脚本时，即使我在网页上看到很多结果，返回的结果数也是 0！</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="3d76" class="md me jb kz b gy ob oc l od oe"><a class="ae mw" href="https://groceries.asda.com/search/yoghurt" rel="noopener ugc nofollow" target="_blank">https://groceries.asda.com/search/yoghurt</a><br/>BeautifulSoup - Number of results 0</span></pre><p id="8524" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当进一步检查页面时，网页上有许多动态特征，这表明 javascript 用于呈现这些结果。<br/>右键单击并选择<code class="fe kw kx ky kz b">View Page Source</code>有许多<code class="fe kw kx ky kz b">&lt;script&gt;</code>元素在使用中，搜索上面包含我们感兴趣的数据的元素不会返回匹配结果。</p><p id="35fe" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">抓取该网页的第一种方法是使用 Selenium web driver 调用浏览器，搜索感兴趣的元素并返回结果。</p><h1 id="5b75" class="nc me jb bd mf nd ne nf mi ng nh ni ml nj nk nl mo nm nn no mr np nq nr mu ns bi translated">使用 Selenium 抓取网页</h1><h2 id="81c1" class="md me jb bd mf mg mh dn mi mj mk dp ml kj mm mn mo kn mp mq mr kr ms mt mu mv bi translated">1.硒配壁虎</h2><p id="6bfd" class="pw-post-body-paragraph jy jz jb ka b kb mx kd ke kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv ij bi translated">由于我们无法使用 Beautiful Soup 访问 web 页面的内容，我们首先需要在 python 脚本中设置一个 web 驱动程序。</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="bd13" class="md me jb kz b gy ob oc l od oe"># import libraries<br/>import urllib.request<br/>from bs4 import BeautifulSoup<br/>from selenium import webdriver<br/>import time<br/>import pandas as pd</span><span id="d002" class="md me jb kz b gy og oc l od oe"># specify the url<br/>urlpage = '<a class="ae mw" href="https://groceries.asda.com/search/yoghurt'" rel="noopener ugc nofollow" target="_blank">https://groceries.asda.com/search/yogurt'</a> <br/>print(urlpage)<br/># run firefox webdriver from executable path of your choice<br/>driver = webdriver.Firefox(executable_path = 'your/directory/of/choice')</span></pre><p id="ba6c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">正如在安装 geckodriver 时提到的，如果可执行文件不在可执行路径中，我们可以在 python 脚本中定义路径。如果它在一个可执行路径中，那么上面的行变成:</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="9332" class="md me jb kz b gy ob oc l od oe"># run firefox webdriver from executable path of your choice<br/>driver = webdriver.Firefox()</span></pre><p id="2fc5" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">设置完成后，我们现在可以连接到网页并找到感兴趣的元素。当在浏览器中加载网页时，结果通常需要一段时间来加载，甚至可能直到我们向下滚动页面时才加载。<br/>考虑到这一点，我们可以为 web 驱动程序添加一些 javascript 来执行这些操作。下面是一个让页面滚动的简单例子，有更有效的方法可以做到这一点，为什么不在这里测试你自己的 javascript，让我在评论中知道什么最适合你！</p><p id="f43d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们还添加了一个睡眠时间，作为等待页面完全加载的另一种方法。</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="de00" class="md me jb kz b gy ob oc l od oe"># get web page<br/>driver.get(urlpage)<br/># execute script to scroll down the page<br/>driver.execute_script("window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;")<br/># sleep for 30s<br/>time.sleep(30)<br/># driver.quit()</span></pre><p id="4ba4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果我们现在运行脚本(你也可以在最后取消对<code class="fe kw kx ky kz b">driver.quit()</code>的注释以确保浏览器关闭)，当你的 python 脚本运行时，Firefox 将打开指定的 url 并向下滚动页面。希望在脚本运行完成之前，您应该已经加载了许多产品。</p><p id="eafc" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我们要获取感兴趣的元素。以前，我们使用 Beautiful Soup 试图根据标签和类属性查找所有元素，但是，在本例中，我们将使用一种稍微不同的方法来访问产品信息。相反，我们可以基于 XML 结构或 css 选择器，通过 xpath 搜索元素。</p><p id="d5bc" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以检查感兴趣的元素，在工具栏中，右键单击突出显示的元素，然后<em class="of">复制&gt;复制 xpath(或复制选择器)</em>。这是理解 html 结构的另一种有趣的方式。在这种情况下，我们将使用 xpath 来查找元素，然后我们可以打印匹配的结果数:</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="1a90" class="md me jb kz b gy ob oc l od oe"># find elements by xpath</span><span id="006f" class="md me jb kz b gy og oc l od oe"># at time of publication, Nov 2018:<br/># results = driver.find_elements_by_xpath("//*[<a class="ae mw" href="http://twitter.com/id" rel="noopener ugc nofollow" target="_blank">@id</a>='componentsContainer']//*[contains(<a class="ae mw" href="http://twitter.com/id" rel="noopener ugc nofollow" target="_blank">@id</a>,'listingsContainer')]//*[<a class="ae mw" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>='product active']//*[<a class="ae mw" href="http://twitter.com/class" rel="noopener ugc nofollow" target="_blank">@class</a>='title productTitle']")</span><span id="0a91" class="md me jb kz b gy og oc l od oe"># updated Nov 2019:<br/>results = driver.find_elements_by_xpath("//*[@class=' co-product-list__main-cntr']//*[@class=' co-item ']//*[@class='co-product']//*[@class='co-item__title-container']//*[@class='co-product__title']")<br/>print('Number of results', len(results))</span></pre><p id="979f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用 xpath 而不是使用元素的一个主要原因是结果有一些元素，其中 id 的词干是带有一些附加单词的<code class="fe kw kx ky kz b">listingsContainer</code>，所以<code class="fe kw kx ky kz b">contains</code>函数被用来选择所有结果，但也排除了容器中的任何其他<code class="fe kw kx ky kz b">div</code>元素，例如广告。</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="584c" class="md me jb kz b gy ob oc l od oe">Firefox Webdriver - Number of results 38</span></pre><p id="332d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们已经有了页面的一些结果，我们可以循环每个结果并保存感兴趣的数据。在这种情况下，我们可以保存产品名称和链接。</p><p id="1d45" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="of">注意，网页上实际上有 38 个以上的结果。根据连接到页面时加载的结果数量，这个数字也可能会有所不同。所有的结果都可以通过改变我们执行的 javascript 来收集，或者其他的方法将在下面的章节中探讨。</em></p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="f5f7" class="md me jb kz b gy ob oc l od oe"># create empty array to store data<br/>data = []<br/># loop over results<br/>for result in results:<br/>    product_name = result.text<br/>    link = result.find_element_by_tag_name('a')<br/>    product_link = link.get_attribute("href")<br/>    # append dict to array<br/>    data.append({"product" : product_name, "link" : product_link})</span></pre><p id="64b2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这个循环之外，我们可以关闭浏览器，因为我们导入了<code class="fe kw kx ky kz b">pandas</code>库，我们可以通过将我们抓取的数据保存到 dataframe 来利用它。我们可以打印数据帧来查看内容。</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="c799" class="md me jb kz b gy ob oc l od oe"># close driver <br/>driver.quit()<br/># save to pandas dataframe<br/>df = pd.DataFrame(data)<br/>print(df)</span></pre><p id="e1fd" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这种格式下，我们可以非常简单地将数据写入 csv。</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="6e46" class="md me jb kz b gy ob oc l od oe"># write to csv<br/>df.to_csv('asdaYogurtLink.csv')</span></pre><p id="9f81" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">将 Selenium 与 geckodriver 一起使用是一种快速抓取使用 javascript 的网页的方法，但也有一些缺点。我发现有时页面无法加载(我确信通过改变我们如上所述执行的 javascript 可以更有效，但是我是 JS 新手，所以这可能需要一些时间)，但是加载浏览器和等待页面加载也需要时间。</p><p id="6da8" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">另一个选择，我们可以使用无头浏览器。这将加快抓取速度，因为我们不必每次都等待浏览器加载。</p><h2 id="897b" class="md me jb bd mf mg mh dn mi mj mk dp ml kj mm mn mo kn mp mq mr kr ms mt mu mv bi translated">2.带无头浏览器的 Selenium</h2><p id="7963" class="pw-post-body-paragraph jy jz jb ka b kb mx kd ke kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv ij bi translated">当使用 PhantomJS 代替 geckodriver 作为无头浏览器时，唯一的区别是 web 驱动程序是如何加载的。这意味着我们可以遵循上面的方法，但是改变初始化 web 驱动程序的行，变成:</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="f5ee" class="md me jb kz b gy ob oc l od oe"># run phantomJS webdriver from executable path of your choice<br/>driver = webdriver.PhantomJS(executable_path = 'your/directory/of/choice')</span></pre><p id="c8b6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里要注意的是，对 PhantomJS 的 Selenium 支持已经贬值，并提供了一个警告。</p><p id="7413" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过使用 headless 选项，还可以对 geckodriver 使用 headless 模式:</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="14cf" class="md me jb kz b gy ob oc l od oe">from selenium import webdriver<br/>from selenium.webdriver.firefox.options import Options</span><span id="2af8" class="md me jb kz b gy og oc l od oe">options = Options()<br/>options.headless = True<br/>driver = webdriver.Firefox(firefox_options=options, <!-- -->executable_path = 'your/directory/of/choice'<!-- -->)</span></pre><p id="d16f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过使用无头浏览器，我们应该看到脚本运行时间的改进，因为我们没有打开浏览器，但不是所有的结果都以类似于在正常模式下使用 firefox webdriver 的方式收集。</p><h1 id="852d" class="nc me jb bd mf nd ne nf mi ng nh ni ml nj nk nl mo nm nn no mr np nq nr mu ns bi translated">发出 API 请求</h1><p id="f8de" class="pw-post-body-paragraph jy jz jb ka b kb mx kd ke kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv ij bi translated">我们将在本教程中讨论的最后一种方法是向 API 发出请求。当检查网页 XHR 文件时，当页面加载时，该页面显示正在进行的请求。在这个列表中有一个<code class="fe kw kx ky kz b">/search</code>请求，它调用一个 API 端点来获得页面上显示的结果。</p><p id="7264" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以使用 REST 客户机或几行 python 来发出同样的请求。</p><p id="998c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果我们检查<code class="fe kw kx ky kz b">search</code>文件，查看文件头、包含关键字的请求 url 以及发出请求所需的其他参数。在一般细节下面是我们以后可能需要的响应和请求头。</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/371fff75ef137da07bb987d84e58d414.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*6shmO69CrKyId1WHMvEHTQ.png"/></div><figcaption class="oi oj gj gh gi ok ol bd b be z dk">Inspect tool showing the search request headers</figcaption></figure><p id="9316" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了获得响应，我们可以获取请求 url，并作为测试将它输入到浏览器的地址栏中。因为参数是在字符串中添加的，所以我们也可以尝试删除除关键字参数之外的所有参数，以测试是否还需要其他参数。在这种情况下，关键字查询在浏览器中返回结果，因此我们也可以使用 REST 客户机或 python 来执行相同的请求。</p><h2 id="3b2f" class="md me jb bd mf mg mh dn mi mj mk dp ml kj mm mn mo kn mp mq mr kr ms mt mu mv bi translated">失眠休息客户</h2><p id="dcb7" class="pw-post-body-paragraph jy jz jb ka b kb mx kd ke kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv ij bi translated">使用失眠症，我们可以输入请求 url 并发送请求。</p><p id="e9e4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这将返回一个包含我们正在寻找的数据的 JSON 响应！</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/3cb4238b02a7a80cd68dbb67ecc4c90b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OZ5iTBk3I97rZ1sZV0KMDw.png"/></div></div><figcaption class="oi oj gj gh gi ok ol bd b be z dk">Preview of JSON response in Insomnia</figcaption></figure><p id="c170" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个例子非常简单，不需要头或安全令牌。对于其他情况，REST 客户机允许您输入任何附加的响应参数，这些参数可以在收集请求细节时从 inspect 工具获得。</p><h2 id="117d" class="md me jb bd mf mg mh dn mi mj mk dp ml kj mm mn mo kn mp mq mr kr ms mt mu mv bi translated">Python 请求</h2><p id="ee98" class="pw-post-body-paragraph jy jz jb ka b kb mx kd ke kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv ij bi translated">我们也可以使用<code class="fe kw kx ky kz b">urllib.request</code>库从 python 发出同样的请求，就像我们在抓取之前连接到网页一样。</p><p id="21d4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过添加一些参数来缩进和排序关键字，可以使 JSON 响应更具可读性，这样我们现在就可以打开文件，并在进行搜索时看到提供给网页的响应数据。</p><pre class="nt nu nv nw gt nx kz ny nz aw oa bi"><span id="317f" class="md me jb kz b gy ob oc l od oe"># import json library<br/>import json</span><span id="080a" class="md me jb kz b gy og oc l od oe"># request url<br/>urlreq = '<a class="ae mw" href="https://groceries.asda.com/api/items/search?keyword=yogurt'" rel="noopener ugc nofollow" target="_blank">https://groceries.asda.com/api/items/search?keyword=yogurt'</a></span><span id="f7b8" class="md me jb kz b gy og oc l od oe"># get response<br/>response = urllib.request.urlopen(urlreq)</span><span id="dfe2" class="md me jb kz b gy og oc l od oe"># load as json<br/>jresponse = json.load(response)</span><span id="3b09" class="md me jb kz b gy og oc l od oe"># write to file as pretty print<br/>with open('asdaresp.json', 'w') as outfile:<br/>    json.dump(jresponse, outfile, sort_keys=True, indent=4)</span></pre><p id="e307" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">目前，我们将保留所有数据。我的下一篇教程将更详细地介绍数据结构和输出，这样我们就可以操作 JSON 并找到相关的数据。</p><h1 id="a57c" class="nc me jb bd mf nd ne nf mi ng nh ni ml nj nk nl mo nm nn no mr np nq nr mu ns bi translated">摘要</h1><p id="4c5d" class="pw-post-body-paragraph jy jz jb ka b kb mx kd ke kf my kh ki kj mz kl km kn na kp kq kr nb kt ku kv ij bi translated">本教程概述了一些我们可以用来抓取使用 javascript 的网页的方法。这些方法包括:</p><h2 id="9f4a" class="md me jb bd mf mg mh dn mi mj mk dp ml kj mm mn mo kn mp mq mr kr ms mt mu mv bi translated">使用 web 驱动程序抓取内容</h2><ul class=""><li id="1cb6" class="lp lq jb ka b kb mx kf my kj on kn oo kr op kv oq lv lw lx bi translated">使用 selenium web 驱动程序连接到带有 Firefox web 驱动程序、PhantomJS 和 headless 浏览器的网页</li><li id="ea18" class="lp lq jb ka b kb ly kf lz kj ma kn mb kr mc kv oq lv lw lx bi translated">使用 web 驱动程序查找感兴趣的元素</li><li id="861d" class="lp lq jb ka b kb ly kf lz kj ma kn mb kr mc kv oq lv lw lx bi translated">循环结果并保存感兴趣的变量</li><li id="98cc" class="lp lq jb ka b kb ly kf lz kj ma kn mb kr mc kv oq lv lw lx bi translated">将数据保存到数据帧</li><li id="1af3" class="lp lq jb ka b kb ly kf lz kj ma kn mb kr mc kv oq lv lw lx bi translated">写入 csv 文件</li></ul><h2 id="6f75" class="md me jb bd mf mg mh dn mi mj mk dp ml kj mm mn mo kn mp mq mr kr ms mt mu mv bi translated">发出 HTTP 请求</h2><ul class=""><li id="ee20" class="lp lq jb ka b kb mx kf my kj on kn oo kr op kv oq lv lw lx bi translated">检查网页以查找 HTTP 请求的详细信息</li><li id="bfc1" class="lp lq jb ka b kb ly kf lz kj ma kn mb kr mc kv oq lv lw lx bi translated">使用浏览器、REST 客户端或 python 发出 GET 请求</li></ul><p id="1127" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">虽然 HTTP 请求方法在本教程中实现起来更快，并且从一个请求中提供了我们需要的所有数据，但情况并不总是这样。并非所有网站都会显示它们的请求，过期的身份验证令牌可能会带来额外的安全性，或者输出数据可能需要大量清理，这比使用带有一些 javascript 的 web 驱动程序来加载所有结果并在所有页面上循环需要更多的工作。本教程提供了一些不同的选择，你可以尝试使用它们来抓取 javascript。</p><p id="4101" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我的下一个教程中，我们将探索数据结构，操作数据和写入输出文件或数据库。</p></div><div class="ab cl or os hu ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="ij ik il im in"><p id="6ea1" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">感谢您的阅读！如果你喜欢我的文章，那么<a class="ae mw" href="https://kaparker.substack.com/" rel="noopener ugc nofollow" target="_blank">订阅我的每月简讯</a>，在那里你可以将我的最新文章和顶级资源直接发送到你的收件箱，或者在我的<a class="ae mw" href="http://kaparker.com/" rel="noopener ugc nofollow" target="_blank">网站</a>上了解更多关于我正在做的事情。</p><p id="2ad5" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您是 python 的新手或者想要提高，请查看我的文章，其中包含学习资源列表，包括数据科学课程:</p><div class="ip iq gp gr ir la"><a rel="noopener follow" target="_blank" href="/learn-to-code-learn-python-efb037b248e8"><div class="lb ab fo"><div class="lc ab ld cl cj le"><h2 class="bd jc gy z fp lf fr fs lg fu fw ja bi translated">学习编码。学习 Python。</h2><div class="lh l"><h3 class="bd b gy z fp lf fr fs lg fu fw dk translated">你想学习编码但是不知道从哪里开始吗？开始您的编码之旅，并决定 python 是否…</h3></div><div class="li l"><p class="bd b dl z fp lf fr fs lg fu fw dk translated">towardsdatascience.com</p></div></div><div class="lj l"><div class="oy l ll lm ln lj lo ix la"/></div></div></a></div></div></div>    
</body>
</html>