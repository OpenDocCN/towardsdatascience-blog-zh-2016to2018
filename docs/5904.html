<html>
<head>
<title>Neural Net from scratch (using Numpy)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始的神经网络(使用 Numpy)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-net-from-scratch-using-numpy-71a31f6e3675?source=collection_archive---------3-----------------------#2018-11-15">https://towardsdatascience.com/neural-net-from-scratch-using-numpy-71a31f6e3675?source=collection_archive---------3-----------------------#2018-11-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/1c32e57a6cda7681f814b20aed2bf011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MaTvYkgr-be3UuY_Z_hxMQ.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">image Source: Data Flair</figcaption></figure><p id="678e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这篇文章是关于使用 Python 中的 numpy 库为分类问题从头构建一个浅层 NeuralNetowrk(nn )(只有一个隐藏层),并与 LogisticRegression(使用 scikit learn)进行性能比较。</p><p id="7f70" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">从头开始构建神经网络有助于理解神经网络在后端如何工作，这对于构建有效的模型是必不可少的。让我们毫不迟疑地从头开始构建简单的浅层神经网络模型。</p><p id="4579" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">完整的代码可在<a class="ae la" href="https://github.com/Msanjayds/NeuralNets/blob/master/Classficatin_NN%20vs%20LogisticReg.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ke ir">这里</strong> </a>。</p><p id="8fcc" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于这项任务，我正在使用 scikit learn 数据集生成器<strong class="ke ir"> make_gaussian_quantiles </strong>函数<strong class="ke ir"> </strong>生成数据集(生成各向同性高斯样本并按分位数标记样本)。生成的输入数据集将有两个特征(“X1”和“X2”)，输出“Y”将有两个类(红色:0，蓝色:1)，共有 200 个示例。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="ccd5" class="lk ll iq lg b gy lm ln l lo lp">def load_extra_datasets():  <br/>    <strong class="lg ir">N = 200</strong><br/>    <strong class="lg ir">gaussian_quantiles</strong> = <strong class="lg ir">sklearn.datasets.make_gaussian_quantiles</strong><br/>     (mean=None, cov=0.7, n_samples=N, <strong class="lg ir">n_features=2, n_classes=2</strong>,  shuffle=True, random_state=None)<br/>    return  gaussian_quantiles</span><span id="c776" class="lk ll iq lg b gy lq ln l lo lp">gaussian_quantiles= load_extra_datasets()<br/>X, Y = gaussian_quantiles<br/>X, Y = X.T, Y.reshape(1, Y.shape[0])</span><span id="9a2a" class="lk ll iq lg b gy lq ln l lo lp"># Visualize the data<br/>plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);</span></pre><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/2f8001a6c071b0c135174d28a323d72f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*M5iJPsYuf3HQtWwqZjhzwg.png"/></div></figure><p id="0e85" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">生成的数据集有两个类，分别标为红色和蓝色的点。我们的目标是建立一个机器学习分类器，在给定 X 和 Y 坐标的情况下预测正确的类别。正如我们在图中看到的，数据不是线性可分的，所以我们不能画一条直线来分隔这两个类。这意味着线性分类器，如逻辑回归，将无法适合这些类型的数据。在这种情况下，神经网络会帮助我们。在神经网络中，不需要特征工程，因为隐藏层会自动学习特征模式，以准确地对数据进行分类。</p><h2 id="a32e" class="lk ll iq bd ls lt lu dn lv lw lx dp ly kn lz ma mb kr mc md me kv mf mg mh mi bi translated"><strong class="ak">逻辑回归</strong>:</h2><p id="922a" class="pw-post-body-paragraph kc kd iq ke b kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv mn kx ky kz ij bi translated">首先，让我们使用输入的 x 和 y 值训练一个 LR 分类器，输出将是预测的类(0 或 1)。我们将使用<em class="mo"> scikit-learn </em>中的回归类</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="d3b8" class="lk ll iq lg b gy lm ln l lo lp">clf = sklearn.linear_model.LogisticRegressionCV()<br/>clf.fit(X.T, Y.T)</span><span id="3fe1" class="lk ll iq lg b gy lq ln l lo lp"># Plot the decision boundary for logistic regression</span><span id="2421" class="lk ll iq lg b gy lq ln l lo lp">plot_decision_boundary(lambda x: clf.predict(x), X, Y)<br/>plt.title("Logistic Regression")</span><span id="7a01" class="lk ll iq lg b gy lq ln l lo lp"># Print accuracy</span><span id="3bb2" class="lk ll iq lg b gy lq ln l lo lp">LR_predictions = clf.predict(X.T)<br/>print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +<br/>       '% ' + "(percentage of correctly labelled datapoints)")</span></pre><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/51018dcd0140799349de7a3f499ab2f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*AOj_70vlodCUJaZ1TfRhLA.png"/></div></figure><p id="31a7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">从上面我们可以看到，LR 只能正确分类 53%的数据点，因为这些数据点不是线性可分的。</p><h1 id="32c0" class="mq ll iq bd ls mr ms mt lv mu mv mw ly mx my mz mb na nb nc me nd ne nf mh ng bi translated"><strong class="ak">神经网络</strong>:</h1><p id="cd96" class="pw-post-body-paragraph kc kd iq ke b kf mj kh ki kj mk kl km kn ml kp kq kr mm kt ku kv mn kx ky kz ij bi translated">现在让我们建立一个简单的神经网络，1 个隐藏层，4 个神经元。输入层将有 2 个节点，因为我们的数据有两个特征(X1 和 X2)，输出层将有一个节点，根据概率阈值，我们将输出分类为红色或蓝色(0 或 1)。</p><figure class="lb lc ld le gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nh"><img src="../Images/a7bb4e9631afb9c56e6bd3b68a8838dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jDPidgu4d5zF6EkYvKo04w.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Source: Andrew Ng’s Coursera</figcaption></figure><p id="176c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们需要做以下步骤来建立我们的神经网络模型。</p><ul class=""><li id="578a" class="ni nj iq ke b kf kg kj kk kn nk kr nl kv nm kz nn no np nq bi translated">定义网络结构(输入单元数量、隐藏单元数量等)。</li><li id="7908" class="ni nj iq ke b kf nr kj ns kn nt kr nu kv nv kz nn no np nq bi translated">初始化模型的参数</li><li id="e09c" class="ni nj iq ke b kf nr kj ns kn nt kr nu kv nv kz nn no np nq bi translated">循环执行以下步骤，直到我们获得最小成本/最佳参数。</li></ul><ol class=""><li id="d1e0" class="ni nj iq ke b kf kg kj kk kn nk kr nl kv nm kz nw no np nq bi translated">实现向前传播</li><li id="a1ee" class="ni nj iq ke b kf nr kj ns kn nt kr nu kv nv kz nw no np nq bi translated">计算损失</li><li id="6f43" class="ni nj iq ke b kf nr kj ns kn nt kr nu kv nv kz nw no np nq bi translated">实现反向传播以获得梯度</li><li id="3def" class="ni nj iq ke b kf nr kj ns kn nt kr nu kv nv kz nw no np nq bi translated">更新参数</li></ol><ul class=""><li id="46df" class="ni nj iq ke b kf kg kj kk kn nk kr nl kv nm kz nn no np nq bi translated">然后将上述所有步骤合并成一个函数，我们称之为‘nn _ model()’。<br/>一旦我们建立了“nn_model()”并学习了正确的参数，我们就可以对新数据进行预测。</li></ul><ol class=""><li id="84c4" class="ni nj iq ke b kf kg kj kk kn nk kr nl kv nm kz nw no np nq bi translated"><strong class="ke ir">定义网络结构</strong>:如前所述，输入层的节点数为 2，隐藏层的节点数为 4。通过在这一层选择更多的节点，我们可以使模型学习复杂的函数。但是进行预测和学习网络参数需要大量的计算。更多的隐藏层和节点也会导致我们的数据过度拟合。</li></ol><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="19ed" class="lk ll iq lg b gy lm ln l lo lp">#X and Y are the input and output variables</span><span id="82ad" class="lk ll iq lg b gy lq ln l lo lp"><strong class="lg ir">    n_x = X.shape[0] # size of input layer`<br/>    n_h = 4<br/>    n_y = Y.shape[0] # size of output layer</strong></span></pre><p id="0f03" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 2。初始化模型的参数</strong> : W1(隐藏层的权重矩阵)和 W2(输出层的权重矩阵)参数使用 numpy 随机函数随机初始化。乘以 0.01，因为我们不希望初始权重过大，因为这会导致学习速度变慢。b1 和 b2 被初始化为零。</p><p id="c4b0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">W1 —隐藏层的形状权重矩阵(n_h，n _ x)<br/>B1—形状偏差向量(n_h，1) <br/> W2 —输出层的形状权重矩阵(n_y，n _ h)<br/>B2—形状偏差向量(n_y，1)</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="92df" class="lk ll iq lg b gy lm ln l lo lp">    W1 = np.random.randn(n_h,n_x) * 0.01<br/>    b1 = np.zeros(shape=(n_h, 1))<br/>    W2 = np.random.randn(n_y,n_h) * 0.01<br/>    b2 = np.zeros(shape=(n_y, 1))</span></pre><p id="22b2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 3。前向传播:</strong>在前向传播期间，输入特征矩阵被馈送到隐藏层中的每个神经元。其将乘以相应的初始权重集(W1)和偏差集(b1)以形成 Z1 矩阵(给定输入的线性变换)。然后，我们通过激活函数将非线性应用于 Z1(以应用非线性)。我们选择“tanh”作为我们的激活函数，因为它适合许多场景。这个激活函数/隐藏层的输出将是 a1(这是一个大小为(4，1)的矩阵，包含来自 4 个神经元的激活，即 A1、a2、a3、a4)。</p><p id="e3ea" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于下一层，在我们的情况下是最终输出层，我们将来自前一层(A1)的输入乘以输出层(W2)的初始权重，加上偏差(b2)以形成 Z2。然后在 Z2 上应用 sigmoid 激活函数以产生最终输出 A2(这是我们的预测)。我们使用 sigmoid 作为最后一层，因为我们希望我们的输出介于 0 和 1 之间。基于概率阈值，我们可以决定输出是红色还是蓝色。这就是 nn 如何在前向传播期间进行预测，这只是一系列矩阵乘法和激活函数的应用。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="ce5e" class="lk ll iq lg b gy lm ln l lo lp"># Implement Forward Propagation to calculate A2 (probabilities)<br/>    Z1 = np.dot(W1,X) + b1<br/>    A1 = np.tanh(Z1)<br/>    Z2 = np.dot(W2,A1) + b2<br/>    A2 = sigmoid(Z2) # Final output prediction</span></pre><p id="4f83" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 4。计算损失:</strong>现在我们有了我们的预测，下一步将是检查我们的预测与实际值有多大差异，即损失/误差。这里我们不使用均方差(MSE)来计算我们的损失，因为我们的预测函数是非线性的(sigmoid)。平方预测将导致具有许多局部最小值的非凸函数。在这种情况下，梯度下降可能找不到最优的全局最小值。因此，我们使用二元<strong class="ke ir">交叉熵</strong>损失(用于误差估计的对数似然方法)，该成本函数本质上是凸的，因此到达全局最小点(最小损失点)将更容易。下面是成本函数公式和代码。</p><p id="d5d3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">m:训练实例的数量</p><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/10e3700324822dc0290c49a9c88540c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*QJP6E2-ivwxFVFF6S85ykw.png"/></div></figure><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="9582" class="lk ll iq lg b gy lm ln l lo lp"># Compute the cross-entropy cost</span><span id="9314" class="lk ll iq lg b gy lq ln l lo lp">logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2)<br/>cost = - np.sum(logprobs) / m</span></pre><p id="fc6d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 5。反向传播/梯度下降(GD) </strong>:反向传播用于计算损失函数相对于模型参数(w1，b1，w2，b2)的梯度(斜率/导数)。为了最小化我们的成本，我们使用 GD 算法，该算法使用计算的梯度来更新参数，使得我们的成本随着迭代不断降低，即，它有助于向全局最小值移动。</p><ul class=""><li id="8801" class="ni nj iq ke b kf kg kj kk kn nk kr nl kv nm kz nn no np nq bi translated">以下是每个模型参数的梯度/斜率计算公式。“m”是训练样本的数量。</li></ul><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d38d9f65a348cd781d8a1e0572b0fdd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*TVHDkZdUG4Xn9eJiyHllKw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Sourced from Coursera</figcaption></figure><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="5736" class="lk ll iq lg b gy lm ln l lo lp">    dZ2 = A2 - Y<br/>    dW2 = (1 / m) * np.dot(dZ2, A1.T)<br/>    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)<br/>    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))<br/>    dW1 = (1 / m) * np.dot(dZ1, X.T)<br/>    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)</span></pre><p id="2a6a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 6。更新参数</strong>:一旦我们计算出我们的梯度，我们将它们乘以一个称为学习率(收敛率)的因子，并从初始参数中减去以获得更新的参数(权重和偏差)。学习率应该是最小的，这样我们就不会错过全局最小点。</p><ul class=""><li id="ddd0" class="ni nj iq ke b kf kg kj kk kn nk kr nl kv nm kz nn no np nq bi translated">用学习率乘以梯度</li><li id="de34" class="ni nj iq ke b kf nr kj ns kn nt kr nu kv nv kz nn no np nq bi translated">从重量中减去</li></ul><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="616c" class="lk ll iq lg b gy lm ln l lo lp">    W1 = W1 - learning_rate * dW1<br/>    b1 = b1 - learning_rate * db1<br/>    W2 = W2 - learning_rate * dW2<br/>    b2 = b2 - learning_rate * db2</span></pre><p id="7c13" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在，我们已经为所有训练示例执行了一轮前向传播和后向传播，即，我们完成了 1 个时期。我们需要在多个时期重复这些步骤，直到我们的成本最小(模型达到全局最小点)或者学习停止(参数没有更新)。</p><p id="4ae2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">下面是函数“nn_model ”,它在给定的时期数(num_iterations)内重复执行上述所有操作，并在每 1000 个时期后打印成本。该函数的输出将是最终的一组优化参数(重量/基数)。</p><figure class="lb lc ld le gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nz"><img src="../Images/b1c7361e9f07866e2935d340e30c5e60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0_ftFqClMPNGb9-sJMSWEQ.png"/></div></div></figure><p id="022d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 7。预测</strong>:下面是使用学习到的参数进行预测的函数，只需向前传播即可。我们将阈值设置为 0.5，如果最终层(A2)的输出是&gt; 0.5，那么我们将其分类为 1:蓝色，否则为 0:红色。</p><figure class="lb lc ld le gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oa"><img src="../Images/0c1fb4fc8ad4a1874aba7443306ca92d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OFGeKW0P1fbI9UAQTipQkQ.png"/></div></div></figure><p id="e1f8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在，让我们通过运行函数<strong class="ke ir"> nn_model </strong>超过 5000 个时期来训练我们的最终模型，并查看结果。</p><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/be963002cd5ab7bc67c5691c66ad710c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*4wEXPwOjCJ_6-9KXR9onQw.png"/></div></figure><figure class="lb lc ld le gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oc"><img src="../Images/3016f36a7d7e92b7662bf67e90d87cc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2bgE2n3lXjWkYB17XDfBOQ.png"/></div></div></figure><p id="b347" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">从结果可以看出，我们的神经网络模型很好地学习了模式。其能够学习分隔类别的非线性决策边界。而我们的成本从 0.69 开始，经过 4000 个纪元达到 0.089。最终的准确率达到 93%,这比我们从逻辑回归得到的只有 53%要高得多。</p><p id="c29f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">调整隐藏层大小</strong>:下一步是决定隐藏层神经元的最佳数量，看看我们的模型是否能在不过度拟合的情况下做得更好。为此，让我们使用不同数量的节点(1、2、3、4、5、20、50)来训练模型，并查看结果。</p><figure class="lb lc ld le gt jr gh gi paragraph-image"><div class="gh gi od"><img src="../Images/1c0e4a2b1169738ee7c92e2fc41ce9eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S-2wpbTgQBaSgAb4CY7WyQ.png"/></div></figure><figure class="lb lc ld le gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/4c4ea5ead4b2a76a576721f4f1692c0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5vK00wQeMAoeKu8eBF1pDA.png"/></div></div></figure><figure class="lb lc ld le gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/142c439d75620a4937c38f900e482889.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tayNE__mMrepnlmxe-8P3g.png"/></div></div></figure><p id="51eb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">以下是这次测试的结果。</p><ul class=""><li id="833b" class="ni nj iq ke b kf kg kj kk kn nk kr nl kv nm kz nn no np nq bi translated">较大的模型(具有更多隐藏单元)能够更好地拟合训练集，直到最大的模型最终过度拟合数据。我们可以看到，在 n_h = 50 时，该模型似乎过拟合(100%精度)。</li><li id="0622" class="ni nj iq ke b kf nr kj ns kn nt kr nu kv nv kz nn no np nq bi translated">最佳隐藏层大小似乎在 n_h = 4 左右。事实上，这里的一个值似乎与数据吻合得很好，而不会引起明显的过度拟合。</li></ul><p id="0f89" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">非常感谢您阅读到最后，请评论任何建议/修改。请吧👏如果你喜欢邮报…😉</p></div></div>    
</body>
</html>