<html>
<head>
<title>Introduction to Various Reinforcement Learning Algorithms. Part II (TRPO, PPO)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">各种强化学习算法介绍。第二部分(TRPO、PPO)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9?source=collection_archive---------3-----------------------#2018-01-17">https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9?source=collection_archive---------3-----------------------#2018-01-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/537b8a182163360983947f7de5bb009d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o3k5SO_pYqnuOr55AqzMyg.png"/></div></div></figure><p id="ace2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">本系列的第一部分<a class="ae kz" rel="noopener" target="_blank" href="/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287"> <em class="la">介绍各种强化学习算法。第一部分(Q-Learning，SARSA，DQN，DDPG) </em> </a> <em class="la">，</em> I <em class="la"> </em>讲述了强化学习(RL)的一些基本概念，并介绍了几种基本的RL算法。在本文中，我将继续讨论两种更高级的RL算法，这两种算法都是去年刚刚发表的。最后，我将对我所讨论的每种算法做一个简单的比较。</p></div><div class="ab cl lb lc hx ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="im in io ip iq"><h1 id="05eb" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">1.入门指南</h1><h2 id="1962" class="mg lj it bd lk mh mi dn lo mj mk dp ls km ml mm lw kq mn mo ma ku mp mq me mr bi translated">定义:</h2><ul class=""><li id="86fb" class="ms mt it kd b ke mu ki mv km mw kq mx ku my ky mz na nb nc bi translated">优势(A): A(s，a) = Q(s，a)- V(s)</li></ul><p id="b029" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">优势是许多高级RL算法中常用的术语，如A3C、NAF和我将要讨论的算法(也许我会为这两种算法写另一篇博文)。为了以一种更直观的方式来看待它，可以把它想成一个动作与特定状态下的平均动作相比有多好。</p><p id="88c6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">但是我们为什么需要优势呢？Q值不够好吗？</p><p id="f55a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我将用这个<a class="ae kz" href="https://datascience.stackexchange.com/questions/15423/understanding-advantage-functions" rel="noopener ugc nofollow" target="_blank">论坛</a>中发布的一个例子来说明优势的想法。</p><p id="a22c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你玩过一个叫“接球”的游戏吗？在游戏中，水果会从屏幕上方落下。你需要向左或向右移动篮子来抓住它们。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/1714b0b52e502559266d481b557092b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*9SpEsk5wD-gvPpykDIaoqQ.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Catch (<a class="ae kz" href="https://datascience.stackexchange.com/questions/15423/understanding-advantage-functions" rel="noopener ugc nofollow" target="_blank">https://datascience.stackexchange.com/questions/15423/understanding-advantage-functions</a>)</figcaption></figure><p id="9e61" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">上图是游戏的草图。上面的圆圈代表一种水果，而下面的小矩形是一个篮子。有三个动作，a1、a2和a3。显然，最好的动作是a2，不要动，因为水果会直接掉进篮子里。现在，假设任何行为都没有负回报。在这种情况下，代理人没有选择最优行动的动机，即上面场景中的a2。为什么？让我们用Q*(s，a)来表示状态s和动作a的最佳Q值，那么我们将得到:</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nm"><img src="../Images/8436a17abe21d2c69a759cd7f6e845cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nVgy0_BZBtmO4Zo_nqLUZA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">(<a class="ae kz" href="https://datascience.stackexchange.com/questions/15423/understanding-advantage-functions" rel="noopener ugc nofollow" target="_blank">https://datascience.stackexchange.com/questions/15423/understanding-advantage-functions</a>)</figcaption></figure><p id="7ff0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">假设贴现因子𝛾仅略小于1。我们可以得到</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nn"><img src="../Images/72c3f983b7fe8d92c7acc90e9a2eb28f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PLduAb0jX5_gYHAIBXLqOA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">(<a class="ae kz" href="https://datascience.stackexchange.com/questions/15423/understanding-advantage-functions" rel="noopener ugc nofollow" target="_blank">https://datascience.stackexchange.com/questions/15423/understanding-advantage-functions</a>)</figcaption></figure><p id="3210" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">由于没有负回报，r(a3)和r(a1)都大于或等于0，暗示Q*(s，a3)和Q*(s，a2)差别不大。因此，在这种情况下，代理人对a2比对a3只有很小的偏好。</p><p id="6d06" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了解决这个问题，我们可以将每个动作的Q值与它们的平均值进行比较，这样我们就可以知道一个动作相对于另一个动作有多好。回想一下上一篇博客，一个州的平均Q值被定义为Value (V)。本质上，我们创造了一个名为<strong class="kd iu"> <em class="la"> advantage </em> </strong>的新操作符，它是通过用该状态的值减去每个动作的Q值来定义的。</p><h1 id="a9ad" class="li lj it bd lk ll no ln lo lp np lr ls lt nq lv lw lx nr lz ma mb ns md me mf bi translated">2.算法图解</h1><h2 id="f810" class="mg lj it bd lk mh mi dn lo mj mk dp ls km ml mm lw kq mn mo ma ku mp mq me mr bi translated">2.1信任区域策略优化(TRPO)</h2><p id="70b3" class="pw-post-body-paragraph kb kc it kd b ke mu kg kh ki mv kk kl km nt ko kp kq nu ks kt ku nv kw kx ky im bi translated">上一篇文章中讨论的深度确定性策略梯度(DDPG)是一个突破，它允许代理在连续空间中执行操作，同时保持下降性能。然而，DDPG的主要问题是你需要选择一个合适的步长。如果太小，训练进度会极其缓慢。如果它太大，相反，它往往会被噪音淹没，导致悲剧性的表现。回想一下，计算时差(TD)误差的目标如下:</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nw"><img src="../Images/adade75729c119a6ad8544d62b83d86a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*htXt8EkoBDUNpPlRntJVQw.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Target for TD error (<a class="ae kz" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><p id="8948" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果步长选择不当，从网络或函数估计器中导出的目标值<em class="la"> yi </em>将不会很好，导致更差的样本和更差的价值函数估计。</p><p id="355f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因此，我们需要一种更新参数的方法来保证政策的改进。也就是说，我们希望<strong class="kd iu">预期贴现长期回报η </strong>总是增加。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/9153d18db979d0756a3b9e031fac27e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*-JOTXwdakK45dI71Bwigow.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Expected Discounted Long-term Reward (<a class="ae kz" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><blockquote class="ny nz oa"><p id="c0ca" class="kb kc la kd b ke kf kg kh ki kj kk kl ob kn ko kp oc kr ks kt od kv kw kx ky im bi translated"><strong class="kd iu">警告:</strong>这部分会有无数的数学方程式和公式。如果您对此不满意，可以直接跳到这一部分的末尾。</p></blockquote><p id="8120" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">与DDPG类似，TRPO也属于政策梯度的范畴。它采用了<strong class="kd iu">actor-critical</strong>架构，但是修改了actor的策略参数的更新方式。</p><p id="ed31" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于新政策π'，η(π')可以看作是政策π'相对于旧政策π'的优势的期望收益。(由于我在键盘上找不到带曲线的π，我将在以下段落中使用π’)</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/110670ae1655f41b5f93dcbf84b7b171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vyRHjLEdX85xisunoy419Q.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">η For New Policy π’ (<a class="ae kz" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><p id="d47a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你可能想知道为什么使用advantage。直觉上，你可以把它看作是衡量新政策相对于旧政策的平均表现有多好。新政策的η可以改写为以下形式，其中<strong class="kd iu"> ⍴ </strong>是<strong class="kd iu">打折的就诊频率</strong>。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi of"><img src="../Images/c2b495d9b0672b84a3d5c1987631289f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*99bRNfNs9MA5w7fMF4qwHw.png"/></div></div></figure><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi og"><img src="../Images/b87c0dcd4a775197129c7c51ad92bc9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_rVD4e8ubtkxhUiUwtin4g.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">η Rewrite &amp; ⍴ (<a class="ae kz" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><p id="133c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然而，由于⍴对新政策π'的高度依赖，上述公式很难进行优化。因此，本文介绍了η(π′)，lπ(π′)的一种近似:</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oh"><img src="../Images/2958e0990dfe37dd758836cbd2f62aec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uj0Qrr6z_9_n4EG7ZXxQgw.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Approximation of η (<a class="ae kz" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><p id="56ad" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">请注意，我们将⍴π替换为⍴π'，假设新旧政策的州访问频率没有太大差异。有了这个等式，我们可以结合众所周知的策略更新方法:</p><p id="ec2a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi">.</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/98216a9b5d35ddf24cf04e17b168a18e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ln3NPShgz-fYAjRl7llAMA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">CPI (<a class="ae kz" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><p id="50ab" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里π_{old}是当前策略，而π'是使L_{πold}最大化的策略的自变量max。然后我们将得到下面的定理(让我们用定理1来表示它)。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/62a9fa601d49d7b79844c9e053860003.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*em6VXFK-iLS06f_4Bd87Vg.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Theorem 1 (<a class="ae kz" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><p id="3b13" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">c表示惩罚系数，而D^{max}_{KL}表示每个状态的两个参数的最大KL散度。KL散度的概念源于信息论，描述了信息的损失。简单来说，你可以把它看成π和π'这两个参数有多大的不同。</p><p id="5e27" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">上式暗示，只要右手边的项最大化，预期的长期回报η就单调提高。为什么？让我们把不等式的右边定义为M_{i}。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/4c4830dc779af88ed65f89a81d5c431f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*lU9dwEaQHf79M0KEfcd7Kw.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">(<a class="ae kz" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><p id="567d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后我们可以证明下面的不等式。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ol"><img src="../Images/a36e292903aa4abff1491947cdd2bd76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nuBzgST3QLJITJPWFXV0Qg.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">(<a class="ae kz" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><p id="7087" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">第一行可以简单地把M_{i}的定义代入定理1得到。第二条线成立，因为π_{i}和π_{i}之间的KL散度为0。将第一条线和第二条线结合起来，我们将得到第三条线。这表明，只要M_{i}在每次迭代中最大化，目标函数η总是在改进的。(我认为第三行末尾的最后一个词应该是Mi而不是m。不确定这是否是论文的打印错误)。因此，我们现在试图解决的复杂问题归结为最大化Mi。即，</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi om"><img src="../Images/a3bb0d394bb89d797a21f81b0c35b7c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d2LANtgkzlZAoZB9Zobcsw.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Objective Function 1 (<a class="ae kz" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><p id="cb34" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下图直观地说明了η与L的近似值:</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi on"><img src="../Images/f9c8ec467d55346aafd223e956432304.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RduAr5cOEbgz53pvDUfhuw.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Visual Illustration of The Approximation (<a class="ae kz" href="https://www.youtube.com/watch?v=xvRrgxcpaHY&amp;t=363s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=xvRrgxcpaHY&amp;t=363s</a>)</figcaption></figure><p id="ee77" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在实践中，如果目标函数中包含惩罚系数，步长会很小，导致训练时间很长。因此，对KL散度的约束用于允许更大的步长，同时保证稳健的性能。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/e92c2055f6b307704fdb430fe2f68f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*fyI0XqNLMwkyyhSE2RmEaA.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Objective Function 2 (<a class="ae kz" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><p id="4c16" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">KL散度约束施加在状态空间中的每个状态上，其最大值应该小于一个小数值𝜹.不幸的是，它是不可解的，因为有无限多的状态。该论文提出了一种解决方案，该解决方案提供了一种启发式近似，该近似具有状态上的预期KL散度，而不是找到最大KL散度。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi op"><img src="../Images/938ece57133f4f537cc1e3f9d203f44c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oz1NTCjtmm20YW0kCjVQuw.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">KL Divergence With State Visitation Frequency ⍴ (<a class="ae kz" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><p id="9ab2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，当我们展开第一行时，目标函数变成如下:</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oq"><img src="../Images/18e6772806e4e47ece0dcfdc021bcaeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v3SwIkyIxdVDOR5kbUZqbw.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Objective Function 3 (<a class="ae kz" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><p id="f667" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">通过用期望代替状态上的σ，用重要抽样估计量代替动作上的σ，如果采用单路径方法，这等同于旧的策略，我们可以将上述重写为:</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi or"><img src="../Images/bf458120342da69982f3385197dfd4c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mcq6AYVfYxHlvIp_DGBQbQ.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Final Objective Function (<a class="ae kz" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><p id="1262" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">目标函数也称为“替代”目标函数，因为它包含当前策略和下一个策略之间的概率比。TPRO成功地解决了DDPG提出的性能不能单调提高的问题。位于约束内的区域子集称为信赖域。只要政策变化相当小，这个近似值与真实的目标函数就不会有太大的不同。通过选择满足KL散度约束的最大化期望的新政策参数，保证了期望长期回报η的下界。这也暗示你不需要太担心TRPO的步长。</p><h2 id="b29d" class="mg lj it bd lk mh mi dn lo mj mk dp ls km ml mm lw kq mn mo ma ku mp mq me mr bi translated">2.2近似策略优化(PPO，OpenAI版本)</h2><p id="a270" class="pw-post-body-paragraph kb kc it kd b ke mu kg kh ki mv kk kl km nt ko kp kq nu ks kt ku nv kw kx ky im bi translated">虽然TRPO已经取得了巨大且持续的高性能，但是它的计算和实现是极其复杂的。在TRPO中，对代理目标函数的约束是新旧策略之间的KL差异。</p><p id="be3f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">Fisher信息矩阵是KL散度的二阶导数，用于近似KL项。这导致计算几个二阶矩阵，这需要大量的计算。在TRPO论文中，共轭梯度(CG)算法被用于解决约束优化问题，从而不需要显式计算Fisher信息矩阵。然而，CG使得实现更加复杂。</p><p id="e027" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">PPO消除了由约束优化产生的计算，因为它提出了一个剪裁的代理目标函数。</p><p id="280c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让rt(𝜽)表示新旧政策之间的比率。用于近似TRPO的长期回报η的替代目标函数变成如下。注下标描述了TRPO所基于的保守策略迭代(CPI)方法。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi os"><img src="../Images/a4c66ac71e4de336de7835c837310186.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eO3E07guKHyqXt7O20HwCQ.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">TRPO Objective Function (<a class="ae kz" href="https://arxiv.org/pdf/1707.06347.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1707.06347.pdf</a>)</figcaption></figure><p id="6436" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">TRPO的约束思想是不允许政策改变太多。因此，PPO没有添加约束，而是稍微修改了TRPO的目标函数，并对过大的策略更新进行了惩罚。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ot"><img src="../Images/e8187122b645a118566f99cbad8be7c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sb-ykXsZLLH_DhXl8o3zjw.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Clipped Objective Function (<a class="ae kz" href="https://arxiv.org/pdf/1707.06347.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1707.06347.pdf</a>)</figcaption></figure><p id="88e4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在右边你可以看到概率比rt(𝜽)被限制在[1- 𝜖，1+𝜖].这表明，如果rt(𝜽)导致目标函数增加到一定程度，其有效性将下降(被修剪)。让我们讨论两种不同的情况:</p><ul class=""><li id="6440" class="ms mt it kd b ke kf ki kj km ou kq ov ku ow ky mz na nb nc bi translated">情况1:当优势ȃt大于0时</li></ul><p id="0ad8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果ȃt大于0，则意味着该动作优于该状态下所有动作的平均值。因此，应通过增加rt(𝜽来鼓励该行动，以便该行动有更高的机会被采纳。由于分母rt(𝜽)不变，旧政策增加rt(𝜽)也意味着新政策增加π𝜽(a(s)。也就是说，增加在给定状态下采取行动的机会。然而，由于修剪，rt(𝜽)只会增长到和1+𝜖.一样多</p><ul class=""><li id="d909" class="ms mt it kd b ke kf ki kj km ou kq ov ku ow ky mz na nb nc bi translated">情况2:当优势ȃt小于0时</li></ul><p id="9363" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">相比之下，如果ȃt小于0，那么这个动作应该被阻止。因此，rt(𝜽)应该减少。同样，由于修剪，rt(𝜽)只会减少到和1-𝜖.一样少</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nm"><img src="../Images/7e7a74ba85ed2f56d52df9bc2df36ad3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FYdM-DQksroO5lPrQqKfBA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Illustration of The Clip (<a class="ae kz" href="https://arxiv.org/pdf/1707.06347.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1707.06347.pdf</a>)</figcaption></figure><p id="bf12" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">本质上，它限制了新政策与旧政策的差异范围；因此，消除了概率比rt(𝜽移出区间的动机。</p><p id="a076" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在实践中，损失函数误差和熵加成也应在实施过程中加以考虑，如下所示。然而，我不打算详细介绍它们，因为最具创新性和最重要的部分仍然是裁剪后的目标函数。</p><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ox"><img src="../Images/d8a929106f01ed3c2209fba9f30ca2f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cz0tz14llIU-qimXzvmf4g.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">PPO Objective Function (<a class="ae kz" href="https://arxiv.org/pdf/1707.06347.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1707.06347.pdf</a>)</figcaption></figure><p id="fc0a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">比较L^{CPI}和L^{CLIP}的目标函数，我们可以看到L^{CLIP}实际上是前者的一个下界。它还消除了KL发散约束。因此，优化该PPO目标函数的计算量比TRPO少得多。经验上也证明了PPO的表现优于TRPO。事实上，由于其轻便和易于实现，PPO已经成为open ai(<a class="ae kz" href="https://blog.openai.com/openai-baselines-ppo/" rel="noopener ugc nofollow" target="_blank">https://blog.openai.com/openai-baselines-ppo/</a>)的默认RL算法。</p><h1 id="aed6" class="li lj it bd lk ll no ln lo lp np lr ls lt nq lv lw lx nr lz ma mb ns md me mf bi translated">3.所讨论算法的比较</h1><figure class="ne nf ng nh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oy"><img src="../Images/20c59410fc54126b8a9b7cd867d651c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BEby_oK1mU8Wq0HABOqeVQ.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Various RL Algorithms I Have Discussed</figcaption></figure><p id="6f61" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所有讨论的RL算法都是无模型的。也就是说，他们都没有试图去估计目标函数。相反，他们基于反复试验来更新他们的知识。在所有这些公司中，只有SARSA是在政策上，根据其当前的行动学习价值。从离散的观察空间到连续的观察空间，DQN是一个巨大的进步，允许代理处理看不见的状态。DDPG是另一个突破，它使智能体能够执行具有策略梯度的连续动作，将RL的应用扩展到更多的任务，如控制。TRPO改进了DDPG的性能，因为它引入了代理目标函数和KL散度约束，保证了长期回报不减少。PPO通过修改代理目标函数进一步优化TRPO，提高了性能，降低了实现和计算的复杂度。</p><h1 id="cd64" class="li lj it bd lk ll no ln lo lp np lr ls lt nq lv lw lx nr lz ma mb ns md me mf bi translated">结论</h1><p id="4c11" class="pw-post-body-paragraph kb kc it kd b ke mu kg kh ki mv kk kl km nt ko kp kq nu ks kt ku nv kw kx ky im bi translated">总之，我介绍了两种更高级的RL算法，并对我讨论过的所有RL算法进行了比较。然而，在TRPO中，数学公式非常复杂。虽然我已经尽力解释了，但我相信对你们中的一些人来说，这可能仍然是令人困惑的。如果您有任何问题，请随时在下面发表评论，或者在<a class="ae kz" href="https://twitter.com/steeve__huang" rel="noopener ugc nofollow" target="_blank">推特</a>上关注我。</p></div></div>    
</body>
</html>