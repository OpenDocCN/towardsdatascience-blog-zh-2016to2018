<html>
<head>
<title>[ ICLR 2017 / Paper Summary ] Exploring loss function topology with cyclical learning rates</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">【ICLR 2017 /论文摘要】探索具有循环学习率的损失函数拓扑</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/iclr-2017-paper-summary-exploring-loss-function-topology-with-cyclical-learning-rates-4213655ba65?source=collection_archive---------16-----------------------#2018-06-29">https://towardsdatascience.com/iclr-2017-paper-summary-exploring-loss-function-topology-with-cyclical-learning-rates-4213655ba65?source=collection_archive---------16-----------------------#2018-06-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/9d17e0e182559ade66b770a8c4a7e31b.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/1*9PL_lYrFxzucKWwSi2YINA.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/xUPGcGA2vckFaMUWbK" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="eb0c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我想知道更多关于循环学习率的信息。</p><blockquote class="kx ky kz"><p id="c00e" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated"><strong class="kb ir">请注意，这篇帖子是让我未来的自己回顾和回顾这篇论文上的材料，而不是从头再看一遍。</strong></p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="https://arxiv.org/pdf/1702.04283.pdf" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="574e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">摘要</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/9b104334701de0b412f1f53c723aad04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z6NG6DdyUIlm2WOZ4pQO_Q.png"/></div></div></figure><p id="a471" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">本文作者训练了一个具有循环学习率(CLR)的残差神经网络。使用线性网络插值，他们能够观察到奇怪的现象。他们还发现 CLR 可以产生更高的测试准确度，尽管他们的学习率很高。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="8014" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">简介</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/29049f16da7d185ca8142ca8dd24eb5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ouk5M2NGENApT2l78CvF0g.png"/></div></div></figure><p id="6faa" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在大多数神经网络论文中，作者提供了 top-k 准确度分数以及训练/测试进行情况的图表。这篇论文的作者认为，必须报告额外的行为。如上图(左图)所示，当学习率设置为 0.14 时，我们可以观察到一个奇怪的现象。训练开始后，准确性实际上在再次上升之前下降了。在右边的图表中，当学习率设定在 0.25 到 1.0 之间时，网络在这些范围内表现很好。本文作者希望用循环学习率来进一步研究这一现象。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="697f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">超收敛</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lx"><img src="../Images/287a11d99104719510bf5b79a05c03e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hKWiz6NuRLLA6tsynurdvA.png"/></div></div></figure><p id="f0a2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">该论文的作者首先尝试了三角循环学习率(图表的左侧部分)。我们可以观察到，随着学习率从 0.1 增加到 0.35，训练损失急剧增加(当学习率在 0.25 左右时)。此外，我们可以观察到测试准确性和测试损失之间奇怪的分歧行为。</p><p id="25e1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在图表的右边可以看到另一个令人惊讶的结果。我们可以看到，通过循环学习率，网络实际上能够比典型的学习率更快地收敛。其中仅在 20，000 个时期内就达到了 93%的准确率。然而，不幸的是，训练以较低的准确率结束。本文作者创造了术语“超收敛”来指代这种现象，即与传统训练相比，网络被训练到更好的最终测试精度，但迭代次数更少，学习率更高。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="d587" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">网络插值</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ly"><img src="../Images/7992f86e5e86201b63a349b2028dd744.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MfImDUNhQ2ymp5E0NXtElg.png"/></div></div></figure><p id="5aba" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果网络的权重初始化不同，则有理由认为网络找到的最优解(最小点)可能不同。将每个网络的权重与下面的等式结合起来，我们可以看到这些最佳点是否彼此相似。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/89c7ae86fa3dd1331b7e3e0e042614ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*hiHbZp9Kbcyfbk8IlzirgA.png"/></div></figure><p id="4c9e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">正如上文所见(图片和图表)，当作者将两种不同状态下的训练(常规学习率)网络结合起来时，我们可以看到一个单一的凹形，指示相似的最小点。但是，当我们结合使用 CLR 训练的网络的两个不同状态的权重时，我们可以观察到网络找到的最优解是不同的。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="0409" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结论</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ma"><img src="../Images/2124227088fb9f459b8509b1f26b6bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uM_b5nw-SRpSjt5uRQbVTA.png"/></div></div></figure><p id="4463" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">总之，本文作者报道了用循环学习率训练残差网络时的一些奇怪现象。作者认为这些模式出现的根本原因是损失函数拓扑。令人惊讶的是，这篇论文中报道的现象只是作者所看到的所有现象的一小部分。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="d4fe" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">最后的话</strong></p><p id="4d05" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">没有多少研究者研究学习速率的动态如何影响网络性能。很高兴看到这些研究正在进行。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="38c2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="4aba" class="mb mc iq kb b kc kd kg kh kk md ko me ks mf kw mg mh mi mj bi translated">l .史密斯和 n .托平(2017 年)。探索具有循环学习率的损失函数拓扑。Arxiv.org。检索于 2018 年 6 月 29 日，来自<a class="ae jy" href="https://arxiv.org/abs/1702.04283" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1702.04283</a></li></ol></div></div>    
</body>
</html>