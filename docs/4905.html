<html>
<head>
<title>Variational Autoencoders Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释了各种自动编码器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/variational-autoencoders-explained-6f9456ee030c?source=collection_archive---------7-----------------------#2018-09-14">https://towardsdatascience.com/variational-autoencoders-explained-6f9456ee030c?source=collection_archive---------7-----------------------#2018-09-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/1d15abcdd9385923edfd3af66cb257e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pWKGRMmF-v9YLUQJlEkdQA.jpeg"/></div></div></figure><p id="8bdb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">想知道变分自动编码器(VAE)模型是如何工作的吗？你想知道 VAE 是如何能够生成与它所训练的数据集相似的新样本的吗？</p><p id="fc00" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">读完这篇文章后，你将对 VAE 的内部运作有一个理论上的理解，并且能够自己实现它。</p><p id="4caf" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在未来的一篇文章中，我将为你提供一个在手写数字图像数据集上训练的 VAE 的工作代码，我们将有一些生成新数字的乐趣！</p></div><div class="ab cl la lb hx lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="im in io ip iq"><h1 id="5424" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">生成模型</h1><p id="52d7" class="pw-post-body-paragraph kb kc it kd b ke mf kg kh ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky im bi translated">VAE 是一个生成模型，它估计训练数据的概率密度函数(PDF)。如果这样的模型是在看起来自然的图像上训练的，它应该给狮子的图像分配高概率值。另一方面，随机乱码的图像应该被赋予低概率值。</p><p id="df5a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">VAE 模型还可以从学习过的 PDF 中抽取样本，这是最酷的部分，因为它能够生成看起来与原始数据集相似的新样本！</p><p id="b406" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我将使用<a class="ae kz" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank"> MNIST </a>手写数字数据集来解释 VAE。模型的输入是 2828 维空间(ℝ[28∙28]).)中的图像如果输入看起来像一个数字，模型应该估计一个高概率值。</p><h1 id="d771" class="lh li it bd lj lk mk lm ln lo ml lq lr ls mm lu lv lw mn ly lz ma mo mc md me bi translated">对图像建模的挑战</h1><p id="cf6e" class="pw-post-body-paragraph kb kc it kd b ke mf kg kh ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky im bi translated">像素之间的相互作用带来了巨大的挑战。如果像素是彼此独立的，我们将需要独立地学习每个像素的 PDF，这很容易。采样本来也是轻而易举的——我们只是独立地对每个像素进行采样。</p><p id="3715" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在数字图像中，像素之间有明显的相关性。如果你看一个图像的左半部分，看到一个 4 的开始，你会非常惊讶地看到右半部分是一个 0 的结束。但是为什么呢？…</p><h1 id="95d3" class="lh li it bd lj lk mk lm ln lo ml lq lr ls mm lu lv lw mn ly lz ma mo mc md me bi translated">潜在空间</h1><p id="1bb4" class="pw-post-body-paragraph kb kc it kd b ke mf kg kh ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky im bi translated">你知道一个数字的每个图像应该包含一个单独的数字。ℝ[28∙28 的输入]并不明确包含该信息。但是它一定存在于某个地方…某个地方的<em class="mp">就是潜在空间。</em></p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mq"><img src="../Images/dc234ebfa448eef364d8a332cc8a616c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*h44BzNDbFsxy44w1.jpg"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">The latent space. Photo by Samuel Zeller on Unsplash</figcaption></figure><p id="94db" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你可以把潜在空间想象成ℝ[k 】,其中每个矢量都包含绘制一幅图像所需的 k 条基本信息。假设第一维包含由数字表示的数字。第二维可以是宽度。第三——角度。诸如此类。</p><p id="6369" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们可以把生成图像的过程想象成一个两步过程。首先，这个人有意识或无意识地决定他要画的数字的所有属性。接下来，这些决定转化为笔触。</p><p id="bace" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">VAE 试图对这一过程进行建模:给定一幅图像<em class="mp"> x </em>，我们想要找到至少一个能够描述它的潜在向量——一个包含生成<em class="mp"> x </em>的指令的向量。使用全概率的<a class="ae kz" href="https://en.wikipedia.org/wiki/Law_of_total_probability" rel="noopener ugc nofollow" target="_blank">定律公式化它，我们得到</a></p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mz"><img src="../Images/43d7be1899304a26cc04bf34e8c0060b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*23JGtwiHZv2g-4bxgR9utw.png"/></div></div></figure><p id="0bf9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们给这个等式注入一些直觉:</p><ul class=""><li id="5411" class="na nb it kd b ke kf ki kj km nc kq nd ku ne ky nf ng nh ni bi translated">积分意味着我们应该在整个潜在空间中寻找候选者。</li><li id="70ab" class="na nb it kd b ke nj ki nk km nl kq nm ku nn ky nf ng nh ni bi translated">对于每一个候选<em class="mp"> z </em>，我们问自己:是否可以使用<em class="mp"> z </em>的指令生成<em class="mp"> x </em>？<em class="mp"> P(x|z) </em>够大吗？例如，如果<em class="mp"> z </em>对数字为 7 的信息进行编码，则 8 的图像是不可能的。然而，1 的图像可能是可能的，因为 1 和 7 看起来相似。</li><li id="304e" class="na nb it kd b ke nj ki nk km nl kq nm ku nn ky nf ng nh ni bi translated">我们找到一个好的<em class="mp"> z </em>？很好！但是等一下…这有可能吗？<em class="mp"> P(z) </em>够大吗？让我们考虑一个颠倒的 7 的给定图像。描述相似外观 7 的潜在向量将是完美匹配，其中角度维度被设置为 180 度。然而，那个<em class="mp"> z </em>不太可能，因为通常数字不会以 180 度角绘制。</li></ul><p id="7248" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">VAE 的训练目标是最大化<em class="mp"> P(x) </em>。我们将使用多元高斯𝓝 <em class="mp"> (f(z)，𝜎I)</em>对<em class="mp"> P(x|z) </em>建模。</p><p id="3ae2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="mp"> f(z) </em>将使用神经网络建模。<em class="mp"> 𝜎 </em>是一个将单位矩阵<em class="mp"> I </em>相乘的超参数。</p><p id="1045" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你应该记住<em class="mp"> f </em>是我们在使用训练好的模型生成新图像时要用到的。强加高斯分布仅用于训练目的。如果我们使用 Dirac delta 函数(即<em class="mp"> x = f(z) </em>确定性)，我们将无法使用梯度下降来训练模型！</p><h1 id="d795" class="lh li it bd lj lk mk lm ln lo ml lq lr ls mm lu lv lw mn ly lz ma mo mc md me bi translated">潜在空间的奇迹</h1><p id="4304" class="pw-post-body-paragraph kb kc it kd b ke mf kg kh ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky im bi translated">潜在空间方法有两个大问题:</p><ol class=""><li id="b911" class="na nb it kd b ke kf ki kj km nc kq nd ku ne ky no ng nh ni bi translated">每个维度包含哪些信息？一些维度可能与抽象的信息有关，例如风格。即使解释所有维度很容易，我们也不想给数据集分配标签。这种方法无法扩展到新的数据集。</li><li id="f112" class="na nb it kd b ke nj ki nk km nl kq nm ku nn ky no ng nh ni bi translated">潜在空间可能是纠缠的，即维度可能是相关的。例如，一个手指画得非常快，可能会导致有角度的和更细的笔触。指定这些依赖关系很难。</li></ol><h1 id="d419" class="lh li it bd lj lk mk lm ln lo ml lq lr ls mm lu lv lw mn ly lz ma mo mc md me bi translated">深度学习拯救世界</h1><p id="c772" class="pw-post-body-paragraph kb kc it kd b ke mf kg kh ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky im bi translated">事实证明，每个分布都可以通过对标准多元高斯函数应用足够复杂的函数来生成。</p><p id="b1a2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因此，我们将选择<em class="mp"> P(z) </em>作为标准多元高斯。由神经网络建模的<em class="mp"> f </em>因此可以分为两个阶段:</p><ol class=""><li id="39cf" class="na nb it kd b ke kf ki kj km nc kq nd ku ne ky no ng nh ni bi translated">第一层将高斯映射到潜在空间的真实分布。我们无法解释维度，但这并不重要。</li><li id="61d7" class="na nb it kd b ke nj ki nk km nl kq nm ku nn ky no ng nh ni bi translated">然后，后面的层将从潜在空间映射到<em class="mp"> P(x|z) </em>。</li></ol><h1 id="4d19" class="lh li it bd lj lk mk lm ln lo ml lq lr ls mm lu lv lw mn ly lz ma mo mc md me bi translated">那么我们该如何训练这只野兽呢？</h1><p id="6697" class="pw-post-body-paragraph kb kc it kd b ke mf kg kh ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky im bi translated"><em class="mp"> P(x) </em>的公式很难处理，所以我们将使用蒙特卡罗方法来近似它:</p><ol class=""><li id="8b96" class="na nb it kd b ke kf ki kj km nc kq nd ku ne ky no ng nh ni bi translated">样本{ <em class="mp"> zᵢ </em> } (i = 1…n)来自先前的<em class="mp"> P(z) </em>。</li><li id="926b" class="na nb it kd b ke nj ki nk km nl kq nm ku nn ky no ng nh ni bi translated">近似使用<em class="mp">p(x)</em>≈<em class="mp">(1/n)∙∑p(x|zᵢ)</em>。</li></ol><p id="aa02" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">太好了！因此，我们只需对一些 z 进行采样，然后让反向传播派对开始！</p><p id="7f17" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">不幸的是，由于<em class="mp"> x </em>具有高维数，因此需要许多样本来获得合理的近似。我的意思是，如果你对 z 进行采样，你最终得到一个看起来和 x 有关的图像的可能性有多大？顺便说一下，这解释了为什么<em class="mp"> P(x|z) </em>必须为任何可能的图像分配正概率值，否则模型将无法学习:采样的<em class="mp"> z </em>将产生几乎肯定不同于<em class="mp"> x </em>的图像，如果概率为 0，梯度将不会传播。</p><p id="fb7c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">那么，我们如何解决这个烂摊子呢？</p><h1 id="0cbb" class="lh li it bd lj lk mk lm ln lo ml lq lr ls mm lu lv lw mn ly lz ma mo mc md me bi translated">我们走捷径吧！</h1><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mq"><img src="../Images/a88251e6b207ebd5f7cd0050cfc066ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gyr-rY2PrjOHt9eJ.jpg"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Photo by Stefan Steinbauer on Unsplash</figcaption></figure><p id="edde" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">大多数被取样的 z 对 P(x)没有任何贡献——它们太差了。如果我们能提前知道从哪里取样就好了…</p><p id="155d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们可以引入<em class="mp"> Q(z|x) </em>。<em class="mp"> Q </em>将被训练为向可能已经生成<em class="mp"> x </em>的<em class="mp"> z </em>给出高概率值。现在，我们可以使用来自<em class="mp"> Q </em>的更少样本来计算蒙特卡罗估计。</p><p id="11f5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">不幸的是，新的问题出现了！而不是最大化</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi np"><img src="../Images/7df7996e762065668aeaa0ace19cce84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oPIBqm1ELITKRwlkIMtwTw.png"/></div></div></figure><p id="e850" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们将最大化以下内容</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/b88108e1ed3f2e2497c9b50c35996250.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*nXb6EZs0gkErp30UepJkSw.png"/></div></figure><p id="415a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这两者之间有什么关系？</p><h1 id="3804" class="lh li it bd lj lk mk lm ln lo ml lq lr ls mm lu lv lw mn ly lz ma mo mc md me bi translated">变分推理</h1><p id="96dc" class="pw-post-body-paragraph kb kc it kd b ke mf kg kh ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky im bi translated">变分推理是一个自成一帖的话题，这里就不细说了。我要说的是，这两者确实通过这个等式联系在一起:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nr"><img src="../Images/e8d0e64a39614993b5ce5cfb9ce58662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CrViuqFsY3xICIQ8AhAGpA.png"/></div></div></figure><p id="8ee8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="mp"> KL </em>是<a class="ae kz" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank">kull back–lei bler 散度</a>，它直观地衡量两个分布有多相似。</p><p id="e017" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一会儿你会看到我们如何最大化等式的右边。这样，左侧也将被最大化:</p><ul class=""><li id="5b66" class="na nb it kd b ke kf ki kj km nc kq nd ku ne ky nf ng nh ni bi translated"><em class="mp"> P(x) </em>将被最大化。</li><li id="da86" class="na nb it kd b ke nj ki nk km nl kq nm ku nn ky nf ng nh ni bi translated">从<em class="mp"> P(z|x) </em>到<em class="mp"> Q(z|x) </em>有多远——我们不知道的<em class="mp">真实</em>后验——将被最小化。</li></ul><p id="d458" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">等式右边背后的直觉是我们有一个张力:</p><ol class=""><li id="6bb3" class="na nb it kd b ke kf ki kj km nc kq nd ku ne ky no ng nh ni bi translated">一方面，我们希望最大化从<em class="mp"> z ~ Q </em>解码<em class="mp"> x </em>的效果。</li><li id="65c9" class="na nb it kd b ke nj ki nk km nl kq nm ku nn ky no ng nh ni bi translated">另一方面，我们希望<em class="mp">Q(z | x)</em>(<em class="mp">编码器</em>)类似于先前的<em class="mp"> P(z) </em>(多元高斯)。人们可以把这个术语看作是正则化。</li></ol><p id="0e30" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">给定正确的分布选择，最小化 KL 散度是容易的。我们将把<em class="mp"> Q(z|x) </em>建模为神经网络，其输出是多元高斯的参数:</p><ul class=""><li id="e527" class="na nb it kd b ke kf ki kj km nc kq nd ku ne ky nf ng nh ni bi translated">一个平均值<em class="mp"> μ_Q </em></li><li id="bcf2" class="na nb it kd b ke nj ki nk km nl kq nm ku nn ky nf ng nh ni bi translated">对角协方差矩阵<em class="mp">σ_ Q</em></li></ul><p id="098f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">KL 散度然后变得解析可解，这对我们(和梯度)是很好的。</p><p id="56ff" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="mp">解码器</em>部分有点棘手。天真的是，我们会用蒙特卡罗来解决这个棘手的问题。但是从<em class="mp"> Q </em>对<em class="mp"> z </em>进行采样不会允许梯度通过<em class="mp"> Q </em>传播，因为采样不是可微分的操作。这是有问题的，因为输出<em class="mp">σ_ Q</em>和<em class="mp"> μ_Q </em>的层的权重不会被更新。</p><h1 id="814b" class="lh li it bd lj lk mk lm ln lo ml lq lr ls mm lu lv lw mn ly lz ma mo mc md me bi translated">重新参数化的技巧</h1><p id="3a28" class="pw-post-body-paragraph kb kc it kd b ke mf kg kh ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky im bi translated">我们可以用无参数随机变量的确定性参数化变换来代替<em class="mp"> Q </em>:</p><ol class=""><li id="4f40" class="na nb it kd b ke kf ki kj km nc kq nd ku ne ky no ng nh ni bi translated">来自标准(无参数)高斯的样本。</li><li id="c713" class="na nb it kd b ke nj ki nk km nl kq nm ku nn ky no ng nh ni bi translated">将样本乘以<em class="mp">σ_ Q</em>的平方根。</li><li id="16ba" class="na nb it kd b ke nj ki nk km nl kq nm ku nn ky no ng nh ni bi translated">将<em class="mp"> μ_Q </em>加到结果上。</li></ol><p id="082a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">结果将具有等于<em class="mp"> Q </em>的分布。现在采样操作将从标准高斯。因此，梯度将能够通过<em class="mp">σ_ Q</em>和<em class="mp"> μ_Q </em>传播，因为现在这些是确定性路径。</p><p id="0646" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">结果呢？该模型将能够学习如何调整<em class="mp"> Q </em>的参数:它将集中于能够产生<em class="mp"> x </em>的好的<em class="mp"> z </em>。</p></div><div class="ab cl la lb hx lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="im in io ip iq"><h1 id="3c34" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">将这些点连接起来</h1><p id="38a1" class="pw-post-body-paragraph kb kc it kd b ke mf kg kh ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky im bi translated">VAE 模式可能很难理解。我们在这里讨论了很多材料，可能会让人不知所措。</p><p id="7161" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因此，让我总结一下实施 VAE 需要掌握的所有步骤。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ns"><img src="../Images/86e3564650b098e6587890156b041587.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-a1ehKCtR43kPt_664WZfQ.png"/></div></div></figure><p id="8c53" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">左侧是模型定义:</p><ol class=""><li id="ca6b" class="na nb it kd b ke kf ki kj km nc kq nd ku ne ky no ng nh ni bi translated">输入图像通过编码器网络。</li><li id="1213" class="na nb it kd b ke nj ki nk km nl kq nm ku nn ky no ng nh ni bi translated">编码器输出分布参数<em class="mp"> Q(z|x) </em>。</li><li id="0096" class="na nb it kd b ke nj ki nk km nl kq nm ku nn ky no ng nh ni bi translated">从<em class="mp"> Q(z|x) </em>中采样潜在向量<em class="mp"> z </em>。如果编码器学会做好它的工作，大多数机会是<em class="mp"> z </em>将包含描述<em class="mp"> x </em>的信息。</li><li id="ec41" class="na nb it kd b ke nj ki nk km nl kq nm ku nn ky no ng nh ni bi translated">解码器将<em class="mp"> z </em>解码成图像。</li></ol><p id="9c3b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">右边是损失:</p><ol class=""><li id="9fe1" class="na nb it kd b ke kf ki kj km nc kq nd ku ne ky no ng nh ni bi translated">重构误差:输出应该类似于输入。</li><li id="923d" class="na nb it kd b ke nj ki nk km nl kq nm ku nn ky no ng nh ni bi translated"><em class="mp"> Q(z|x) </em>应该类似于先验(多元标准高斯)。</li></ol><p id="f234" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了生成新的图像，可以直接从先验分布中采样一个潜在向量，并将其解码成图像。</p></div><div class="ab cl la lb hx lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="im in io ip iq"><p id="1ed0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在下一篇文章中，我会给你提供一个 VAE 的工作代码。此外，我将向您展示如何使用一个巧妙的技巧来调节潜在向量，以便您可以决定要为哪个数字生成图像。敬请期待:)</p><h2 id="234d" class="nt li it bd lj nu nv dn ln nw nx dp lr km ny nz lv kq oa ob lz ku oc od md oe bi translated">笔记</h2><p id="0463" class="pw-post-body-paragraph kb kc it kd b ke mf kg kh ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky im bi translated">这篇文章是基于我的直觉和以下来源:</p><ul class=""><li id="83cb" class="na nb it kd b ke kf ki kj km nc kq nd ku ne ky nf ng nh ni bi translated"><a class="ae kz" href="https://pure.uva.nl/ws/files/17891313/Thesis.pdf" rel="noopener ugc nofollow" target="_blank">变分推理和深度学习:一种新的综合</a></li><li id="f351" class="na nb it kd b ke nj ki nk km nl kq nm ku nn ky nf ng nh ni bi translated"><a class="ae kz" href="https://arxiv.org/abs/1606.05908" rel="noopener ugc nofollow" target="_blank">变型自动编码器教程</a></li></ul></div><div class="ab cl la lb hx lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="im in io ip iq"><p id="11b3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这个帖子最初是我在<a class="ae kz" href="http://anotherdatum.com" rel="noopener ugc nofollow" target="_blank">www.anotherdatum.com</a>发的。</p></div></div>    
</body>
</html>