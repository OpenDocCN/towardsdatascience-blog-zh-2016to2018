<html>
<head>
<title>Early Stopping</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">提前停止</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/early-stopping-2f92c29ce0ae?source=collection_archive---------11-----------------------#2018-10-08">https://towardsdatascience.com/early-stopping-2f92c29ce0ae?source=collection_archive---------11-----------------------#2018-10-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="de48" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有时候不值得走到最后，尤其是在超参数调优中</p><p id="c499" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di"> M </span> ost 机器学习模型具有由用户固定的超参数，以便在底层数据集上构建这些模型的训练。例如，在训练随机森林时，您需要指定树的深度和数量(以及其他超参数)。还有许多其他的<a class="ae ku" rel="noopener" target="_blank" href="/demystifying-hyper-parameter-tuning-acb83af0258f">【真实世界】</a>超参数的例子。一旦设置了超参数，模型的训练就可以用标准的优化器如梯度下降来处理。指定不当的超参数会导致更长的训练时间或有偏差的模型。所选超参数的质量通常是在一个公开的测试数据集上测量的。</p><p id="df16" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/the-intuitions-behind-bayesian-optimization-with-gaussian-processes-7e00fcc898a0">贝叶斯优化</a>是一种旨在尽可能少的迭代中识别最佳超参数的方法，但每次迭代都需要在评估超参数的质量并进入下一次迭代之前完成给定超参数配置的模型训练。在许多超参数调优问题中，数据科学家可能在训练结束(或开始)之前就知道某些配置不会产生很好的结果！).在本文中，我们将研究一种叫做冻融的方法，它将贝叶斯优化中超参数调整的<strong class="jp ir"> <em class="kv">【提前停止】</em> </strong>系统化。因此，这种方法可以更有效地利用超参数调优中的计算资源，并减少整体调优时间。</p><h1 id="9dc1" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">冻融贝叶斯优化</h1><blockquote class="lu lv lw"><p id="4984" class="jn jo kv jp b jq jr js jt ju jv jw jx lx jz ka kb ly kd ke kf lz kh ki kj kk ij bi translated"><a class="ae ku" href="https://arxiv.org/pdf/1406.3896.pdf" rel="noopener ugc nofollow" target="_blank">冻融</a>使用在训练机器学习模型的过程中获得的部分信息，以便决定是否暂停训练并开始新的模型，或者恢复先前考虑的模型的训练。</p></blockquote><p id="a105" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在模型训练期间提供的部分信息实质上是模型的训练损失，其指示模型对训练数据的预测有多差。一个例子是均方误差，它是每个例子的平均平方损失，由以下公式给出:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ma"><img src="../Images/14d51c09b40ec0d54554b30b49b898a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kGVljOJktCST_j7iGRZong.png"/></div></div></figure><p id="930c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，线性回归的最佳权重使损失最小化，这就是为什么在图 1 中，损失比第二种情况高得多(并且模型差得多)。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mm"><img src="../Images/4d96959e0d72b49d44537eb46053299a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j2iMyb40bSEk92846aPSww.png"/></div></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">Figure 1: loss of the trained model (red) for linear regression models (blue) trained on data points (yellow)(<a class="ae ku" href="https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="671d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">冻融的工作原理是假设大多数机器学习模型的训练损失大致遵循朝向未知最终值的指数衰减。这种假设被编码在他们的贝叶斯优化方法的先验中，这允许他们预测部分训练模型的最终结果。</p><p id="c3c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更简单地说:</p><ul class=""><li id="f3d9" class="mr ms iq jp b jq jr ju jv jy mt kc mu kg mv kk mw mx my mz bi translated">贝叶斯优化为我们正在优化的底层函数构建了一个代理模型</li><li id="de05" class="mr ms iq jp b jq na ju nb jy nc kc nd kg ne kk mw mx my mz bi translated">在这种情况下，这是一个高斯过程，通过观察函数值和预测域的其他点来建立。</li><li id="4363" class="mr ms iq jp b jq na ju nb jy nc kc nd kg ne kk mw mx my mz bi translated">在冻结-解冻 BO 中，来自模型训练的部分信息用于预测最终损失，然后将最终损失纳入替代模型</li><li id="d7cd" class="mr ms iq jp b jq na ju nb jy nc kc nd kg ne kk mw mx my mz bi translated">换句话说，代理模型是对模型训练结束时的预测</li></ul><p id="b5a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦已经预测了替代物，贝叶斯优化策略适于“打包”一篮子<strong class="jp ir"> B = B_old + B_new </strong>候选模型，这些模型已经用不同的超参数值进行了部分训练或者没有进行训练。然后，贝叶斯优化程序将确定尝试哪些新配置以及恢复哪些“冻结”配置。更详细的解释可以在他们的<a class="ae ku" href="https://arxiv.org/pdf/1406.3896.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中找到。</p><p id="8082" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在众所周知的问题上，冻融法相对于其他贝叶斯优化方法的性能增益如图 2 所示。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nf"><img src="../Images/590418961cac3a4ad286951b123c3218.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R1zfzi4SI98UzSQHwUtH1A.png"/></div></div><figcaption class="mn mo gj gh gi mp mq bd b be z dk">FT vs GP EI MCMC on MNIST (a), LDA (b), PMF/Movie Lens (c)</figcaption></figure><p id="a143" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">收益是显著的，但是严重依赖于训练损失遵循指数衰减的假设。这项工作的扩展可以研究这个假设不成立的模型。提前停止是<a class="ae ku" href="http://mindfoundry.ai" rel="noopener ugc nofollow" target="_blank"> Mind Foundry </a>的一个重要研究领域，很快将在我们的贝叶斯优化 API<a class="ae ku" href="https://optaas.mindfoundry.ai" rel="noopener ugc nofollow" target="_blank">opta as</a>中实现。</p><p id="1779" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">【<strong class="jp ir">更新</strong>:我开了一家科技<a class="ae ku" href="http://www.legislate.tech" rel="noopener ugc nofollow" target="_blank">公司</a>。您可以在此了解更多<a class="ae ku" href="https://medium.com/legislate/eliminate-the-work-in-paperwork-c053bfb0188c" rel="noopener"/></p><p id="7dcc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1: K .斯维斯基、j .斯诺克和 R. P .亚当斯。冻融贝叶斯优化。<a class="ae ku" href="https://arxiv.org/abs/1406.3896" rel="noopener ugc nofollow" target="_blank"> <br/> arXiv:1406.3896 </a>【统计。ML]</p></div></div>    
</body>
</html>