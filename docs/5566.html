<html>
<head>
<title>Reaching for the gut of Machine Learning: A brief intro to CLT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">触及机器学习的本质:CLT 简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-analyze-learning-short-tour-of-computational-learning-theory-9d93b15fc3e5?source=collection_archive---------7-----------------------#2018-10-26">https://towardsdatascience.com/how-to-analyze-learning-short-tour-of-computational-learning-theory-9d93b15fc3e5?source=collection_archive---------7-----------------------#2018-10-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cf04" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">作为机器学习的实践者，了解计算学习理论的基础可以极大地增强你的能力。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5226932ae322d3adab94b002cef9d475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FRmr0Iq0Zxz1A_hbpI62Yw.jpeg"/></div></div></figure><h1 id="3179" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">介绍</h1><p id="6455" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">假设，这是一个阳光明媚的日子，你有朋友来访，你最喜欢的餐馆在 12 英里外开了一家分店。一般来说，你会避免长途驾驶，但是今天你会出去吃午饭吗？你是否有这种情况的过去的例子(一些因素是积极的，一些是消极的)，从中你制定了一个规则？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/0c56450781cb7622cc40159fe6d573c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tDCYtlK61ehmGBVipNRX-g.jpeg"/></div></div></figure><p id="67d8" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这就是我们如何从过去的经验和行为中学习，形成规则，并将其应用于当前的情况。机器也没有什么不同。但是这种机器学习背后也有一个理论。</p><p id="4a17" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu"> C </strong>计算<strong class="lo iu"> L </strong>收入<strong class="lo iu"> T </strong>理论(<strong class="lo iu"> CLT </strong>)是统计学/机器学习/人工智能(总的来说)的一个分支，它处理关于分析我们(人和机器)从数据中学习规则和模式的能力的基本界限和定理。</p><p id="7d92" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">它<strong class="lo iu">超越了我们经常听到的特定算法</strong>的领域——<em class="mo">回归、决策树、支持向量机</em>或<em class="mo">深度神经网络</em>——并试图回答<strong class="lo iu">关于整个机器学习企业的限制和可能性的根本问题</strong>。</p><blockquote class="mp"><p id="98be" class="mq mr it bd ms mt mu mv mw mx my mh dk translated">这就是我们如何从过去的经验和行为中学习，形成规则，并将其应用于当前的情况。机器也没有什么不同</p></blockquote><p id="5320" class="pw-post-body-paragraph lm ln it lo b lp mz ju lr ls na jx lu lv nb lx ly lz nc mb mc md nd mf mg mh im bi translated">听起来很刺激？请继续阅读，快速浏览这个领域…</p><h2 id="2293" class="ne kv it bd kw nf ng dn la nh ni dp le lv nj nk lg lz nl nm li md nn no lk np bi translated">有哪些基本问题？</h2><p id="6203" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">当研究机器学习时，很自然会想知道什么一般规律可以支配机器(和非机器)学习者。举个例子，</p><ul class=""><li id="b4fc" class="nq nr it lo b lp mj ls mk lv ns lz nt md nu mh nv nw nx ny bi translated">是否有可能独立于学习算法，识别出天生困难或容易的学习问题类别？</li><li id="8390" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh nv nw nx ny bi translated">你能描述出确保成功学习所需的或足够的<strong class="lo iu">培训示例的数量吗？</strong></li><li id="38d5" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh nv nw nx ny bi translated">如果允许学习者向培训师提问，而不是观察随机抽样的培训示例，这个数字会受到什么影响？</li><li id="d2fa" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh nv nw nx ny bi translated">在学习目标函数之前，你能描述一个学习者会犯的错误的数量吗？</li><li id="9d7b" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh nv nw nx ny bi translated">有人能描述学习问题类固有的计算复杂性(T21)吗？</li></ul><blockquote class="oe of og"><p id="252e" class="lm ln mo lo b lp mj ju lr ls mk jx lu oh ml lx ly oi mm mb mc oj mn mf mg mh im bi translated">下面是令人失望的答案:所有这些问题的一般答案还不知道。</p></blockquote><p id="17ce" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">但是我们可以专注于任何实际机器学习任务中最常出现的特定设置，<strong class="lo iu"/><strong class="lo iu">—</strong><a class="ae ok" href="https://www.netlanguages.com/blog/index.php/2017/06/28/what-is-inductive-learning/" rel="noopener ugc nofollow" target="_blank"><strong class="lo iu">归纳学习</strong> </a> <strong class="lo iu">一个未知的目标函数，只给定这个目标函数的训练示例和一组候选假设</strong>。</p><p id="2e58" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">在这种情况下，我们主要关心的问题是，</p><ul class=""><li id="a328" class="nq nr it lo b lp mj ls mk lv ns lz nt md nu mh nv nw nx ny bi translated"><em class="mo">多少个训练示例足以成功学习目标函数</em>，以及</li><li id="0a47" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh nv nw nx ny bi translated"><em class="mo">学习者会犯多少错误才能成功</em>？</li></ul><p id="b73f" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">正如我们将看到的，根据学习问题的属性，为这些度量设置数量界限是可能的，</p><ul class=""><li id="806f" class="nq nr it lo b lp mj ls mk lv ns lz nt md nu mh nv nw nx ny bi translated">学习者考虑的假设空间的大小或复杂性</li><li id="1876" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh nv nw nx ny bi translated">目标概念必须达到的近似精度</li><li id="793f" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh nv nw nx ny bi translated">学习者输出成功假设的概率</li><li id="c042" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh nv nw nx ny bi translated">向学习者展示培训示例的方式</li></ul><blockquote class="mp"><p id="eab4" class="mq mr it bd ms mt ol om on oo op mh dk translated">CLT 试图回答关于整个机器学习事业的限制和可能性的基本问题。</p></blockquote><h2 id="c333" class="ne kv it bd kw nf oq dn la nh or dp le lv os nk lg lz ot nm li md ou no lk np bi translated">关键词的图示</h2><p id="b843" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">为了进行全面的讨论，我在这里尝试定义一下在 CLT 普遍使用的基本关键词。</p><p id="4547" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">数据</strong>:一组给定的例子，我们试图从中学习目标概念。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/3ac293bcee89b3959831f40edd15956b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*10ARSOvsm7TRll4AU4651g.png"/></div></div></figure><p id="98f1" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">目标概念(又名规则)</strong>:这是我们试图从数据中学习的隐藏模式例如，“我们出去吃午饭<strong class="lo iu"> <em class="mo">如果</em> </strong>天气晴朗<strong class="lo iu"><em class="mo"/></strong>我们有朋友<strong class="lo iu"> <em class="mo">或</em> </strong> <strong class="lo iu"> <em class="mo">如果</em> </strong>最喜欢的美食餐馆就在附近<strong class="lo iu"> <em class="mo">和</em> </strong>我们有朋友<strong class="lo iu"/></p><p id="a286" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这种规则是根据属于<a class="ae ok" href="https://www.geeksforgeeks.org/proposition-logic/" rel="noopener ugc nofollow" target="_blank">命题逻辑</a>的陈述编写的，即用 AND、OR、NOT 表达的真值。</p><p id="d71d" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">假设空间</strong>:这是一组假设，我们希望从中发现目标概念。在这个例子中，假设的“智能”选择应该看起来像我们上面写的作为目标概念的逻辑连接短语。但是它们可以更简单或者看起来不同，</p><ul class=""><li id="43eb" class="nq nr it lo b lp mj ls mk lv ns lz nt md nu mh nv nw nx ny bi translated">如果天气晴朗</li><li id="146e" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh nv nw nx ny bi translated">如果我们有朋友或者天气晴朗</li></ul><p id="780f" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">注意上面两个假设和目标概念之间的细微差别。<strong class="lo iu">这两个假设不够丰富，不足以抓住真正的目标概念</strong>，如果应用于给定的数据，它们会有误差。</p><p id="7e8f" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">为什么？</p><p id="6a23" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">因为它们不属于“析取陈述的连词”形式，即(X1 &amp; X2)|(X3 &amp; X4)|(X5 &amp; amp！X6)。它们过于简单和笼统，无法捕捉所呈现数据的所有细微差别。</p><p id="55bd" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">因此，这个故事的寓意是，你在机器学习(这里是分类)任务中搜索目标概念是否会成功，很大程度上取决于你选择工作的假设空间的丰富性和复杂性。</p><p id="39d4" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">预测器和响应变量</strong>:这些是不言自明的。“<em class="mo">晴</em>？”、‘<em class="mo">最喜欢的餐厅距离</em>、‘<em class="mo">有朋友吗？“T9”是预测变量，“出去吃午饭”是响应变量。</em></p><h2 id="b62c" class="ne kv it bd kw nf ng dn la nh ni dp le lv nj nk lg lz nl nm li md nn no lk np bi translated">感觉抽象？需要具体例子吗？</h2><p id="418e" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">还记得分类的决策树吗？这里有两个这样的树，以及它们在关于上述学习问题的假设空间方面的丰富性。</p><p id="7cdc" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">如果你阅读从根开始向下到一片叶子的树的边所满足的条件，你会自动得到和上面一样的命题逻辑陈述。试试看。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/950de275d7517ac11c1950a36abeec22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lUCF0EciNBXrb5UYR5GbsQ.png"/></div></div><figcaption class="ox oy gj gh gi oz pa bd b be z dk">Rich and ‘poor’ hypothesis space illustrations</figcaption></figure><blockquote class="mp"><p id="fce5" class="mq mr it bd ms mt ol om on oo op mh dk translated">因此，这个故事的寓意是，你在机器学习(这里是分类)任务中搜索目标概念是否会成功，很大程度上取决于你选择工作的假设空间的丰富性和复杂性。</p></blockquote><h2 id="3e05" class="ne kv it bd kw nf oq dn la nh or dp le lv os nk lg lz ot nm li md ou no lk np bi translated">假设空间的大小？那里有个陷阱！</h2><p id="9d88" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">那么，这里假设空间的大小是多少呢？</p><p id="a03c" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">如果你用所有三个预测变量构建树，你可以从其中的任何一个开始，然后你可以在深度<strong class="lo iu">)</strong>处有<em class="mo"> n </em>的叶子，如果你使用所有的预测变量。但是你可以选择在所有的叶子上写“是”和“不是”。所以，大小可以和 O(n*2^(2^n)).一样大对于这个例子，有 3 个变量，我们可以得到 3*2^(2^(3–1)= 48 棵树！这 48 棵树分别代表一种不同的假设。很多都是多余的，用简单的思考就能消除，但那是题外话。</p><blockquote class="oe of og"><p id="ae8a" class="lm ln mo lo b lp mj ju lr ls mk jx lu oh ml lx ly oi mm mb mc oj mn mf mg mh im bi translated">那么，如果我们考虑一个足够丰富、足够大的假设空间，是否保证能找到目标概念(给定足够的数据)？</p></blockquote><p id="e55e" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">没有。</p><p id="18d3" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">考虑一个简单线性回归问题</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/10a48f277268bf449806c3da4ce82e11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PC3ksJb_U1_XxHCKCQ0zgw.png"/></div></div></figure><p id="2a6e" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">有多少预测值？<strong class="lo iu"> 1 </strong>。那么，什么是假设空间大小呢？2?4?</p><p id="f93d" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">答案是——<strong class="lo iu">无限</strong>。</p><p id="a86d" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">你能在一个平面上画多少条直线？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/e143edc70ffae2e71ff1e7f5050ffcbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oEqzFfEfIKtkKwa5LoaCog.png"/></div></div></figure><p id="80c7" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">你对无害的最小二乘误差最小化回归算法的尊重是不是越来越大了？毕竟，<a class="ae ok" rel="noopener" target="_blank" href="/where-did-the-least-square-come-from-3f1abc7f7caf">能够从那些无限的可能性中找到最优的路线【T3:-)</a></p><blockquote class="oe of og"><p id="10c2" class="lm ln mo lo b lp mj ju lr ls mk jx lu oh ml lx ly oi mm mb mc oj mn mf mg mh im bi translated">但问题是——即使假设空间无限大，我们也不能指望在这个回归任务中找到真正的目标概念。</p></blockquote><p id="0120" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这是因为<strong class="lo iu">真正的概念存在于另一个空间，其大小是一个‘更大的无限’</strong>。在这个例子中，<strong class="lo iu"> <em class="mo"> y </em> </strong>和<strong class="lo iu"> <em class="mo"> x </em> </strong>之间的真实函数关系可能是二次函数关系。这意味着二次项的无限可能性+线性项的无限可能性+截距的无限可能性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/a139af74ad41ea7c6a65f29d21d075d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C_FUn-ismZczIcCLWpLrIg.png"/></div></div></figure><p id="3800" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">除非我们决定在我们的假设空间中包含一个二次项，否则我们将永远无法找到真正的函数，即使在只有线性项(和截距)的无限大的空间中。</p><blockquote class="oe of og"><p id="fc94" class="lm ln mo lo b lp mj ju lr ls mk jx lu oh ml lx ly oi mm mb mc oj mn mf mg mh im bi translated"><strong class="lo iu">所以，很多时候，我们需要更丰富的空间，而不一定是更大的。</strong>而且，像灭霸一样，我们需要不断担心和寻找哪个无限大小的空间最适合我们必须学习的特定数据集！</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/93ddb27cf0c7362eabcd7ec29502ebaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*UTIE4oW3CaRLM2ZJ-l04fQ.png"/></div></div></figure><h2 id="28be" class="ne kv it bd kw nf ng dn la nh ni dp le lv nj nk lg lz nl nm li md nn no lk np bi translated">那么，CLT 试图估算的关键量是什么呢？</h2><p id="392d" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">在很大程度上，CLT 关注的不是个别的学习算法，而是以他们考虑的假设空间、训练样本的呈现等为特征的学习算法的大类。它试图推理的关键量是，</p><ul class=""><li id="516f" class="nq nr it lo b lp mj ls mk lv ns lz nt md nu mh nv nw nx ny bi translated"><strong class="lo iu"> <em class="mo">样本复杂度</em> </strong>:一个学习者<br/>需要多少训练样本才能收敛(大概率)到一个成功的假设？</li><li id="3f77" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh nv nw nx ny bi translated"><strong class="lo iu"> <em class="mo">计算复杂度</em> </strong>:一个学习者需要多大的计算努力才能收敛(大概率)到一个成功的假设？</li><li id="a163" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh nv nw nx ny bi translated"><strong class="lo iu"> <em class="mo">错误界限</em> </strong>:在收敛到一个成功的假设之前，学习者会错误分类<br/>多少训练样本？</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/e0b12f565d5699a7794868372c2548f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P6AVFX-M5T-7NU1skfw9HA.png"/></div></div><figcaption class="ox oy gj gh gi oz pa bd b be z dk"><strong class="bd pf">Model complexity and learning curve show how much data and what class of hypothesis space we need</strong></figcaption></figure><p id="47a6" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">有各种各样的方法来说明对学习者来说“成功”意味着什么我们可以指定，为了成功，学习者必须输出一个与目标概念相同的<br/>假设。或者，我们可以简单地要求它输出一个大多数时候与目标概念一致的假设，或者它通常输出这样一个假设。</p><p id="b5f2" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">同样，我们必须指定学习者如何获得训练样本。训练示例有可能是由一位乐于助人的老师提供的，或者是由学习者执行精心计划的实验获得的(<strong class="lo iu">想一想 A/B 测试或科学实验</strong>)，或者是根据学习者控制之外的一些自然过程随机生成的(<strong class="lo iu">想一想随机点击流，癌细胞与药物的相互作用</strong>)。正如我们所料，上述问题的答案取决于我们心目中的特定环境或学习模式。</p><h2 id="f135" class="ne kv it bd kw nf ng dn la nh ni dp le lv nj nk lg lz nl nm li md nn no lk np bi translated">“大概近似正确”(PAC)的神奇世界</h2><p id="392a" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">在这种理论设置下，我们假设所有观察到的数据都是从一个未知但固定(非时变)的分布过程中产生的，该分布过程的参数不会受到我们从中抽取样本的过程的影响。此外，不失一般性，我们可以假设无噪声测量过程。</p><p id="f912" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">设<strong class="lo iu"> <em class="mo"> H </em> </strong>为假设空间，<strong class="lo iu"> <em class="mo"> D </em> </strong>为未知真分布，<strong class="lo iu"> <em class="mo"> c </em> </strong>为目标概念。假设在观察了一些数据<em class="mo"> </em> <strong class="lo iu"> <em class="mo"> d </em> </strong>后，学习算法<strong class="lo iu"> <em class="mo"> L </em> </strong>输出一个它认为是<strong class="lo iu"><em class="mo">【c</em></strong>的最佳逼近的假设<strong class="lo iu"><em class="mo"/></strong>。</p><p id="22f5" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这里衡量错误/成功的标准是什么？误差在哪里<strong class="lo iu"> <em class="mo"> c </em> </strong>和<strong class="lo iu"> <em class="mo"> h </em> </strong>的预测不一致。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/937343c21785f5bdcc83913fa4b1c863.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*aMFCIDaxEjG0dX-6HlVKxg.png"/></div></div><figcaption class="ox oy gj gh gi oz pa bd b be z dk"><strong class="bd pf">Image credit</strong>: “Machine Learning”, by Tom Mitchell</figcaption></figure><blockquote class="oe of og"><p id="88ad" class="lm ln mo lo b lp mj ju lr ls mk jx lu oh ml lx ly oi mm mb mc oj mn mf mg mh im bi translated">我们的目的是描述目标概念的类别，这些目标概念可以从<strong class="lo iu">合理数量的</strong>随机抽取的训练样本和<strong class="lo iu">合理数量的计算</strong>中<strong class="lo iu">可靠地学习</strong>。</p></blockquote><p id="d1f7" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们没有无限的时间、计算能力、存储或传感器带宽。因此，我们并不试图使误差为零。为此，我们需要看到由<strong class="lo iu"><em class="mo"/></strong>生成的每一个可能的实例。出于同样的原因，我们限制自己检查有限的训练样本，并使用多项式时间算法。</p><p id="ee2c" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">此外，假设训练样本是随机抽取的，<br/>学习者遇到的训练样本<br/>总会有一些非零概率是误导的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/a1f867963918736af644fa5980f00b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vUBzWyntF4vQw1e9nTS5iQ.png"/></div></div></figure><p id="39a8" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">因此，我们将只要求它的<strong class="lo iu">误差被某个常数<br/><strong class="lo iu"/>限制，该常数可以任意变小</strong>。此外，我们不会要求学习者对随机抽取的训练样本的每个序列都成功，相反，我们的要求只是它的<strong class="lo iu">失败概率由某个常数限制，该常数可以任意变小</strong>。</p><blockquote class="mp"><p id="c0d8" class="mq mr it bd ms mt mu mv mw mx my mh dk translated">简而言之，我们只要求学习者<strong class="ak"> <em class="pi">大概学习</em> </strong>一个<strong class="ak"> <em class="pi">近似正确</em> </strong>的假设。</p></blockquote><p id="4a85" class="pw-post-body-paragraph lm ln it lo b lp mz ju lr ls na jx lu lv nb lx ly lz nc mb mc md nd mf mg mh im bi translated">有了这些设置，我们说我们的概念类<strong class="lo iu"> <em class="mo"> C </em> </strong>，目标概念<strong class="lo iu"> <em class="mo"> c </em> </strong>所属的， <a class="ae ok" href="https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture28-pac.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="lo iu">PAC-learnable</strong></a>如果学习算法在用数据<strong class="lo iu"><em class="mo">【d】</em></strong>进行训练后能够输出假设<strong class="lo iu"> <em class="mo"> h </em> </strong>具有一定高的<strong class="lo iu"> <em class="mo">概率</em> </strong>和一定低的<strong class="lo iu"> <em class="mo">误差</em> </strong>并且计算量是 1/ <strong class="lo iu"> <em class="mo">误差</em> </strong></p><h2 id="820b" class="ne kv it bd kw nf ng dn la nh ni dp le lv nj nk lg lz nl nm li md nn no lk np bi translated">样本复杂性界限</h2><p id="c31a" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">这里我们不讨论形式上的证明，但是根据上一节中讨论的概念，可以推导出一个非常简单和优雅的数学界限来回答下面的问题。</p><blockquote class="oe of og"><p id="e74a" class="lm ln mo lo b lp mj ju lr ls mk jx lu oh ml lx ly oi mm mb mc oj mn mf mg mh im bi translated">“当假设空间具有一定大小时，以一定概率学习近似正确的假设所需的最小训练样本数是多少？”</p></blockquote><p id="a353" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">界限由下式给出，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/3408fd6c35cf3e7302831b58f73d925b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*TkNT15e4fjtTeOEiiVJ5AQ.png"/></div></figure><p id="c038" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这叫做<a class="ae ok" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.125.6931" rel="noopener ugc nofollow" target="_blank"> <strong class="lo iu">豪斯勒束缚</strong> </a>。这个不等式很容易证明我们在任何机器学习任务的实践中所经历的以下事实，</p><ul class=""><li id="fb02" class="nq nr it lo b lp mj ls mk lv ns lz nt md nu mh nv nw nx ny bi translated"><strong class="lo iu">我们需要更多的训练数据来减少测试/泛化错误</strong></li><li id="76e0" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh nv nw nx ny bi translated"><strong class="lo iu">该算法需要“看到”更多的训练样本，以增加其学习的信心，即减少出错的可能性</strong></li><li id="7dad" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh nv nw nx ny bi translated"><strong class="lo iu">更丰富、更大的假设空间意味着在寻找正确假设时要搜索更大的维度，这就需要更多的训练样本</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/4bd2088a47501dc1ee3ed85c4445a177.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qaUGdnii-6KwyQtma3X69A.png"/></div></div></figure><h2 id="4555" class="ne kv it bd kw nf ng dn la nh ni dp le lv nj nk lg lz nl nm li md nn no lk np bi translated">我们短暂的旅行到此结束，但更多的“学习”还在前面</h2><p id="398d" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">对这种类型的复杂性界限有一种普遍的批评，认为它们过于悲观，不能代表实际的学习问题。关于这个话题有很多争论，你可以从我在本文末尾提供的参考资料中读到一些。</p><p id="c83a" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">一个关于无限假设空间的自然问题出现了。样本界是否趋于无穷大？原来有一个概念叫做'<a class="ae ok" href="http://disi.unitn.it/moschitti/Teaching-slides/VC-dim.pdf" rel="noopener ugc nofollow" target="_blank"> VC-dimension </a> ' (V 在这里代表<a class="ae ok" href="https://en.wikipedia.org/wiki/Vladimir_Vapnik" rel="noopener ugc nofollow" target="_blank"> Vladimir Vapnik </a>，支持向量机的发明者)，它有助于保持许多实际学习问题的样本复杂度有限，即使是线性回归或核机器这样的无限假设空间。但是我们可以在另一篇文章中讨论这些高级概念。</p><p id="3613" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我希望这篇文章能够引发一些关于基本机器学习理论的有趣概念，并有助于激发您学习这些主题的兴趣。</p><p id="94e1" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">要获得关于这些主题的优秀教程，请观看此视频，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pl pm l"/></div></figure></div><div class="ab cl pn po hx pp" role="separator"><span class="pq bw bk pr ps pt"/><span class="pq bw bk pr ps pt"/><span class="pq bw bk pr ps"/></div><div class="im in io ip iq"><h2 id="f8e1" class="ne kv it bd kw nf ng dn la nh ni dp le lv nj nk lg lz nl nm li md nn no lk np bi translated">可供参考的资料</h2><ol class=""><li id="e89a" class="nq nr it lo b lp lq ls lt lv pu lz pv md pw mh px nw nx ny bi translated">《机器学习》，汤姆·米切尔(1997)。</li><li id="4419" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh px nw nx ny bi translated"><a class="ae ok" href="http://www.cs.cmu.edu/~./awm/tutorials/pac.html" rel="noopener ugc nofollow" target="_blank">CMU PAC 学习教程</a></li><li id="9326" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh px nw nx ny bi translated"><a class="ae ok" href="http://www.cs.cmu.edu/~guestrin/Class/10701-S05/slides/pac-vc.pdf" rel="noopener ugc nofollow" target="_blank"> PAC 学习、VC 维度和基于边界的边界</a></li><li id="f821" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh px nw nx ny bi translated">"<a class="ae ok" href="https://arxiv.org/pdf/1808.06324.pdf" rel="noopener ugc nofollow" target="_blank"> PAC 学习是不可判定的</a>"</li><li id="5544" class="nq nr it lo b lp nz ls oa lv ob lz oc md od mh px nw nx ny bi translated">"<a class="ae ok" href="https://pdfs.semanticscholar.org/733d/d84bc10fa96c65a10c7856f6f9bd1b389700.pdf" rel="noopener ugc nofollow" target="_blank">学习算法的效率和计算限制</a>"</li></ol></div><div class="ab cl pn po hx pp" role="separator"><span class="pq bw bk pr ps pt"/><span class="pq bw bk pr ps pt"/><span class="pq bw bk pr ps"/></div><div class="im in io ip iq"><p id="4c39" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi py translated"><span class="l pz qa qb bm qc qd qe qf qg di">如果</span>您有任何问题或想法要分享，请联系作者在<a class="ae ok" href="mailto:tirthajyoti@gmail.com" rel="noopener ugc nofollow" target="_blank"><strong class="lo iu">tirthajyoti【AT】Gmail . com</strong></a>。此外，您可以查看作者的<a class="ae ok" href="https://github.com/tirthajyoti?tab=repositories" rel="noopener ugc nofollow" target="_blank"> <strong class="lo iu"> GitHub 资源库</strong> </a>中其他有趣的 Python、R 或 MATLAB 代码片段和机器学习资源。如果你像我一样对机器学习/数据科学充满热情，请随时<a class="ae ok" href="https://www.linkedin.com/in/tirthajyoti-sarkar-2127aa7/" rel="noopener ugc nofollow" target="_blank">在 LinkedIn 上添加我</a>或<a class="ae ok" href="https://twitter.com/tirthajyotiS" rel="noopener ugc nofollow" target="_blank">在 Twitter 上关注我。</a></p></div></div>    
</body>
</html>