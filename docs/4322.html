<html>
<head>
<title>The Best Embedding Method for Sentiment Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">情感分类的最佳嵌入方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/blog-md-34c5d082a8c5?source=collection_archive---------13-----------------------#2018-08-07">https://towardsdatascience.com/blog-md-34c5d082a8c5?source=collection_archive---------13-----------------------#2018-08-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/fb5e7a34111f4065c66eec83cd005e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/0*gCk6fd7E7qiFXAJw.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">image from <a class="ae kb" href="http://houseofbots.com/news-detail/2491-4-5-things-you-need-to-know-about-sentiment-analysis-and-classification" rel="noopener ugc nofollow" target="_blank">http://houseofbots.com/news-detail/2491-4-5-things-you-need-to-know-about-sentiment-analysis-and-classification</a></figcaption></figure><h1 id="b672" class="kc kd it bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">1.背景</h1><p id="ec74" class="pw-post-body-paragraph la lb it lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">你可能认为单词级表示是情感分类任务最有效的特征。但是，这是真的吗？我将展示一个有趣的实验结果，让你大吃一惊。</p><p id="f1e3" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">如果你不想读整篇文章，这里有一样东西你可以带走。如果数据集有许多非正式单词，子单词级嵌入方法对于情感分类是有用的。</p><p id="bca3" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">本文的其余部分组织如下。</p><ul class=""><li id="9b1b" class="md me it lc b ld ly lh lz ll mf lp mg lt mh lx mi mj mk ml bi translated">情感分析中使用的嵌入方法适用于 ACL 2018</li><li id="ff4e" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">讨论为什么单词级嵌入如此广泛地用于情感分析</li><li id="5370" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">实验设置和结果</li></ul><p id="9f1f" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">你可以在这里找到所有代码:<a class="ae kb" href="https://github.com/BrambleXu/nlp-beginner-guide-keras/tree/master/sentiment-comparison" rel="noopener ugc nofollow" target="_blank">情绪比较</a></p><h1 id="3d51" class="kc kd it bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">2.ACL 2018 中情感分析的嵌入方法</h1><p id="61ac" class="pw-post-body-paragraph la lb it lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">计算语言学协会(ACL)是一个非常著名的 NLP 会议。如果你扫了一眼 ACL 2018 中的情感分析相关作品，你可能会注意到大部分作品都选择了词级嵌入方法。</p><p id="c47e" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">单词嵌入方法已经被证明是一种有效的自然语言处理技术。但是除了单词嵌入之外，还有另外两种嵌入方法，字符级嵌入和子单词级嵌入。</p><figure class="ms mt mu mv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mr"><img src="../Images/475227f746b716168e59b6f86928ed5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-pgsO-u001Yx0lwizNABQ.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">word/character/subword representations</figcaption></figure><p id="9f58" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">我计算 ACL 2018 关于情感分析任务的长论文和短论文。这表明了词级嵌入在情感分析中的广泛应用。</p><p id="5130" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">共有 12 篇长论文，5 篇短论文，标题包含关键词“情绪”。根据我们的调查，有 15 篇论文是基于单词级嵌入的，1 篇是基于字符级嵌入的，1 篇是字符级和单词级相结合的。</p><figure class="ms mt mu mv gt ju gh gi paragraph-image"><div class="gh gi na"><img src="../Images/3d25314971c3eec00c234701d8f5b268.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*W4fLmBD1bDDVXJX_Bs08uQ.png"/></div></figure><p id="6d72" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">我在下面列出了所有等级分类的论文。<em class="nb"> SP </em>的意思是短纸。</p><p id="74fc" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated"><strong class="lc iu"> A .字级</strong>:</p><p id="a501" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">Word2Vec</p><ul class=""><li id="7b88" class="md me it lc b ld ly lh lz ll mf lp mg lt mh lx mi mj mk ml bi translated">为跨领域情感分类识别跨领域的可转移信息</li><li id="e023" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">SemAxis:一个轻量级框架，用于描述情感之外的特定领域词汇语义</li><li id="adf4" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">情绪自适应的端到端对话系统</li><li id="7051" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">双语情感嵌入:跨语言情感的联合投射</li><li id="a9f2" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">(SP)利用文档知识进行方面级情感分类</li><li id="191d" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">幽默识别中的情感关联建模</li></ul><p id="ad51" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">手套</p><ul class=""><li id="810d" class="md me it lc b ld ly lh lz ll mf lp mg lt mh lx mi mj mk ml bi translated">援助之手:用于深度情感分析的迁移学习</li><li id="bd78" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">冷启动感知用户和产品对情感分类的关注</li><li id="2ebc" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">面向情感分类的目标敏感记忆网络。</li><li id="3d76" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">面向目标的情感分类转换网络</li><li id="aaa2" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">具有目标领域特定信息的跨领域情感分类</li><li id="51a2" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">基于特征的门控卷积网络情感分析</li></ul><p id="94d6" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">组合 Word2Vec，手套</p><ul class=""><li id="60d1" class="md me it lc b ld ly lh lz ll mf lp mg lt mh lx mi mj mk ml bi translated">(SP)用于改进情感分类的领域适应单词嵌入</li></ul><p id="63da" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">学习单词级情感信息</p><ul class=""><li id="09c2" class="md me it lc b ld ly lh lz ll mf lp mg lt mh lx mi mj mk ml bi translated">学习领域敏感和情感感知的单词嵌入</li></ul><p id="093e" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">无预训练模型(随机生成)</p><ul class=""><li id="03f8" class="md me it lc b ld ly lh lz ll mf lp mg lt mh lx mi mj mk ml bi translated">不成对的情感到情感的翻译:一种循环强化学习方法</li></ul><p id="06d0" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated"><strong class="lc iu"> B .人物等级</strong></p><ul class=""><li id="2e66" class="md me it lc b ld ly lh lz ll mf lp mg lt mh lx mi mj mk ml bi translated">(SP)用未标记的对话数据预训练情感分类器</li></ul><p id="67fd" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated"><strong class="lc iu"> C .字级和词级结合</strong></p><ul class=""><li id="8f92" class="md me it lc b ld ly lh lz ll mf lp mg lt mh lx mi mj mk ml bi translated">用于情感分类的多情感资源增强注意网络</li></ul><h1 id="1234" class="kc kd it bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">3.为什么词级嵌入在情感分析中如此受欢迎？</h1><p id="3ba7" class="pw-post-body-paragraph la lb it lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">也许我们应该改变这个问题，为什么大多数研究人员不考虑情感分析任务的其他嵌入方法？对于机器翻译、命名实体识别、语言建模等自然语言处理任务，字符级嵌入和子词级嵌入已经显示出巨大的改进。但是对于情感分析任务，这些嵌入方法似乎被忽略了。</p><p id="975b" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">我们知道字符级表示和子词级表示的最大优势是处理词汇外(OOV)问题。这是否意味着 OOV 词对于情感分析任务并不重要？</p><p id="f7a7" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">在某种程度上，是的。在情感分类问题中，我们希望提取情感特征来预测一个句子是肯定的还是否定的。但通常情况下，未登录词并不包含有用的情感信息。直观地，情感特征可以在词级表示中被充分提取。这就是为什么情感词典方法可以获得很好的性能。例如，<code class="fe nc nd ne nf b">'happy'</code>是一个正面词，词级嵌入方法很容易从上下文中学习到这个情感信息。</p><p id="dfb7" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">至于字符级表示，即使它解决了 OOV 问题，我们也很难说从文本中提取情感特征是容易的。例如，我们可以将<code class="fe nc nd ne nf b">'happy'</code>表示为<code class="fe nc nd ne nf b">'h', 'a', 'p', 'p', 'y'</code>。情感词被拆分成字符，从这种上下文中提取情感特征变得越来越困难。因为单个字符很难传达情感信息。</p><p id="0e45" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">子词级表示怎么样？我们可以将子词级的<code class="fe nc nd ne nf b">'happy'</code>表示为<code class="fe nc nd ne nf b">'hap', 'py'</code>，这看起来像是字符级的表示，没有太多的意义。但是，如果我们处理一些包含许多未知单词的数据集，情况可能会有所不同。考虑到这些词<code class="fe nc nd ne nf b">'coooool'</code>和‘呜哇’。即使我们知道这些单词包含强烈的情感信息，它们也会在单词级表示的预处理过程中被去除。另一方面，如果我们在子词级表示这些词，<code class="fe nc nd ne nf b">'co', 'ooo', 'ol'</code>和<code class="fe nc nd ne nf b">'wo', 'oooo', 'ow'</code>，子词级表示可以从<code class="fe nc nd ne nf b">'oooo'</code>中捕获情感信息。这些非正式词汇在 Twitter 和 Reddit 等社交网络短信中非常常见。</p><p id="fe3b" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">单词级表示非常紧凑，但存在 OOV 问题，而字符级表示过于稀疏，但不存在 OOV 问题。子词级别的表示法似乎很好地平衡了这两者。因此，如果一个数据集有许多非正式的词，我们认为<strong class="lc iu">子词级别的表示能够足够好地捕捉情感特征</strong>。为了证明我们的假设，我们进行了一些实验。</p><h1 id="ec2d" class="kc kd it bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">4.实验对比</h1><p id="4fd2" class="pw-post-body-paragraph la lb it lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我们选择两个数据集进行比较。对于每个数据集，我们选择三个表示级别。对于每个级别，我们实现了两个深度学习模型，CNN 和 LSTM</p><ul class=""><li id="c0a2" class="md me it lc b ld ly lh lz ll mf lp mg lt mh lx mi mj mk ml bi translated">两个数据集:电影评论，推特</li><li id="1887" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">三个表示级别:单词、字符、子单词</li><li id="c793" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">两个深度学习模型:CNN，LSTM</li></ul><h1 id="46bf" class="kc kd it bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">4.1 实验设置</h1><h1 id="159e" class="kc kd it bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">数据集</h1><p id="c545" class="pw-post-body-paragraph la lb it lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我们使用两个数据集进行实验。两个数据集都是关于二元情感分类的。一个是<a class="ae kb" href="http://www.cs.cornell.edu/people/pabo/movie-review-data/" rel="noopener ugc nofollow" target="_blank">影评数据集</a>，可以从<a class="ae kb" href="http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz" rel="noopener ugc nofollow" target="_blank">句子极性数据集 v1.0 </a>下载。还有一个是<a class="ae kb" href="http://help.sentiment140.com/for-students" rel="noopener ugc nofollow" target="_blank"> Twitter 数据集</a>，你可以从网页下载。</p><p id="dd4a" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">使用 LSTM 来训练整个 Twitter 数据集将花费太长时间(一个纪元 9 个小时！)，所以我们取 50，000 个样本作为小数据集，用于 CNN/LSTM 比较，并在整个数据集上训练 CNN。</p><figure class="ms mt mu mv gt ju gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/d0f3a5c683b2546903634859616732c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*6CiuK0MxV8sYxMJ9YHaWRg.png"/></div></figure><h1 id="35eb" class="kc kd it bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">嵌入方法</h1><p id="1489" class="pw-post-body-paragraph la lb it lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">对于每个数据集，我们将在单词/字符/子单词三个嵌入级别上进行实验。</p><figure class="ms mt mu mv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mr"><img src="../Images/475227f746b716168e59b6f86928ed5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-pgsO-u001Yx0lwizNABQ.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">word/character/subword representations</figcaption></figure><h2 id="029a" class="nh kd it bd ke ni nj dn ki nk nl dp km ll nm nn kq lp no np ku lt nq nr ky ns bi translated">A.单词级嵌入</h2><p id="32ff" class="pw-post-body-paragraph la lb it lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">在这一级，我们将预先训练的单词嵌入(手套嵌入)作为单词向量。单词级的数据预处理非常简单。首先我们用<code class="fe nc nd ne nf b">data_helpers.load_data_and_labels</code> ( <a class="ae kb" href="https://github.com/BrambleXu/nlp-beginner-guide-keras/blob/master/cnn-text-classification/data_helpers.py#L31" rel="noopener ugc nofollow" target="_blank">源代码</a>)加载和清理数据，然后用 keras 函数对文本进行分词和填充。在我们知道最大长度之后，我们在填充过程中将<code class="fe nc nd ne nf b">maxlen</code>设置为 56。</p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="89fa" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">Jupyter 笔记本:<a class="ae kb" href="https://github.com/BrambleXu/nlp-beginner-guide-keras/blob/master/sentiment-comparison/word-level/preprocess_word_mv.ipynb" rel="noopener ugc nofollow" target="_blank">单词级-预处理-电影</a></p><p id="45aa" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">对于 Twitter 数据集的预处理，清理有更多的步骤。</p><ul class=""><li id="b9fc" class="md me it lc b ld ly lh lz ll mf lp mg lt mh lx mi mj mk ml bi translated">HTML 解码</li><li id="e42e" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">删除@提及</li><li id="02aa" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">删除 URL 链接</li><li id="29cf" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">拉丁 1 BOM(字节顺序标记)，用<code class="fe nc nd ne nf b">?</code>替换签名字符<code class="fe nc nd ne nf b">ï»¿</code></li><li id="44b2" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">删除标签和数字</li></ul><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="72f7" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">我们将清理后的数据集保存为<code class="fe nc nd ne nf b">clean_tweet.csv</code></p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="dddd" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">Af 首先我打算使用拼写校正来预处理文本，但是<a class="ae kb" href="http://norvig.com/spell-correct.html" rel="noopener ugc nofollow" target="_blank">拼写校正算法</a>的输出似乎不够好，所以我没有使用拼写校正。如果你对拼写校正感兴趣，那么<a class="ae kb" href="https://github.com/sloria/TextBlob/blob/dev/textblob/blob.py#L113" rel="noopener ugc nofollow" target="_blank">文本块</a>已经实现了我上面提到的拼写校正算法。你可以在 beblow notebook 中找到拼写纠正的输出。</p><p id="f759" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">Jupyter 笔记本:<a class="ae kb" href="https://github.com/BrambleXu/nlp-beginner-guide-keras/blob/master/sentiment-comparison/word-level/preprocess_word_twitter_1.ipynb" rel="noopener ugc nofollow" target="_blank">单词级-预处理-twitter </a></p><h2 id="343d" class="nh kd it bd ke ni nj dn ki nk nl dp km ll nm nn kq lp no np ku lt nq nr ky ns bi translated">B.字符级嵌入</h2><p id="a44a" class="pw-post-body-paragraph la lb it lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">因为我们使用字符级表示，所以电影评论数据集的预处理非常简单。</p><p id="e254" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">我们总共有 70 个角色:</p><blockquote class="nv nw nx"><p id="2324" class="la lb nb lc b ld ly lf lg lh lz lj lk ny ma ln lo nz mb lr ls oa mc lv lw lx im bi translated">abcdefghijklmnopqrstuvwxyz 0123456789，；。！？:'\"/\\|_@#$%^&amp;*~`+-= &lt;&gt; ()[]{}</p></blockquote><p id="4c84" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">在这一关，我们采用三种不同的方法为每个角色分配向量。首先是<strong class="lc iu">一键编码</strong>。我们给每个字符分配一个热点向量。我们需要将<code class="fe nc nd ne nf b">embedding_weights</code>传递给嵌入层。<code class="fe nc nd ne nf b">embedding_weights</code>是包含每个字符的所有向量的矩阵。</p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="5a09" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">如果您不熟悉 one-hot 编码，这里有一篇很棒的文章适合您。<a class="ae kb" href="https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/" rel="noopener ugc nofollow" target="_blank">如何用 Python 对序列数据进行热编码</a>。</p><p id="dff3" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">第二种方法是我们<strong class="lc iu">生成随机值</strong>作为每个字符的向量。当初始化嵌入层时，我们不会将<code class="fe nc nd ne nf b">embedding_weights</code>传递给<code class="fe nc nd ne nf b">weights</code>参数。</p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="38ce" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">第三种方法是我们使用<strong class="lc iu">预训练字符嵌入</strong>。这里我用手套字嵌入生成字符嵌入。这个脚本可以帮助我们完成这项工作，<a class="ae kb" href="https://github.com/minimaxir/char-embeddings/blob/master/create_embeddings.py" rel="noopener ugc nofollow" target="_blank"> create_embeddings.py </a>。然后我们会得到一个包含字符嵌入的文件。在初始化嵌入层时，我们用其他字符嵌入的平均值手动设置空格字符，用随机值设置未知字符。</p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="7f72" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">我用 CNN 模型对电影评论数据集实现了这三种方法。结果表明，随机初始化方法的性能最好。所以我对所有角色级别的实验都使用随机初始化。</p><figure class="ms mt mu mv gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/fd660b242650a5665b3fdf15d5d74bda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*nDCvMWbhQuPCr7YHeUlfng.png"/></div></figure><p id="2b8c" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">你可以在<a class="ae kb" href="https://github.com/BrambleXu/nlp-beginner-guide-keras/tree/master/sentiment-comparison/char-level" rel="noopener ugc nofollow" target="_blank">这个目录</a>中找到三种嵌入方法。</p><p id="f95c" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">Jupyter 笔记本:<a class="ae kb" href="https://github.com/BrambleXu/nlp-beginner-guide-keras/blob/master/sentiment-comparison/char-level/preprocess_char_mv.ipynb" rel="noopener ugc nofollow" target="_blank">字符级-预处理-电影</a></p><p id="6f40" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">至于角色级表示的 Twitter 数据集，预处理比电影评论数据集简单。字符级表示可以处理未知单词和非正式单词。因此，没有必要删除标点符号和纠正拼写。</p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="7987" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">我们将清洗后的数据集保存为<code class="fe nc nd ne nf b">clean_tweet_char.csv</code>经过预处理后，有些样本可能会变成 NAN，所以在加载数据集时，我们应该删除 NAN 样本。</p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="144f" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">Jupyter 笔记本:<a class="ae kb" href="https://github.com/BrambleXu/nlp-beginner-guide-keras/blob/master/sentiment-comparison/char-level/preprocess_char_twitter_1.ipynb" rel="noopener ugc nofollow" target="_blank">字符级-预处理-推特</a></p><h2 id="1f85" class="nh kd it bd ke ni nj dn ki nk nl dp km ll nm nn kq lp no np ku lt nq nr ky ns bi translated">C.子字级嵌入</h2><p id="c82e" class="pw-post-body-paragraph la lb it lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">至于子词级预处理，我们从<a class="ae kb" href="https://github.com/bheinzerling/bpemb" rel="noopener ugc nofollow" target="_blank"> BPEmb </a>下载 vocab 和预训练的子词嵌入格式。我选择合并操作为 25000 和 50 dims。下面是一个简单的脚本，用于将句子分割成子单词级别的表示，<a class="ae kb" href="https://github.com/bheinzerling/bpemb/blob/master/bpe.py" rel="noopener ugc nofollow" target="_blank"> bpe.py </a></p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="b9a3" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">朱庇特笔记本:</p><ul class=""><li id="fdfb" class="md me it lc b ld ly lh lz ll mf lp mg lt mh lx mi mj mk ml bi translated"><a class="ae kb" href="https://github.com/BrambleXu/nlp-beginner-guide-keras/blob/master/sentiment-comparison/subword-level/preprocess_subword_mv.ipynb" rel="noopener ugc nofollow" target="_blank">子词级预处理电影</a></li><li id="0848" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated"><a class="ae kb" href="https://github.com/BrambleXu/nlp-beginner-guide-keras/blob/master/sentiment-comparison/subword-level/preprocess_subword_twitter.ipynb" rel="noopener ugc nofollow" target="_blank">子词级预处理推特</a></li></ul><h1 id="7e8d" class="kc kd it bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">4.1.3 深度学习模型</h1><p id="381d" class="pw-post-body-paragraph la lb it lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">对于每个级别，我们采取两种深度学习模型。一个是 CNN，我们实现了 Kim 的 CNN <a class="ae kb" href="http://arxiv.org/abs/1408.5882" rel="noopener ugc nofollow" target="_blank">卷积神经网络的修改版本，用于句子分类</a>。你可以在这里查看我的实现，<a class="ae kb" href="https://github.com/BrambleXu/nlp-beginner-guide-keras/tree/master/cnn-text-classification" rel="noopener ugc nofollow" target="_blank">CNN-text-class ification</a>。</p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nt nu l"/></div></figure><figure class="ms mt mu mv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi oc"><img src="../Images/2534985282d59286dd8cc0194b625e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*axrkBAebGenoOBLY"/></div></div></figure><p id="0152" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">下面列出了修改版本的超参数。</p><figure class="ms mt mu mv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi od"><img src="../Images/99390bda11c26ec4ab9ac5af0da4ba9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*fokpN-98VDNTZMgaBfdExg.png"/></div></div></figure><p id="cd3d" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">设置修改的嵌入维度 50 过滤器大小(3，8)过滤器数量 10 池化最大池化</p><p id="e1a4" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">第二个模型是 LSTM，我们堆叠了两个 LSTM 层。</p><figure class="ms mt mu mv gt ju"><div class="bz fp l di"><div class="nt nu l"/></div></figure><figure class="ms mt mu mv gt ju gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/2440adc71a0c4796f3e7d6c0885ef89d.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/0*hFgvB2IOer_lBi9o"/></div></figure><h1 id="f583" class="kc kd it bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">4.2 实验结果</h1><p id="5334" class="pw-post-body-paragraph la lb it lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我们将结果总结在下表中。</p><figure class="ms mt mu mv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi of"><img src="../Images/583dc979720c68c2ee363535d1bf6a00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rqKpXbb6JLejUNTw.png"/></div></div></figure><h1 id="6ec8" class="kc kd it bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">4.3 观察和分析</h1><p id="1a10" class="pw-post-body-paragraph la lb it lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">对于情感分类的不同组合有一些观察。</p><ul class=""><li id="ffc9" class="md me it lc b ld ly lh lz ll mf lp mg lt mh lx mi mj mk ml bi translated">在 Twitter 数据集中，子词级表示比词级表示具有更好的性能。这和我们的假设是一样的。如果数据集包含少量非正式单词并且预处理良好，则单词级嵌入确实在情感分类中实现了最佳性能。但是，如果数据集包含太多的非正式单词，子单词级嵌入可以获得比单词级嵌入更好的结果。</li><li id="f85a" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">CNN 比 LSTM 好。在小型 Twitter 数据集上，子词级表示获得了最佳性能。尽管 CNN 和 LSTM 的准确率非常接近，但考虑到 CNN 的训练速度非常快，毫无疑问，CNN 是比 LSTM 更好的选择。</li><li id="84c9" class="md me it lc b ld mm lh mn ll mo lp mp lt mq lx mi mj mk ml bi translated">字符级表示在三个表示级别中准确性最低。原因可能是字符嵌入缺少良好的预训练字符向量。CNN/LSTM 很难从人物层面捕捉情感信息。</li></ul><h1 id="5ad7" class="kc kd it bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">5.摘要</h1><p id="871b" class="pw-post-body-paragraph la lb it lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">这一次，我没有在优化超参数上花费太多时间。LSTM 模式可能过于简单。但是这个实验仍然表明子词级表示对于情感分类是有用的，特别是对于非正式文本。所以对于情感分析分类来说，子词-词嵌入是一个很好的选择。</p><p id="7bdd" class="pw-post-body-paragraph la lb it lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx im bi translated">你可以在这里找到所有代码:<a class="ae kb" href="https://github.com/BrambleXu/nlp-beginner-guide-keras/tree/master/sentiment-comparison" rel="noopener ugc nofollow" target="_blank">情感对比</a></p><blockquote class="nv nw nx"><p id="b802" class="la lb nb lc b ld ly lf lg lh lz lj lk ny ma ln lo nz mb lr ls oa mc lv lw lx im bi translated"><strong class="lc iu"> <em class="it">查看我的其他帖子</em> </strong> <a class="ae kb" href="https://medium.com/@bramblexu" rel="noopener"> <strong class="lc iu"> <em class="it">中等</em> </strong> </a> <strong class="lc iu"> <em class="it">同</em> </strong> <a class="ae kb" href="https://bramblexu.com/posts/eb7bd472/" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu"> <em class="it">一分类查看</em> </strong> </a> <strong class="lc iu"> <em class="it">！<br/>GitHub:</em></strong><a class="ae kb" href="https://github.com/BrambleXu" rel="noopener ugc nofollow" target="_blank"><strong class="lc iu"><em class="it">bramble Xu</em></strong></a><strong class="lc iu"><em class="it"><br/>LinkedIn:</em></strong><a class="ae kb" href="https://www.linkedin.com/in/xu-liang-99356891/" rel="noopener ugc nofollow" target="_blank"><strong class="lc iu"><em class="it">徐亮</em> </strong> </a> <strong class="lc iu"> <em class="it"> <br/>博客:</em></strong><a class="ae kb" href="https://bramblexu.com" rel="noopener ugc nofollow" target="_blank"><strong class="lc iu"><em class="it">bramble Xu</em></strong></a></p></blockquote></div></div>    
</body>
</html>