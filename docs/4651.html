<html>
<head>
<title>Mathematical Underpinnings: SVMs + Optimisation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数学基础:支持向量机+优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mathematical-underpinnings-svms-optimisation-6495776215c3?source=collection_archive---------9-----------------------#2018-08-28">https://towardsdatascience.com/mathematical-underpinnings-svms-optimisation-6495776215c3?source=collection_archive---------9-----------------------#2018-08-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="1aab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本周我们将看到<strong class="jp ir">拉格朗日乘数将如何帮助我们解决 SVM 最优化问题。因此，让我们提醒自己到目前为止已经涵盖了什么。</strong></p><p id="4c85" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae kl" href="https://medium.com/@emilymuller1991/mathematical-underpinnings-svms-89a1c786eff9" rel="noopener">第 1 周</a>中，我们定义了学习模型参数的支持向量机<strong class="jp ir"> <em class="km"> w </em> </strong>和<em class="km"> b </em>，它们最大化了从决策边界到特征空间中最近点的间隔。此外，我们假设我们的数据点是线性可分的，并且是正确分类的(约束)。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi kn"><img src="../Images/c4adab5785f3d05c93bd23d3359d1779.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Da298rT5-mZ-HQgnM7wug.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk">Week 1 optimisation equation and constraints.</figcaption></figure><p id="1124" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后在第 2 周的<a class="ae kl" href="https://medium.com/@emilymuller1991/mathematical-underpinnings-svms-lagrange-multipliers-3fa057db30cb" rel="noopener">中，我们揭示了拉格朗日乘子的原理，用于解决一个受不等式约束的优化问题。让我们回顾一下。</a></p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi ld"><img src="../Images/f069edc429a5015e0589eca60aea6353.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*39UXB71puN0itSQ-O2z8Lg.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk">Week 2 Lagrange function.</figcaption></figure><p id="3faf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">偏导数 wrt 对<strong class="jp ir"> <em class="km"> x </em> </strong>恢复平行法线约束，偏导数 wrt <em class="km"> λ </em>恢复约束条件，<em class="km">g(</em><strong class="jp ir"><em class="km">x</em></strong><em class="km">)= 0</em>。还记得 KKT 条件吗？</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi le"><img src="../Images/272eca731c839403305133b9b223c858.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/format:webp/1*GBFVsfprYYf-OttF9mFIJA.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk">KKT conditions.</figcaption></figure><p id="f721" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">拉格朗日函数可以扩展到不止一个约束。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi lf"><img src="../Images/43b1bfbeba61ebb24dcdc875d566248c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZlXrzn6p3poxuQu96rOcJw.png"/></div></div></figure><h1 id="2829" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">使用拉格朗日乘子的 SVM 优化</h1><p id="3003" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">我们的优化函数是由|| <strong class="jp ir"> <em class="km"> w </em> </strong> ||^-1.最大化的这反过来相当于最小化|| <strong class="jp ir"> <em class="km"> w </em> </strong> ||。我们的约束条件保持不变。这是<em class="km">二次规划</em>的一个实例；最小化受线性约束的二次函数。我们可以将拉格朗日函数表述如下</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/e2da36ad46378244df39310f51b9aac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*M0JWDbSqOvRFbb7p8DJjTw.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk">Lagrange Function</figcaption></figure><p id="ce41" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对<strong class="jp ir"> <em class="km"> w </em> </strong>和<em class="km"> b </em>进行偏导数，我们得到</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/6c7d868b40967f7bffc70e637594e3cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*hHMy_tnQe6Th0n7bR4SmyQ.png"/></div></figure><p id="0d36" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在将优化拉格朗日对偶表示。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/e1c50c541b902f2fd7238dc926a679c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*EhcVxYRgShcKuKO-VZVReg.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk">The Lagrangian Dual Problem from R. Berwick slides.</figcaption></figure><p id="f173" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这来自于<a class="ae kl" href="https://en.wikipedia.org/wiki/Duality_(optimization)" rel="noopener ugc nofollow" target="_blank">对偶原理</a>，该原理指出优化问题可以被视为原始的(在这种情况下，最小化超过<strong class="jp ir"> <em class="km"> w </em> </strong>和<em class="km"> b) </em>或者对偶的(在这种情况下，最大化超过<strong class="jp ir"> <em class="km"> a </em> </strong>)。对于凸优化问题，原始和对偶有相同的最优解。</p><p id="de81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">拉格朗日对偶表示(通过替换偏导数得到)是:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi mm"><img src="../Images/437a11bc5678fa4825c376fc24781cb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BIZQd3Ir3OizCd5xzN9gjQ.png"/></div></div><figcaption class="kz la gj gh gi lb lc bd b be z dk">Dual Representation of the Lagrange function of SVM optimisation, [Bishop — MLPR].</figcaption></figure><p id="2b79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在有一个关于<strong class="jp ir"> <em class="km">和</em> </strong>的优化问题。要求核函数是正定的，这导致了凸优化问题，给出了与原始原始问题相同的解。</p><p id="635b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们内核的内积起着非常重要的作用。为了深入了解，回想一下两个向量的点积返回它们之间角度的余弦值。可以算是一种<em class="km">相似度</em>。现在，在两个向量完全不同的情况下，内积为 0，拉格朗日量没有增加任何东西。在两个向量相似且具有相同输出值的情况下，内积为 1，拉格朗日中的第二项将保持为正。这导致拉格朗日量减少，因此该算法降低了做出相同预测的相似向量的等级。在两个向量相似的情况下，预测不同的结果类别，第二项增加了拉格朗日，这些是我们感兴趣的点。<strong class="jp ir">区分两个阶级的要点。</strong></p><p id="9022" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">稍后，核还将允许我们通过定义非线性核来对非线性可分的点进行分类。</p><p id="aad7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在优化我们的拉格朗日乘数<strong class="jp ir"> <em class="km"> a </em> </strong>之后，我们可以对新的数据点进行如下分类</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/fbca789c42687235457db1d227884248.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*SzrZD88i-hcpnR1t9Q6ZLg.png"/></div></figure><p id="30cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回想一下现在关于我们的 SVM 优化问题的 KKT 条件。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/eb434deb715b3fcf4c6968442df5ebcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*QZo-Crdt5VLjQeChW8o9IA.png"/></div></figure><p id="3670" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我们要么让拉格朗日乘数等于零，要么不等于零。对于所有的零乘数，它们对<em class="km">y(</em><strong class="jp ir"><em class="km">)x</em></strong><em class="km">)</em>的总和没有贡献，因此在新点分类中不起作用。剩余的向量是支持向量，使得</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/26bd04ad09a0fa2d9157fb2f48b1c011.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*viRe-N-Fmf6okn4XRwaQ1g.png"/></div></figure><p id="cb1c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">并且它们对应于位于最大边缘超平面上的向量，如下所示。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/1478eded7bbc7ac5500b97c1902761ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*wafQ7IL-egUP8OPxAS3NKg.png"/></div><figcaption class="kz la gj gh gi lb lc bd b be z dk">Support vectors shown in blue, [Bishop — MLPR].</figcaption></figure></div></div>    
</body>
</html>