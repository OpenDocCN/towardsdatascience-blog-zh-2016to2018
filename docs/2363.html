<html>
<head>
<title>Speed Up Your Python Code With Broadcasting and PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用广播和PyTorch加速你的Python代码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/speed-up-your-python-code-with-broadcasting-and-pytorch-64fbd31b359?source=collection_archive---------6-----------------------#2018-01-15">https://towardsdatascience.com/speed-up-your-python-code-with-broadcasting-and-pytorch-64fbd31b359?source=collection_archive---------6-----------------------#2018-01-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/09ecb71eb624ef09b8bb1d4daa3f1d11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0M8Vql454XfDRgxh0TIbWg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Image credit: Saffu at <a class="ae jd" href="https://unsplash.com/photos/E4kKGI4oGaU" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/E4kKGI4oGaU</a></figcaption></figure><div class=""/><div class=""><h2 id="e032" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">广播使得向量化您的代码成为可能，在Numpy的底层C实现中执行数组运算，而不必制作不必要的数据副本。根据不同的情况，这可以给你的代码带来显著的加速。此外，事实证明，如果您在Numpy中使用广播，您可以轻松地将代码移植到PyTorch，然后在您的GPU上运行相同的代码，从而获得更高的速度！</h2></div><p id="96ce" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当我做硕士论文的时候，我花了很多时间处理大量的激光雷达数据。其中一个步骤是移除属于场景中静态对象(建筑物、栅栏等)的所有点测量。场景中的每个静态对象都被建模为一个矩形对象，这实质上意味着我必须检查每个激光雷达测量值是否落在任何矩形内。我论文中使用的激光雷达工作在10Hz，每次扫描包含大约100，000到150，000次测量，这意味着一秒钟的激光雷达数据对应于需要处理的1-150万个激光雷达点。那时我不了解Python或广播，所以我的这个处理步骤的实现不是那么快或有效。现在，我将以这个问题为例，展示如何使用broadcasting和PyTorch编写一个非常快速的算法实现。</p><h2 id="8397" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">如何确定一个点是否在矩形中</h2><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mk"><img src="../Images/09e20070ddcebc2c3000763806a3b54b.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*Ld3qbPlOZ2PbY4B-IeupbA.png"/></div></div></figure><p id="4879" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有一个简单的公式可以用来确定一个点是否在矩形内。如果我们假设矩形的每个角(x，y)被表示为A，B，C，D，并且我们所讨论的点位于(x，y)点P，那么公式可以被表示为</p><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mp"><img src="../Images/7c10f2910cb2119835e88b699968e45e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rMnR3BjTVYUkIzFfCtlqnA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">AP denotes the vector from point A to point P. The · sign denotes dot multiplication between the vectors.</figcaption></figure><p id="d90d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果这两个条件都满足，则该点位于矩形内，否则不满足。</p><h2 id="2c49" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">没有广播的实现</h2><p id="23a5" class="pw-post-body-paragraph kv kw jg kx b ky mq kh la lb mr kk ld le ms lg lh li mt lk ll lm mu lo lp lq ij bi translated">如果我们以“标准”的方式实现算法，我们会得到这样的结果:</p><pre class="ml mm mn mo gt mv mw mx my aw mz bi"><span id="8ff9" class="lr ls jg mw b gy na nb l nc nd">def in_boxes(boxes, points):<br/>  # (N, 4, 2) = boxes.shape<br/>  # (M, 2) = points.shape</span><span id="6902" class="lr ls jg mw b gy ne nb l nc nd">  w = np.zeros(points.shape[0])<br/>  for (i, point) in enumerate(points):<br/>   in_box = False<br/>   for box in boxes:<br/>     (A, B, C, D) = box<br/>     AP = (point — A)<br/>     AB = (B — A)<br/>     AD = (D — A)<br/>     cond0 = 0 &lt; np.dot(AP, AB) &lt; np.dot(AB, AB)<br/>     cond1 = 0 &lt; np.dot(AP, AD) &lt; np.dot(AD, AD)<br/>     in_box = in_box or (cond0 and cond1)<br/>   if in_box:<br/>     w[i] = 1<br/>   else:<br/>     w[i] = 0<br/>  return w</span></pre><p id="edb6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">输入参数对应于出现的框和我们想要检查的点。盒子由形状为<code class="fe nf ng nh mw b">(N,4,2)</code>的数组表示，其中<code class="fe nf ng nh mw b">N </code>对应于盒子的数量，<code class="fe nf ng nh mw b">4</code>反映了我们有角(A，B，C，D)，而<code class="fe nf ng nh mw b">2</code>反映了每个角的xy位置。我们想要研究的点由形状为<code class="fe nf ng nh mw b">(M, 2)</code>的数组表示，其中<code class="fe nf ng nh mw b">M</code>对应于点的数量，<code class="fe nf ng nh mw b">2</code>反映每个点的xy位置。</p><p id="4037" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我的MBP 2014上用3个盒子和1，000，000个积分运行这个功能大约需要<strong class="kx jh"> 21.47秒。</strong>让我们看看使用Numpy广播重写函数时会发生什么。</p><h2 id="fa08" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">广播，我怎么用？</h2><p id="be58" class="pw-post-body-paragraph kv kw jg kx b ky mq kh la lb mr kk ld le ms lg lh li mt lk ll lm mu lo lp lq ij bi translated">在我们继续实现之前，让我们快速讨论一下如何使用广播。判断一个点是否在矩形中的算法的核心部分是计算向量<code class="fe nf ng nh mw b">AP</code>，即每个点和每个坐标的<code class="fe nf ng nh mw b">P-A</code>。所有的A坐标可以存储在一个形状为<code class="fe nf ng nh mw b">(N, 2)</code>的数组<code class="fe nf ng nh mw b">A</code>中。类似地，我们有<code class="fe nf ng nh mw b">M</code>点，这些点存储在形状为<code class="fe nf ng nh mw b">(M, 2)</code>的数组<code class="fe nf ng nh mw b">P</code>中。现在，如果我们以下面的方式给数组增加一个额外的维度，我们将增加秩(维度的数量),数组将有新的形状</p><pre class="ml mm mn mo gt mv mw mx my aw mz bi"><span id="717f" class="lr ls jg mw b gy na nb l nc nd">&gt;&gt;&gt; A.shape<br/>(M, 2)<br/>&gt;&gt;&gt; P.shape<br/>(N, 2)<br/>&gt;&gt;&gt; A = A[:, None, :]<br/>&gt;&gt;&gt; P = P[None, ...]<br/>&gt;&gt;&gt; A.shape<br/>(M, 1, 2)<br/>&gt;&gt;&gt; P.shape<br/>(1, N, 2)</span></pre><p id="d81c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正是在这个阶段，广播开始发挥作用。如果我们现在选择计算两个数组之间的差，我们将得到一个形状为<code class="fe nf ng nh mw b">(M, N, 2)</code>的数组</p><pre class="ml mm mn mo gt mv mw mx my aw mz bi"><span id="ecd8" class="lr ls jg mw b gy na nb l nc nd">&gt;&gt;&gt; (P - A).shape<br/>(M, N, 2)</span></pre><p id="c633" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，Numpy正在<em class="ni">广播</em>大小为1的每个维度中的数据，以便匹配两个数组的大小。这意味着Numpy本质上在第一维度上重复了N次<code class="fe nf ng nh mw b">A</code>数组，在第0维度上重复了M次<code class="fe nf ng nh mw b">P</code>数组。这种数据重复是在底层C实现中执行的，并且利用了指针，这意味着不会为了执行重复而复制数据(也就是说，我们不会因为必须复制数据而耗尽空间)。</p><p id="356f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这基本上展示了如何使用广播。通过向我们的数组添加额外的1维，并确保它们以相同的秩结束，我们可以执行任何类型的算术运算，并以我们想要的方式结束结果排序，而不必使用for循环。至此，让我们看看如何使用广播实现<code class="fe nf ng nh mw b">in_boxes</code>算法</p><h2 id="b983" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">广播的实现</h2><p id="effc" class="pw-post-body-paragraph kv kw jg kx b ky mq kh la lb mr kk ld le ms lg lh li mt lk ll lm mu lo lp lq ij bi translated">使用广播实现算法非常简单。为了更容易理解发生了什么，我在每一行都添加了注释，描述每个操作的结果形状。由于Numpy的<code class="fe nf ng nh mw b">np.dot()</code>方法不支持广播，我将数组逐元素相乘(因为它支持广播)，然后在最后一个维度上求和，因为这个操作对应于点积。</p><pre class="ml mm mn mo gt mv mw mx my aw mz bi"><span id="7694" class="lr ls jg mw b gy na nb l nc nd">def in_boxes_bc(boxes, points):<br/>  # (N, 4, 2) = boxes.shape<br/>  # (M, 2) = points.shape</span><span id="e900" class="lr ls jg mw b gy ne nb l nc nd">  (A, B, C, D) = np.split(boxes, 4, axis=1)  # (N, 1, 2)<br/>  AM = (points[None, ...] - A)  # (N, M, 2)<br/>  AB = (B - A)  # (N, 1, 2)<br/>  AD = (D - A)  # (N, 1, 2) <br/>    <br/>  AM_AB = np.sum(AM * AB, axis=-1)  # (N, M)<br/>  AB_AB = np.sum(AB * AB, axis=-1)  # (N, 1)<br/>  AM_AD = np.sum(AM * AD, axis=-1)  # (N, M)<br/>  AD_AD = np.sum(AD * AD, axis=-1)  # (N, 1)<br/>    <br/>  cond0 = (0 &lt; AM_AB) &amp; (AM_AB &lt; AB_AB)  # (N, M)<br/>  cond1 = (0 &lt; AM_AD) &amp; (AM_AD &lt; AD_AD)  # (N, M)<br/>    <br/>  in_box = cond0 &amp; cond1  # (N, M) = in_box.shape<br/>  w = np.any(in_box, axis=0)<br/>  return w</span></pre><p id="23bd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我的MBP 2014上用3个盒子和1，000，000个点运行这个版本的算法大约需要<strong class="kx jh"> 0.36秒。</strong>与之前花费的时间21.47秒相比，这意味着我们实现了<strong class="kx jh">到60倍</strong>的速度提升！</p><h2 id="629d" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">使用PyTorch在GPU上运行</h2><p id="f2e6" class="pw-post-body-paragraph kv kw jg kx b ky mq kh la lb mr kk ld le ms lg lh li mt lk ll lm mu lo lp lq ij bi translated">该算法的广播变体的真正巧妙之处在于，它非常容易移植到PyTorch，这允许我们在GPU上运行相同的算法，从而可以进一步加快算法的速度！Numpy和PyTorch都以相同的方式实现广播，唯一的警告是PyTorch对一些数学函数及其参数有稍微不同的命名方案。(<code class="fe nf ng nh mw b">cuda()</code>函数用于将数据移动到GPU，而<code class="fe nf ng nh mw b">cpu()</code>用于将数据从GPU移回)。</p><pre class="ml mm mn mo gt mv mw mx my aw mz bi"><span id="6f72" class="lr ls jg mw b gy na nb l nc nd">def in_boxes_torch(boxes, points):<br/>  boxes = torch.DoubleTensor(boxes).cuda()<br/>  points = torch.DoubleTensor(points).cuda()<br/>    <br/>  dd = torch.chunk(boxes, 4, dim=1)<br/>  (A, B, C, D) = dd</span><span id="06d5" class="lr ls jg mw b gy ne nb l nc nd">  AM = (points[None, ...] - A)<br/>  AB = (B - A)<br/>  AD = (D - A)</span><span id="b3a9" class="lr ls jg mw b gy ne nb l nc nd">  AM_AB = torch.sum(AM * AB, dim=-1)<br/>  AB_AB = torch.sum(AB * AB, dim=-1)<br/>  AM_AD = torch.sum(AM * AD, dim=-1)<br/>  AD_AD = torch.sum(AD * AD, dim=-1)</span><span id="5f0c" class="lr ls jg mw b gy ne nb l nc nd">  cond0 = (0 &lt; AM_AB) &amp; (AM_AB &lt; AB_AB)<br/>  cond1 = (0 &lt; AM_AD) &amp; (AM_AD &lt; AD_AD)<br/>    <br/>  in_box = cond0 &amp; cond1<br/>  # PyTorch does not have anything corresponding to np.any()<br/>  w = (torch.sum(in_box, dim=0) &gt; 0)<br/>  return w.cpu()</span></pre><p id="67dd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用与之前相同的场景(3个盒子和1，000，000个点)，但这次在GTX 980 Ti上运行大约需要<strong class="kx jh"> 0.026秒* </strong>，与使用广播的版本相比速度增加了<strong class="kx jh"> ~14x </strong>，与算法的第一个版本相比速度增加了<strong class="kx jh"> ~826x </strong>！</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><p id="90e1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">广播可以用来使某些类型的算法运行得更快，这很酷(尽管应该注意到<a class="ae jd" href="http://scipy.github.io/old-wiki/pages/EricsBroadcastingDoc" rel="noopener ugc nofollow" target="_blank">情况并不总是这样</a>，所以在开始使用广播进行任何类型的计算之前要小心)。能够快速移植代码在GPU上运行的额外好处是非常惊人的，特别是因为ML/DL领域的大多数人通常都可以使用非常强大的GPU。速度上的差异很重要，因为这可能是等待几分钟算法完成与等待几小时(甚至几天)算法完成之间的差异！).</p><p id="b87b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">非常感谢<a class="ae jd" href="https://www.linkedin.com/in/adam-fjeldsted-84779a86/" rel="noopener ugc nofollow" target="_blank"> Adam Fjeldsted </a>校对这篇文章！</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><p id="ee16" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="ni">关于在GPU上运行代码，有两点值得注意:</em></p><ol class=""><li id="8a0f" class="nq nr jg kx b ky kz lb lc le ns li nt lm nu lq nv nw nx ny bi translated"><em class="ni">功能</em> <code class="fe nf ng nh mw b"><em class="ni">in_boxes_torch</em></code> <em class="ni">包括将数据移动到GPU </em></li><li id="f19a" class="nq nr jg kx b ky nz lb oa le ob li oc lm od lq nv nw nx ny bi translated"><em class="ni">第一次使用PyTorch将数据移动到GPU比后续的移动花费更多的时间，因为建立到GPU的连接会有一些开销(这种开销与您试图移动到GPU的数据大小无关，在我的机器上大约是2秒)。因此，在运行和计时</em> <code class="fe nf ng nh mw b"><em class="ni">in_boxes_torch</em></code> <em class="ni">函数之前，我先将一些虚拟数据移入和移出GPU。</em></li></ol></div></div>    
</body>
</html>