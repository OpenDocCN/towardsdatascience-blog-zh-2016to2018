<html>
<head>
<title>[ Google DeepMind ] — Deep Learning for Medical Image Segmentation with Interactive Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[ Google DeepMind ] —利用交互式代码进行医学图像分割的深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/google-deepmind-deep-learning-for-medical-image-segmentation-with-interactive-code-4634b6fd6a3a?source=collection_archive---------5-----------------------#2018-04-08">https://towardsdatascience.com/google-deepmind-deep-learning-for-medical-image-segmentation-with-interactive-code-4634b6fd6a3a?source=collection_archive---------5-----------------------#2018-04-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/2879f5afc8e949ed4d04c071dfb847a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*QGZ2Idl45SBGw9Rxsh_yTQ.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Gif from this <a class="ae jy" href="https://giphy.com/gifs/ge-dogs-brain-scans-ge8rBpX40ZjR6/download" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="c0ec" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><a class="ae jy" href="https://matthewlai.ca/blog/" rel="noopener ugc nofollow" target="_blank"> Matthew Lai </a>是Deep Mind的研究工程师，也是“<a class="ae jy" href="https://arxiv.org/abs/1509.01549" rel="noopener ugc nofollow" target="_blank"> <em class="kx">长颈鹿，利用深度强化学习下棋</em> </a>的创造者。但是他的硕士Msc项目是在MRI图像上，也就是“<a class="ae jy" href="https://arxiv.org/pdf/1505.02000.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="kx">医学图像分割的深度学习</em> </a>”，所以我想深入看看他的项目。</p><p id="6d2a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">因为这不是一篇传统的会议论文，而是一个主要项目，我会用不同的方式来处理这个问题。我会做一个论文总结(对我不知道的东西做笔记)和实施。</p><p id="7e62" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">请注意，在原始论文中，Matthew使用了ADNI老年痴呆症MRI数据集，不幸的是，我无法获得这些数据，所以我将使用“<a class="ae jy" href="https://www.isi.uu.nl/Research/Databases/DRIVE/" rel="noopener ugc nofollow" target="_blank"> <em class="kx">驱动:用于血管提取的数字视网膜图像</em> </a>”数据集。还请注意，由于使用了不同的数据集，以及硬件限制，网络架构与原始论文的差异很小，但是我试图保持总体结构相似。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><figure class="lf lg lh li gt jr"><div class="bz fp l di"><div class="lj lk l"/></div></figure></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="604b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> 2.2网络节点和激活功能</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ll"><img src="../Images/3d4ff6a02730aa57ffbd0aa79999e964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4s3yh3C5c_RpzCogurymsA.png"/></div></div></figure><p id="c180" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这里我了解到激活功能必须满足三个标准。(我只认识其中两个。)并且它们是差分的、非线性的和单调的。(我不知道单调。)</p><p id="70be" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">微分</strong> →执行反向传播<br/> <strong class="kb ir">非线性</strong> →赋予模型计算非线性函数的能力<br/> <strong class="kb ir">单调</strong> →防止产生更多的局部最小值</p><div class="lf lg lh li gt ab cb"><figure class="lq jr lr ls lt lu lv paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/c724f84264785625023c732686998222.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*CkBRPSwZmvvbKm3dU5_yig.png"/></div></figure><figure class="lq jr lr ls lt lu lv paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/9b33c050d07978f2e374b545c882c1e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*xvPlzx1Zcq0lT4iY_sD7ng.png"/></div></figure><figure class="lq jr lr ls lt lu lv paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/c642350d5271a7d363672b60dcfa3046.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*ubmTp9yn7UvBd-8GaJGxyQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk lw di lx ly">Image from <a class="ae jy" href="https://en.wikipedia.org/wiki/Monotonic_function" rel="noopener ugc nofollow" target="_blank">Wiki</a></figcaption></figure></div><p id="7de3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →单调递增的函数<br/> <strong class="kb ir">中图</strong> →单调递减的函数<br/> <strong class="kb ir">右图</strong> →非单调的函数</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="28bf" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> 2.3训练神经网络</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lz"><img src="../Images/b19313b74912509e106173d172773069.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o3rqV7KX6ljNMn1vLTDp9w.png"/></div></div></figure><p id="3875" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这里，我了解到Rprop反向传播的存在，这种反向传播方法只考虑偏导数的符号，而不考虑大小。如果您想了解更多信息，请<a class="ae jy" href="https://en.wikipedia.org/wiki/Rprop" rel="noopener ugc nofollow" target="_blank">点击此处</a>。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="3f46" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> 3.1为什么要建立深度网络？</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ma"><img src="../Images/354bf7a47b4915462da9b0f05d1aa34b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZVohbD3LWs6UmBZWDB-PPg.png"/></div></div></figure><p id="765a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这里，我了解到我们需要使用深度神经网络的确切原因。我学到了几件事…</p><ol class=""><li id="f046" class="mb mc iq kb b kc kd kg kh kk md ko me ks mf kw mg mh mi mj bi translated">从理论上讲，用两层模型来模拟任何功能都是可能的，这意味着我们真的不需要一个比两层更深的模型。</li><li id="c3ad" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">但是使用深度神经网络的好处其实是节点效率。当我们有一个更小但更深的神经网络时，用更高的精度逼近复杂函数是可能的。</li></ol><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mp"><img src="../Images/2b75ab7712ebc31d4950dd1b6d833e33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bh8aCGKgkR5dhp6Pr5c2wA.png"/></div></div></figure><p id="8370" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">本质上，更小更深的神经网络更有效，因为(每个节点)完成的冗余工作量减少了。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="03d7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> 3.2消失渐变</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mq"><img src="../Images/7f47fcc84881996361f96e24b9d83f8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wQmS0N1LMEn38nGXsWaZ5A.png"/></div></div></figure><p id="1c94" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这里，我学到了三种解决消失梯度问题的方法。我已经知道了辍学和ReLU，但我从来不知道ReLU激活功能是用来克服消失梯度。我也不知道分层预训练。</p><p id="237a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> <em class="kx"> 3.2.1方案一:分层预训练</em> </strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mr"><img src="../Images/4c1952681d214a4959bc690b4ec54e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ptd2HQP3_5_3s25EtMJn7w.png"/></div></div></figure><p id="8bec" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这个想法非常有趣，因为我们将首先以无人监督的方式训练每一层，然后一起使用它。</p><p id="3c31" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> <em class="kx"> 3.2.2解决方案2:整流线性激活单元</em> </strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ms"><img src="../Images/7818b38e3b314200d7c413d7a1a92210.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*heEJGwRQKnVij5m_Pe_FZw.png"/></div></div></figure><p id="6bfd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">虽然ReLU()激活可能在两个方面存在问题，因为</p><p id="63d6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">a.0处没有导数。<br/> b .正区域内没有边界。<br/>然而，它仍然可以使用，因为它们在进行反向传播时不会降低梯度(导数为1)。我不知道ReLU()激活层是用来克服渐变消失问题的。</p><p id="bdb7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> <em class="kx"> 3.3辍学</em> </strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mt"><img src="../Images/85c33428408dc9badc549018dd2171e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pFoPxuCDwu7w6TlkEyvr7w.png"/></div></div></figure><p id="6163" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">通过丢弃某些节点的值，我们可以使每个节点独立地从另一个节点进化，这使得网络更加健壮。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="aba7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> 3.5使深网变浅</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mu"><img src="../Images/dce4f27606677380b422d0192115a24a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kvi5SYYYU7hWO2i6xFaoPw.png"/></div></div></figure><p id="1910" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这里，我学到了一种提高网络性能的方法，这种方法非常有趣。来自论文“<a class="ae jy" href="https://arxiv.org/abs/1312.6184" rel="noopener ugc nofollow" target="_blank"> <em class="kx">深网真的需要深吗？</em> </a>“我们可以看到一个深度NN被训练来指导一个较浅网络的情况。并且这种引导的浅层网络比用给定数据直接训练的浅层网络表现得更好。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mv"><img src="../Images/60880b7985344be4f02d5eb5cf8941e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jx9FcGBs1OL3HUEZKF3MKw.png"/></div></div></figure></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="54fd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">网络架构(描述形式)</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ma"><img src="../Images/d8293e2fbe09fcc41c5a578954df8bb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WbqpkbQoZEjcyFC8bkbUnA.png"/></div></div></figure><p id="8a15" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红框</strong> →网络架构描述</p><p id="f5d5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">因为其他两个网络与这个非常相似，所以我们只实现第一个。我从未真正使用全连接网络进行过分段，但我认为这会非常有趣。此外，请注意，由于使用不同的数据集，我将增加一个层，只是为了适应我们的数据。然而，网络的一般结构是相似的。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="3435" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">网络架构(图形形式)</strong></p><div class="lf lg lh li gt ab cb"><figure class="lq jr mw ls lt lu lv paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/41926ad187ed9d701e9a2dafb52a71d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*DOwRMDBkel8i2aGnNq0sLg.png"/></div></figure><figure class="lq jr mx ls lt lu lv paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/dbc5aaf2ae192423d804ad96842d7d54.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*k1iLO0Tq1glF12GIxF0s6g.png"/></div></figure></div><p id="5518" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">黑色矩形</strong> →输入图像<br/> <strong class="kb ir">蓝色矩形</strong> →卷积层<br/> <strong class="kb ir">红色矩形</strong> →全连通层(输入图像矢量化)<br/> <strong class="kb ir">橙色矩形</strong> →软max层</p><p id="bc3a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">网络本身很简单，现在让我们来看看OOP的形式。此外，为了克服消失梯度，让我们使用ReLU()激活功能，并tanh()激活在完全连接的层。</p><blockquote class="my mz na"><p id="e76e" class="jz ka kx kb b kc kd ke kf kg kh ki kj nb kl km kn nc kp kq kr nd kt ku kv kw ij bi translated">**注意**由于硬件限制，我不得不将两个卷积层的滤波器大小减半，以及批量大小(2)。</p></blockquote><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ne"><img src="../Images/cac049a6d3d55f0c51d2e2eecff6ab0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1K_O_IT44VmSzTQR_TWS5Q.png"/></div></div></figure><p id="f764" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">红线</strong> →添加图层</p><p id="ce24" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">整个网络(由于其深度)可以在一个屏幕截图中查看。最后，我们将使用随机梯度下降优化器。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="6c92" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">结果</strong></p><div class="lf lg lh li gt ab cb"><figure class="lq jr lr ls lt lu lv paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/78fdc0eb31bd0dd6e63ad79a87fba501.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*2nsVgtG2nDLo5h4sHRb38w.png"/></div></figure><figure class="lq jr lr ls lt lu lv paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/b27af5bca0b376c89f5dde631fe86ef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*nkjqr9MFNVXYuf8F7F4yQw.png"/></div></figure><figure class="lq jr lr ls lt lu lv paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/706d6720a2c90ea2ab7376a46e68cfca.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*O7nLmVMW2xkvYnldYbsPZg.png"/></div></figure></div><div class="ab cb"><figure class="lq jr lr ls lt lu lv paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/ccbab6104a0691b51a55ef2fffb3428c.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*MqIykAGv3UfOlwH4RvoHoQ.png"/></div></figure><figure class="lq jr lr ls lt lu lv paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/d5ec32412d5393804c784a02f6af6390.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*859esLzkK9gV9o80KCHm4w.png"/></div></figure><figure class="lq jr lr ls lt lu lv paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/3550d975b829cce2613b52cf36c865ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*dooNJzJhSDAtnkFGC1arkQ.png"/></div></figure></div><p id="b57e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">右图</strong> →原图<br/> <strong class="kb ir">中图</strong> →二值掩码<br/> <strong class="kb ir">地面真相左图</strong> →从网络生成二值掩码</p><p id="52ef" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">网络给出的结果很差，但是我相信这是由于我使用了不同的数据集。我没有ADNI老年痴呆症的核磁共振数据集，但我相信网络的最终输出不是图像的矢量化版本。可能是海马体所在位置的坐标等等…</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="0769" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">GIF格式的结果</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/f6166a80313781da91b6db88603320f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/1*gG6TyIfmAMZ7UMlFP4f1bA.gif"/></div></figure><p id="4b92" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">显示图像的顺序</strong> → 1。原图→ 2。地面真实二进制掩码→ 3。生成的二进制掩码→ 4。原始图像上的地面真实遮罩叠加→ 5。原始图像上生成的蒙版覆盖。</p><p id="3d51" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，随着训练的继续，我们可以看到所生成的滤波器变得更加清晰，但是所生成的掩模不会从一幅图像改变到另一幅图像，并且在正确分割图像方面表现不佳。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="1b03" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">互动代码/透明度</strong></p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ng"><img src="../Images/7aa4b3e366ab484d9a5b5dd7c312420c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pG176thMsISEZPxF0CnmGg.png"/></div></div></figure><p id="60aa" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于Google Colab，你需要一个Google帐户来查看代码，而且你不能在Google Colab中运行只读脚本，所以在你的操场上复制一份。最后，我永远不会请求允许访问你在Google Drive上的文件，仅供参考。编码快乐！</p><p id="f79d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要访问Google Colab上的代码，<a class="ae jy" href="https://colab.research.google.com/drive/1ilCOiqlyfpj6eEa8EfR1XB5sV0m9JnXF" rel="noopener ugc nofollow" target="_blank">请点击这里。</a></p><ul class=""><li id="27b9" class="mb mc iq kb b kc kd kg kh kk md ko me ks mf kw nh mh mi mj bi translated">*注**:我不想在github上存放私人医疗数据，因为我可能会违反他们的数据使用政策。所以这段代码不能直接在线运行。</li><li id="f32a" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw nh mh mi mj bi translated">为了让这个实验更加透明，我上传了我所有的命令输出到我的Github，如果你想看<a class="ae jy" href="https://github.com/JaeDukSeo/Only_Numpy_Basic/blob/master/U-net/math.txt" rel="noopener ugc nofollow" target="_blank">，请点击这里。</a></li></ul></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="da63" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">最后的话</strong></p><p id="0665" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这篇硕士论文在英语和内容方面都写得非常好，我希望在我做硕士论文时能写出类似的内容。</p><p id="8ee7" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到jae.duk.seo@gmail.com给我，如果你想看我所有写作的列表，请点击这里查看我的网站。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的推特<a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>关注我，访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube频道</a>了解更多内容。如果你感兴趣，我还在这里做了解耦神经网络<a class="ae jy" href="https://becominghuman.ai/only-numpy-implementing-and-comparing-combination-of-google-brains-decoupled-neural-interfaces-6712e758c1af" rel="noopener ugc nofollow" target="_blank">的比较。</a></p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="6ae3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="8643" class="mb mc iq kb b kc kd kg kh kk md ko me ks mf kw mg mh mi mj bi translated">赖，男(2015)。长颈鹿:利用深度强化学习下棋。Arxiv.org。检索于2018年4月8日，来自<a class="ae jy" href="https://arxiv.org/abs/1509.01549" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1509.01549</a></li><li id="f399" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">赖，男(2015)。用于医学图像分割的深度学习。Arxiv.org。检索于2018年4月8日，来自<a class="ae jy" href="https://arxiv.org/abs/1505.02000" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1505.02000</a></li><li id="9f98" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">单调函数。(2018).En.wikipedia.org。检索于2018年4月8日，来自<a class="ae jy" href="https://en.wikipedia.org/wiki/Monotonic_function" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Monotonic_function</a></li><li id="3f28" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">Rprop。(2018).En.wikipedia.org。检索于2018年4月8日，来自<a class="ae jy" href="https://en.wikipedia.org/wiki/Rprop" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Rprop</a></li><li id="2cc0" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">驱动:用于血管提取的数字视网膜图像。(2018).isi . uu . nl . 2018年4月8日检索，来自<a class="ae jy" href="https://www.isi.uu.nl/Research/Databases/DRIVE/" rel="noopener ugc nofollow" target="_blank">https://www.isi.uu.nl/Research/Databases/DRIVE/</a></li><li id="567f" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">ADNI |阿尔茨海默病神经影像倡议。(2018).Adni.loni.usc.edu。检索于2018年4月8日，来自<a class="ae jy" href="http://adni.loni.usc.edu/" rel="noopener ugc nofollow" target="_blank">http://adni.loni.usc.edu/</a></li><li id="255f" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">导数双曲线。(2018).Math.com。2018年4月8日检索，来自<a class="ae jy" href="http://www.math.com/tables/derivatives/more/hyperbolics.htm" rel="noopener ugc nofollow" target="_blank">http://www.math.com/tables/derivatives/more/hyperbolics.htm</a></li><li id="aae3" class="mb mc iq kb b kc mk kg ml kk mm ko mn ks mo kw mg mh mi mj bi translated">j . brown lee(2017年)。深度学习的Adam优化算法的温和介绍-机器学习掌握。机器学习精通。检索于2018年4月8日，来自<a class="ae jy" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/Adam-optimization-algorithm-for-deep-learning/</a></li></ol></div></div>    
</body>
</html>