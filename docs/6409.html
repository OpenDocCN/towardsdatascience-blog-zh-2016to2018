<html>
<head>
<title>OpenAI Gym from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始开放健身房</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/openai-gym-from-scratch-619e39af121f?source=collection_archive---------6-----------------------#2018-12-12">https://towardsdatascience.com/openai-gym-from-scratch-619e39af121f?source=collection_archive---------6-----------------------#2018-12-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1c32" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">从环境开发到训练有素的网络。</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/bc1a35803b6a34ddbf09579538e2ade9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eo0g3I--K_xDLd4i"/></div></div></figure><p id="b715" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">有很多工作和教程解释了如何使用 OpenAI Gym toolkit，以及如何使用 Keras 和 TensorFlow 来训练使用一些现有 OpenAI Gym 结构的现有环境。然而在本教程中，我将解释如何从头开始创建一个 OpenAI 环境，并在其上训练一个代理。</p><p id="9333" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我还将解释如何创建一个模拟器来开发环境。该教程分为 4 个部分:问题陈述，模拟器，健身房环境和培训。</p><h1 id="0851" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">1 —问题陈述</h1><p id="95f8" class="pw-post-body-paragraph ks kt iq ku b kv mg jr kx ky mh ju la lb mi ld le lf mj lh li lj mk ll lm ln ij bi translated">这里提出的问题是基于我最后的毕业设计。目标是创造一个人工智能代理来控制船只在整个航道的航行。</p><p id="d976" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">如今，在受限水域(如航道和港口)中的航行基本上是基于飞行员对环境条件(如给定位置的风和水流)的了解。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ml"><img src="../Images/774efa338bf81d571ccbc44c2c59a636.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VRxUiYtJncHWnPWr.jpg"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Figure1: Ship accident</figcaption></figure><p id="9968" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">让我们说，人类仍然在犯错误，有时会花费数十亿美元，人工智能是一种可能的选择，可以应用于导航，以减少事故的数量。</p><h1 id="4022" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">2 —模拟器</h1><p id="3b0f" class="pw-post-body-paragraph ks kt iq ku b kv mg jr kx ky mh ju la lb mi ld le lf mj lh li lj mk ll lm ln ij bi translated">为了创造一个人工智能代理来控制一艘船，我们需要一个人工智能代理可以执行导航体验的环境，并通过自己的错误来学习如何正确地在整个航道中导航。此外，因为我们不能使用真实的船只来训练 AI 智能体，所以最好的替代方法是使用模拟器来模拟真实世界船只的动力学行为。为此，我们可以使用现有的商业软件(付费选项)，<strong class="ku ir">，但</strong>在本教程中，我们将创建<strong class="ku ir">我们自己的船舶模拟器。</strong></p><p id="a51e" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">为了做到这一点，一些假设被采用，如:船舶是一个刚体，唯一的外力驱动船舶是水阻力(没有风，没有水流)，此外，推进和舵控制力被用来控制方向和速度的船舶。控制船舶动力学的完整方程很复杂，可以在参考文献[1]中找到。这里我们将使用一个非常简单的 3DOF 模型，如下所示:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mq"><img src="../Images/3518bf560236d67e23046300770dbdb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5czPYa7e22kHqotsp7jk2Q.png"/></div></div></figure><p id="22b2" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在该图中，<strong class="ku ir"> u </strong>是船相对于固定在船 CG 上的框架的纵向速度，<strong class="ku ir"> v </strong>是吃水速度，<strong class="ku ir"> dψ/dt </strong>是相对于固定参照物的角速度，<strong class="ku ir"> ψ </strong>是相对于固定框架 OXY 测量的船的攻角。速度<strong class="ku ir"> U，V </strong>(固定框架)通过 2x2 旋转矩阵链接 T10<strong class="ku ir">U，v </strong>。φ是相对于如图所示的移动框架测得的舵角。</p><p id="3bd2" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">阻力和推进力的公式超出了本教程的范围，但是，总而言之，阻力与船的运动方向相反，与船的速度成正比。方向舵和推进力与[1，1]中的参数<strong class="ku ir"> Al 和</strong> [0，1]中的参数<strong class="ku ir"> Ap 成比例。这些参数与舵角和推进力(Tp)成正比。注意，Al 和 Ap 是可控参数，因此:</strong></p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mr"><img src="../Images/eb3dafd36b053c3b121d2577eb23fce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bI2-t9-bO8kR4sV8Mw_MnQ.png"/></div></div></figure><p id="7650" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">现在我们有了模型微分方程，我们可以使用一个积分器来建立我们的模拟器。让我们写下我们的模拟器。</p><ul class=""><li id="dbcc" class="ms mt iq ku b kv kw ky kz lb mu lf mv lj mw ln mx my mz na bi translated"><strong class="ku ir">第一步:模拟器方程式</strong>。写出上面的等式，分离出加速度项，我们得到:</li></ul><pre class="kh ki kj kk gt nb nc nd ne aw nf bi"><span id="21fb" class="ng lp iq nc b gy nh ni l nj nk">def simulate_scipy(self, t, global_states):<br/>    local_states = self._global_to_local(global_states)<br/>    return self._local_ds_global_ds(global_states[2], self.simulate(local_states))<br/><br/>        def simulate(self, local_states):<br/>        <em class="nl">"""<br/>        :param local_states: Space state<br/>        :return df_local_states<br/>        """<br/>        </em>x1 = local_states[0]  # u<br/>        x2 = local_states[1]  # v<br/>        x3 = local_states[2]  # theta (not used)<br/>        x4 = local_states[3]  # du<br/>        x5 = local_states[4]  # dv<br/>        x6 = local_states[5]  # dtheta<br/><br/>        Frx, Fry, Frz = self.compute_rest_forces(local_states)<br/><br/>        # Propulsion model<br/>        Fpx, Fpy, Fpz = self.compute_prop_forces(local_states)<br/><br/>        # Derivative function<br/><br/>        fx1 = x4<br/>        fx2 = x5<br/>        fx3 = x6<br/><br/>        # main model simple<br/>        fx4 = (Frx + Fpx) / (self.M)<br/>        fx5 = (Fry + Fpy) / (self.M)<br/>        fx6 = (Frz + Fpz) / (self.Iz)<br/><br/>    fx = np.array([fx1, fx2, fx3, fx4, fx5, fx6])<br/>    return fx</span></pre><p id="1d6f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在前 6 行中，我们只定义了变量名从<strong class="ku ir"> x1 </strong>到<strong class="ku ir"> x6 </strong>，<strong class="ku ir"> beta </strong>和<strong class="ku ir"> alpha </strong>是用于控制方向舵和推进控制的控制常数，在我们计算阻力并最终分离出导数项<strong class="ku ir"> fx1，fx2 …，fx6 </strong>后，我们得出:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mq"><img src="../Images/9bfa7d42612390b828bfb643bcb46a21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BFBrsaTZoR5aR2Do6gC9rQ.png"/></div></div></figure><ul class=""><li id="5d85" class="ms mt iq ku b kv kw ky kz lb mu lf mv lj mw ln mx my mz na bi translated"><strong class="ku ir">第二步:积分器。</strong>作为模拟的积分器，我们使用 scipy 库的 5 阶龙格-库塔积分器。</li></ul><pre class="kh ki kj kk gt nb nc nd ne aw nf bi"><span id="1fa6" class="ng lp iq nc b gy nh ni l nj nk">from scipy.integrate import RK45</span></pre><p id="e121" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们定义一个函数，它使用 scipy RK45 来整合一个函数<strong class="ku ir"> fun </strong>使用一个起点<strong class="ku ir"> y0 </strong>。从<strong class="ku ir"> t0 </strong>到<strong class="ku ir"> t_bound </strong>进行积分，相对公差<strong class="ku ir"> rtol </strong>和绝对公差<strong class="ku ir"> atol </strong>。</p><pre class="kh ki kj kk gt nb nc nd ne aw nf bi"><span id="b9f8" class="ng lp iq nc b gy nh ni l nj nk">def scipy_runge_kutta(self, fun, y0, t0=0, t_bound=10):<br/>    return RK45(fun, t0, y0, t_bound,                rtol=self.time_span/self.number_iterations, atol=1e-4)</span></pre><p id="8415" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">因为我们使用全局参考(OXY)来定位船只，使用局部参考来积分方程(oxyz)，所以我们定义了一个“掩码”函数用于积分器。该功能主要将函数<em class="nl">模拟</em>中输出的差分向量转换为全局参考。</p><pre class="kh ki kj kk gt nb nc nd ne aw nf bi"><span id="2832" class="ng lp iq nc b gy nh ni l nj nk">def simulate_scipy(self, t, global_states):<br/>    local_states = self._global_to_local(global_states)<br/>    return self._local_ds_global_ds(global_states[2], self.simulate(local_states))</span></pre><ul class=""><li id="6598" class="ms mt iq ku b kv kw ky kz lb mu lf mv lj mw ln mx my mz na bi translated"><strong class="ku ir"> Step3:步进功能</strong></li></ul><p id="9b37" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">每走一步，我们都要经过一个方向舵(<strong class="ku ir"> angle_level) </strong>和一个旋转杠杆(<strong class="ku ir"> rot_level </strong>)来控制推进装置传递的推力。</p><pre class="kh ki kj kk gt nb nc nd ne aw nf bi"><span id="6d4d" class="ng lp iq nc b gy nh ni l nj nk">def step(self, angle_level, rot_level):<br/>    self.current_action = np.array([angle_level, rot_level])<br/>    while not (self.integrator.status == 'finished'):<br/>        self.integrator.step()<br/>    self.last_global_state = self.integrator.y<br/>    self.last_local_state = self._global_to_local(self.last_global_state)<br/>    self.integrator = self.scipy_runge_kutta(self.simulate_scipy, self.get_state(), t0=self.integrator.t, t_bound=self.integrator.t+self.time_span)<br/>    return self.last_global_state</span></pre><p id="698c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在第一行中，我们存储当前动作向量，在第二行中，我们使用 RK45<em class="nl">self . integrator . step()</em>进行积分，直到它达到最终时间跨度。最后，我们通过<strong class="ku ir"> self.integrator </strong>更新<strong class="ku ir"> self.last_global_state、self.last_local_state 和</strong>积分间隔。最后，我们返回全局状态。</p><ul class=""><li id="b5e0" class="ms mt iq ku b kv kw ky kz lb mu lf mv lj mw ln mx my mz na bi translated"><strong class="ku ir">第四步:复位功能。</strong>重置功能用于在每次新的迭代中设置模拟的初始条件，如初始位置和速度。它使用一个全局变量并更新<strong class="ku ir"> self.last_global_state，self.last_local_state。</strong></li></ul><pre class="kh ki kj kk gt nb nc nd ne aw nf bi"><span id="87ee" class="ng lp iq nc b gy nh ni l nj nk"><strong class="nc ir">def reset_start_pos(self, global_vector)</strong>:<br/>    x0, y0, theta0, vx0, vy0, theta_dot0 = global_vector[0], global_vector[1], global_vector[2], global_vector[3], global_vector[4], global_vector[5]<br/>    self.last_global_state = np.array([x0, y0, theta0, vx0, vy0, theta_dot0])<br/>    self.last_local_state = self._global_to_local(self.last_global_state)<br/>    self.current_action = np.zeros(2)<br/>    self.integrator = self.scipy_runge_kutta(self.simulate_scipy, self.get_state(), t_bound=self.time_span)</span></pre><p id="94cf" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">最后的代码可以在这里查看<a class="ae nm" href="https://github.com/jmpf2018/SimpleShipAI/blob/master/simulator_simple.py" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="68b3" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">3 —健身房环境</h1><p id="0e7e" class="pw-post-body-paragraph ks kt iq ku b kv mg jr kx ky mh ju la lb mi ld le lf mj lh li lj mk ll lm ln ij bi translated">有了模拟器后，我们现在可以创建一个健身房环境来训练代理。</p><h2 id="ee1b" class="ng lp iq bd lq nn no dn lu np nq dp ly lb nr ns ma lf nt nu mc lj nv nw me nx bi translated">3.1 国家</h2><p id="a134" class="pw-post-body-paragraph ks kt iq ku b kv mg jr kx ky mh ju la lb mi ld le lf mj lh li lj mk ll lm ln ij bi translated">状态是代理可以“看到”世界的环境变量。代理使用这些变量来确定自己在环境中的位置，并决定采取什么行动来完成提议的任务。在我们的问题中，任务被表述为:</p><blockquote class="ny nz oa"><p id="9d82" class="ks kt nl ku b kv kw jr kx ky kz ju la ob lc ld le oc lg lh li od lk ll lm ln ij bi translated"><em class="iq">使用方向舵控制，在给定的恒定推进动作下，沿航道执行规定的线性导航路径。</em></p></blockquote><p id="db24" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">为在任务中应用 RL 而选择的状态如下:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/57c50ed307be3a419e2f27b34c8452e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/0*S6xKk06FFAslCr_W"/></div></figure><p id="76e5" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">式中<strong class="ku ir"> d </strong>为船舶质心到基准线的距离；<strong class="ku ir"> θ </strong>是船舶纵轴与基准线之间的角度；<strong class="ku ir"> vx </strong>是船在其质心的水平速度(在引导线方向；<strong class="ku ir"> vy </strong>是船在其质心的垂直速度(垂直于引导线)；<strong class="ku ir"> dθ/dt </strong>是船的角速度。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi of"><img src="../Images/168cf5c8f45fcf03e3a4dcfc03d8c988.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/0*LjpZaA_Wg_VbQwta"/></div></figure><h2 id="d0db" class="ng lp iq bd lq nn no dn lu np nq dp ly lb nr ns ma lf nt nu mc lj nv nw me nx bi translated">3.2 奖励</h2><p id="30f4" class="pw-post-body-paragraph ks kt iq ku b kv mg jr kx ky mh ju la lb mi ld le lf mj lh li lj mk ll lm ln ij bi translated">奖励函数负责惩罚没有遵循指导方针的代理，如果他没有太大的动摇就奖励他。定义的奖励函数是:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi og"><img src="../Images/1d3e95545d530efbcbd0b3d28c09b6c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/0*-qO2IDBj1ia9jX5Y"/></div></figure><p id="1d93" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">其中:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oh"><img src="../Images/890bd42937c91803d33b01ec8046158f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fheQrr4Oz4z9WbVk"/></div></div></figure><h2 id="71f8" class="ng lp iq bd lq nn no dn lu np nq dp ly lb nr ns ma lf nt nu mc lj nv nw me nx bi translated">3.3 行动</h2><p id="eaa4" class="pw-post-body-paragraph ks kt iq ku b kv mg jr kx ky mh ju la lb mi ld le lf mj lh li lj mk ll lm ln ij bi translated">这些动作是控制船舶操纵运动的输入参数。使船可控的力是方向舵和推进力。这些动作具有矢量形式<strong class="ku ir">Av =【Al，Ap】</strong>，其中<strong class="ku ir"> Al </strong>为无量纲方向舵指令，<strong class="ku ir"> Ap </strong>为无量纲推进指令，因此<strong class="ku ir"> Al 在[-1，1] </strong>中，<strong class="ku ir"> Ap 在[0，1] </strong>中。</p><h2 id="9f7b" class="ng lp iq bd lq nn no dn lu np nq dp ly lb nr ns ma lf nt nu mc lj nv nw me nx bi translated">3.4 代码</h2><p id="220a" class="pw-post-body-paragraph ks kt iq ku b kv mg jr kx ky mh ju la lb mi ld le lf mj lh li lj mk ll lm ln ij bi translated">现在我们已经定义了环境的主要方面，我们可以写下代码了。</p><p id="d1e5" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">首先，我们定义我们的船的界限和我们的可观察空间状态(特征)的“盒子”的种类，我们也定义初始条件盒子。</p><pre class="kh ki kj kk gt nb nc nd ne aw nf bi"><span id="1f93" class="ng lp iq nc b gy nh ni l nj nk"><strong class="nc ir">self.observation_space</strong> = spaces.Box(low=np.array([0, -np.pi / 2, 0, -4, -0.2]), high=np.array([150, np.pi / 2, 4.0, 4.0, 0.2]))</span><span id="c845" class="ng lp iq nc b gy oi ni l nj nk"><strong class="nc ir">self.init_space</strong> = spaces.Box(low=np.array([0, -np.pi / 15, 1.0, 0.2, -0.01]), high=np.array([30, np.pi / 15, 1.5, 0.3, 0.01]))<br/>self.ship_data = None</span></pre><p id="f1dd" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">之后，我们定义一个函数将模拟器空间状态转换为环境空间状态。注意，我们镜像了<strong class="ku ir"> vy </strong>速度<strong class="ku ir"> θ </strong>角度和距离<strong class="ku ir"> d </strong>以使 AI 更容易学习(减少空间状态维度)。</p><pre class="kh ki kj kk gt nb nc nd ne aw nf bi"><span id="8723" class="ng lp iq nc b gy nh ni l nj nk"><strong class="nc ir">def convert_state(self, state):</strong><br/>    <em class="nl">"""<br/>    This method generated the features used to build the reward    function<br/>    </em><strong class="nc ir"><em class="nl">:param</em></strong><em class="nl"> state: Global state of the ship<br/>    """<br/>    </em>ship_point = Point((state[0], state[1]))<br/>    side = np.sign(state[1] - self.point_a[1])<br/>    d = ship_point.distance(self.guideline)  # meters<br/>    theta = side*state[2]  # radians<br/>    vx = state[3]  # m/s<br/>    vy = side*state[4]  # m/s<br/>    thetadot = side * state[5]  # graus/min<br/>    obs = np.array([d, theta, vx, vy, thetadot])<br/>    return obs</span></pre><p id="4f93" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">然后我们定义一个函数来计算之前定义的奖励</p><pre class="kh ki kj kk gt nb nc nd ne aw nf bi"><span id="262f" class="ng lp iq nc b gy nh ni l nj nk">d, theta, vx, vy, thetadot = obs[0], obs[1]*180/np.pi, obs[2], obs[3], obs[4]*180/np.pi<br/>if not self.observation_space.contains(obs):<br/>    return -1000<br/>else:<br/>    return 1-8*np.abs(theta/90)-np.abs(thetadot/20)-5*np.abs(d)/150-     np.abs(vy/4)-np.abs(vx-2)/2</span></pre><p id="432f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们还必须定义阶跃函数。该功能由代理在导航时使用，在每一步，代理选择一个动作并在 10 秒内运行模拟(在我们的积分器中),并一次又一次地这样做，直到它到达通道的末端或到达通道边缘。</p><pre class="kh ki kj kk gt nb nc nd ne aw nf bi"><span id="4d92" class="ng lp iq nc b gy nh ni l nj nk">def step(self, action):<br/>    side = np.sign(self.last_pos[1])<br/>    angle_action = action[0]*side<br/>    rot_action = 0.2<br/>    state_prime = self.simulator.step(angle_level=angle_action, rot_level=rot_action)<br/>    # convert simulator states into obervable states<br/>    obs = self.convert_state(state_prime)<br/>    # print('Observed state: ', obs)<br/>    dn = self.end(state_prime=state_prime, obs=obs)<br/>    rew = self.calculate_reward(obs=obs)<br/>    self.last_pos = [state_prime[0], state_prime[1], state_prime[2]]<br/>    self.last_action = np.array([angle_action, rot_action])<br/>    info = dict()<br/>    return obs, rew, dn, info</span></pre><p id="3ffc" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">因为我们镜像状态，所以我们也必须镜像方向舵的动作，将它乘以<strong class="ku ir">侧。</strong>在本教程中，我们将创建一个网络，仅控制方向舵的动作，并保持旋转角度不变(<strong class="ku ir"> rot_action = 0.2 </strong>)。</p><p id="77a4" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们还使用库<a class="ae nm" href="https://docs.python.org/3/library/turtle.html" rel="noopener ugc nofollow" target="_blank"> turtle </a>创建了一个查看器，你可以在这里查看代码<a class="ae nm" href="https://github.com/jmpf2018/SimpleShipAI/blob/master/viewer.py" rel="noopener ugc nofollow" target="_blank">。它用于显示学习过程或培训后的表现。查看器在函数<strong class="ku ir"> render </strong>中被调用。</a></p><pre class="kh ki kj kk gt nb nc nd ne aw nf bi"><span id="e5f1" class="ng lp iq nc b gy nh ni l nj nk">def render(self, mode='human'):<br/>    if self.viewer is None:<br/>        self.viewer = Viewer()<br/>        self.viewer.plot_boundary(self.borders)<br/>        self.viewer.plot_guidance_line(self.point_a, self.point_b)</span><span id="78a5" class="ng lp iq nc b gy oi ni l nj nk">    img_x_pos = self.last_pos[0] - self.point_b[0] * (self.last_pos[0] // self.point_b[0])<br/>    if self.last_pos[0]//self.point_b[0] &gt; self.number_loop:<br/>        self.viewer.end_episode()<br/>        self.viewer.plot_position(img_x_pos, self.last_pos[1], self.last_pos[2], 20 * self.last_action[0])<br/>        self.viewer.restart_plot()<br/>        self.number_loop += 1<br/>    else:<br/>        self.viewer.plot_position(img_x_pos, self.last_pos[1], self.last_pos[2], 20 * self.last_action[0])</span></pre><p id="3648" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">最后，我们定义了设置初始空间状态和重置的函数，它们在每次新的迭代开始时使用。</p><pre class="kh ki kj kk gt nb nc nd ne aw nf bi"><span id="b5f4" class="ng lp iq nc b gy nh ni l nj nk"><strong class="nc ir">def set_init_space(self, low, high):</strong><br/>    self.init_space = spaces.Box(low=np.array(low), high=np.array(high))</span><span id="b5a1" class="ng lp iq nc b gy oi ni l nj nk"><strong class="nc ir">def reset(self):</strong><br/>    init = list(map(float, self.init_space.sample()))<br/>    init_states = np.array([self.start_pos[0], init[0], init[1], init[2] * np.cos(init[1]), init[2] * np.sin(init[1]), 0])<br/>    self.simulator.reset_start_pos(init_states)<br/>    self.last_pos = np.array([self.start_pos[0], init[0],  init[1]])<br/>    print('Reseting position')<br/>    state = self.simulator.get_state()<br/>    if self.viewer is not None:<br/>        self.viewer.end_episode()<br/>    return self.convert_state(state)</span></pre><p id="34b5" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">完整的代码可以在<a class="ae nm" href="https://github.com/jmpf2018/SimpleShipAI" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="4959" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">4 —培训</h1><p id="a97b" class="pw-post-body-paragraph ks kt iq ku b kv mg jr kx ky mh ju la lb mi ld le lf mj lh li lj mk ll lm ln ij bi translated">为了培训我们的代理人，我们使用了来自<a class="ae nm" href="https://github.com/keras-rl/keras-rl" rel="noopener ugc nofollow" target="_blank"> Keras-rl 项目</a>的 DDPG 代理人。关于 DDPG 方法的细节可以在<a class="ae nm" href="https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="5898" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">然后我们导入所有使用过的方法来建立我们的神经网络。</p><pre class="kh ki kj kk gt nb nc nd ne aw nf bi"><span id="3f03" class="ng lp iq nc b gy nh ni l nj nk">from keras.models import Sequential, Model<br/>from keras.layers import Dense, Activation, Flatten, Input, Concatenate<br/>from keras.optimizers import Adam</span><span id="560a" class="ng lp iq nc b gy oi ni l nj nk">from rl.agents import DDPGAgent<br/>from rl.memory import SequentialMemory<br/>from rl.random import OrnsteinUhlenbeckProcess<br/>from ship_env import ShipEnv</span></pre><p id="c23d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">所使用的神经网络具有以下结构(演员-评论家结构):</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/fe6dbd92c4b47913defc811f4c40b541.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*TdNYX-UbKtRWJpgJ6hlcUw.png"/></div></figure><p id="f98d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">实现该结构的代码如下:</p><pre class="kh ki kj kk gt nb nc nd ne aw nf bi"><span id="02e2" class="ng lp iq nc b gy nh ni l nj nk"># Next, we build a very simple model.<br/>actor = Sequential()<br/>actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))<br/>actor.add(Dense(400))<br/>actor.add(Activation('relu'))<br/>actor.add(Dense(300))<br/>actor.add(Activation('relu'))<br/>actor.add(Dense(nb_actions))<br/>actor.add(Activation('softsign'))<br/>print(actor.summary())<br/></span><span id="bba1" class="ng lp iq nc b gy oi ni l nj nk">action_input = Input(shape=(nb_actions,), name='action_input')<br/>observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')<br/>flattened_observation = Flatten()(observation_input)<br/>x = Concatenate()([action_input, flattened_observation])<br/>x = Dense(400)(x)<br/>x = Activation('relu')(x)<br/>x = Dense(300)(x)<br/>x = Activation('relu')(x)<br/>x = Dense(1)(x)<br/>x = Activation('linear')(x)<br/>critic = Model(inputs=[action_input, observation_input], outputs=x)<br/>print(critic.summary())<br/></span><span id="6dbd" class="ng lp iq nc b gy oi ni l nj nk"># Finally, we configure and compile our agent. You can use every built-in Keras optimizer and<br/># even the metrics!<br/>memory = SequentialMemory(limit=2000, window_length=1)<br/>random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=0.6, mu=0, sigma=0.3)<br/>agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,<br/>                  memory=memory, nb_steps_warmup_critic=2000, nb_steps_warmup_actor=10000,<br/>                  random_process=random_process, gamma=.99, target_model_update=1e-3)<br/>agent.compile(Adam(lr=0.001,  clipnorm=1.), metrics=['mae'])</span></pre><p id="f1e9" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">最后，我们使用 300.000 次迭代来训练我们的代理，并且在训练之后，我们保存网络权重和训练历史:</p><pre class="kh ki kj kk gt nb nc nd ne aw nf bi"><span id="ade7" class="ng lp iq nc b gy nh ni l nj nk">hist = agent.fit(env, nb_steps=300000, visualize=False, verbose=2, nb_max_episode_steps=1000) # train our agent and store training in hist</span><span id="a970" class="ng lp iq nc b gy oi ni l nj nk">filename = '300kit_rn4_maior2_mem20k_target01_theta3_batch32_adam2'</span><span id="baf8" class="ng lp iq nc b gy oi ni l nj nk"># we save the history of learning, it can further be used to plot reward evolution</span><span id="edb6" class="ng lp iq nc b gy oi ni l nj nk">with open('_experiments/history_ddpg__redetorcs'+filename+'.pickle', 'wb') as handle:<br/>     pickle.dump(hist.history, handle, protocol=pickle.HIGHEST_PROTOCOL)<br/>#<br/>After training is done, we save the final weights.<br/>agent.save_weights('h5f_files/ddpg_{}_weights.h5f'.format('600kit_rn4_maior2_mem20k_target01_theta3_batch32_adam2_action_lim_1'), overwrite=True)</span></pre><h2 id="43b4" class="ng lp iq bd lq nn no dn lu np nq dp ly lb nr ns ma lf nt nu mc lj nv nw me nx bi translated">最后经过训练我们有了以下结果:</h2><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="10e5" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">代理人已经学会如何控制船舵以及如何停留在航道中线。</p><blockquote class="ny nz oa"><p id="4c23" class="ks kt nl ku b kv kw jr kx ky kz ju la ob lc ld le oc lg lh li od lk ll lm ln ij bi translated">如果你已经读到这里，谢谢！如有任何问题，请在下方留言</p></blockquote><h2 id="e6a2" class="ng lp iq bd lq nn no dn lu np nq dp ly lb nr ns ma lf nt nu mc lj nv nw me nx bi translated">关于这个项目的更多信息:</h2><p id="18d0" class="pw-post-body-paragraph ks kt iq ku b kv mg jr kx ky mh ju la lb mi ld le lf mj lh li lj mk ll lm ln ij bi translated">在我最后一年的项目中，我使用了一个更详细的船模型，还包括由人工智能代理控制的推进动作。项目存储库可以在这里找到<a class="ae nm" href="https://github.com/jmpf2018/ShipAI" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="778e" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">参考文献:</strong></p><p id="8824" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><em class="nl">【1】FOSSEN，Thor I .船舶流体力学与运动控制手册。约翰·威利父子有限公司，2011 年。</em></p></div></div>    
</body>
</html>