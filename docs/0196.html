<html>
<head>
<title>A Beginner’s Guide to Neural Networks: Part Two</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络初学者指南:第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-guide-to-neural-networks-part-two-bd503514c71a?source=collection_archive---------0-----------------------#2017-03-27">https://towardsdatascience.com/a-beginners-guide-to-neural-networks-part-two-bd503514c71a?source=collection_archive---------0-----------------------#2017-03-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8535" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">偏置、激活函数、隐藏层以及构建更高级的前馈神经网络架构。</h2></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><p id="79cf" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi li translated"><span class="l lj lk ll bm lm ln lo lp lq di">在</span> <a class="ae lr" href="https://medium.com/luminous-ai/a-beginners-guide-to-neural-networks-b6be0d442fa4#.24wy4o50n" rel="noopener">第一部分，</a>我解释了感知器如何接受输入，应用线性组合，如果线性组合大于或小于某个阈值，则分别产生 1 或 0 的输出。在数学语言中，它看起来像这样:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/ed5fe319831e584881d3908da734bfc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/1*7uqFwiPG5_fvnW8tjaYCjA.gif"/></div></figure><p id="944e" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">在上面的公式中，希腊字母适马∑用于表示<strong class="ko ir">求和</strong>，下标<em class="ma"> i </em>用于迭代输入(<em class="ma"> x) </em>和权重(<em class="ma"> w) </em>配对。</p><p id="968c" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">为了让后面的<em class="ma">训练</em>简单一点，我们对上面的公式做一个小的调整。让我们将<em class="ma">阈值</em>移动到<a class="ae lr" href="https://www.mathsisfun.com/algebra/inequality.html" rel="noopener ugc nofollow" target="_blank">不等式</a>的另一边，并用神经元的<em class="ma">偏差来代替它。</em>现在我们可以将等式改写为:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/b92ea98a80032c4da1a360d0f00b6e7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/1*C7AkWHjMWgNan5WQ3IJKDw.gif"/></div></figure><p id="7224" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">有效，<em class="ma">偏差= —阈值。</em>你可以把<em class="ma">偏置</em>想象成让神经元输出 1 有多容易——如果偏置非常大，神经元输出 1 就很容易，但如果偏置非常负，那就很难了。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/598622309efbe4acc5af8b35887630fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*7c-ZVZ7h-AwjapAhQgeKdQ.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk"><a class="ae lr" href="http://smbc-comics.com/index.php?id=4105" rel="noopener ugc nofollow" target="_blank">Source: SMBC Comics</a></figcaption></figure><p id="8ed8" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">在定义中做这个小改动有几个原因(稍后还会有一些改动；很高兴看到做出这些改变背后的直觉)。正如我在第一部分中提到的，我们随机分配权重数字(我们事先不知道正确的数字)，随着神经网络<em class="ma">训练</em>，它对这些权重进行增量更改，以产生更准确的输出。类似地，我们不知道正确的<em class="ma">阈值</em>，并且像权重一样，网络将需要改变阈值以产生更精确的输出。现在有了<em class="ma">偏差</em>，我们只需要对等式的左边进行修改，而右边可以保持不变为零，很快，你就会明白为什么这是有用的。</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><p id="49df" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">为输出神经元的决策转换值或陈述条件的函数被称为<strong class="ko ir">激活函数</strong>。上面的数学公式只是深度学习中使用的几个激活函数之一(也是最简单的)，它被称为<strong class="ko ir"> Heaviside 阶跃函数。</strong></p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/59faa0aa5ad658a9ebdf8c640f6dabf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*rdSZijQqNoiZPImMI4ENeg.png"/></div></figure><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/d782605bee1de4777af40174088862d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/1*LWWzDCgUONGAKko67oMzcw.gif"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">General formula (<em class="mj">h </em>being the condition) and graph of the Heaviside step function. The solid circle is the y-value you take (1.0 for x=0 in this case) , not the hollow circle. The line the hollow and solid circles are on is the ‘step’. (Source: Udacity)</figcaption></figure><p id="a62a" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">其他激活功能包括<em class="ma"> sigmoid </em>、<em class="ma"> tanh </em>和<em class="ma"> softmax </em>功能，它们各有其用途。(在这篇文章中，我将只解释 sigmoid 函数，但我会在其他函数出现时进行解释)。</p><h2 id="fd53" class="mk ml iq bd mm mn mo dn mp mq mr dp ms kv mt mu mv kz mw mx my ld mz na nb nc bi translated">Sigmoid 函数</h2><p id="4228" class="pw-post-body-paragraph km kn iq ko b kp nd jr kr ks ne ju ku kv nf kx ky kz ng lb lc ld nh lf lg lh ij bi translated">即使在处理绝对数(1 和 0；yes 和 nos)，让输出给出一个中间值是有益的。这有点像在被问到一个你不知道的是或否的问题时回答“也许”，而不是猜测。这实质上是 sigmoid 函数优于 Heaviside 函数的地方。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/015d8032b17a70896df0cf623c41fd3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*UzqjTkSHgkrU55P71NKNMA.png"/></div></figure><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/51ae35e2a2194adb106ac0254760d3ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/1*7fiuQEWfNRUCDQBAUZz6Rw.gif"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">The graph and formula of the sigmoid function. Source: Udacity</figcaption></figure><p id="780f" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">像亥维赛阶梯函数一样，sigmoid 的值在 0 和 1 之间。但是这一次，没有<em class="ma">步</em>；它已经被平滑，以创建一个<em class="ma">连续</em>线。因此，输出可以被认为是成功的<em class="ma">概率</em>(1)，或者是肯定的。在日常生活中，0.5 的输出意味着网络不知道它是肯定的还是否定的，而 0.8 的输出意味着网络“非常肯定”它是肯定的。</p><p id="8eef" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">拥有这一特性对于网络的学习能力尤为重要，我们将在以后的文章中看到这一点。现在，只要把它想象成更容易<em class="ma">教会</em>一个网络逐步走向正确答案，而不是直接从 0 跳到 1(反之亦然)。</p><h2 id="92d1" class="mk ml iq bd mm mn mo dn mp mq mr dp ms kv mt mu mv kz mw mx my ld mz na nb nc bi translated">隐藏层</h2><p id="8ec6" class="pw-post-body-paragraph km kn iq ko b kp nd jr kr ks ne ju ku kv nf kx ky kz ng lb lc ld nh lf lg lh ij bi translated">到目前为止，我们已经探索了感知器(最简单的神经网络模型)的架构，并看到了两个激活函数:Heaviside 阶跃函数和 sigmoid 函数。(要了解感知机如何用于计算逻辑功能，如 AND、OR 和 NAND，请查看迈克尔·尼尔森在线书籍中的<a class="ae lr" href="http://neuralnetworksanddeeplearning.com/chap1.html" rel="noopener ugc nofollow" target="_blank">第 1 章)。</a></p><p id="75ed" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">现在让我们使我们的网络稍微复杂一点。在这里，以及所有的神经网络图中，最左边的层是输入层(即你馈入的数据)，最右边的层是输出层(网络的预测/答案)。这两层之间的任意数量的层被称为<strong class="ko ir">隐藏层</strong>。层数越多，决策能得到的<em class="ma">细致入微</em>就越多。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/21e624c3bbc1c2e06829f27db228227f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*hGT-tQnvJ76fqzr8sLhaqQ.jpeg"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">(Source: <a class="ae lr" href="http://cs231n.github.io/neural-networks-1/" rel="noopener ugc nofollow" target="_blank">Stanford CS231n</a>)</figcaption></figure><p id="5953" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">网络通常有不同的名称:<em class="ma">深度前馈网络、前馈神经网络、</em>或<em class="ma">多层感知器(MLP)。</em>(为了使事情不那么混乱，我将坚持使用前馈神经网络)。它们被称为<em class="ma">前馈</em>网络，因为信息在一个总的(正向)方向上流动，在每个阶段都应用了数学函数。事实上，它们之所以被称为“网络”<em class="ma">是因为这个功能链的</em>(第一层功能的输出是第二层的输入，第三层的输出也是第三层的输入，依此类推)。那条链的长度给出了模型的<em class="ma">深度</em>，这实际上就是深度学习中的术语“<em class="ma">deep”</em>的来源！</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/19276b8d17cb44050b4ba914c8875d17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*MgJbWBmn9kB0pKi_f5kX4Q.jpeg"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Not quite the same ‘hidden’. (Source: Bill Watterson)</figcaption></figure><p id="00bf" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">添加隐藏层可以让神经网络做出更复杂的决定，但更多的是，以及神经网络如何通过第三部分(即将推出)中的<em class="ma">反向传播、</em>过程进行学习！</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="71cf" class="mk ml iq bd mm mn mo dn mp mq mr dp ms kv mt mu mv kz mw mx my ld mz na nb nc bi translated">资源</h2><ol class=""><li id="0095" class="nk nl iq ko b kp nd ks ne kv nm kz nn ld no lh np nq nr ns bi translated"><a class="ae lr" href="http://neuralnetworksanddeeplearning.com/chap1.html" rel="noopener ugc nofollow" target="_blank"> <em class="ma">利用神经网络识别手写数字</em> </a> <em class="ma"> </em>迈克尔尼尔森。</li><li id="7f75" class="nk nl iq ko b kp nt ks nu kv nv kz nw ld nx lh np nq nr ns bi translated"><a class="ae lr" href="http://cs231n.github.io/neural-networks-1/" rel="noopener ugc nofollow" target="_blank"> <em class="ma">神经网络第一部分:搭建架构</em>，</a>斯坦福 CS231n。</li><li id="f7f4" class="nk nl iq ko b kp nt ks nu kv nv kz nw ld nx lh np nq nr ns bi translated"><a class="ae lr" href="http://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank"> <em class="ma">深度学习书籍</em> </a>伊恩·古德菲勒、约舒阿·本吉奥和亚伦·库维尔。</li></ol></div></div>    
</body>
</html>