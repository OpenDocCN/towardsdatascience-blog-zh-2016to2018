<html>
<head>
<title>Training a Goal-Oriented Chatbot with Deep Reinforcement Learning — Part II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用深度强化学习训练面向目标的聊天机器人——第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-ii-dqn-agent-f84122cc995c?source=collection_archive---------6-----------------------#2018-12-01">https://towardsdatascience.com/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-ii-dqn-agent-f84122cc995c?source=collection_archive---------6-----------------------#2018-12-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="433a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">第二部分:DQN 代理人</h2></div><p id="3f9c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你还不知道什么是面向目标的聊天机器人，我们正在使用的数据库和我们对话系统的训练循环，看看这个系列教程的<a class="ae lb" href="https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383" rel="noopener">的前一部分</a>！</p><p id="ea48" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这一部分，我们将深入探讨代理人，在这种情况下，代理人由 DQN 代表。</p><p id="33c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本教程和附带的代码是基于 MiuLab 的 TC-Bot。本教程的代码可以在<a class="ae lb" href="https://github.com/maxbren/GO-Bot-DRL" rel="noopener ugc nofollow" target="_blank">这里</a>找到。我们将从<code class="fe lc ld le lf b"><a class="ae lb" href="https://github.com/maxbren/GO-Bot-DRL/blob/master/dqn_agent.py" rel="noopener ugc nofollow" target="_blank">dqn_agent.py</a></code>开始这一部分的工作。</p><p id="27f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们正在遵循的总体图表:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/95c8931d8c7beae082c62a80ec16ee9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R3dY-PNLzYTIRAwKpywVyA.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Dialogue flow of a single round</figcaption></figure></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h1 id="1eb0" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">代理的目的是什么？</h1><p id="ba12" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">面向目标(GO)的聊天机器人代理的目的是接受训练，以便熟练地与真实用户交谈，从而完成一个目标，例如找到一个符合用户约束的预订或电影票。代理的主要工作是采取一种状态，并产生一个接近最优的行动。具体来说，代理从对话状态跟踪器(ST)接收表示当前对话历史的状态，并选择要采取的对话响应。</p><h2 id="fe0c" class="na me iq bd mf nb nc dn mj nd ne dp mn ko nf ng mp ks nh ni mr kw nj nk mt nl bi translated">深度 Q 网(DQN)</h2><p id="5285" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">dqn 的细节超出了本教程的范围，所以请查看这些资源以了解更多信息:<a class="ae lb" href="https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/" rel="noopener ugc nofollow" target="_blank">编码一，</a> <a class="ae lb" href="https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26" rel="noopener ugc nofollow" target="_blank">以不同的方式编码一</a></p><p id="31a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们来看一些代码！</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h2 id="6c9d" class="na me iq bd mf nb nc dn mj nd ne dp mn ko nf ng mp ks nh ni mr kw nj nk mt nl bi translated">代理的对话配置</h2><p id="30c6" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">以下是代理使用的<code class="fe lc ld le lf b"><a class="ae lb" href="https://github.com/maxbren/GO-Bot-DRL/blob/master/dialogue_config.py" rel="noopener ugc nofollow" target="_blank">dialogue_config.py</a></code>中的对话配置常量:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="6525" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lc ld le lf b">agent_inform_slots</code>是座席代表通知的所有可能的关键值。<code class="fe lc ld le lf b">agent_request_slots</code>是代理请求的所有可能的键值。还显示了可能的代理操作。</p><h2 id="8d84" class="na me iq bd mf nb nc dn mj nd ne dp mn ko nf ng mp ks nh ni mr kw nj nk mt nl bi translated">神经网络模型</h2><p id="061e" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">我们使用<a class="ae lb" href="https://github.com/keras-team/keras" rel="noopener ugc nofollow" target="_blank"> Keras </a>来建立代理的模型。该模型是一个单隐层神经网络。这很简单，但是对这个问题有很好的效果。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="cdf3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该代码片段中的实例变量被分配给<code class="fe lc ld le lf b"><a class="ae lb" href="https://github.com/maxbren/GO-Bot-DRL/blob/master/constants.json" rel="noopener ugc nofollow" target="_blank">constants.json</a></code>中“agent”下的常量</p><h1 id="2bc9" class="md me iq bd mf mg no mi mj mk np mm mn jw nq jx mp jz nr ka mr kc ns kd mt mu bi translated">政策</h1><p id="060e" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">在给定的状态下，代理用来选择动作的策略取决于对话是处于预热阶段还是训练阶段。在训练前运行热身，通常使用随机策略来填充代理的记忆。在这种情况下，代理在预热期间使用非常基本的基于规则的策略。在训练中，行为模型被用来选择一个动作。在这种情况下，<code class="fe lc ld le lf b">use_rule</code>表示预热。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="f40f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个方法返回动作的索引和动作本身。</p><h2 id="a996" class="na me iq bd mf nb nc dn mj nd ne dp mn ko nf ng mp ks nh ni mr kw nj nk mt nl bi translated">基于规则的策略</h2><p id="0762" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">在预热期间，采用一个简单的基于规则的策略。</p><p id="52d9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先注意代理的重置方法，该方法仅用于重置该基于规则的策略的几个变量:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="6111" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该策略简单地请求请求槽列表中的下一个槽，直到它没有更多的请求为止，然后它采取“找到匹配”动作，最后在最后一轮采取“完成”动作。</p><p id="ac8c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将在第三部分和第四部分讨论 match found 和 done 含义。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="caf0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是一个使用这个简单策略的单集示例:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="6290" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个策略对于以某种有意义的方式热启动代理很重要。这是一个简单的策略，但比采取随机行动能改善结果。</p><h2 id="025e" class="na me iq bd mf nb nc dn mj nd ne dp mn ko nf ng mp ks nh ni mr kw nj nk mt nl bi translated">DQN 政策</h2><p id="6c07" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">培训期间使用:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h1 id="7f00" class="md me iq bd mf mg no mi mj mk np mm mn jw nq jx mp jz nr ka mr kc ns kd mt mu bi translated">训练方法</h1><p id="c4ba" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">正如在系列的第一部分中所描述的,<code class="fe lc ld le lf b">dqn_agent.train()</code>在训练中每隔几集就会被调用一次。</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="nm nn l"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Note: Some of this code is based off of an awesome tutorial by <a class="ae lb" href="https://github.com/jaromiru" rel="noopener ugc nofollow" target="_blank">Jaromír</a> which can be found <a class="ae lb" href="https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/" rel="noopener ugc nofollow" target="_blank">here</a>.</figcaption></figure><p id="854d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我不打算遍历这段代码的大部分，因为它是非常基本的，你们中的许多人应该已经看到了 DQN 代码，看起来很像现在这样。然而，我要指出的是，与许多其他 DQN 训练方法不同的是，这个代码并不是随机抽取一批样本。相反，它会计算当前内存中有多少批次，然后根据这些批次训练权重。这有点奇怪，但对 TC-Bot 的原始代码是准确的。摆弄一下这个，看看你能否用不同的批量取样技术得到更好的结果！</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><figure class="lh li lj lk gt ll gh gi paragraph-image"><a href="http://languagelog.ldc.upenn.edu/nll/?p=34864"><div class="gh gi nt"><img src="../Images/47ea7657b478fc625843b30c2d310be9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aLMeYSuaNUILVF59HDJ5iw.png"/></div></a></figure><p id="089b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<strong class="kh ir">摘要</strong>中，该代理根据一个状态选择一个动作，该状态的策略要么是预热期间的一个简单请求列表，要么是训练期间的单个隐藏层行为模型。这种训练方法很简单，与其他 DQN 训练方法只有几处不同。尝试模型的架构，添加<a class="ae lb" href="https://arxiv.org/abs/1511.05952" rel="noopener ugc nofollow" target="_blank">优先体验回放</a>并制定更高级的基于规则的策略！</p><p id="1775" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iii-dialogue-state-d29c2828ce2a" rel="noopener">下一部分</a>将涵盖 ST 和查询系统！在那里，您将了解 ST 如何更新其历史/对对话的理解，以及状态准备是什么样的。</p></div></div>    
</body>
</html>