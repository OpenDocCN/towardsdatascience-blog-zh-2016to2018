<html>
<head>
<title>Self Learning AI-Agents Part I: Markov Decision Processes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自学习人工智能代理第一部分:马尔可夫决策过程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f?source=collection_archive---------2-----------------------#2018-10-14">https://towardsdatascience.com/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f?source=collection_archive---------2-----------------------#2018-10-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cce4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深度强化学习理论的数学指南</h2></div><p id="4ec2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是关于自我学习人工智能代理的多部分系列的第一篇文章，或者更准确地说，是深度强化学习。本系列的目的不仅仅是让你对这些话题有一个直觉。相反，我想为您提供对最流行和最有效的深度强化学习方法背后的理论、数学和实现的更深入的理解。</p><h2 id="760f" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">自学习人工智能代理系列—目录</h2><ul class=""><li id="423e" class="lx ly it kk b kl lz ko ma kr mb kv mc kz md ld me mf mg mh bi translated">第一部分:马尔可夫决策过程(<strong class="kk iu">本文</strong>)</li><li id="35e1" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated"><a class="ae mn" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47">第二部分:深度 Q 学习</a></li><li id="dbdb" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated"><a class="ae mn" rel="noopener" target="_blank" href="/deep-double-q-learning-7fca410b193a">第三部分:深度(双)Q 学习</a></li><li id="fa8f" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated"><a class="ae mn" rel="noopener" target="_blank" href="/self-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20">第四部分:持续行动空间的政策梯度</a></li><li id="ae79" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">第五部分:决斗网络</li><li id="9477" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">第六部分:异步演员-评论家代理</li><li id="f4c9" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi">…</li></ul><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mo"><img src="../Images/9557478bd7e57d088b378a4e7f6d4e46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*aqp94KVRB72uoFWAjVAgWg.gif"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Fig. 1. AI agent learned how to run and overcome obstacles.</figcaption></figure><h2 id="ed65" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">马尔可夫决策过程—目录</h2><ul class=""><li id="c00a" class="lx ly it kk b kl lz ko ma kr mb kv mc kz md ld me mf mg mh bi translated"><strong class="kk iu"> 0。简介</strong></li><li id="622f" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="kk iu"> 1。简而言之强化学习</strong></li><li id="a360" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="kk iu"> 2。马尔可夫决策过程</strong></li><li id="94dd" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">2.1 马尔可夫过程</li><li id="df85" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">2.2 马尔可夫奖励过程</li><li id="759f" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">2.3 价值函数</li><li id="7750" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="kk iu"> 3。贝尔曼方程</strong></li><li id="0696" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">3.1 马尔可夫报酬过程的贝尔曼方程</li><li id="5e69" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">3.2 马尔可夫决策过程— <em class="ne">定义</em></li><li id="f1ae" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">3.3 政策</li><li id="818a" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">3.4 动作值函数</li><li id="d3e5" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">3.5 最佳政策</li><li id="c55f" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">3.6 贝尔曼最优方程</li></ul></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h2 id="5144" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">如果你喜欢这篇文章，想分享你的想法，问问题或保持联系，请随时通过 LinkedIn 与我联系。</h2></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h1 id="0c6c" class="nm lf it bd lg nn no np lj nq nr ns lm jz nt ka lp kc nu kd ls kf nv kg lv nw bi translated">0.介绍</h1><p id="94be" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">深度强化学习正在兴起。近年来，深度学习的其他子领域没有被研究人员和全球大众媒体谈论得更多。深度学习中的大多数杰出成就都是由于深度强化学习而取得的。从谷歌的阿尔法围棋(Alpha Go)击败了世界上最好的人类棋手(这一成就在几年前被认为是不可能的),到 DeepMind 的人工智能智能代理(AI agents ),它们可以自学行走、奔跑和克服障碍(图 1-3)。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/ef4d371ddad83019c5f3e21d2f250448.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/1*dean9FM6YSsm6QD7Cx48LA.gif"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Fig. 2. AI agent learned how to run and overcome obstacles.</figcaption></figure><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/1397f64c92c965daaa9679f0c1dccad3.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/1*mGttdEJW4Hd_oSAwo3A3Nw.gif"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Fig. 3. AI agent learned how to run and overcome obstacles.</figcaption></figure><p id="e288" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自 2014 年以来，其他人工智能代理在玩老派雅达利游戏如突破(图 4)时超过了人类水平。在我看来，所有这些最令人惊讶的事情是，这些人工智能代理中没有一个是由人类明确编程或教会如何解决这些任务的。他们是靠深度学习和强化学习的力量自己学会的。这个多部分系列的第一篇文章的目标是为您提供必要的数学基础，以便在接下来的文章中处理这个人工智能子领域中最有前途的领域。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/3f12d23480383ae73c042534194033c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*wXIZwFnRNmSbO4QWj0cjbg.gif"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Fig. 4 AI agent learned how to play Atari’s Breakthrough.</figcaption></figure></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h2 id="217a" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">1.简而言之，深度强化学习</h2><p id="7033" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">深度强化学习可以概括为构建一个直接从与环境的交互中学习的算法(或 AI agent)(图 5)。环境可以是真实的世界、计算机游戏、模拟甚至是棋盘游戏，如围棋或象棋。像人类一样，人工智能<strong class="kk iu">代理</strong>从其<strong class="kk iu">动作</strong>的结果中学习，而不是从被明确教导中学习。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi od"><img src="../Images/b45d17e0b44aeb5ec4b06e01f3bc3041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7GERX5blVYnRRkW-vVkd0Q.jpeg"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Fig. 5 Schematic depiction of deep reinforcement learning</figcaption></figure><p id="a181" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在深度强化学习中，<strong class="kk iu">代理</strong>由神经网络表示。神经网络直接与环境互动。它观察<strong class="kk iu">环境</strong>的当前<strong class="kk iu">状态</strong>并决定采取哪个<strong class="kk iu">动作</strong>(例如向左、向右移动等。)基于当前<strong class="kk iu">状态</strong>和过去的经验。基于所采取的<strong class="kk iu">动作</strong>，AI 代理接收<strong class="kk iu">奖励。</strong>奖励<strong class="kk iu">的数量</strong>决定了所采取的<strong class="kk iu">行动</strong>在解决给定问题方面的质量(例如学习如何走路)。一个<strong class="kk iu">代理</strong>的目标是学习在任何给定的情况下采取<strong class="kk iu">行动</strong>，使累积的<strong class="kk iu">回报</strong>随着时间的推移而最大化。</p></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h1 id="79cf" class="nm lf it bd lg nn no np lj nq nr ns lm jz nt ka lp kc nu kd ls kf nv kg lv nw bi translated">2.马尔可夫决策过程</h1><p id="5ffc" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">一个<strong class="kk iu">马尔可夫决策过程</strong> ( <strong class="kk iu"> MDP </strong>)是一个离散时间随机控制过程。MDP 是迄今为止我们为人工智能代理的复杂环境建模的最好方法。代理旨在解决的每个问题都可以被认为是一系列状态<em class="ne"> S1、S2、S3……Sn</em>(例如，一个状态可以是围棋/国际象棋棋盘配置)。代理采取行动并从一种状态转移到另一种状态。在接下来的内容中，你将学习在任何给定的情况下，决定代理人必须采取何种行动的数学方法。</p><h2 id="4938" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">2.1 马尔可夫过程</h2><p id="e9e1" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated"><strong class="kk iu">马尔可夫过程</strong>是描述一系列可能状态的随机模型，其中当前状态仅依赖于先前状态。这也称为马尔可夫性质(等式)。1).对于强化学习，这意味着人工智能主体的下一个状态只依赖于上一个状态，而不是之前的所有状态。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/391b65171bfcbed913bceaed0592ae10.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*mXpLcRAzuazibFpkEi35Zg.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 1 Markov Property</figcaption></figure><p id="03de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">马尔可夫过程是一个随机过程。意味着从当前状态<strong class="kk iu"> <em class="ne"> s </em> </strong> <em class="ne"> </em>到下一个状态<strong class="kk iu"><em class="ne">s’</em></strong>的过渡只能以一定的概率发生<strong class="kk iu"><em class="ne">Pss</em>’</strong>(等式。2).在马尔可夫过程中，被告知向左的代理只会以某个概率(例如 0.998)向左。在小概率情况下，由环境来决定代理的最终位置。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/ae77b359d2124014deb88b12b0bcd277.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*GS4Ea3D2DKKjTp3Msdx7sw.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 2 Transition probability from state<strong class="bd og"> s</strong> to state<strong class="bd og"> s’.</strong></figcaption></figure><p id="360c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"><em class="ne">Pss</em>’</strong>可以被认为是状态转移矩阵<strong class="kk iu"> <em class="ne"> P </em> </strong>中的一个条目，该矩阵定义了从所有状态<strong class="kk iu"> <em class="ne"> s </em> </strong>到所有后继状态<strong class="kk iu"><em class="ne">’s</em></strong>的转移概率。3).</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/93d77183df91ea7ab768d38aefef03b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*TSNR-z9LWO-S0UtJa9LxBg.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 3. Transition probability matrix.</figcaption></figure><blockquote class="oi oj ok"><p id="e9f3" class="ki kj ne kk b kl km ju kn ko kp jx kq ol ks kt ku om kw kx ky on la lb lc ld im bi translated"><strong class="kk iu">记住</strong>:一个马尔可夫过程(或马尔可夫链)是一个元组<strong class="kk iu"> &lt; <em class="it"> S </em> </strong>，<strong class="kk iu"> <em class="it"> P </em> &gt; </strong>。<strong class="kk iu"> S </strong>是状态的(有限)集合。<strong class="kk iu"> P </strong>是状态转移概率矩阵。</p></blockquote><h2 id="f962" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">2.2 马尔可夫奖励过程</h2><p id="720f" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">一个马氏奖励过程是一个元组<strong class="kk iu"> &lt; <em class="ne"> S，P，R </em> &gt; </strong>。这里<strong class="kk iu"> <em class="ne"> R </em> </strong>是代理人在状态<strong class="kk iu"> <em class="ne"> s </em> </strong> (Eq。4).这一过程的动机是，对于一个旨在实现某个目标(例如赢得一场国际象棋比赛)的人工智能主体来说，某些状态(比赛配置)在策略和赢得比赛的潜力方面比其他状态更有希望。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/65cc99b5a6f6e90e283696fd9e3e43bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*Wqqe4F2XdRPQ2brDI13NIg.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 4. Expected reward in a state <strong class="bd og">s</strong>.</figcaption></figure><p id="4323" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">兴趣的首要话题是总奖励<strong class="kk iu"> <em class="ne"> Gt </em> </strong> (Eq。5)这是代理将在所有状态序列中收到的预期累积奖励。每个奖励由所谓的折扣因子γ ∈ [0，1]加权。贴现回报在数学上是方便的，因为它避免了循环马尔可夫过程中的无限回报。此外，贴现因子意味着我们越是在未来，回报就变得越不重要，因为未来往往是不确定的。如果奖励是经济上的，即时奖励可能比延迟奖励获得更多的利息。此外，动物/人类的行为显示出对即时回报的偏好。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi op"><img src="../Images/aca6649dd56b211f2caf476b881d8888.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*mzE3SlmZeAxwiJ27F8okhw.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 5. Total reward across all states.</figcaption></figure></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h2 id="a36a" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">2.3 价值函数</h2><p id="bb05" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">另一个重要的概念是价值函数<strong class="kk iu"> v(s) </strong>的概念。value 函数将一个值映射到每个状态<strong class="kk iu"> <em class="ne"> s </em> </strong>。状态<strong class="kk iu"> <em class="ne"> s </em> </strong>的值被定义为如果 AI 代理在状态<strong class="kk iu"> <em class="ne"> s </em> </strong>中开始其进程，它将接收的<em class="ne">预期</em>总奖励(等式。6).</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/7356f8e575589e5070d566f0250428fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*knZcR-GgeSthgfVTajAnDQ.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 6. Value function, the expected return starting in state <strong class="bd og">s</strong>.</figcaption></figure><p id="c28c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">价值函数可以分解为两部分:</p><ul class=""><li id="8603" class="lx ly it kk b kl km ko kp kr or kv os kz ot ld me mf mg mh bi translated">代理人在状态<em class="ne"/><strong class="kk iu"><em class="ne">【s .</em></strong>下收到的即时报酬 R(t+1)</li><li id="4b3c" class="lx ly it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated">状态<strong class="kk iu"><em class="ne"/></strong>之后的下一个状态的贴现值 v(s(t+1))</li></ul><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/28b6ae1e9563eadcace87011f6096033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*6F-jTAkiK3DonHKca6c8Ug.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 7 Decomposition of the value function.</figcaption></figure><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/ec711ae9e6c7d1258ca0d33c00d0ef97.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*nkI217UjWAZRBTP8XAm-xQ.png"/></div></figure><h1 id="e972" class="nm lf it bd lg nn ow np lj nq ox ns lm jz oy ka lp kc oz kd ls kf pa kg lv nw bi translated">3.贝尔曼方程</h1><h2 id="3004" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">3.1<strong class="ak">马尔可夫奖励</strong>过程的贝尔曼方程</h2><p id="c94c" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">分解的值函数(等式。8)也称为马尔可夫奖励过程的贝尔曼方程。该功能可以在节点图中可视化(图 6)。从状态<strong class="kk iu"> <em class="ne"> s </em> </strong>开始导致值<strong class="kk iu"> <em class="ne"> v(s) </em> </strong>。处于状态<strong class="kk iu"> <em class="ne"> s </em> </strong>我们有一定的概率<strong class="kk iu"><em class="ne">【Pss’</em></strong>结束于下一个状态<strong class="kk iu"><em class="ne">s’</em></strong>。在这个特例中，我们有两个可能的下一个状态。为了获得值<strong class="kk iu"> v(s) </strong>，我们必须对可能的下一个状态<strong class="kk iu"> <em class="ne"> </em> </strong>的值<strong class="kk iu"><em class="ne">s v(s)</em></strong>求和，该值由概率<strong class="kk iu"><em class="ne">【Pss’</em></strong>加权，并加上从处于状态<strong class="kk iu"> <em class="ne"> s </em> </strong>中得到的直接奖励。这产生了等式。如果我们在等式中执行期望操作符<em class="ne"> E </em>，这就是等式 8。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/b4247fe9f791c2cd3e36ccb192363c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*1HducD_Lv0Sd84I2mn2_fQ.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 8 Decomposed value function.</figcaption></figure><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/d907667c5ff12c0ce9481c006fc27491.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*31TUqkhNYSsXxrtrtMTpNw.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Fig. 6 Stochastic transition from <strong class="bd og">s</strong> to <strong class="bd og">s’</strong>.</figcaption></figure><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/a3e9d7deefdc71b2283ec447e21fb4ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*gqyv7Uw63u_82syf8P3tAQ.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 9 Bellman Equation after execution of the expectation operator E.</figcaption></figure><h2 id="c8a3" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">3.2 马尔可夫决策过程—定义</h2><p id="710c" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">马尔可夫决策过程是一个带有决策的马尔可夫回报过程。马尔可夫决策过程由一组元组<strong class="kk iu"> &lt; S，A，P，R &gt;来描述，A </strong>是代理在状态<strong class="kk iu"> s </strong>中可以采取的有限的可能动作集合。因此，处于状态<strong class="kk iu"> <em class="ne"> s </em> </strong>的直接回报现在也取决于代理在这种状态下采取的动作<strong class="kk iu"><em class="ne"/></strong>(等式。10).</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/1f7364cf3922eb59a39ffb564b6f0028.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*2U-JtOFT945pv41ZGxBpEw.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 10 Expected reward depending on the action in a state <strong class="bd og">s</strong>.</figcaption></figure><h2 id="687c" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">3.3 政策</h2><p id="ddea" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">在这一点上，我们将讨论代理如何决定在一个特定的状态下必须采取的行动。这是由所谓的策略<strong class="kk iu"><em class="ne">【π】</em></strong>(等式。11).从数学上来说，策略是给定状态<strong class="kk iu"> <em class="ne"> s </em> </strong>下所有动作的分布。该策略确定从状态<strong class="kk iu"><em class="ne"/></strong>到代理必须采取的动作<strong class="kk iu"><em class="ne"/></strong>的映射。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/a745256d25fa919d3bd31e37a34654ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*EecaG2A-QvTVg8Vqs65vhw.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 11 Policy as a mapping from <strong class="bd og">s</strong> to <strong class="bd og">a</strong>.</figcaption></figure><blockquote class="oi oj ok"><p id="842b" class="ki kj ne kk b kl km ju kn ko kp jx kq ol ks kt ku om kw kx ky on la lb lc ld im bi translated"><strong class="kk iu">记住</strong>:直观地说，策略π可以描述为代理根据当前状态<strong class="kk iu"> s </strong>选择某些动作的策略。</p></blockquote><p id="1b90" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该政策导致了国家价值函数的新定义<strong class="kk iu"> v(s) </strong>(等式。我们现在把它定义为从状态<strong class="kk iu"> <em class="ne"> s </em> </strong>开始，然后遵循一个策略<strong class="kk iu"> π的期望收益。</strong></p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/61e91f0208835dc1e2746edcb38739e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*3T_7Y8v23p-U31CoQDyrow.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 12 State-value function.</figcaption></figure><h2 id="9c16" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">3.4 动作值函数</h2><p id="755a" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">除了状态值函数之外，另一个重要的函数是所谓的动作值函数<strong class="kk iu"> q(s，a) </strong>(等式。13) <strong class="kk iu">。</strong>行动值函数是我们从状态<strong class="kk iu"> s </strong>开始，采取行动<strong class="kk iu"> a </strong>，然后遵循策略<strong class="kk iu"> π </strong>所获得的预期收益。注意，对于状态<strong class="kk iu"> <em class="ne"> s </em> </strong>，<strong class="kk iu"> <em class="ne"> q(s，a) </em> </strong>可以取几个值，因为在状态<strong class="kk iu"> <em class="ne"> s </em> </strong>中代理可以采取几个动作。<strong class="kk iu"> <em class="ne"> Q(s，a) </em> </strong>的计算由神经网络实现。给定一个状态<strong class="kk iu"> <em class="ne"> s </em> </strong>作为输入，网络以标量形式计算该状态下每个可能动作的质量(图 7)。更高的质量意味着对于给定的目标更好的行动。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/cb745d4e272466f5d53ce37b89cc188a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*HT3IqCLPp_cX--G83sJIzA.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Fig. 7 Illustration of the action-value function.</figcaption></figure><blockquote class="oi oj ok"><p id="291f" class="ki kj ne kk b kl km ju kn ko kp jx kq ol ks kt ku om kw kx ky on la lb lc ld im bi translated"><strong class="kk iu">记住:</strong>行动价值函数告诉我们，在特定状态下采取特定行动有多好。</p></blockquote><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/cef48db28492830037b11fb6a4fce74c.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*yg4b4BpJuqHc8oc1lQB_ag.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 13 Action-value function.</figcaption></figure><p id="8736" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">之前，状态值函数<strong class="kk iu"> <em class="ne"> v(s) </em> </strong>可以分解为以下形式:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/8adb20c09afc4af5c9362daa026f8d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*-XMV6gAt0zqXaXWLJSXknw.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 14 Decomposed state-value function.</figcaption></figure><p id="4b7f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样的分解可以应用于动作值函数:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/97600bd39717fd128355186bede7ae0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*XDKYLAmdBO78M-4crHYYEQ.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 15 Decomposed action-value function.</figcaption></figure><p id="6fad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此时，让我们讨论一下<strong class="kk iu"><em class="ne">【v(s)</em></strong>和<strong class="kk iu"> <em class="ne"> q(s，a) </em> </strong>是如何相互关联的。这些函数之间的关系可以在图表中再次可视化:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/fbfc54e0ba9e86f67642c8047b566ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*xHWYthaBFm-v6v1bytkE_Q.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Fig. 8 Visualization of the relation between<strong class="bd og"> v(s) </strong>and <strong class="bd og">q(s,a)</strong>.</figcaption></figure><p id="ed91" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个例子中，处于状态<strong class="kk iu"> <em class="ne"> s </em> </strong>允许我们采取两种可能的动作<strong class="kk iu"> <em class="ne"> a </em> </strong>。根据定义，在特定的状态下采取特定的动作给了我们动作值<strong class="kk iu"> <em class="ne"> q(s，a) </em> </strong>。价值函数<strong class="kk iu"><em class="ne">【v(s)</em></strong>是在状态<strong class="kk iu"><em class="ne">【s】</em></strong>(Eq)中采取行动<strong class="kk iu"><em class="ne"/></strong>的概率(非策略<strong class="kk iu"> π </strong>)加权的可能<strong class="kk iu"><em class="ne">【q(s，a) </em> </strong>之和。16) <strong class="kk iu"> <em class="ne">。</em> </strong></p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/399b3ff72794134f7f1978b65f498466.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*s_UR_zVnJDIk4g-xMer-RA.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 16 State-value function as weighted sum of action-values.</figcaption></figure><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/9ef8c9c824cd98d839374a5dcc5469a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/1*XgB-Sjd4ZuwQgvxLijnnDw.gif"/></div></figure><p id="cbc3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们考虑图 9 中相反的情况。二叉树的根现在是一种状态，在这种状态下我们选择采取一个特定的动作<strong class="kk iu"><em class="ne"/></strong>。记住马尔可夫过程是随机的。采取行动并不意味着你会百分百确定地到达你想去的地方。严格地说，你必须考虑采取行动后在其他州结束的可能性。在这种特殊情况下，在采取行动<strong class="kk iu"><em class="ne"/></strong>之后，你可能会以两种不同的下一个状态<strong class="kk iu"><em class="ne"/></strong>结束:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi po"><img src="../Images/fc4eed6dcf944e6aa56f7f1e4b6c0d9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*x01YpecwlGz7auoV57O1MQ.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Fig. 9 Visualization of the relation between<strong class="bd og"> v(s) </strong>and <strong class="bd og">q(s,a)</strong>.</figcaption></figure><p id="7ac1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要获得行动值，您必须获得由概率<strong class="kk iu">Pss’</strong>加权的贴现状态值，以结束所有可能的状态(在本例中只有 2 个),并添加即时奖励:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/48ef259e0838605e40da3db34864c7b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*dlKrffEs6_x3xm0uU2vJ4A.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 17 Relation between <strong class="bd og">q(s,a) </strong>and <strong class="bd og">v(s)</strong>.</figcaption></figure><p id="00a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们知道了这些函数之间的关系，我们可以从等式中插入<strong class="kk iu"><em class="ne">【v(s)】</em></strong>。16 从 Eq 变成<strong class="kk iu"> <em class="ne"> q(s，a) </em> </strong>。17.我们得到等式。18，可以注意到，当前<strong class="kk iu"> q(s，a) </strong>和下一个动作值<strong class="kk iu">q(s’，a’)</strong>之间存在递归关系。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/44e3ec3707bbcde79394beaef7ee5acd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*IiIjG_beZPMAwAl7L8QOmA.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 18 Recursive nature of the action-value function.</figcaption></figure><p id="512f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种递归关系可以在二叉树中再次可视化(图 10)。我们从<strong class="kk iu"> q(s，a) </strong>开始，以某个概率在下一个状态<strong class="kk iu"> s' </strong>中结束<strong class="kk iu"> <em class="ne"> Pss' </em> </strong>从那里我们可以以概率<strong class="kk iu"><em class="ne">【π</em></strong>采取行动<strong class="kk iu"><em class="ne">' a '</em></strong>并以行动值<strong class="kk iu">结束为了获得 q(s，a) </strong>,我们必须沿着树向上，对所有概率进行积分，如等式 1 所示。18.</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/2cbd1c93337318aede31416c95f2b01a.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*ZwsekDt6wps8yoi4Tot9Ug.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Fig. 10 Visualization of the recursive behavior of q(s,a).</figcaption></figure><h2 id="6a6f" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">3.5 最佳政策</h2><p id="6d39" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">深度强化学习中最重要的感兴趣的主题是找到最佳动作值函数<strong class="kk iu"> q* </strong>。寻找<strong class="kk iu"> q* </strong>意味着代理人确切地知道在任何给定状态下动作的质量。此外，代理可以根据质量决定必须采取的行动。让我们定义一下<strong class="kk iu"> q* </strong>的意思。最可能的行动价值函数是遵循最大化行动价值的政策的函数:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/3e9d322c6dbb68f405df3eb8e06a7d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*x-pJEMjQDrbRHM1S4T_Z8w.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 19 Definition of the best action-value function.</figcaption></figure><p id="4e81" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了找到可能的最佳策略，我们必须在<strong class="kk iu">q(s，a)</strong>上最大化。最大化意味着我们只从所有可能的动作中选择动作<strong class="kk iu"><em class="ne"/></strong>，其中<strong class="kk iu"> <em class="ne"> q(s，a) </em> </strong>具有最高值。这就产生了最优策略π的如下定义:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/0bdb4726d951284da1eae305599b75c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*4CHLqHG9OvahwDqbX9zo4w.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 20 Optimal policy. Take actions that maximize q(s,a).</figcaption></figure><h2 id="a03e" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">3.6 贝尔曼最优方程</h2><p id="ed8b" class="pw-post-body-paragraph ki kj it kk b kl lz ju kn ko ma jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">最优策略的条件可以代入方程。18.从而为我们提供了<em class="ne">贝尔曼最优性方程:</em></p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/82d87b9cd856f9fbfed5cb5480a63f7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*JT4IHmXcis9x2tXVPGYfSg.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Eq. 21 <em class="pv">Bellman Optimality Equation</em></figcaption></figure><p id="0858" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果人工智能代理可以解决这个方程，那么它基本上意味着给定环境中的问题得到了解决。代理人知道在任何给定的状态或情况下任何可能的行动的质量，并能相应地行动。</p><blockquote class="oi oj ok"><p id="3ee6" class="ki kj ne kk b kl km ju kn ko kp jx kq ol ks kt ku om kw kx ky on la lb lc ld im bi translated">解决<em class="it">贝尔曼</em>最优方程<em class="it"> </em>将是接下来文章的主题。在接下来的文章中，我将向你展示第一种叫做深度 Q 学习的技术。</p></blockquote></div></div>    
</body>
</html>