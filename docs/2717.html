<html>
<head>
<title>Connected components at scale in PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark中的大规模连接元件</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/connected-components-at-scale-in-pyspark-4a1c6423b9ed?source=collection_archive---------6-----------------------#2018-02-25">https://towardsdatascience.com/connected-components-at-scale-in-pyspark-4a1c6423b9ed?source=collection_archive---------6-----------------------#2018-02-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/42fd62988caabf433cee26cc0c3e0181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zyoor62U2IIUNQDb0rZ04A.jpeg"/></div></div></figure><p id="8008" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">不久前，我有一个节点网络，我需要为它计算<a class="ae kw" href="https://en.wikipedia.org/wiki/Connected_component_(graph_theory)" rel="noopener ugc nofollow" target="_blank">个连通分量</a>。这并不是一件特别困难的事情。Python <a class="ae kw" href="https://networkx.github.io/" rel="noopener ugc nofollow" target="_blank"> networkx </a>库有一个很好的实现，使它变得特别容易，但是即使你想推出自己的函数，这也是一个简单的<a class="ae kw" href="https://en.wikipedia.org/wiki/Breadth-first_search" rel="noopener ugc nofollow" target="_blank">广度优先搜索</a>。(如果你想要一个温和的介绍，可汗学院给出了一个很好的概述。从概念上讲，它只涉及一组队列。您选择一个节点，找到连接到该节点的所有节点，找到连接到这些节点的所有节点，依此类推。每当您找到一个节点的连接时，您就将该节点移动到一个“完成”列表中。当您找不到任何不在“完成”列表中的连接时，那么您已经找到了您的连接组件。然后，对图中的所有剩余节点重复该过程。</p><p id="a394" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">虽然广度优先搜索在概念上很简单，但要大规模实现却是一件难事。这是因为像Spark这样的“大数据”系统区分了转化和行动。转型是一个计划——一组预期的行动。您可以构建一个很长的转换列表，并在最后调用一个操作，比如收集最终结果的操作。这就是使用大型数据集的成本:你可以存储比你个人电脑多得多的信息，但访问这些信息的计算量要比你只是从本地机器的内存中提取信息大得多。这就是为什么大规模计算连通分量是复杂的:广度优先搜索根据以前转换的结果来确定未来的转换，这意味着您必须一直调用操作。这可能是一个真正的资源负担。当我需要这样做的时候，我正在处理一个有几千万个节点和上亿条边的图。</p><p id="1d85" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Spark的<a class="ae kw" href="https://spark.apache.org/graphx/" rel="noopener ugc nofollow" target="_blank"> GraphX </a>库有一个connected components函数，但当时我正在寻找这样做的方法，我的整个工作流程是Python，而GraphX只在Scala中实现。有一个<a class="ae kw" href="https://github.com/graphframes/graphframes" rel="noopener ugc nofollow" target="_blank"> GraphFrames </a>项目为GraphX提供了一个Python包装器，在某些情况下还提供了一个替代实现，但据我所知，该项目并没有太频繁地更新。我担心我的项目依赖于一个大而复杂的依赖项，而这个依赖项并没有得到持续的支持。此外，我并不需要那个庞大复杂的依赖关系中的所有东西。我只需要一种计算连通分量的方法。</p><p id="f828" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这让我开始在互联网上搜索，给了我一些想法，坦率地说，这些想法有点超出了我翻译成工作代码的能力(这是我的错，不是他们的——例如:<a class="ae kw" href="https://www.linkedin.com/pulse/connected-component-using-map-reduce-apache-spark-shirish-kumar/" rel="noopener ugc nofollow" target="_blank">这里</a>，这里<a class="ae kw" href="https://github.com/kwartile/connected-component" rel="noopener ugc nofollow" target="_blank">这里</a>，这里<a class="ae kw" href="https://dl.acm.org/citation.cfm?doid=2670979.2670997" rel="noopener ugc nofollow" target="_blank">这里</a>。)但是查看这些实现至少给了我一个真正有用的视角:对于广度优先搜索，你不需要队列；你只需要知道两个节点之间的关系是否已经被探索过。排队是一种简单的方法，但不是唯一的方法。</p><p id="2cbb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以你可以这样做:给你的每个节点分配一个数字散列。假设一个无向图(其中，如果节点a连接到节点b，那么节点b也连接到节点a)，您可以创建一个邻接表(两列，每一列都充满了节点散列，其中每一行表示一条边)，然后按左侧列的散列分组，并取右侧列中散列的最小值。将这些重新加入到你的邻接表中，并替换新的、较低的左列散列。然后不断重复，直到一切都不再改变。</p><p id="ad9b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面，您可以看到一个使用PySpark的概念验证。这要求您通过将数据帧保存到文件来检查数据帧。通常，我发现将数据帧保存(缓存)在内存中更好，因为这样我就不必记得事后清理了，但是如果缓存次数太多，Spark就会花费太多时间来跟踪转换的整个过程，以至于根本没有时间调用操作。在我的用例中，我发现每3-4次迭代进行一次检查点检查，并在所有其他迭代中坚持下去，会产生可接受的性能。</p><pre class="kx ky kz la gt lb lc ld le aw lf bi"><span id="93f4" class="lg lh iq lc b gy li lj l lk ll"><strong class="lc ir">import</strong> <!-- -->pyspark.sql.functions as f<br/><strong class="lc ir">import</strong> <!-- -->subprocess</span><span id="0be0" class="lg lh iq lc b gy lm lj l lk ll"><strong class="lc ir">def</strong> <!-- -->pyspark_connected_components(<br/>    ss, adj, a, b, label, checkpoint_dir, <br/>    checkpoint_every<strong class="lc ir">=</strong>2, max_n<strong class="lc ir">=</strong>None):<br/>    <em class="ln">"""<br/>    This function takes an adjacency list stored in a Spark <br/>    data frame and calculates connected components. This <br/>    implementation only deals with use cases: assuming an <br/>    undirected graph (if a is connected to b then b is<br/>    connected to a).</em></span><span id="3692" class="lg lh iq lc b gy lm lj l lk ll"><em class="ln">    Args:<br/>    ss (pyspark.sql.session.SparkSession): an active SparkSession<br/>    adj (pyspark.sql.DataFrame): A data frame with at least two<br/>        columns, where each entry is a node of a graph and each row<br/>        represents an edge connecting two nodes.<br/>    a (str): the column name indicating one of the node pairs in the<br/>        adjacency list. The column must be numeric.<br/>    b (str): the column name indicating the other of the node pairs<br/>        in the adjacency list. The column must be numeric.<br/>    label (str): a label for the output column of nodes<br/>    checkpoint_dir (str): a location on HDFS in which to save <br/>        checkpoints<br/>    checkpoint every (int): how many iterations to wait before<br/>        checkpointing the dataframe<br/>    max_n (int): the maximum number of iterations to run</em></span><span id="4c72" class="lg lh iq lc b gy lm lj l lk ll"><em class="ln">    Returns:<br/>    A PySpark DataFrame with one column (named by the label <br/>        argument) listing all nodes, and another columns <br/>        ("component") listing component labels.<br/>    """</em></span><span id="c2e6" class="lg lh iq lc b gy lm lj l lk ll">    <em class="ln"># set up a few strings for use in the code further down</em><br/>    b2 <strong class="lc ir">=</strong> <!-- -->b <strong class="lc ir">+</strong> <!-- -->'join'<br/>    filepattern <strong class="lc ir">=</strong> <!-- -->'pyspark_cc_iter{}.parquet'</span><span id="fed1" class="lg lh iq lc b gy lm lj l lk ll">    <em class="ln"># cache the adjacency matrix because it will be used many times</em><br/>    adj_cache <strong class="lc ir">=</strong> <!-- -->adj.persist()</span><span id="534d" class="lg lh iq lc b gy lm lj l lk ll">    <em class="ln"># for each id in the `a` column, take the minimum of the <br/>    # ids in the `b` column</em><br/>    mins <strong class="lc ir">=</strong> <!-- -->(<br/>        adj_cache<br/>        .groupby(a)<br/>        .agg(f.min(f.col(b)).alias('min1'))<br/>        .withColumn('change', f.lit(0))<br/>        .persist()<br/>    )</span><span id="9408" class="lg lh iq lc b gy lm lj l lk ll"><strong class="lc ir">    if</strong> <!-- -->max_n <strong class="lc ir">is</strong> <strong class="lc ir">not</strong> <!-- -->None:<br/>        <em class="ln"># ensure a global minimum id that is less than <br/>        # any ids in the dataset</em><br/>        minimum_id <strong class="lc ir">= </strong>mins.select(<br/>            f.min(f.col('min1')).alias('n')<br/>        ).collect()[0]['n'] <strong class="lc ir">-</strong> <!-- -->1000000</span><span id="5edb" class="lg lh iq lc b gy lm lj l lk ll">    <em class="ln"># transformation used repeatedly to replace old labels with new</em><br/>    fill_in <strong class="lc ir">=</strong> <!-- -->f.coalesce(f.col('min2'), f.col('min1'))</span><span id="c8c6" class="lg lh iq lc b gy lm lj l lk ll">    <em class="ln"># join conditions for comparing nodes</em><br/>    criteria <strong class="lc ir">=</strong> <!-- -->[<br/>        f.col(b) <strong class="lc ir">==</strong> <!-- -->f.col(b2), f.col('min1') &gt; f.col('min3')<br/>    ]</span><span id="0a3f" class="lg lh iq lc b gy lm lj l lk ll">    <em class="ln"># initial conditions for the loop</em><br/>    not_done <strong class="lc ir">=</strong> <!-- -->mins.count()<br/>    old_done <strong class="lc ir">=</strong> <!-- -->not_done <strong class="lc ir">+</strong> <!-- -->1<br/>    new_done <strong class="lc ir">=</strong> <!-- -->not_done<br/>    niter <strong class="lc ir">=</strong> <!-- -->0<br/>    used_filepaths <strong class="lc ir">=</strong> <!-- -->[]<br/>    filepath <strong class="lc ir">=</strong> <!-- -->None</span><span id="4cbe" class="lg lh iq lc b gy lm lj l lk ll">    <em class="ln"># repeat until done</em><br/>    <strong class="lc ir">while</strong> <!-- -->not_done &gt; 0:<br/>        niter <strong class="lc ir">+=</strong> <!-- -->1<br/>        <br/>        <strong class="lc ir">if</strong> <!-- -->max_n <strong class="lc ir">is</strong> <strong class="lc ir">not</strong> <!-- -->None:<br/>            min_a <strong class="lc ir">=</strong> <!-- -->mins.filter(f.col('min1') !<strong class="lc ir">=</strong> <!-- -->minimum_id)<br/>        <strong class="lc ir">else</strong>:<br/>            min_a <strong class="lc ir">=</strong> <!-- -->mins</span><span id="86a6" class="lg lh iq lc b gy lm lj l lk ll">            <em class="ln"># get new minimum ids</em><br/>            newmins <strong class="lc ir">=</strong> <!-- -->(<br/>                adj_cache<br/>                .join(min_a, a, 'inner')<br/>                .join(<br/>                    mins.select(<br/>                        f.col(a).alias(b2), <br/>                        f.col('min1').alias('min3')<br/>                    ), <br/>                    criteria, <br/>                    'inner'<br/>                )<br/>                .groupby(f.col('min1').alias('min1'))<br/>                .agg(f.min(f.col('min3')).alias('min2'))<br/>            )</span><span id="1a77" class="lg lh iq lc b gy lm lj l lk ll">        <em class="ln"># reconcile new minimum ids with the <br/>        # minimum ids from the previous iteration</em><br/>        mins <strong class="lc ir">=</strong> <!-- -->(<br/>            mins<br/>            .join(newmins, 'min1', 'left')<br/>            .select(<br/>                a,<br/>                fill_in.alias('min1'),<br/>                (f.col('min1') !<strong class="lc ir">=</strong> <!-- -->fill_in).alias('change')<br/>            )<br/>        )</span><span id="091f" class="lg lh iq lc b gy lm lj l lk ll">        <em class="ln"># if there is a max_n, assign the global minimum id to <br/>        # any components with more than max_n nodes</em><br/>        <strong class="lc ir">if</strong> <!-- -->max_n <strong class="lc ir">is</strong> <strong class="lc ir">not</strong> <!-- -->None:<br/>            mins <strong class="lc ir">=</strong> <!-- -->(<br/>                mins<br/>                .withColumn(<br/>                    'n',<br/>                    f.count(f.col(a)).over(<br/>                        w.partitionBy(<br/>                            f.col('min1')).orderBy(f.lit(None)<br/>                        )<br/>                    )<br/>                )<br/>                .withColumn(<br/>                    'min1', <br/>                    f.when(f.col('n') &gt;<strong class="lc ir">=</strong> <!-- -->max_n, f.lit(minimum_id))<br/>                    .otherwise(f.col('min1'))<br/>                )<br/>                .drop('n')<br/>            )</span><span id="3e9c" class="lg lh iq lc b gy lm lj l lk ll">            <em class="ln"># logic for deciding whether to persist of checkpoint</em><br/>            <strong class="lc ir">if</strong> <!-- -->(niter <strong class="lc ir">%</strong> <!-- -->checkpoint_every) <strong class="lc ir">==</strong> <!-- -->0:<br/>                filepath <strong class="lc ir">=</strong> (<br/>                    <!-- -->checkpoint_dir <strong class="lc ir">+</strong> <!-- -->filepattern.format(niter)<br/>                )<br/>                used_filepaths.append(filepath)<br/>                mins.write.parquet(filepath, mode<strong class="lc ir">=</strong>'overwrite')<br/>                mins <strong class="lc ir">=</strong> <!-- -->ss.read.parquet(filepath)<br/>            <strong class="lc ir">else</strong>:<br/>                mins <strong class="lc ir">=</strong> <!-- -->mins.persist()</span><span id="0012" class="lg lh iq lc b gy lm lj l lk ll">        <em class="ln"># update inputs for stopping logic</em><br/>        not_done <strong class="lc ir">=</strong> <!-- -->mins.filter(f.col('change') <strong class="lc ir">==</strong> <!-- -->True).count()<br/>        old_done <strong class="lc ir">=</strong> <!-- -->new_done<br/>        new_done <strong class="lc ir">=</strong> <!-- -->not_done<br/>        n_components <strong class="lc ir">=</strong> <!-- -->mins.select(<br/>            f.countDistinct(f.col(m1)).alias('n')<br/>        ).collect()[0]['n']<br/>        <strong class="lc ir">print</strong>(niter, not_done, n_components)</span><span id="dc18" class="lg lh iq lc b gy lm lj l lk ll">    output <strong class="lc ir">=</strong> <!-- -->mins.select(<br/>        f.col(a).alias(label), <br/>        f.col(m1).alias('component')<br/>    )</span><span id="ac2f" class="lg lh iq lc b gy lm lj l lk ll"><strong class="lc ir">    if</strong> <!-- -->max_n <strong class="lc ir">is</strong> <strong class="lc ir">not</strong> <!-- -->None:<br/>        output <strong class="lc ir">=</strong> <!-- -->(<br/>            output<br/>            .withColumn(<br/>                'component',<br/>                f.when(<br/>                    f.col('component') <strong class="lc ir">==</strong> <!-- -->minimum_id, <br/>                    f.lit(None)<br/>                ).otherwise(f.col('component'))<br/>            )<br/>        )</span><span id="b018" class="lg lh iq lc b gy lm lj l lk ll">    output <strong class="lc ir">=</strong> <!-- -->output.persist()</span><span id="f44c" class="lg lh iq lc b gy lm lj l lk ll">    n_components <strong class="lc ir">=</strong> <!-- -->output.select(<br/>        f.countDistinct(f.col('component')).alias('n')<br/>    ).collect()[0]['n']<br/>    <strong class="lc ir">print</strong>('total components:', n_components)</span><span id="661d" class="lg lh iq lc b gy lm lj l lk ll">    <em class="ln"># clean up checkpointed files</em><br/>    <strong class="lc ir">for</strong> <!-- -->filepath <strong class="lc ir">in</strong> <!-- -->used_filepaths:<br/>        subprocess.call(["hdfs", "dfs", "-rm", "-r", filepath])</span><span id="deff" class="lg lh iq lc b gy lm lj l lk ll"><strong class="lc ir">return</strong> <!-- -->output</span></pre></div></div>    
</body>
</html>