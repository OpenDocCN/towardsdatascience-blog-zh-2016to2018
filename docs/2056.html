<html>
<head>
<title>Predicting Employee Turnover</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测员工流动率</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-employee-turnover-7ab2b9ecf47e?source=collection_archive---------1-----------------------#2017-12-11">https://towardsdatascience.com/predicting-employee-turnover-7ab2b9ecf47e?source=collection_archive---------1-----------------------#2017-12-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/017ec9cbf97aec8cb8c872cabab5bd18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oRKwlu787m3gjN-wDEKL_g.png"/></div></div></figure><h1 id="0e2e" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">介绍</h1><p id="5a2a" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="ky ir">员工流动率</strong>是指离开一个组织并被新员工取代的员工的百分比。对组织来说，这是非常昂贵的，费用包括但不限于:离职、空缺、招聘、培训和替换。平均来说，组织投资四周到三个月来培训新员工。如果新员工在第一年就决定离开，这项投资对公司来说将是一项损失。此外，由于定期更换<em class="lu">的客户代表</em>和/或<em class="lu">的顾问</em>，咨询公司等组织的客户满意度会下降，这将导致失去客户。</p><p id="da19" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">在本帖中，我们将研究来自<a class="ae ma" href="https://www.kaggle.com/ludobenistant/hr-analytics" rel="noopener ugc nofollow" target="_blank"> kaggle </a>的模拟人力资源数据，以构建一个分类器，帮助我们预测在给定某些属性的情况下，哪种员工更有可能离职。这种分类器将有助于组织预测雇员流动，并积极主动地帮助解决这种代价高昂的问题。我们将限制自己使用最常见的分类器:随机森林，梯度推进树，K-最近邻，逻辑回归和支持向量机。</p><p id="047c" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">数据有14999个例子(样本)。下面是每一种的功能和定义:</p><ul class=""><li id="51ca" class="mb mc iq ky b kz lv ld lw lh md ll me lp mf lt mg mh mi mj bi translated">satisfaction_level:满意度{ 0–1 }。</li><li id="bcd0" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">last_evaluationTime:自上次绩效评估以来的时间(以年为单位)。</li><li id="066b" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">number_project:工作时完成的项目数。</li><li id="f4d6" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">average_montly_hours:工作场所每月平均工作时间。</li><li id="0bc5" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">time_spend_company:在公司工作的年数。</li><li id="f6ab" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">Work_accident:员工是否发生了工作场所事故。</li><li id="dcb5" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">left:员工是否离开了工作场所{0，1}。</li><li id="6759" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">promotion_last_5years:员工在过去五年中是否获得晋升。</li><li id="9ccb" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">销售:员工工作的部门。</li><li id="1326" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">工资:工资的相对水平{低、中、高}。</li></ul><p id="308a" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">创建这篇文章的源代码可以在<a class="ae ma" href="https://nbviewer.jupyter.org/github/ImadDabbura/blog-posts/blob/master/notebooks/Employee-Turnover.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="efce" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">数据预处理</h1><p id="a397" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">让我们看一下数据(检查是否有缺失值以及每个要素的数据类型):</p><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mv"><img src="../Images/7e0c1d7e5fbe46434735be3aef22b368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9dYxrE0obUIShJzcWde_4Q.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Data oveview</figcaption></figure><p id="4eb3" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">因为没有缺失值，所以我们不必做任何插补。但是，需要一些数据预处理:</p><ol class=""><li id="3ad1" class="mb mc iq ky b kz lv ld lw lh md ll me lp mf lt na mh mi mj bi translated">将<strong class="ky ir">销售</strong>功能名称改为<strong class="ky ir">部门</strong>。</li><li id="cba5" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt na mh mi mj bi translated">将<strong class="ky ir">薪金</strong>转换为<em class="lu">有序分类</em>特征，因为在低、中、高之间有内在的顺序。</li><li id="010b" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt na mh mi mj bi translated">从<strong class="ky ir">部门</strong>特征创建虚拟特征，并删除第一个特征，以避免某些学习算法可能遇到的线性依赖性。</li></ol><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="5182" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">数据现在可以用于建模了。最终的特征数量现在是17个。</p><h1 id="40c2" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">建模</h1><p id="e507" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">让我们先来看看每一类的比例，看看我们处理的是平衡的还是不平衡的数据，因为每一类都有自己的一套工具在拟合分类器时使用。</p><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/64c6231b6eb249e8dc6773237b8971d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*l6KHGCpLeUvsNgK5iYshzg.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Class counts</figcaption></figure><p id="79da" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">如图所示，我们有一个不平衡的数据集。因此，当我们在这样的数据集上拟合分类器时，在比较诸如<em class="lu"> f1-score </em>或<em class="lu">AUC</em>(ROC曲线下面积)等模型时，我们应该使用准确性以外的度量。此外，类不平衡通过隐式学习基于数据集中的多数类优化预测的模型，使决策规则偏向多数类，从而影响训练期间的学习算法。有三种方法可以处理这个问题:</p><ol class=""><li id="b475" class="mb mc iq ky b kz lv ld lw lh md ll me lp mf lt na mh mi mj bi translated">对少数群体的错误预测给予更大的惩罚。</li><li id="465e" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt na mh mi mj bi translated">对少数类进行上采样或对多数类进行下采样。</li><li id="b47f" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt na mh mi mj bi translated">生成合成训练示例。</li></ol><p id="fe9c" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">尽管如此，没有明确的指南或最佳实践来处理这种情况。因此，我们必须尝试所有这些方法，看看哪种方法最适合手头的问题。我们将限制自己使用前两个，即，在分类器中使用<code class="fe nc nd ne nf b">class_weight</code>为来自少数类的错误预测分配更大的惩罚，这允许我们这样做，并评估对训练数据的上采样/下采样，以查看哪一个给出更高的性能。</p><p id="9b04" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">首先，使用80/20拆分将数据拆分为训练集和测试集；80%的数据将用于训练模型，20%用于测试模型的性能。第二，对少数类进行上采样，对多数类进行下采样。对于这个数据集，正类是少数类，负类是多数类。</p><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="3bf9" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">原始形状:(11999，17) (11999，)<br/>上采样形状:(18284，17) (18284，)<br/>下采样形状:(5714，17) (5714，)</p><p id="b83e" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">我认为我们不需要应用PCA之类的降维方法，因为:1)我们想知道每个特征在决定谁会离开与谁不会离开(推断)中的重要性。2)数据集的维数适当(17个特征)。然而，很高兴看到需要多少主成分来解释数据中90%、95%和99%的变化。</p><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/6aceb046082c58aa09f5ac4f8bc61636.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZJDae9tHyrLkBMibDinbXw.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">PCA</figcaption></figure><p id="35c9" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">看起来它需要14、15和16个主成分来分别捕捉数据中90%、95%和99%的变化。换句话说，这意味着数据已经在一个良好的空间中，因为特征值彼此非常接近，并进一步证明我们不需要压缩数据。</p><p id="9ce0" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">我们在构建分类器时将遵循的方法如下:</p><ol class=""><li id="09fd" class="mb mc iq ky b kz lv ld lw lh md ll me lp mf lt na mh mi mj bi translated">使用scikit-learn的<code class="fe nc nd ne nf b">make_pipeline</code>构建一个处理所有步骤的管道，该管道有两个步骤:</li></ol><p id="8f9e" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">I .将数据标准化，以加快收敛速度，并使所有特征具有相同的比例。</p><p id="b09e" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">二。我们想要用来拟合模型的分类器(<code class="fe nc nd ne nf b">estimator</code>)。</p><p id="f74e" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">2.使用<code class="fe nc nd ne nf b">GridSearchCV</code>通过10重交叉验证调整超参数。我们可以使用更快的<code class="fe nc nd ne nf b">RandomizedSearchCV</code>，尤其是如果我们有两个以上的超参数，并且每个超参数的范围都很大时，它可能会优于<code class="fe nc nd ne nf b">GridSearchCV</code>；然而，<code class="fe nc nd ne nf b">GridSearchCV</code>将工作得很好，因为我们只有两个超参数和下降范围。</p><p id="0218" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">3.使用训练数据拟合模型。</p><p id="b244" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">4.使用测试数据为最佳估计量绘制混淆矩阵和ROC曲线。</p><p id="fac8" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">对<em class="lu">随机森林、梯度推进树、K近邻、逻辑回归和支持向量机</em>重复上述步骤。接下来，选择交叉验证f1值最高的分类器。注意:一些超参数范围将由论文<a class="ae ma" href="https://arxiv.org/pdf/1708.05070.pdf" rel="noopener ugc nofollow" target="_blank">将机器学习应用于生物信息学问题的数据驱动建议</a>指导。</p><h1 id="2c91" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">随机森林</h1><p id="01e0" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">首先，我们将从使用<em class="lu">未采样、上采样和下采样</em>数据拟合随机森林分类器开始。其次，我们将使用交叉验证(CV) f1得分评估每种方法，并选择CV f1得分最高的方法。最后，我们将使用该方法来拟合其余的分类器。</p><p id="265f" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">我们将调整的唯一超参数是:</p><ul class=""><li id="3d0a" class="mb mc iq ky b kz lv ld lw lh md ll me lp mf lt mg mh mi mj bi translated"><code class="fe nc nd ne nf b">max_feature</code>:每次分割随机考虑多少个特征。这将有助于避免在每次分割时挑选很少的强特征，并让其他特征有机会做出贡献。因此，预测的相关性将会降低，每棵树的方差将会减少。</li><li id="f8b1" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated"><code class="fe nc nd ne nf b">min_samples_leaf</code>:每次拆分要有多少个实例才能成为最终叶节点。</li></ul><p id="7580" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">随机森林是一个集合模型，有多棵树(<code class="fe nc nd ne nf b">n_estimators</code>)。最终预测将是所有估计者预测的加权平均值(回归)或模式(分类)。注意:大量的树不会导致过度拟合。</p><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/4514b6e0889d548c90234864668e2b5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*pWToEMAqe4JsOMfzBHpTsw.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Random Forest hyperparameter tuning results</figcaption></figure><p id="7c37" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">上采样产生了最高的CV f1值，为99.8%。因此，我们将使用上采样数据来拟合其余的分类器。新数据现在有18，284个例子:50%属于积极类，50%属于消极类。</p><p id="48a8" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">让我们使用上面调整的最佳超参数，用向上采样的数据重新调整随机森林，并使用测试数据绘制混淆矩阵和ROC曲线。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/9943aa20c2aceb15adca10fa425a36ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kx0k9OjhPpTglUDQ5C0EEg.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Random Forest</figcaption></figure><h1 id="c339" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">梯度推进树</h1><p id="52b7" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">梯度增强树与随机森林相同，除了:</p><ul class=""><li id="58e5" class="mb mc iq ky b kz lv ld lw lh md ll me lp mf lt mg mh mi mj bi translated">它从小树开始，并通过考虑成年树的残差从成年树开始学习。</li><li id="dee5" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">更多的树会导致过度拟合；与随机森林相对。</li></ul><p id="ff1c" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">因此，我们可以把每一棵树都看作一个弱学习者。除了<code class="fe nc nd ne nf b">max_features</code>和<code class="fe nc nd ne nf b">n_estimators</code>之外，我们要调整的另外两个超参数是:</p><ul class=""><li id="17ef" class="mb mc iq ky b kz lv ld lw lh md ll me lp mf lt mg mh mi mj bi translated"><code class="fe nc nd ne nf b">learning_rate</code>:速率树学习，越慢越好。</li><li id="121f" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated"><code class="fe nc nd ne nf b">max_depth</code>:每次树生长时的分裂次数，限制了每棵树的节点数。</li></ul><p id="3171" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">让我们使用测试数据拟合GB分类器并绘制混淆矩阵和ROC曲线。</p><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/d9354de2cdbfce7de48a282f74d552ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*KkDKTm6srWczh3bEKbW9Og.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Gradient Boosting Trees hyperparameter tuning results</figcaption></figure><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/01b7a622aa5b6279d960064b26f9020f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0aMfO91rn32wPpW6m_vmKg.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Gradient Boosting Trees</figcaption></figure><h1 id="7405" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">k-最近邻</h1><p id="172d" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">KNN被称为懒惰学习算法，因为它不学习也不适合任何参数。它从训练数据中提取最接近我们感兴趣的点的<code class="fe nc nd ne nf b">n_neighbors</code>点来预测它的类别，并将邻近点的类别的模式(多数投票)作为它的类别。我们要调整的两个超参数是:</p><ul class=""><li id="7b94" class="mb mc iq ky b kz lv ld lw lh md ll me lp mf lt mg mh mi mj bi translated"><code class="fe nc nd ne nf b">n_neighbors</code>:用于预测的邻居数量。</li><li id="b0b2" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated"><code class="fe nc nd ne nf b">weights</code>:根据以下条件为邻居分配多少权重:</li><li id="2cf3" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">“均匀”:所有相邻点的权重相同。</li><li id="526c" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">“距离”:使用预测中使用的每个相邻点的欧几里德距离的倒数。</li></ul><p id="f8c6" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">让我们拟合KNN分类器，并绘制混淆矩阵和ROC曲线。</p><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/f1309e250de05b0ab46cab6ce752467e.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*6VmZG-P-PBy9-lrK_AojvQ.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">K-Nearest Neighbors hyperparameter tuning results</figcaption></figure><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/1b94f06ae9633ce46b5a98dc9e46c15e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zQgNF3L51-2YHP8MRP_UlQ.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">K-Nearest Neighbors</figcaption></figure><h1 id="8ec9" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">逻辑回归</h1><p id="7dd3" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">对于逻辑回归，我们将调整三个超参数:</p><ul class=""><li id="9b97" class="mb mc iq ky b kz lv ld lw lh md ll me lp mf lt mg mh mi mj bi translated"><code class="fe nc nd ne nf b">penalty</code>:正则化类型，L2或L1正则化。</li><li id="2d7b" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated"><code class="fe nc nd ne nf b">C</code>:参数λ正则化的反面。C越高，正则化程度越低。我们将使用覆盖非正则化到完全正则化之间的整个范围的值，其中模型是示例标签的模式。</li><li id="7c72" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated"><code class="fe nc nd ne nf b">fit_intercept</code>:是否包含拦截。</li></ul><p id="811e" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">我们不会使用任何非线性，如多项式特征。</p><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/bde8125e6479883136d090fb55da4687.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*_URQCyj65FjfhzlHU4dW0w.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Logistic Regression hyperparameter tuning results</figcaption></figure><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/8b36b634f0f958e50eefe6b649129711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jiIUOdssR3bMEyeqPMUS5Q.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Logistic Regression</figcaption></figure><h1 id="1f02" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">支持向量机(SVM)</h1><p id="9021" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">SVM调整其超参数的计算成本非常高，原因有二:</p><ol class=""><li id="28ff" class="mb mc iq ky b kz lv ld lw lh md ll me lp mf lt na mh mi mj bi translated">对于大数据集，它变得非常慢。</li><li id="46d2" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt na mh mi mj bi translated">它有大量的超参数需要优化，这需要在CPU上花费很长时间来优化。</li></ol><p id="3213" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">因此，我们将使用我们之前提到的论文中推荐的超参数值，该论文显示在Penn Machine Learning Benchmark 165数据集上产生最佳性能。我们通常希望调整的超参数有:</p><ul class=""><li id="bcf3" class="mb mc iq ky b kz lv ld lw lh md ll me lp mf lt mg mh mi mj bi translated"><code class="fe nc nd ne nf b">C</code>、<code class="fe nc nd ne nf b">gamma</code>、<code class="fe nc nd ne nf b">kernel</code>、<code class="fe nc nd ne nf b">degree</code>和<code class="fe nc nd ne nf b">coef0</code></li></ul><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="91f5" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated"><strong class="ky ir">10倍CV f1分为:96.38% </strong></p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/57654c59857195c10489d48a72cbc4e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6oqS7pscraI3fawErTMbHg.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Support Vector Machine</figcaption></figure><h1 id="371e" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">结论</strong></h1><p id="116f" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">最后，让我们打印出到目前为止我们训练的所有分类器的测试准确率，并绘制ROC曲线。然后我们将挑选ROC曲线下面积最大的分类器。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/c9335dd0c9a9870974d03477cc543c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lJR9dd2yb-W3P0NyKEKZ4g.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Comparing ROC curves for all classifiers</figcaption></figure><p id="c812" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">尽管随机森林和梯度增强树具有几乎相等的AUC，但随机森林具有更高的准确率和f1值，分别为99.27%和99.44%。因此，我们有把握地说，随机森林优于其余的分类器。让我们看看随机森林分类器的特征重要性。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi np"><img src="../Images/b599891cc09ecf11920a8dbc9117aa0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jYO45lrn_f7LOOt9K44ebQ.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">Random Forest feature importance</figcaption></figure><p id="2e0d" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">看起来最重要的五个特征是:</p><ul class=""><li id="1c73" class="mb mc iq ky b kz lv ld lw lh md ll me lp mf lt mg mh mi mj bi translated">满意度_级别</li><li id="b4da" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">时间_花费_公司</li><li id="552b" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">平均每月小时数</li><li id="424e" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">数字_项目</li><li id="7f3e" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">lats _评估</li></ul><p id="905e" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated">带回家的信息如下:</p><ul class=""><li id="6f08" class="mb mc iq ky b kz lv ld lw lh md ll me lp mf lt mg mh mi mj bi translated">在处理不平衡类时，精度不是模型评估的好方法。AUC和f1分数是我们可以使用的指标示例。</li><li id="34a2" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">上采样/下采样、数据合成和使用平衡的类权重是尝试提高不平衡类数据集的分类器精度的好策略。</li><li id="ab21" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated"><code class="fe nc nd ne nf b">GridSearchCV</code>帮助调整每个学习算法的超参数。<code class="fe nc nd ne nf b">RandomizedSearchCV</code>更快，可能会超过<code class="fe nc nd ne nf b">GridSearchCV</code>，尤其是当我们有两个以上的超参数需要优化时。</li><li id="3cc2" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">主成分分析(PCA)并不总是被推荐，特别是如果数据在一个好的特征空间中，并且它们的特征值彼此非常接近。</li><li id="b5b6" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">正如所料，在大多数情况下，集成模型优于其他学习算法。</li></ul><p id="da0e" class="pw-post-body-paragraph kw kx iq ky b kz lv lb lc ld lw lf lg lh lx lj lk ll ly ln lo lp lz lr ls lt ij bi translated"><em class="lu">原载于2017年12月11日</em><a class="ae ma" href="https://imaddabbura.github.io/posts/employee-turnover/Employee-Turnover.html" rel="noopener ugc nofollow" target="_blank"><em class="lu">imaddabbura . github . io</em></a><em class="lu">。</em></p></div></div>    
</body>
</html>