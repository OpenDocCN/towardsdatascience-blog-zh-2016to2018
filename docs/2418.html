<html>
<head>
<title>Fine tuning a classifier in scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 scikit-learn 中微调分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65?source=collection_archive---------0-----------------------#2018-01-24">https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65?source=collection_archive---------0-----------------------#2018-01-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/cc8223e1db7a7f0d69749a64cf672a5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*za9Jt26YZC9CVmqjoGeBjA.jpeg"/></div></div></figure><p id="a46b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">很容易理解，许多机器学习问题受益于作为其最佳性能度量的精确度或召回率，但实现这一概念需要详细过程的知识。我最初几次尝试为回忆(灵敏度)微调模型很困难，所以我决定分享我的经验。</p><p id="aaa4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这篇文章来自我的第一个 Kaggle 内核，我的目的不是构建一个健壮的分类器，而是想展示优化分类器的实用性。在下图 A 中，目标是将决策阈值向左移动。这最大限度地减少了假阴性，这在为本文选择的数据集中尤其麻烦。它包含 357 个良性和 212 个恶性乳腺活检图像的特征。假阴性样本相当于错过了恶性肿瘤的诊断。数据文件可以在<a class="ae kw" href="https://www.kaggle.com/uciml/breast-cancer-wisconsin-data" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi kx"><img src="../Images/9f5543c840917d833713341951933324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*J8kWRRekByVSJi8XDTSsgw.jpeg"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">The goal of this post is to outline how to move the decision threshold to the left in Figure A, reducing false negatives and maximizing sensitivity.</figcaption></figure><p id="eb08" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用 scikit-learn，针对召回调整分类器可以(至少)通过两个主要步骤来实现。</p><ol class=""><li id="d369" class="lg lh iq ka b kb kc kf kg kj li kn lj kr lk kv ll lm ln lo bi translated">使用<code class="fe lp lq lr ls b"><a class="ae kw" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">GridSearchCV</a></code>通过搜索最佳超参数并保留具有最高召回分数的分类器来调整您的模型。</li><li id="f941" class="lg lh iq ka b kb lt kf lu kj lv kn lw kr lx kv ll lm ln lo bi translated">使用精确回忆曲线和 roc 曲线调整决策阈值，这是一个更复杂的方法，我将详细介绍。</li></ol><p id="6323" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先加载必要的库和数据。</p><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="ly lz l"/></div></figure><p id="d8a6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通过统计<code class="fe lp lq lr ls b">diagnosis</code>列可以找到阶级分布。b 代表良性，M 代表恶性。</p><pre class="ky kz la lb gt ma ls mb mc aw md bi"><span id="8a1e" class="me mf iq ls b gy mg mh l mi mj">B    357<br/>M    212<br/>Name: diagnosis, dtype: int64</span></pre><p id="2cc1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">转换类别标签，并将数据分成训练集和测试集。使用<code class="fe lp lq lr ls b">stratify=True</code>的<code class="fe lp lq lr ls b">train_test_split</code>导致训练集和测试集之间的一致类别分布:</p><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="ly lz l"/></div></figure><pre class="ky kz la lb gt ma ls mb mc aw md bi"><span id="6be0" class="me mf iq ls b gy mg mh l mi mj"># show the distribution<br/>print('y_train class distribution')<br/>print(y_train.value_counts(normalize=True))</span><span id="c3b5" class="me mf iq ls b gy mk mh l mi mj">print('y_test class distribution')<br/>print(y_test.value_counts(normalize=True))</span><span id="c8d0" class="me mf iq ls b gy mk mh l mi mj">y_train class distribution<br/>0    0.626761<br/>1    0.373239<br/>Name: diagnosis, dtype: float64<br/>y_test class distribution<br/>0    0.629371<br/>1    0.370629<br/>Name: diagnosis, dtype: float64</span></pre><p id="96ee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">既然已经准备好了数据，就可以构建分类器了。</p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><h1 id="87da" class="ms mf iq bd mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no bi translated">第一个策略:使用 GridSearchCV 和 scoring 参数优化灵敏度。</h1><p id="7ac4" class="pw-post-body-paragraph jy jz iq ka b kb np kd ke kf nq kh ki kj nr kl km kn ns kp kq kr nt kt ku kv ij bi translated">首先建立一个通用分类器并设置一个参数网格；随机森林有许多可调参数，这使它适合于<code class="fe lp lq lr ls b">GridSearchCV</code>。<code class="fe lp lq lr ls b">scorers</code>字典可以用作<code class="fe lp lq lr ls b">GridSearchCV</code>中的<code class="fe lp lq lr ls b">scoring</code>参数。当多个分数通过时，<code class="fe lp lq lr ls b">GridSearchCV.cv_results_</code>将返回所提供的每种分数类型的评分标准。</p><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="ly lz l"/></div></figure><p id="6002" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面的函数使用<a class="ae kw" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank"> GridSearchCV </a>根据<code class="fe lp lq lr ls b">param_grid</code>中的参数组合来拟合几个分类器。来自<code class="fe lp lq lr ls b">scorers</code>的分数被记录下来，最佳模型(由<code class="fe lp lq lr ls b">refit</code>参数评分)将被选择并“改装”成完整的训练数据，供下游使用。这也对伸出的<code class="fe lp lq lr ls b">X_test</code>进行预测，并打印混淆矩阵以显示性能。</p><p id="0e9d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">包装器功能的要点是根据所选择的评分标准类型快速重用代码以适合最佳分类器。首先，试试<code class="fe lp lq lr ls b">precision_score</code>，它可以限制误报的数量。这不太适合最大灵敏度的目标，但允许我们快速显示针对<code class="fe lp lq lr ls b">precision_score</code>优化的分类器和针对<code class="fe lp lq lr ls b">recall_score</code>优化的分类器之间的差异。</p><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="ly lz l"/></div></figure><pre class="ky kz la lb gt ma ls mb mc aw md bi"><span id="a5cb" class="me mf iq ls b gy mg mh l mi mj">grid_search_clf = grid_search_wrapper(refit_score='precision_score')</span><span id="e18a" class="me mf iq ls b gy mk mh l mi mj">Best params for precision_score<br/>{'max_depth': 15, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 300}<br/><br/>Confusion matrix of Random Forest optimized for precision_score on the test data:<br/>     pred_neg  pred_pos<br/>neg        85         5<br/>pos         3        50</span></pre><p id="ce72" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe lp lq lr ls b">param_grid</code>中每个参数组合的精度、召回率和准确度分数都存储在<code class="fe lp lq lr ls b">cv_results_</code>中。这里，pandas 数据框架有助于可视化每次分类器迭代的得分和参数。这是为了表明，尽管分类器之间的准确率可能相对一致，但很明显，准确率和召回率之间存在权衡。按精度排序，最好的评分模型应该是第一条记录。这可以通过查看第一条记录的参数并将其与上面的<code class="fe lp lq lr ls b">grid_search.best_params_</code>进行比较来检查。</p><pre class="ky kz la lb gt ma ls mb mc aw md bi"><span id="c823" class="me mf iq ls b gy mg mh l mi mj">results = pd.DataFrame(grid_search_clf.cv_results_)<br/>results = results.sort_values(by='mean_test_precision_score', ascending=False)</span><span id="270f" class="me mf iq ls b gy mk mh l mi mj">results[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_accuracy_score', 'param_max_depth', 'param_max_features', 'param_min_samples_split', 'param_n_estimators']].round(3).head()</span></pre><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="nu lz l"/></div></figure><p id="db5c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该分类器针对精度进行了优化。为了比较，为了显示<code class="fe lp lq lr ls b">GridSearchCV</code>如何选择最佳分类器，下面的函数调用返回一个为召回优化的分类器。网格可能与上面的网格相似，唯一的不同是召回率最高的分类器将被重新调整。这将是癌症诊断分类问题中最理想的度量，在测试集混淆矩阵上应该有更少的假阴性。</p><pre class="ky kz la lb gt ma ls mb mc aw md bi"><span id="35a8" class="me mf iq ls b gy mg mh l mi mj">grid_search_clf = grid_search_wrapper(refit_score='recall_score')</span><span id="25df" class="me mf iq ls b gy mk mh l mi mj">Best params for recall_score<br/>{'max_depth': 5, 'max_features': 3, 'min_samples_split': 5, 'n_estimators': 100}<br/><br/>Confusion matrix of Random Forest optimized for recall_score on the test data:<br/>     pred_neg  pred_pos<br/>neg        84         6<br/>pos         3        50</span></pre><p id="770b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">再次复制生成结果表的相同代码，只是这一次最好的分数将是<code class="fe lp lq lr ls b">recall</code>。</p><pre class="ky kz la lb gt ma ls mb mc aw md bi"><span id="b990" class="me mf iq ls b gy mg mh l mi mj">results = pd.DataFrame(grid_search_clf.cv_results_)<br/>results = results.sort_values(by='mean_test_precision_score', ascending=False)</span><span id="50fe" class="me mf iq ls b gy mk mh l mi mj">results[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_accuracy_score', 'param_max_depth', 'param_max_features', 'param_min_samples_split', 'param_n_estimators']].round(3).head()</span></pre><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="nu lz l"/></div></figure><p id="5ae6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第一种策略没有为<code class="fe lp lq lr ls b">recall_score</code>产生令人印象深刻的结果，与为<code class="fe lp lq lr ls b">precision_score</code>优化的分类器相比，它没有显著减少(如果有的话)假阴性的数量。理想情况下，在设计癌症诊断测试时，分类器应尽可能减少假阴性。</p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><h1 id="33a9" class="ms mf iq bd mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no bi translated">策略二:调整决策阈值，确定操作点</h1><p id="3011" class="pw-post-body-paragraph jy jz iq ka b kb np kd ke kf nq kh ki kj nr kl km kn ns kp kq kr nt kt ku kv ij bi translated"><code class="fe lp lq lr ls b">precision_recall_curve</code>和<code class="fe lp lq lr ls b">roc_curve</code>是可视化分类器中灵敏度-特异性权衡的有用工具。它们有助于告知数据科学家在何处设置模型的决策阈值，以最大化灵敏度或特异性。这被称为模型的“工作点”。</p><blockquote class="nv nw nx"><p id="49c6" class="jy jz ny ka b kb kc kd ke kf kg kh ki nz kk kl km oa ko kp kq ob ks kt ku kv ij bi translated">理解如何在 scikit-learn 中微调分类器的关键是理解方法<code class="fe lp lq lr ls b">.predict_proba()</code>和<code class="fe lp lq lr ls b">.decision_function()</code>。这些返回样本被预测在一个类中的原始概率。这是与调用<code class="fe lp lq lr ls b">.predict()</code>方法返回的绝对类预测的一个重要区别。</p></blockquote><p id="87d9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了使这种方法适用于 scikit-learn 中的所有分类器，要知道有些分类器(如 RandomForest)使用<code class="fe lp lq lr ls b">.predict_proba()</code>，而有些分类器(如 SVC)使用<code class="fe lp lq lr ls b">.decision_function()</code>。<code class="fe lp lq lr ls b">RandomForestClassifier</code>的默认阈值是 0.5，所以以此为起点。创建一个名为<code class="fe lp lq lr ls b">y_scores</code>的类概率数组。</p><pre class="ky kz la lb gt ma ls mb mc aw md bi"><span id="0f9d" class="me mf iq ls b gy mg mh l mi mj">y_scores = grid_search_clf.predict_proba(X_test)[:, 1]</span><span id="22e4" class="me mf iq ls b gy mk mh l mi mj"># for classifiers with decision_function, this achieves similar results<br/># y_scores = classifier.decision_function(X_test)</span></pre><p id="ae3c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为分类器生成精确召回曲线:</p><pre class="ky kz la lb gt ma ls mb mc aw md bi"><span id="c9e8" class="me mf iq ls b gy mg mh l mi mj">p, r, thresholds = precision_recall_curve(y_test, y_scores)</span></pre><p id="a3a4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里的<code class="fe lp lq lr ls b">adjusted_classes</code>是一个简单的函数，用于返回上面计算的<code class="fe lp lq lr ls b">y_scores</code>的修改版本，只是现在将根据概率阈值<code class="fe lp lq lr ls b">t</code>分配类别标签。下面的另一个函数绘制了相对于给定阈值<code class="fe lp lq lr ls b">t</code>的精度和召回率。</p><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="ly lz l"/></div></figure><p id="6ae9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">多次重复执行该功能，每次改变<code class="fe lp lq lr ls b">t</code>，调整阈值，直到出现 0 个假阴性。在这次特别的运行中，我不得不一直降到 0.29，然后将假阴性降低到 0。</p><pre class="ky kz la lb gt ma ls mb mc aw md bi"><span id="2fec" class="me mf iq ls b gy mg mh l mi mj">precision_recall_threshold(p, r, thresholds, 0.30)</span><span id="a63e" class="me mf iq ls b gy mk mh l mi mj">pred_neg  pred_pos<br/>neg        79        11<br/>pos         1        52</span></pre><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/1196d122a5822a2a4f963aaff7c8b82e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*cGoL45jYxDu9O0Ajx_yGgw.png"/></div></figure><p id="9164" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">查看精确度和召回率之间权衡的另一种方式是将它们一起绘制为决策阈值的函数。</p><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="ly lz l"/></div></figure><pre class="ky kz la lb gt ma ls mb mc aw md bi"><span id="d654" class="me mf iq ls b gy mg mh l mi mj"># use the same p, r, thresholds that were previously calculated<br/>plot_precision_recall_vs_threshold(p, r, thresholds)</span></pre><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/b45716897dffaffb57d7268aa55774a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*kCn4y9dCYJ9yx9xZ13YzdA.png"/></div></figure><p id="36b3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，ROC 曲线显示，为了实现 1.0 的召回，模型的用户必须选择一个允许某些假阳性率&gt; 0.0 的操作点。</p><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="ly lz l"/></div></figure><pre class="ky kz la lb gt ma ls mb mc aw md bi"><span id="4145" class="me mf iq ls b gy mg mh l mi mj">fpr, tpr, auc_thresholds = roc_curve(y_test, y_scores)<br/>print(auc(fpr, tpr)) # AUC of ROC<br/>plot_roc_curve(fpr, tpr, 'recall_optimized')</span><span id="993c" class="me mf iq ls b gy mk mh l mi mj">0.9914046121593292</span></pre><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/20c5adc5559de1cc31e76464bcc3ecc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*tXhR8eh6OprNx3rS-PQXnw.png"/></div></figure><p id="7c8e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">感谢跟随。针对特异性和敏感性调整模型的概念应该更加清晰，并且您应该能够轻松地在您的 scikit-learn 模型中实现这些方法。我很想听到改进代码和/或分类器的建议。</p></div></div>    
</body>
</html>