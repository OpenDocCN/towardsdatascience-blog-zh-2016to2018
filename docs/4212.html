<html>
<head>
<title>Not-So-Deep Reinforcement Learning for dummies — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">虚拟人的非深度强化学习——第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/not-so-deep-reinforcement-learning-for-dummies-part-2-854216d1fe0d?source=collection_archive---------8-----------------------#2018-07-29">https://towardsdatascience.com/not-so-deep-reinforcement-learning-for-dummies-part-2-854216d1fe0d?source=collection_archive---------8-----------------------#2018-07-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f9e1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">马尔可夫决策过程——强化学习初学者指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/8e49688a60f48bd53827672fd62eb59a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0LBL_ca6xeFtDH0B-U6xQw.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">This image is meant to signify an agent trying to decide between two actions. Photo by <a class="ae kv" href="https://unsplash.com/photos/J3JMyXWQHXU?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Caleb Jones</a> on <a class="ae kv" href="https://unsplash.com/search/photos/path?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="f060" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我之前的帖子中，我们讨论了什么是强化学习，关于代理、奖励、状态和环境。在这篇文章中，我们将讨论如何将现实世界的问题表述为<strong class="ky ir">马尔可夫决策过程</strong> ( MDP)，以便我们可以使用强化学习来解决它。在我们开始之前，让我引用《T2 强化学习:导论》一书中的一些例子和可能的应用。</p><blockquote class="ls lt lu"><p id="febd" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">出生几分钟后，一只小瞪羚挣扎着站起来。半小时后，它以每小时 20 英里的速度运行。</p><p id="ed28" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">象棋大师走了一步棋。这种选择是由计划(预测可能的回复和反回复)和对特定位置和移动的愿望的即时、直觉判断两者提供的。</p><p id="c5bf" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">一个移动机器人决定它是否应该进入一个新的房间寻找更多的垃圾来收集，或者开始试图找到回到它的电池充电站的路。它根据电池的当前充电水平以及过去找到充电器的速度和难易程度来做出决定。</p></blockquote><p id="d674" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，让我们试着把移动机器人的第三个例子公式化为 MDP。让我们看看机器人拥有和能够采取的状态和动作。</p><p id="2028" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">状态</strong>—<em class="lv">{高，低} </em>。机器人可以处于高电量状态，不担心充电，专注于<strong class="ky ir">搜寻</strong>垃圾；或者，它可以有一个低电量，并优先<strong class="ky ir">充电</strong>超过搜索。</p><p id="0588" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">行动</strong> —在每个状态中，代理可以决定去<strong class="ky ir">搜索</strong>垃圾，<strong class="ky ir">等待</strong>有人将垃圾带到那里，或者返回充电站<strong class="ky ir">充电</strong>。当电池电量很高时，充电就没有意义了。因此，我们有以下动作集:<em class="lv"> A(高)——{搜索、等待} </em>和<em class="lv"> A(低)——{搜索、等待、充电} </em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lz"><img src="../Images/efc60be46faa8dda05e453f2641a9b3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EmYiXaxLGxvLXFSquBFGrQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">A figure showing the states of the robot, and the transitional probabilities and respective rewards. (Source: <a class="ae kv" href="https://in.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893" rel="noopener ugc nofollow" target="_blank">Udacity</a>)</figcaption></figure><p id="919a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">转移概率&amp;奖励</strong>——转移概率或者他们正式的称呼；<strong class="ky ir">单步动态</strong>，是给定一个动作，从一个状态转换到另一个状态的概率。假设奖励大部分时间是 0，代理每捡一个垃圾获得+1。参考上图，我们来看一些观察结果。</p><p id="b0ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当机器人的状态为<strong class="ky ir">高</strong>时，它可以决定搜索或等待。</p><ol class=""><li id="24a6" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mf mg mh mi bi translated">如果它决定<strong class="ky ir">搜索</strong>(消耗电池)，有 30%的可能性(0.3 的概率)代理可能从高转换到低，有 70%的可能性(0.7 的概率)该状态将保持高。假设在这两种情况下，机器人设法找到了 4 个垃圾，因此环境给了它 4.0 的奖励。</li><li id="72dc" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">当处于状态<strong class="ky ir">高</strong>的机器人决定<strong class="ky ir">等待</strong>时。我们 100%确定(1.0 概率)机器人会保持同样的状态。在等待的时候，假设有人过来给了机器人 1 个垃圾，机器人因此获得了 1 英镑的奖励。</li></ol><p id="d83c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似地，对于状态低，机器人具有它可以采取的动作；<em class="lv"> A(低)——{搜索、等待、充电}。</em></p><ol class=""><li id="3aac" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mf mg mh mi bi translated">如果电池<strong class="ky ir">低</strong>，并且代理决定<strong class="ky ir">搜索</strong>，则电池很有可能(0.8)会耗尽，我们必须进行干预，将代理带到充电站，稍后结束于<strong class="ky ir">高</strong>状态。在这种状态下成功完成搜索操作的概率非常低(0.2)，此后代理仍将保持在<strong class="ky ir">低</strong>状态。在决定搜索时，假设电池耗尽，需要人工干预才能到达充电站。在这里，我们需要惩罚机器人即使在电池电量低的时候也决定搜索，并给它-3 的奖励，无论它是否能够找到任何垃圾。类似地，在决定搜索时，假设它设法找到了 4 个垃圾桶而没有耗尽电池，因此它获得了 4 的奖励。</li><li id="2262" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">如果在这种状态下，代理决定采取动作<strong class="ky ir">再充电</strong>，则有 100%的可能性(1.0 概率)代理在采取该动作后将处于<strong class="ky ir">高</strong>状态。在决定充值时，代理人只是去了充值站，没有发现任何垃圾，因此最终奖励为 0。</li><li id="9a32" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">同样，如果它决定<strong class="ky ir">等待</strong>，代理将肯定(1.0 概率)处于相同的状态<strong class="ky ir">低电平</strong>。与上面类似，假设在等待的时候，有人过来给了机器人 1 个垃圾，机器人因此获得了 1 的奖励。</li></ol><h1 id="e39f" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">那么，什么是马尔可夫决策过程呢？</h1><p id="a718" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">为了使用强化学习解决现实世界中的问题，我们需要指定环境的 MDP，这将清楚地定义我们希望代理解决的问题。我们上面讨论的例子是一个有限 MDP 的例子。形式上，有限 MDP 被定义为—</p><ol class=""><li id="db53" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mf mg mh mi bi translated">状态的有限集合。</li><li id="de4b" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">一组有限的动作。</li><li id="5ec7" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">奖励的有限集合，r。</li><li id="3e34" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">环境的单态动力学。</li><li id="f54f" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">贴现率(我们稍后会讨论这个问题)</li></ol><h1 id="4c9e" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">结论</h1><p id="1d8d" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">总之，这是我们需要用来准备问题的框架，以便我们可以使用强化学习算法来解决它。这适用于间断任务和连续任务。</p><p id="6b54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止，我们已经学习了什么是强化学习(第一部分)，以及如何通过指定环境的 MDP 来正式准备环境。在下一章，我们将讨论如何解决它。敬请关注。</p></div></div>    
</body>
</html>