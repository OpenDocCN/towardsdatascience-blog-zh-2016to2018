<html>
<head>
<title>Andrew Ng’s Machine Learning Course in Python (Logistic Regression)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">吴恩达的 Python(逻辑回归)机器学习课程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-logistic-regression-c0ae25509feb?source=collection_archive---------6-----------------------#2018-12-13">https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-logistic-regression-c0ae25509feb?source=collection_archive---------6-----------------------#2018-12-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/b04838ba86da1c7bb010b59334d2913c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0LX-S4dR7dyeZz62WIHfew.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Machine Learning – Andrew Ng</figcaption></figure><p id="44d7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">继续这个系列，这将是吴恩达关于逻辑回归的机器学习课程的 python 实现。</p><p id="f2af" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi ld translated">与标签为连续变量的线性回归相比，逻辑回归用于标签为离散类别数的分类问题。</p></div><div class="ab cl lm ln hx lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="im in io ip iq"><p id="4606" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">和往常一样，我们从导入库和数据集开始。这个数据集包含 2 个不同的学生考试成绩和他们的大学录取状态。我们被要求根据学生的考试成绩来预测他们是否被大学录取。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="8781" class="mc md it ly b gy me mf l mg mh">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span><span id="d3d8" class="mc md it ly b gy mi mf l mg mh">df=pd.read_csv("ex2data1.txt",header=None)</span></pre><p id="80bf" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">理解数据</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="92f2" class="mc md it ly b gy me mf l mg mh">df.head()<br/>df.describe()</span></pre><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mj"><img src="../Images/11bcb813905d41f46e286b4b83ff60ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7q7MBD-0XZTF2vK4jYuNSA.png"/></div></div></figure><p id="740a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">数据绘图</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="a64c" class="mc md it ly b gy me mf l mg mh">pos , neg = (y==1).reshape(100,1) , (y==0).reshape(100,1)<br/>plt.scatter(X[pos[:,0],0],X[pos[:,0],1],c="r",marker="+")<br/>plt.scatter(X[neg[:,0],0],X[neg[:,0],1],marker="o",s=10)<br/>plt.xlabel("Exam 1 score")<br/>plt.ylabel("Exam 2 score")<br/>plt.legend(["Admitted","Not admitted"],loc=0)</span></pre><p id="0400" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">由于这不是标准的散点图或线图，为了便于理解，我将逐步分解代码。对于分类问题，我们将独立变量彼此相对绘制，并识别不同的类别以观察它们之间的关系。因此，我们需要区分导致大学录取的 x1 和 x2 的组合和没有导致大学录取的组合。变量<code class="fe mk ml mm ly b">pos</code>和<code class="fe mk ml mm ly b">neg</code>就是这样做的。通过用不同的颜色和标记画出被大学录取的 x1 和 x2 的组合，我们成功地将这种关系可视化。</p><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mn"><img src="../Images/628f0def62c0aacb52ac7c3eac403a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9PDmIMtJjGupzwmcDOeR7g.png"/></div></div></figure><p id="a2d7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">两次考试分数都较高的学生果然被大学录取了。</p><p id="6350" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">区分逻辑回归和线性回归的 sigmoid 函数</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="a0b9" class="mc md it ly b gy me mf l mg mh">def sigmoid(z):<br/>    """<br/>    return the sigmoid of z<br/>    """<br/>    <br/>    return 1/ (1 + np.exp(-z))</span><span id="c3b2" class="mc md it ly b gy mi mf l mg mh"># testing the sigmoid function<br/>sigmoid(0)</span></pre><p id="cdfa" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">运行<code class="fe mk ml mm ly b">sigmoid(0)</code>功能返回 0.5</p><p id="9fe3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">计算成本函数 J(θ)和梯度(J(θ)相对于每个θ的偏导数)</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="8e5c" class="mc md it ly b gy me mf l mg mh">def costFunction(theta, X, y):<br/>    """<br/>    Takes in numpy array theta, x and y and return the logistic regression cost function and gradient<br/>    """<br/>    <br/>    m=len(y)<br/>    <br/>    predictions = sigmoid(np.dot(X,theta))<br/>    error = (-y * np.log(predictions)) - ((1-y)*np.log(1-predictions))</span><span id="c7ac" class="mc md it ly b gy mi mf l mg mh">cost = 1/m * sum(error)<br/>    <br/>    grad = 1/m * np.dot(X.transpose(),(predictions - y))<br/>    <br/>    return cost[0] , grad</span></pre><p id="0668" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">设置 initial_theta 并测试成本函数</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="6892" class="mc md it ly b gy me mf l mg mh">m , n = X.shape[0], X.shape[1]<br/>X= np.append(np.ones((m,1)),X,axis=1)<br/>y=y.reshape(m,1)<br/>initial_theta = np.zeros((n+1,1))<br/>cost, grad= costFunction(initial_theta,X,y)<br/>print("Cost of initial theta is",cost)<br/>print("Gradient at initial theta (zeros):",grad)</span></pre><p id="1a38" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">打印语句将打印:初始角增量的成本是<code class="fe mk ml mm ly b">0.693147180559946</code>初始角增量处的梯度(零):<code class="fe mk ml mm ly b">[-0.1],[-12.00921659],[-11.26284221]</code></p><p id="8b62" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在是优化算法。在任务本身中，我们被告知使用 Octave 中的<code class="fe mk ml mm ly b">fminunc</code>函数来寻找无约束函数的最小值。至于 python 实现，有一个库可以用于类似的目的。你可以在这里找到官方文档<a class="ae mo" href="https://docs.scipy.org/doc/scipy-0.10.0/reference/tutorial/optimize.html" rel="noopener ugc nofollow" target="_blank"/>。有各种各样优化方法可供选择，在我之前，许多其他人都在他们的 python 实现中使用过这些方法。在这里，我决定使用梯度下降来进行优化，并将结果与 Octave 中的<code class="fe mk ml mm ly b">fminunc</code>进行比较。</p><p id="d636" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在进行梯度下降之前，永远不要忘记对多元问题进行特征缩放。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="2f9f" class="mc md it ly b gy me mf l mg mh">def featureNormalization(X):<br/>    """<br/>    Take in numpy array of X values and return normalize X values,<br/>    the mean and standard deviation of each feature<br/>    """<br/>    mean=np.mean(X,axis=0)<br/>    std=np.std(X,axis=0)<br/>    <br/>    X_norm = (X - mean)/std<br/>    <br/>    return X_norm , mean , std</span></pre><p id="b53f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">正如讲座中提到的，梯度下降算法与线性回归非常相似。唯一的区别是假设<em class="mp"> h(x) </em>现在是 g(θ^tx)其中 g 是 sigmoid 函数。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="69da" class="mc md it ly b gy me mf l mg mh">def gradientDescent(X,y,theta,alpha,num_iters):<br/>    """<br/>    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps<br/>    with learning rate of alpha<br/>    <br/>    return theta and the list of the cost of theta during each iteration<br/>    """<br/>    <br/>    m=len(y)<br/>    J_history =[]<br/>    <br/>    for i in range(num_iters):<br/>        cost, grad = costFunction(theta,X,y)<br/>        theta = theta - (alpha * grad)<br/>        J_history.append(cost)<br/>    <br/>    return theta , J_history</span></pre><p id="a94c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我一直很喜欢干(不要重复自己)在编码上的说法。由于我们之前已经有了一个计算梯度的函数，我们就不重复计算了，在这里添加一个α项来更新θ。</p><p id="3586" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">由于作业没有实现梯度下降，我不得不测试一些 alpha 和 num_iters 值来找到最佳值。</p><p id="ba72" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">使用<code class="fe mk ml mm ly b">alpha=0.01, num_iters=400</code>，</p><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mj"><img src="../Images/ce5059e1899146b75ff20da2e81ca324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IvEmYvINS4DrzCvfsHSUUQ.png"/></div></div></figure><p id="26c0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">梯度下降在每次迭代中降低成本函数，但是我们可以做得更好。用<code class="fe mk ml mm ly b">alpha = 0.1, num_iters =400</code>，</p><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/c34bcf20f4d984ade753193f42dab380.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*8DAw71bU_gxL8JbbueR5Eg.png"/></div></figure><p id="f88a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">好多了，但我会尝试另一个值，只是为了确保。与<code class="fe mk ml mm ly b">alpha=1, num_iters=400</code>，</p><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/069faf81d1ab81a4a93135577e299da7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*1iA_SkqQSbgvAcbrq6kMiw.png"/></div></figure><p id="702c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">下降更剧烈，并且成本函数在 150 次迭代左右达到平稳。使用这个 alpha 和 num_iters 值，优化的θ是<code class="fe mk ml mm ly b">[1.65947664],[3.8670477],[3.60347302]</code>，结果成本是<code class="fe mk ml mm ly b">0.20360044248226664</code>。与最初的<code class="fe mk ml mm ly b">0.693147180559946</code>相比有了显著的改进。与 Octave 中使用<code class="fe mk ml mm ly b">fminunc</code>的优化成本函数相比，它与任务中获得的<code class="fe mk ml mm ly b">0.203498</code>相差不远。</p><p id="b740" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来是使用优化的θ绘制决策边界。在课程资源中有关于如何绘制决策边界的逐步解释。链接可以在<a class="ae mo" href="https://www.coursera.org/learn/machine-learning/resources/fz4AU" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="9437" class="mc md it ly b gy me mf l mg mh">plt.scatter(X[pos[:,0],1],X[pos[:,0],2],c="r",marker="+",label="Admitted")<br/>plt.scatter(X[neg[:,0],1],X[neg[:,0],2],c="b",marker="x",label="Not admitted")<br/>x_value= np.array([np.min(X[:,1]),np.max(X[:,1])])<br/>y_value=-(theta[0] +theta[1]*x_value)/theta[2]<br/>plt.plot(x_value,y_value, "r")<br/>plt.xlabel("Exam 1 score")<br/>plt.ylabel("Exam 2 score")<br/>plt.legend(loc=0)</span></pre><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/e1ad8e67eb8cb1788fc6c07c608dddf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*PZLH-OskNmmRBkxCTSl6_g.png"/></div></figure><p id="e5f1" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">使用优化的 theta 进行预测</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="2820" class="mc md it ly b gy me mf l mg mh">x_test = np.array([45,85])<br/>x_test = (x_test - X_mean)/X_std<br/>x_test = np.append(np.ones(1),x_test)<br/>prob = sigmoid(x_test.dot(theta))<br/>print("For a student with scores 45 and 85, we predict an admission probability of",prob[0])</span></pre><p id="c652" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">打印语句将打印:对于一个分数为 45 和 85 的学生，我们预测录取概率为<code class="fe mk ml mm ly b">0.7677628875792492</code>。使用<code class="fe mk ml mm ly b">fminunc</code>非常接近<code class="fe mk ml mm ly b">0.776291</code>。</p><p id="a424" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了找到分类器的准确性，我们计算训练集上正确分类的百分比。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="65ca" class="mc md it ly b gy me mf l mg mh">def classifierPredict(theta,X):<br/>    """<br/>    take in numpy array of theta and X and predict the class <br/>    """<br/>    predictions = X.dot(theta)<br/>    <br/>    return predictions&gt;0</span><span id="b235" class="mc md it ly b gy mi mf l mg mh">p=classifierPredict(theta,X)<br/>print("Train Accuracy:", sum(p==y)[0],"%")</span></pre><p id="5cf2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果被大学录取的概率大于 0.5，函数<code class="fe mk ml mm ly b">classifierPredict </code>返回一个带有<code class="fe mk ml mm ly b">True</code>的布尔数组，否则返回<code class="fe mk ml mm ly b">False</code>。取<code class="fe mk ml mm ly b">sum(p==y)</code>将所有正确预测 y 值的实例相加。</p><p id="ea51" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">print 语句 print: Train Accuracy: 89 %，表明我们的分类器正确预测了 89 %的训练集。</p></div><div class="ab cl lm ln hx lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="im in io ip iq"><p id="8101" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这都是为了逻辑回归。像往常一样，Jupyter 笔记本上传到我的 GitHub，网址是(<a class="ae mo" href="https://github.com/Benlau93/Machine-Learning-by-Andrew-Ng-in-Python" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/Ben lau 93/Machine-Learning-by-Andrew-Ng-in-Python</a>)。</p><p id="76f2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于本系列中的其他 python 实现，</p><ul class=""><li id="4ab9" class="mt mu it kh b ki kj km kn kq mv ku mw ky mx lc my mz na nb bi translated"><a class="ae mo" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-linear-regression-dd04fba8e137" rel="noopener">线性回归</a></li><li id="e048" class="mt mu it kh b ki nc km nd kq ne ku nf ky ng lc my mz na nb bi translated"><a class="ae mo" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-regularized-logistic-regression-lasso-regression-721f311130fb" rel="noopener">正则化逻辑回归</a></li><li id="b29c" class="mt mu it kh b ki nc km nd kq ne ku nf ky ng lc my mz na nb bi translated"><a class="ae mo" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-neural-networks-e526b41fdcd9" rel="noopener">神经网络</a></li><li id="2132" class="mt mu it kh b ki nc km nd kq ne ku nf ky ng lc my mz na nb bi translated"><a class="ae mo" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-support-vector-machines-435fc34b7bf9" rel="noopener">支持向量机</a></li><li id="6a6e" class="mt mu it kh b ki nc km nd kq ne ku nf ky ng lc my mz na nb bi translated"><a class="ae mo" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-kmeans-clustering-pca-b7ba6fafa74" rel="noopener">无监督学习</a></li><li id="e850" class="mt mu it kh b ki nc km nd kq ne ku nf ky ng lc my mz na nb bi translated"><a class="ae mo" href="https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-anomaly-detection-1233d23dba95" rel="noopener">异常检测</a></li></ul><p id="a72e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">感谢您的阅读。</p></div></div>    
</body>
</html>