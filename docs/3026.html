<html>
<head>
<title>Coding Neural Network — Forward Propagation and Backpropagtion</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">编码神经网络——前向传播和反向传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76?source=collection_archive---------1-----------------------#2018-04-01">https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76?source=collection_archive---------1-----------------------#2018-04-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="179a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">为什么是神经网络？</strong></p><p id="6975" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">根据<em class="kl">通用逼近定理</em>，给定足够大的层和期望的误差范围，神经网络可以逼近、学习和表示任何函数。神经网络学习真正功能的方式是在简单的基础上建立复杂的表示。在每个隐藏层上，神经网络通过首先计算给定输入的仿射(线性)变换，然后应用非线性函数来学习新的特征空间，该非线性函数又将成为下一层的输入。这一过程将继续下去，直到我们到达输出层。因此，我们可以将神经网络定义为从输入端通过隐含层流向输出端的信息流。对于一个三层神经网络，学习的函数将是:<em class="kl"> f(x) = f_3(f_2(f_1(x))) </em>其中:</p><ul class=""><li id="897e" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated"><em class="kl">f1(x)</em>:在第一个隐藏层学习的函数</li><li id="a56b" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated"><em class="kl">F2(x)</em>:在第二隐藏层学习的函数</li><li id="cd4e" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated"><em class="kl">F3(x)</em>:在输出层学习的功能</li></ul><p id="c640" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，在每一层上，我们学习不同的表现，随着后面的隐藏层变得更加复杂。下面是一个3层神经网络的例子(我们不考虑输入层):</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi la"><img src="../Images/644a1d4a5812c44d52dafd499b4adc59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hzIQ5Fs-g8iBpVWq.jpg"/></div></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk"><strong class="bd lq">Figure 1:</strong> Neural Network with two hidden layers</figcaption></figure><p id="2003" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">比如计算机不能直接理解图像，不知道如何处理像素数据。然而，神经网络可以在识别边缘的早期隐藏层中建立图像的简单表示。给定第一个隐藏层输出，它可以学习拐角和轮廓。给定第二个隐藏层，它可以学习鼻子等部位。最后，它可以学习对象身份。</p><p id="4af4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于<strong class="jp ir">真相从来都不是线性的</strong>并且表示对于机器学习算法的性能非常关键，神经网络可以帮助我们建立非常复杂的模型，并将其留给算法来学习这种表示，而不必担心特征工程，这需要从业者花费非常长的时间和精力来策划一个好的表示。</p><p id="6bfb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章有两部分:</p><ol class=""><li id="8825" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk lr ks kt ku bi translated">神经网络编码:这需要编写所有的助手函数，使我们能够实现一个多层神经网络。在这样做的时候，我会尽可能地解释理论部分，并给出一些实现上的建议。</li><li id="7d4f" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk lr ks kt ku bi translated">应用:我们将实现我们在第一部分中编写的关于图像识别问题的神经网络，看看我们构建的网络是否能够检测图像中是否有猫或狗，并看到它在工作:)</li></ol><p id="8765" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章将是一系列文章中的第一篇，涵盖了在numpy中实现神经网络，包括<em class="kl">梯度检查，参数初始化，L2正则化，丢失</em>。创建这篇文章的源代码可以在<a class="ae ls" href="https://nbviewer.jupyter.org/github/ImadDabbura/blog-posts/blob/master/notebooks/Coding-Neural-Network-Forwad-Back-Propagation.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="lb lc ld le gt lt lu lv lw aw lx bi"><span id="ec83" class="ly lz iq lu b gy ma mb l mc md"># Import packages<br/>import h5py import<br/>matplotlib.pyplot as plt<br/>import numpy as np<br/>import seaborn as sns</span></pre><h1 id="3042" class="me lz iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">I .神经网络编码</h1><h1 id="d2ee" class="me lz iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">正向传播</h1><p id="8fc7" class="pw-post-body-paragraph jn jo iq jp b jq nb js jt ju nc jw jx jy nd ka kb kc ne ke kf kg nf ki kj kk ij bi translated">输入<em class="kl"> X </em>提供初始信息，然后传播到每层的隐藏单元，并最终产生输出y^.。网络的架构需要确定其深度、宽度和每层使用的激活函数。<strong class="jp ir">深度</strong>是隐藏层数。<strong class="jp ir">宽度</strong>是每个隐藏层上的单元(节点)数量，因为我们不控制输入层和输出层的尺寸。有相当多组激活函数，如<em class="kl">整流线性单元、Sigmoid、双曲线正切等</em>。研究已经证明，更深的网络优于具有更多隐藏单元的网络。因此，培养一个更深层次的关系网(收益递减)总是更好，也不会有坏处。</p><p id="b9cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们首先介绍一些将在本文中使用的符号:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi ng"><img src="../Images/eb1c3808b6dca1b9d305be5348cb260d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eJyGOkakBkSC466ilLGslQ.png"/></div></div></figure><p id="3799" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们将以通用形式写下多层神经网络的维度，以帮助我们进行矩阵乘法，因为实现神经网络的主要挑战之一是获得正确的维度。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi nh"><img src="../Images/df1d142dc02bc56ce869dd098430370d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oM7DK8Yf8ev-pjpBTErNHg.png"/></div></div></figure><p id="e4fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们需要实现前向传播的两个方程是:这些计算将发生在每一层上。</p><h1 id="188d" class="me lz iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">参数初始化</h1><p id="5b9b" class="pw-post-body-paragraph jn jo iq jp b jq nb js jt ju nc jw jx jy nd ka kb kc ne ke kf kg nf ki kj kk ij bi translated">我们将首先初始化权重矩阵和偏差向量。值得注意的是，我们不应该将所有参数初始化为零，因为这样做会导致梯度相等，并且每次迭代的输出都是相同的，学习算法不会学到任何东西。因此，将参数随机初始化为0到1之间的值很重要。还建议将随机值乘以小标量，如0.01，以使激活单元处于活动状态，并位于激活函数导数不接近零的区域。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="ni nj l"/></div></figure><h1 id="5edc" class="me lz iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">激活功能</h1><p id="d5a2" class="pw-post-body-paragraph jn jo iq jp b jq nb js jt ju nc jw jx jy nd ka kb kc ne ke kf kg nf ki kj kk ij bi translated">对于哪种激活功能在特定问题上效果最好，没有明确的指导。这是一个反复试验的过程，人们应该尝试不同的功能集，看看哪一个最适合手头的问题。我们将介绍4种最常用的激活功能:</p><ul class=""><li id="b692" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated"><strong class="jp ir">乙状结肠函数(</strong>σ<strong class="jp ir">)</strong>:<em class="kl">g(z)</em>=<em class="kl">1/(1+e^{-z})</em>。建议仅在输出图层上使用，这样我们可以很容易地将输出解释为概率，因为它将输出限制在0和1之间。在隐藏层上使用sigmoid函数的一个主要缺点是，在其域的大部分上，梯度非常接近于零，这使得学习算法学习起来缓慢且困难。</li><li id="39ef" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated"><strong class="jp ir">双曲正切函数</strong>:<em class="kl">g(z)</em>=<em class="kl">(e^z -e^{-z})/(e^z+e^{-z})</em>。它优于sigmoid函数，在sigmoid函数中，其输出的平均值非常接近零，换句话说，它将激活单元的输出集中在零附近，并使值的范围非常小，这意味着学习速度更快。它与sigmoid函数共有的缺点是在域的好的部分上梯度非常小。</li><li id="32b5" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated"><strong class="jp ir">整流线性单元(ReLU)</strong>:<em class="kl">g(z)</em><em class="kl">= max { 0，z} </em>。接近线性的模型易于优化。由于ReLU与线性函数有许多相同的性质，它在大多数问题上都表现得很好。唯一的问题是导数没有定义在<em class="kl"> z = 0 </em>，我们可以通过在<em class="kl"> z = 0 </em>将导数赋值为0来解决这个问题。然而，这意味着对于z ≤ 0，梯度为零，再次无法学习。</li><li id="83ea" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated"><strong class="jp ir">漏整流线性单元</strong>:<em class="kl">g(z)</em>=<em class="kl">max {α* z，z} </em>。它克服了ReLU的零梯度问题，并为<em class="kl"> z </em> ≤ 0指定了一个小值<em class="kl"> α </em>。</li></ul><p id="b991" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你不确定选择哪个激活函数，从ReLU开始。接下来，我们将实现上面的激活函数，并为每个函数绘制一个图表，以便更容易地看到每个函数的域和范围。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="ni nj l"/></div></figure><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="ni nj l"/></div></figure><h1 id="a47e" class="me lz iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">正向输送</h1><p id="2dbe" class="pw-post-body-paragraph jn jo iq jp b jq nb js jt ju nc jw jx jy nd ka kb kc ne ke kf kg nf ki kj kk ij bi translated">给定来自前一层的输入，每个单元计算仿射变换<em class="kl"> z = W^Tx + b </em>，然后应用激活函数<em class="kl"> g(z) </em>，例如ReLU元素方式。在这个过程中，我们将存储(缓存)在每一层上计算和使用的所有变量，以便在反向传播中使用。我们将编写将在L模型正向传播中使用的前两个助手函数，以使其更易于调试。请记住，在每一层上，我们可能有不同的激活函数。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="ni nj l"/></div></figure><h1 id="69c5" class="me lz iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">费用</h1><p id="f784" class="pw-post-body-paragraph jn jo iq jp b jq nb js jt ju nc jw jx jy nd ka kb kc ne ke kf kg nf ki kj kk ij bi translated">我们将使用二元<strong class="jp ir">交叉熵</strong>成本。它使用对数似然法来估计其误差。代价是:上述代价函数是凸的；然而，神经网络通常会陷入局部极小值，不能保证找到最优参数。我们将在这里使用基于梯度的学习。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="ni nj l"/></div></figure><h1 id="86ed" class="me lz iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">反向传播</h1><p id="08dc" class="pw-post-body-paragraph jn jo iq jp b jq nb js jt ju nc jw jx jy nd ka kb kc ne ke kf kg nf ki kj kk ij bi translated">允许信息通过网络从成本回溯，以计算梯度。因此，以相反的拓扑顺序从最终节点开始循环遍历节点，以计算最终节点输出相对于每条边的节点尾部的导数。这样做将帮助我们知道谁对最大的错误负责，并在那个方向上改变参数。下面的导数公式将帮助我们编写反向传播函数:因为<em class="kl"> b^l </em>总是一个向量，所以总和将是跨行的(因为每一列都是一个例子)。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="ni nj l"/></div></figure><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="ni nj l"/></div></figure><h1 id="b484" class="me lz iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">二。应用</h1><p id="ad7d" class="pw-post-body-paragraph jn jo iq jp b jq nb js jt ju nc jw jx jy nd ka kb kc ne ke kf kg nf ki kj kk ij bi translated">我们将要处理的数据集有209张图片。每幅图像的RGB比例为64 x 64像素。我们将建立一个神经网络来分类图像是否有猫。因此，<em class="kl"/>∈<em class="kl">{ 0，1} </em>。</p><ul class=""><li id="11bb" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">我们将首先加载图像。</li><li id="1d29" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">显示猫的样本图像。</li><li id="f4ca" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">改变输入矩阵的形状，使每一列都成为一个示例。此外，由于每张图片的大小为64 x 64 x 3，因此每张图片有12，288个特征。因此，输入矩阵应为12，288 x 209。</li><li id="6285" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">将数据标准化，这样梯度就不会失控。此外，它将有助于隐藏单位有类似的价值范围。现在，我们将每个像素除以255，这应该不成问题。但是，最好将数据标准化为平均值为0，标准差为1。</li></ul><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="ni nj l"/></div></figure><pre class="lb lc ld le gt lt lu lv lw aw lx bi"><span id="9b79" class="ly lz iq lu b gy ma mb l mc md">Original dimensions:<br/>--------------------<br/>Training: (209, 64, 64, 3), (209,)<br/>Test: (50, 64, 64, 3), (50,)</span><span id="9327" class="ly lz iq lu b gy nk mb l mc md">New dimensions:<br/>---------------<br/>Training: (12288, 209), (1, 209)<br/>Test: (12288, 50), (1, 50)</span></pre><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/987660d3e2769abbfa8047937ff4c34d.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/0*9gUGxKkZneMIbWIu.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk"><strong class="bd lq">Figure 3:</strong> Sample image</figcaption></figure><p id="c4a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们的数据集已准备好用于测试我们的神经网络实现。先写<strong class="jp ir">多层模型</strong>函数，用预定义的迭代次数和学习速率实现基于梯度的学习。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="4473" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们将训练两个版本的神经网络，其中每个将在隐藏层上使用不同的激活函数:一个将使用整流线性单元(<strong class="jp ir"> ReLU </strong>)，第二个将使用双曲正切函数(<strong class="jp ir"> tanh </strong>)。最后，我们将使用从两个神经网络中获得的参数对训练示例进行分类，并计算每个版本的训练准确率，以查看哪个激活函数在这个问题上效果最好。</p><pre class="lb lc ld le gt lt lu lv lw aw lx bi"><span id="c9cf" class="ly lz iq lu b gy ma mb l mc md"># Setting layers dims<br/>layers_dims = [X_train.shape[0], 5, 5, 1]</span><span id="f11b" class="ly lz iq lu b gy nk mb l mc md"># NN with tanh activation fn<br/>parameters_tanh = L_layer_model( X_train, y_train, layers_dims, learning_rate=0.03, num_iterations=3000, hidden_layers_activation_fn="tanh")</span><span id="dac9" class="ly lz iq lu b gy nk mb l mc md"># Print the accuracy<br/>accuracy(X_test, parameters_tanh, y_test, activation_fn="tanh")</span><span id="445e" class="ly lz iq lu b gy nk mb l mc md">The cost after 100 iterations is: 0.6556 <br/>The cost after 200 iterations is: 0.6468<br/>The cost after 300 iterations is: 0.6447<br/>The cost after 400 iterations is: 0.6441<br/>The cost after 500 iterations is: 0.6440<br/>The cost after 600 iterations is: 0.6440<br/>The cost after 700 iterations is: 0.6440<br/>The cost after 800 iterations is: 0.6439<br/>The cost after 900 iterations is: 0.6439<br/>The cost after 1000 iterations is: 0.6439<br/>The cost after 1100 iterations is: 0.6439<br/>The cost after 1200 iterations is: 0.6439<br/>The cost after 1300 iterations is: 0.6438<br/>The cost after 1400 iterations is: 0.6438<br/>The cost after 1500 iterations is: 0.6437<br/>The cost after 1600 iterations is: 0.6434<br/>The cost after 1700 iterations is: 0.6429<br/>The cost after 1800 iterations is: 0.6413<br/>The cost after 1900 iterations is: 0.6361<br/>The cost after 2000 iterations is: 0.6124<br/>The cost after 2100 iterations is: 0.5112<br/>The cost after 2200 iterations is: 0.5288<br/>The cost after 2300 iterations is: 0.4312<br/>The cost after 2400 iterations is: 0.3821<br/>The cost after 2500 iterations is: 0.3387<br/>The cost after 2600 iterations is: 0.2349<br/>The cost after 2700 iterations is: 0.2206<br/>The cost after 2800 iterations is: 0.1927<br/>The cost after 2900 iterations is: 0.4669<br/>The cost after 3000 iterations is: 0.1040 </span><span id="abe6" class="ly lz iq lu b gy nk mb l mc md">'The accuracy rate is: 68.00%.'</span></pre><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/d423cbb9be336b05997491dc37c166c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/0*iS4jIq0zdJ7pKYfd.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk"><strong class="bd lq">Figure 4:</strong> Loss curve with tanh activation function</figcaption></figure><pre class="lb lc ld le gt lt lu lv lw aw lx bi"><span id="63ad" class="ly lz iq lu b gy ma mb l mc md"># NN with relu activation fn<br/>parameters_relu = L_layer_model( X_train, y_train, layers_dims, learning_rate=0.03, num_iterations=3000, hidden_layers_activation_fn="relu")</span><span id="aa76" class="ly lz iq lu b gy nk mb l mc md"># Print the accuracy<br/>accuracy(X_test, parameters_relu, y_test, activation_fn="relu")</span><span id="406e" class="ly lz iq lu b gy nk mb l mc md">The cost after 100 iterations is: 0.6556<br/>The cost after 200 iterations is: 0.6468<br/>The cost after 300 iterations is: 0.6447<br/>The cost after 400 iterations is: 0.6441<br/>The cost after 500 iterations is: 0.6440<br/>The cost after 600 iterations is: 0.6440 <br/>The cost after 700 iterations is: 0.6440 <br/>The cost after 800 iterations is: 0.6440 <br/>The cost after 900 iterations is: 0.6440 <br/>The cost after 1000 iterations is: 0.6440 <br/>The cost after 1100 iterations is: 0.6439 <br/>The cost after 1200 iterations is: 0.6439 <br/>The cost after 1300 iterations is: 0.6439 <br/>The cost after 1400 iterations is: 0.6439 <br/>The cost after 1500 iterations is: 0.6439 <br/>The cost after 1600 iterations is: 0.6439 <br/>The cost after 1700 iterations is: 0.6438 <br/>The cost after 1800 iterations is: 0.6437 <br/>The cost after 1900 iterations is: 0.6435 <br/>The cost after 2000 iterations is: 0.6432 <br/>The cost after 2100 iterations is: 0.6423 <br/>The cost after 2200 iterations is: 0.6395 <br/>The cost after 2300 iterations is: 0.6259 <br/>The cost after 2400 iterations is: 0.5408 <br/>The cost after 2500 iterations is: 0.5262 <br/>The cost after 2600 iterations is: 0.4727 <br/>The cost after 2700 iterations is: 0.4386 <br/>The cost after 2800 iterations is: 0.3493 <br/>The cost after 2900 iterations is: 0.1877 <br/>The cost after 3000 iterations is: 0.3641</span><span id="c0ca" class="ly lz iq lu b gy nk mb l mc md">'The accuracy rate is: 42.00%.'</span></pre><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/80118399d2d0f5a04ba03dc526f141c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/0*mIpo_T_3vw9gpZtN.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk"><strong class="bd lq">Figure 5:</strong> Loss curve with ReLU activation function</figcaption></figure><p id="463b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，上述准确率预计会高估概化准确率。</p><h1 id="7152" class="me lz iq bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">结论</h1><p id="6feb" class="pw-post-body-paragraph jn jo iq jp b jq nb js jt ju nc jw jx jy nd ka kb kc ne ke kf kg nf ki kj kk ij bi translated">这篇文章的目的是一步一步地编写深度神经网络，并解释其中的重要概念。我们现在并不关心准确率，因为我们可以做很多事情来提高准确率，这将是后续帖子的主题。以下是一些要点:</p><ul class=""><li id="f405" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">即使神经网络可以表示任何函数，它也可能因为两个原因而无法学习:</li></ul><ol class=""><li id="badf" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk lr ks kt ku bi translated">优化算法可能无法找到期望(真实)函数的参数的最佳值。它会陷入局部最优。</li><li id="d870" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk lr ks kt ku bi translated">由于过拟合，学习算法可能发现不同于预期函数的不同函数形式。</li></ol><ul class=""><li id="cbd2" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">即使神经网络很少收敛，总是陷入局部极小值，它仍然能够显著降低成本，并以高测试精度提出非常复杂的模型。</li><li id="a1fe" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">我们在这篇文章中使用的神经网络是标准的全连接网络。然而，还有另外两种网络:</li></ul><ol class=""><li id="174e" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk lr ks kt ku bi translated">卷积神经网络:不是所有的节点都连接在一起。图像识别类最好。</li><li id="a5b2" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk lr ks kt ku bi translated">递归神经网络:有一个反馈连接，模型的输出反馈到它自己。它主要用于序列建模。</li></ol><ul class=""><li id="29bf" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">完全连接的神经网络也会忘记前面步骤中发生的事情，并且也不知道关于输出的任何事情。</li><li id="bc4b" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">我们可以使用交叉验证来调整许多超参数，以获得最佳的网络性能:</li></ul><ol class=""><li id="4db3" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk lr ks kt ku bi translated">学习率(α):决定每次参数更新的步长。</li></ol><p id="8aef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">A.小α会导致收敛缓慢，并且可能会在计算上变得非常昂贵。</p><p id="0c8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">B.大的α可能会导致超调，我们的学习算法可能永远不会收敛。</p><p id="0383" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.隐藏的层数(深度):隐藏的层数越多越好，但是要付出计算的代价。</p><p id="3327" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.每层隐藏单元的数量(宽度):研究证明，每层大量的隐藏单元并不能改善网络。</p><p id="7bfd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4.激活函数:在不同的应用程序和领域中，在隐藏层上使用的函数是不同的。这是一个反复试验的过程，尝试不同的功能，看看哪一个效果最好。</p><p id="e67e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">5.迭代次数。</p><ul class=""><li id="cd38" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">标准化数据将有助于激活单元具有相似的值范围，并避免梯度失控。</li></ul></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><p id="6d8d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">原载于2018年4月1日</em><a class="ae ls" href="https://imaddabbura.github.io/posts/coding-nn/fwd-bkwd-propagation/Coding-Neural-Network-Forwad-Back-Propagation.html" rel="noopener ugc nofollow" target="_blank"><em class="kl">imaddabbura . github . io</em></a><em class="kl">。</em></p></div></div>    
</body>
</html>