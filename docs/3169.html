<html>
<head>
<title>Fast Word Segmentation of Noisy Text</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">噪声文本的快速分词</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fast-word-segmentation-for-noisy-text-2c2c41f9e8da?source=collection_archive---------3-----------------------#2018-04-16">https://towardsdatascience.com/fast-word-segmentation-for-noisy-text-2c2c41f9e8da?source=collection_archive---------3-----------------------#2018-04-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/b0794c375052eefa10fba7b873737985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKZq0upvq-q5U-A2hX1Xow.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="https://de.wikipedia.org/wiki/Vergilius_Vaticanus#/media/File:Vergilius_Vat_Folio_31v.jpg" rel="noopener ugc nofollow" target="_blank">Vergilius Vaticanus</a></figcaption></figure><h1 id="c701" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">TL；速度三角形定位法(dead reckoning)</h1><p id="151b" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="ld ir">更快</strong>用三角矩阵代替动态规划分词。集成的<strong class="ld ir">拼写纠正</strong>允许<strong class="ld ir">嘈杂的输入文本</strong>。GitHub上的<a class="ae kc" href="https://github.com/wolfgarbe/SymSpell" rel="noopener ugc nofollow" target="_blank"> C#源代码</a>。</p></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><p id="cb6f" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">对于西方人来说，单词之间用空格分隔似乎是显而易见的，而在汉语、日语、朝鲜语(CJK语言)、泰语和爪哇语中，单词之间是没有空格的。</p><p id="2198" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">甚至古典希腊语和晚期古典拉丁语都没有这些空格。这被称为<a class="ae kc" href="https://en.wikipedia.org/wiki/Scriptio_continua" rel="noopener ugc nofollow" target="_blank">continua</a>脚本。</p><p id="d260" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">似乎我们还没有失去我们的能力:我们可以很容易地破译</p><p id="6d0d" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir">quickbrownfoxjumpsoverthelazydog</strong></p><p id="2756" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">如同</p><p id="083b" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">敏捷的棕色狐狸跳过懒惰的狗</p><p id="d132" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">我们的大脑以某种方式直觉地和无意识地做到这一点。我们的阅读速度稍微慢了一点，这是因为我们的大脑需要处理所有的背景信息。如果我们尝试以编程的方式来做，我们将会看到它到底有多少。</p><h1 id="7fb9" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">为什么？</h1><p id="62af" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">但是，如果人们无论如何都能阅读未分段的文本，我们为什么要以编程方式实现分词呢？</p><p id="0f6b" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir"> <em class="ml">对于单词间没有空格的CJK语言来说</em> </strong> <em class="ml">，</em>就更明显了。</p><p id="f817" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">像谷歌或百度这样的网络搜索引擎必须以一种允许高效快速检索的方式对文本进行索引。那些网络搜索引擎是基于倒排索引的。在抓取每个单词的过程中，会创建一个单独的链接列表。每个列表都包含指向出现该特定单词的所有网页的链接。</p><p id="f677" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">如果我们搜索一个单词，那么列表中包含的所有链接都是有效的搜索结果。如果我们对两个单词进行布尔搜索(AND ),那么这两个列表相交，并且只有包含在两个列表中的链接作为结果返回。</p><p id="464f" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir">拼写纠正</strong>允许纠正拼错的单词。这是通过在字典中查找可能拼错的单词来完成的。如果找到该单词，则该单词被认为是正确的。如果没有找到该单词，那么字典中最接近候选的那些单词(根据编辑距离度量，如<a class="ae kc" href="https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance" rel="noopener ugc nofollow" target="_blank"> Damerau-Levenshtein </a>)被呈现为拼写纠正建议。当然，计算每个字典条目的编辑距离的简单方法是非常低效的。<a class="ae kc" href="https://seekstorm.com/blog/symspell-vs-bk-tree/" rel="noopener ugc nofollow" target="_blank">符号拼写</a>是一个快得多的算法。但无论如何，前提条件是首先识别单词边界，以便首先为拼写校正算法提供输入。</p><p id="514e" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir">机器翻译，语言理解</strong>，S <strong class="ld ir">感知分析</strong>等很多信息处理任务也是基于单词的。</p><p id="5fa3" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">但是对于单词之间已经有空格的语言，我们为什么还需要分词呢？嗯，即使在那些通常有空格的语言中，有时也会缺少空格！应用领域多得惊人:</p><ul class=""><li id="32d7" class="mm mn iq ld b le mg li mh lm mo lq mp lu mq ly mr ms mt mu bi translated"><strong class="ld ir">规范化书写可变的英语复合名词</strong>:例如，ice box = ice-box = icebox猪圈=猪圈=猪圈)进行搜索&amp;索引。</li><li id="e871" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated"><strong class="ld ir">复合词分词:</strong>原词和拆分词部分都应编入索引。</li><li id="f9b9" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">打字错误可能导致空格丢失。</li><li id="cfe8" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated"><strong class="ld ir">转换错误</strong>:在转换过程中，word之间的一些空格可能会丢失，例如，通过删除换行符而不是用空格替换它们。</li><li id="9a5f" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated"><strong class="ld ir"> OCR错误</strong>:原始文档或手写文本的质量较差，可能会导致无法正确识别单词之间的所有空格。</li><li id="6c2d" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated"><strong class="ld ir">传输错误:</strong>在噪声信道上传输期间，可能会丢失空格或引入拼写错误。</li><li id="e157" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated"><strong class="ld ir">关键字提取</strong>从URL地址、域名、表格列描述或无空格书写的编程变量。</li><li id="111f" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">对于<strong class="ld ir">密码分析</strong>，可能需要从密码中提取术语。</li><li id="0a50" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated"><strong class="ld ir">语音识别</strong>，如果在口语中不能正确识别单词之间的空格。</li><li id="2ced" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated"><strong class="ld ir"/><a class="ae kc" href="https://en.wikipedia.org/wiki/Camel_case" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">自动编程变量</strong> </a>。</li></ul><p id="4243" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">但是，除了自然语言之外，单词分割也可能是令人感兴趣的，例如，将DNA序列分割成单词  (PDF)。</p><h1 id="a1d2" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">分段变体生成</h1><p id="cbde" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">一个字符串可以用几种方式来划分。每个不同的分割变体被称为一个<strong class="ld ir">组合</strong>。存在多少种不同的成分？</p><ul class=""><li id="46c4" class="mm mn iq ld b le mg li mh lm mo lq mp lu mq ly mr ms mt mu bi translated">在长度为n的字符串中，<strong class="ld ir">潜在字边界的数量</strong>是<strong class="ld ir">n’= n-1</strong>(在字符串的每个字符之间)。</li><li id="31e3" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">这些位置中的每一个实际上都可以用作字边界，也可以不用作字边界。我们可以认为这是一个二进制形式的数字，其中每一位都可以设置或不设置。</li><li id="13a6" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">在二进制数字系统中，一个n位数可以代表x=2^n数。因此，分割变体的数量也是<strong class="ld ir">x=2^n'</strong>(n’潜在单词边界的数量)</li><li id="5155" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">每个长度为n的字符串可以被分割成<strong class="ld ir">个2^n−1 </strong>个可能的<a class="ae kc" href="https://en.wikipedia.org/wiki/Composition_(combinatorics)" rel="noopener ugc nofollow" target="_blank">个组合</a>。</li></ul><p id="014f" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">必须根据两个标准评估钻柱的成分:</p><ol class=""><li id="16b8" class="mm mn iq ld b le mg li mh lm mo lq mp lu mq ly na ms mt mu bi translated">作文是否是有效的分词，即由真实的单词组成(我们将学习如何处理未知单词)。</li><li id="8e5b" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly na ms mt mu bi translated">对于特定的输入字符串，哪个有效的单词分段(可能存在几个)具有最高的概率，并且将被选为最终结果。</li></ol><h1 id="fbb5" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">递归方法</h1><p id="8228" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">最简单的方法是生成所有可能的变体，将字符串分割成子字符串(候选单词),并在字典中查找这些候选单词。所有子字符串都是有效单词的那些分段变量将作为结果返回。</p><p id="9a6c" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">以下递归算法枚举所有合成:</p><figure class="nb nc nd ne gt jr"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="d7c0" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">对于“<strong class="ld ir"> isit </strong>”作为输入，它将生成所有<strong class="ld ir"> 8个可能的组合</strong>:</p><pre class="nb nc nd ne gt nh ni nj nk aw nl bi"><span id="b9a2" class="nm ke iq ni b gy nn no l np nq">i s i t<br/>i s it<br/>i si t<br/><strong class="ni ir">i sit</strong><br/>is i t<br/><strong class="ni ir">is it</strong><br/>isi t<br/>isit</span></pre><p id="43e5" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">两个作文由真正的英语单词组成，并且将是有效的分段建议。在<strong class="ld ir">分段变量选择</strong>部分，我们将学习如何确定两个变量中哪一个更有可能。</p><p id="90c9" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">但是有一个问题。长度为n的每一串可以被分割成<strong class="ld ir"> 2^n−1 </strong>可能的<a class="ae kc" href="https://en.wikipedia.org/wiki/Composition_(combinatorics)" rel="noopener ugc nofollow" target="_blank">成分</a>。这是巨大的！</p><blockquote class="nr ns nt"><p id="9893" class="lb lc ml ld b le mg lg lh li mh lk ll nu mi lo lp nv mj ls lt nw mk lw lx ly ij bi translated">朴素的算法是<strong class="ld ir">指数</strong>和<strong class="ld ir">不会缩放</strong>更长的字符串！</p></blockquote><p id="ea2b" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">例如，我们的字符串<strong class="ld ir">quickbrownfoxjumpsoverthelazydog</strong>的长度为35。这导致产生超过<strong class="ld ir">170亿</strong>个切分变体，它们的所有单词都要在字典中进行验证，最后，这些变体按概率进行排序。</p><p id="aca1" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">我们通常没有那么多时间和计算能力。我们需要想出更聪明、更有效的方法。</p><h1 id="cc9d" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">局部最优的增量计算</h1><blockquote class="nr ns nt"><p id="005a" class="lb lc ml ld b le mg lg lh li mh lk ll nu mi lo lp nv mj ls lt nw mk lw lx ly ij bi translated">我们<strong class="ld ir">不需要为了<strong class="ld ir">找到最好的</strong>而生成所有的</strong>作文！</p></blockquote><p id="e979" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">在朴素算法中，生成分割变量的整个树。对于<strong class="ld ir">重复子串</strong>，也重复生成<strong class="ld ir">分段</strong>。并且仅在最后，所有生成的完整分割被比较以找到最佳分割。</p><p id="bfe1" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">但是<strong class="ld ir">每个特定的子串我们只需要分割一次</strong>。我们只为每个子串计算一次最佳部分分割，并在整个输入串的增量分割期间重用它。</p><p id="63c3" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">局部最优的<strong class="ld ir">选择基于<strong class="ld ir">假设连续的部分分割是独立的</strong>。</strong></p><blockquote class="nr ns nt"><p id="d9a1" class="lb lc ml ld b le mg lg lh li mh lk ll nu mi lo lp nv mj ls lt nw mk lw lx ly ij bi translated">只计算一次每个子串的最佳分段并重用它<strong class="ld ir">节省了生成和存储重复模式所需的大量时间和内存</strong>。</p></blockquote><p id="0f31" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">下一节将介绍两种不同的方法来实现这一点。</p><h1 id="81b0" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">动态规划方法</h1><p id="79b5" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">改进算法的一种方法是使用<a class="ae kc" href="https://en.wikipedia.org/wiki/Dynamic_programming" rel="noopener ugc nofollow" target="_blank">动态编程</a>:</p><p id="deb6" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">这是一种解决复杂问题的方法，将复杂问题分解成一系列更简单的子问题，每个子问题只解决一次，然后存储它们的解。下一次出现相同的子问题时，不用重新计算它的解，只需查找先前计算的解，从而节省计算时间。每个子问题解都以某种方式被索引，通常基于其输入参数的值，以便于查找。将子问题的解存储起来而不是重新计算的技术叫做“ <a class="ae kc" href="https://en.wikipedia.org/wiki/Memoization" rel="noopener ugc nofollow" target="_blank"> <em class="ml">记忆化</em> </a> <em class="ml">”。【来源:维基百科】</em></p><p id="a9a8" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">每次在我们的程序进行递归调用来分割剩余的字符串之前，我们都要检查这个精确的子串以前是否被分割过。如果是这样，我们只需消除递归调用，并从缓存中检索该子串的最佳分割结果(在特定子串第一次被分割时存储在缓存中)。</p><p id="398d" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">这种技术已经被多次用于分词:</p><ul class=""><li id="1c38" class="mm mn iq ld b le mg li mh lm mo lq mp lu mq ly mr ms mt mu bi translated">彼得·诺维格的<a class="ae kc" href="http://norvig.com/ngrams" rel="noopener ugc nofollow" target="_blank">分词Python代码</a>详见本书<a class="ae kc" href="http://oreilly.com/catalog/9780596157111/" rel="noopener ugc nofollow" target="_blank"> <em class="ml">自然语言语料库数据<a class="ae kc" href="http://norvig.com/ngrams/ch14.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ml">章节【优美数据</em> </a></em></a></li><li id="851a" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">grant Jenks<a class="ae kc" href="https://github.com/grantjenks/python-wordsegment" rel="noopener ugc nofollow" target="_blank">python _ word segment</a>。</li><li id="77e7" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">杰昆<a class="ae kc" href="https://jeremykun.com/2012/01/15/word-segmentation/" rel="noopener ugc nofollow" target="_blank">分词</a></li></ul><p id="b406" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">由于我在网上找不到C#端口，下面是我对动态编程方法的实现:</p><figure class="nb nc nd ne gt jr"><div class="bz fp l di"><div class="nf ng l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="https://gist.github.com/wolfgarbe/fd1d02d05b6da9828ed8fd9a28648338" rel="noopener ugc nofollow" target="_blank">WordSegmentationDP source code</a></figcaption></figure><h1 id="8173" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">三角矩阵方法</h1><p id="f765" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="ld ir">动态规划</strong>方法使用<strong class="ld ir">递归</strong>和<strong class="ld ir">字典</strong>来记忆<strong class="ld ir">剩余子串</strong>的最佳分段。</p><p id="cdc1" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir">三角矩阵方法</strong>使用<strong class="ld ir">嵌套循环</strong>和<strong class="ld ir">循环数组</strong>来存储<strong class="ld ir">前缀子串</strong>的最佳分段。</p><p id="080b" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">生成长度递增的部件的三角形矩阵，组织成圆形阵列。此<strong class="ld ir">允许恒定的O(1)内存消耗</strong>(与动态编程的线性O(n)相比)，因为一旦不再需要中间分段变量，它们就会被覆盖。不是存储完整的分段串，而是仅使用潜在空间位置的<strong class="ld ir">位向量。这<strong class="ld ir">减少了存储分段变量的内存消耗</strong>(1位而不是1字符)并且<strong class="ld ir">垃圾收集友好</strong>。</strong></p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nx"><img src="../Images/ea8781e5be155af8942697f3439edbdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fH2REX6iCiZBk1mvTwl6ZQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Triangular Matrix algorithm</figcaption></figure><p id="d9a7" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">以及三角矩阵方法的C#实现:</p><figure class="nb nc nd ne gt jr"><div class="bz fp l di"><div class="nf ng l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="https://gist.github.com/wolfgarbe/f2e04bcbb7b3d92c83466e0a9354348d" rel="noopener ugc nofollow" target="_blank">WordSegmentationTM source code</a></figcaption></figure><h1 id="3280" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">动态规划与三角矩阵</h1><p id="cc0a" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">那么，这两种方法的相同点和不同点是什么呢？</p><ul class=""><li id="8a81" class="mm mn iq ld b le mg li mh lm mo lq mp lu mq ly mr ms mt mu bi translated">两种算法提供<strong class="ld ir">相同的结果</strong>。</li><li id="4c0e" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">两者具有相同的线性计算复杂度O(n)。</li><li id="9fe8" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">两者都存储特定子串的最佳分段变量，因此只需计算一次。</li><li id="8756" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">DP连续计算特定<strong class="ld ir">剩余子串</strong><strong class="ld ir"/>的所有分段变量。最好的变体是存储在缓存字典(哈希表)中的<strong class="ld ir">。为了访问特定余数的分割结果，子串<strong class="ld ir">被用作关键字</strong>。</strong></li><li id="e7d5" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">TM在内部循环的特定位置计算特定<strong class="ld ir">前缀子串</strong> <strong class="ld ir">非连续</strong>的分段变量。最好的变体是存储在数组中的<strong class="ld ir">。为了访问特定前缀子串的分割结果，循环位置被用作索引</strong>。</li><li id="3652" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">TM有一个<strong class="ld ir">常量内存消耗</strong> <em class="ml">(数组大小= O(1)= maxSegmentationWordLength)</em>，而DP有一个<strong class="ld ir">线性内存消耗</strong> <em class="ml">(缓存字典条目数= O(n) =输入文本长度)。</em></li><li id="bb54" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">此外，即使对于相同数量的条目，数组(MT)的<strong class="ld ir">内存消耗也低于字典(DP)的</strong>内存消耗。</li><li id="bc45" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">此外，TM仅存储潜在空间位置的<strong class="ld ir">位向量，而DP存储完整的分段串。这进一步<strong class="ld ir">减少了存储分段变量的内存消耗</strong>(1位而不是1字符)并且<strong class="ld ir">垃圾收集友好</strong>。</strong></li><li id="a8f3" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">通过索引(MT)访问数组的<strong class="ld ir">比通过键(DP)访问缓存字典的</strong>更快。</li><li id="1909" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">两者的词频词典的<strong class="ld ir">查找号码类似于</strong>。</li><li id="3eda" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated">迭代通常比递归快，并防止可能的递归限制(超过最大递归深度/堆栈溢出)。</li></ul><blockquote class="nr ns nt"><p id="4de8" class="lb lc ml ld b le mg lg lh li mh lk ll nu mi lo lp nv mj ls lt nw mk lw lx ly ij bi translated">在我的基准测试中，<strong class="ld ir">三角矩阵方法比动态编程方法快2倍。它有<strong class="ld ir">更低的内存消耗</strong>，<strong class="ld ir">更好的可伸缩性</strong>，并且<strong class="ld ir">是GC友好的</strong>。</strong></p></blockquote><p id="e79a" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">最初我认为这两种方法完全不同。  <em class="ml">但在比较了它们的特点后，我认为三角矩阵方法是</em> <strong class="ld ir"> <em class="ml">只是一种特殊形式的动态编程</em> </strong> <em class="ml">，它使用一个</em> <strong class="ld ir"> <em class="ml">数组代替字典</em> </strong> <em class="ml">进行记忆，使用</em> <strong class="ld ir"> <em class="ml">循环代替递归</em> </strong> <em class="ml">并增量优化</em> <strong class="ld ir"> <em class="ml">前缀字符串而不是</em></strong></p><h1 id="706d" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">最大字长</h1><p id="f668" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">候选单词是从最大长度的字符串中生成的。Norvig将最大单词长度任意选择为20。由于<a class="ae kc" href="https://en.wikipedia.org/wiki/Longest_words" rel="noopener ugc nofollow" target="_blank">最长的单词是语言相关的</a>(中文最多6个字符)，最好从字典中存在的最长单词中导出。虽然这是试探性的，但是不太可能存在大于字典中包含的最长单词的有效真实单词。它还可以防止查找我们已经知道不在字典中的单词，并创建包含这些不存在的单词的组合。这将确保最大的分割速度。</p><h1 id="cdb7" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">计算的复杂性</h1><p id="f4a5" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">动态规划和三角矩阵方法都有一个<strong class="ld ir">线性运行时间O(n) </strong>来寻找最佳组合。</p><p id="ea66" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">文本长度n和运行时间的确切关系是<strong class="ld ir"> O(n*Min(n，最大字典字长)/ 2) </strong>。</p><h1 id="5d8a" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">分段变量选择</h1><p id="908a" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">不过，将字符串拆分成单词可能有几种有效的方法:</p><p id="c80e" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir"> isit </strong>可能是<strong class="ld ir"> i sit </strong>或者是<strong class="ld ir">it</strong></p><p id="2289" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">词频(或单词出现概率)用于确定最可能的分段变量。</p><blockquote class="nr ns nt"><p id="3565" class="lb lc ml ld b le mg lg lh li mh lk ll nu mi lo lp nv mj ls lt nw mk lw lx ly ij bi translated"><strong class="ld ir">单词</strong> <strong class="ld ir">出现概率P =字数C /语料库规模N </strong></p></blockquote><p id="213b" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">语料库大小N是用于生成频率词典的文本语料库中的总字数。只有当字典是完整的，n才等于频率字典中所有计数c的和，但如果字典被截断或过滤，n就不等于。</p><h1 id="9aa0" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">朴素贝叶斯规则</h1><p id="6d8a" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们假设两个单词的单词概率是独立的。因此，单词组合的朴素贝叶斯概率是两个单词概率的乘积:</p><blockquote class="nr ns nt"><p id="ed1c" class="lb lc ml ld b le mg lg lh li mh lk ll nu mi lo lp nv mj ls lt nw mk lw lx ly ij bi translated"><strong class="ld ir"> P(AB)=P(A)*P(B) </strong></p></blockquote><p id="7acd" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">使用<strong class="ld ir">双词概率</strong>代替<strong class="ld ir">单字概率</strong>可以进一步提高切分质量。</p><h1 id="98d1" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">对数标度</h1><p id="380a" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们不是计算概率的<strong class="ld ir">乘积</strong>，而是计算概率对数的<strong class="ld ir">和。因为单词的概率是关于10^-10的，所以许多这样的小数字的乘积可能超过(下溢)浮点数的范围并变成零。</strong></p><blockquote class="nr ns nt"><p id="1aed" class="lb lc ml ld b le mg lg lh li mh lk ll nu mi lo lp nv mj ls lt nw mk lw lx ly ij bi translated"><strong class="ld ir">log(P(AB))= log(P(A))+log(P(B))</strong></p></blockquote><h1 id="61a1" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">已知单词</strong></h1><p id="d06a" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">为了计算单词出现的概率，我们可以使用不同的方法:</p><ol class=""><li id="c992" class="mm mn iq ld b le mg li mh lm mo lq mp lu mq ly na ms mt mu bi translated">通过统计大型文本语料库中的单词来创建我们自己的词频词典。<strong class="ld ir"> </strong>如果字典来源于与你要处理的文档相似的语料库，那么<strong class="ld ir">相似词出现概率</strong>分布将确保<strong class="ld ir">最优结果</strong>。</li><li id="144a" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly na ms mt mu bi translated">使用包含字数的现有频率词典，例如<a class="ae kc" href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html" rel="noopener ugc nofollow" target="_blank"> Google Books Ngram数据</a>。缺点是Google Books Ngram数据不仅包含有效单词，还包含拼写错误和其他片段。虽然许多拼写错误由于概率低，无论如何都不会有影响，但这并不总是有保证的。例如，常用词的频繁拼写错误可以战胜真正的稀有词。</li><li id="f441" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly na ms mt mu bi translated">词典质量对于分段质量至关重要。为了实现这一点，两个数据源可以通过交集来组合:巨大的<a class="ae kc" href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html" rel="noopener ugc nofollow" target="_blank"> Google Books Ngram数据</a>提供了<strong class="ld ir">有代表性的词频</strong>(但是包含许多有拼写错误的条目)<strong class="ld ir">，甚至对于罕见的单词</strong>和<a class="ae kc" href="http://wordlist.aspell.net/" rel="noopener ugc nofollow" target="_blank">SCOWL——面向拼写检查器的单词列表</a>，其<strong class="ld ir">确保了真实的英语词汇</strong>(但是不包含在相同编辑距离内对建议进行排序所需的词频)。</li></ol><p id="e2dd" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">我已经决定使用变体3，即真正的英语单词词典与来自Google Books Ngram语料库的词频的交集，因为即使对于罕见的单词也有相关的词频。但是<strong class="ld ir">与变体1相比的缺点</strong>是<strong class="ld ir"> </strong>谷歌语料库来源于不同世纪的<strong class="ld ir">书籍</strong>。随着时间的推移，语言的使用已经发生了变化，语言的使用在不同的领域也有所不同(如医学、法律、散文、新闻、科学)。</p><h1 id="42e9" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">未知单词</h1><p id="130e" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们不能完全依赖字典。没有一本字典是完整的。有生僻词、新词、外来词、人名、品牌名、产品名、缩略语、非正式用词、俚语、拼写错误。即使对于字典中不包含的单词，我们也希望分词算法选择最可能的分词变体。</p><p id="bdb9" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">因此，我们必须估计一个词出现的概率，以未知的词。我们拥有的甚至是未知单词的信息就是它们的长度。</p><p id="9d4e" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">乔治·金斯利·齐夫观察到单词的长度和它们的频率成反比。经常使用的词，如“the”，往往很短(Zipf g . The psycho biology of Language)。Zipf用“最小努力原则”(Zipf g . Human Behavior and the Principle of less Effort)对此进行了解释。</p><p id="2058" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated"><a class="ae kc" href="http://norvig.com/ngrams/ch14.pdf" rel="noopener ugc nofollow" target="_blank"> Peter Norvig在《自然语言语料库数据》第224页</a>中提出了以下计算未知单词概率的公式:</p><blockquote class="nr ns nt"><p id="d080" class="lb lc ml ld b le mg lg lh li mh lk ll nu mi lo lp nv mj ls lt nw mk lw lx ly ij bi translated"><strong class="ld ir">近似词出现概率P=10 / (N * 10^word长度l) </strong></p></blockquote><p id="b1a8" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">我们必须意识到，这是一个包含<strong class="ld ir">启发式参数</strong>的<strong class="ld ir">原始估计值</strong>，可能需要针对除英语之外的<strong class="ld ir">语言进行调整。关于单词长度和词频关系的更多信息可以在Miller，Newmann，Friedmann 的<a class="ae kc" href="https://www.sciencedirect.com/science/article/pii/S0019995858902298" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">书面英语的长度-频率统计</strong>和Strauss，Grzybek，Altmann </a>的<a class="ae kc" href="http://www.peter-grzybek.eu/science/publications/2006/grzybek_us_ga_2006_word_length_frequency.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">【单词长度和词频】</strong>中的不同语言方面找到。</a></strong></p><p id="a0e3" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">当然，我们可以想到更复杂的概率计算，例如，通过使用<strong class="ld ir">马尔可夫链</strong>或<strong class="ld ir">已知前缀和后缀</strong>来估计特定子串成为真正单词的概率。这将使正确分割连续的未知单词成为可能。</p><h1 id="b8b5" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">语言</h1><p id="4671" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">分词<strong class="ld ir">算法本身是语言独立的</strong>。但是<strong class="ld ir">词频词典是特定于语言的</strong>。为了对以某种语言编写的文本进行分词，需要该语言的频率词典。<a class="ae kc" href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html" rel="noopener ugc nofollow" target="_blank"> Google Books n-gram数据</a>包括许多语言的词频。</p><p id="3d74" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">此外，如上所述，根据未知单词的长度来估计未知单词的概率的公式包含启发式参数，必须针对每种语言单独调整这些参数，以获得最佳分割结果。</p><h1 id="6a97" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">分词和拼写纠正</h1><p id="a0ba" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如上所述，存在几种分词解决方案。</p><blockquote class="nr ns nt"><p id="2a0c" class="lb lc ml ld b le mg lg lh li mh lk ll nu mi lo lp nv mj ls lt nw mk lw lx ly ij bi translated">符号拼写。分词<strong class="ld ir">结合了分词和拼写纠正</strong>。</p></blockquote><p id="0fdc" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">在切分过程中，会生成许多潜在的候选单词，所有候选单词都需要进行拼写检查，并对候选建议进行排序。</p><p id="84eb" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">这只有通过利用SymSpell 的<a class="ae kc" href="https://seekstorm.com/blog/symspell-vs-bk-tree/" rel="noopener ugc nofollow" target="_blank">极限查找速度才是可行的:</a></p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/3ac7fc25c14f7c65a7f020e95c48fd4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1l_5pOYU3AhoijKfVD-Qag.png"/></div></div></figure><p id="e851" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">通过<strong class="ld ir">最大编辑距离</strong>的一个参数，我们可以控制允许多少拼写纠正。在maxEditDistance=0的情况下，我们有没有拼写校正的纯单词分割。在maxEditDistance=2的情况下，如果字典中的所有单词都在一个<a class="ae kc" href="https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance" rel="noopener ugc nofollow" target="_blank"> Damerau-Levenshtein </a>编辑距离≤2的范围内，我们就认为它们是子字符串的有效候选词。</p><p id="e3a5" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">更高的最大编辑距离允许<strong class="ld ir">校正更多的错误</strong>，但是以<strong class="ld ir">假阳性校正</strong>和更低的分割性能为代价。实际的权衡取决于投入材料的质量。它可以使有错误的输入变得可读，但也可能给原本完美的输入带来错误。</p><p id="3155" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir">例1 </strong></p><pre class="nb nc nd ne gt nh ni nj nk aw nl bi"><span id="ecb6" class="nm ke iq ni b gy nn no l np nq">input:                 isit<br/>Norvig segment:        is it             -&gt; no spelling correction<br/>WordSegmentation ed=0: is it             -&gt; no spelling correction<br/><strong class="ni ir">WordSegmentation ed=1: visit             -&gt; spelling correction<br/></strong>LookupCompound ed=0:   i sit             -&gt; no spelling correction<br/><strong class="ni ir">LookupCompound ed=1:   visit             </strong>-<strong class="ni ir">&gt; spelling correction</strong></span></pre><p id="6914" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir">例2 </strong></p><pre class="nb nc nd ne gt nh ni nj nk aw nl bi"><span id="c8cc" class="nm ke iq ni b gy nn no l np nq">input:                 independend<br/>Norvig segment:        in depend end      -&gt; no spelling correction<br/>WordSegmentation ed=0: in depend end      -&gt; no spelling correction<br/><strong class="ni ir">WordSegmentation ed=1: independent        -&gt; spelling correction<br/></strong>LookupCompound ed=0:   independend        -&gt; no spelling correction<br/><strong class="ni ir">LookupCompound ed=1:   independent        -&gt; spelling correction</strong></span></pre><p id="c744" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir">例3 </strong></p><pre class="nb nc nd ne gt nh ni nj nk aw nl bi"><span id="df04" class="nm ke iq ni b gy nn no l np nq">input:                 whocouqdn’tread<br/>Norvig segment:        who couqdn’t read  -&gt; no spelling correction<br/>WordSegmentation ed=0: who co uqdn’t read -&gt; no spelling correction<br/><strong class="ni ir">WordSegmentation ed=1: who couldn’t read  -&gt; segmentation+correction<br/></strong>LookupCompound ed=0:   whocouqdn’tread    -&gt; more than 1 space/token<br/>LookupCompound ed=1:   whocouqdn’tread    -&gt; more than 1 space/token</span></pre><p id="070a" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated"><em class="ml">*为了使SymSpell的算法具有可比性。分词和Norvig的分词相同</em> <a class="ae kc" href="https://github.com/wolfgarbe/SymSpell/blob/master/SymSpell/frequency_dictionary_en_82_765.txt" rel="noopener ugc nofollow" target="_blank"> <em class="ml">频词典</em> </a> <em class="ml">一直在使用。</em></p><h1 id="1186" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">单词分段与查找复合</h1><p id="f102" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><a class="ae kc" href="https://github.com/wolfgarbe/SymSpell" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir"> SymSpell </strong> </a>拼写纠正和模糊搜索库附带一个方法<a class="ae kc" href="https://github.com/wolfgarbe/SymSpell#compound-aware-automatic-spelling-correction" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">lookup compound</strong></a>。它支持多单词输入字符串的复合感知自动拼写纠正，有三种情况:</p><ol class=""><li id="d2ae" class="mm mn iq ld b le mg li mh lm mo lq mp lu mq ly na ms mt mu bi translated">在正确的单词中错误地插入空格导致了两个不正确的术语</li><li id="814a" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly na ms mt mu bi translated">错误地省略了两个正确单词之间的空格，导致了一个不正确的组合术语</li><li id="778a" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly na ms mt mu bi translated">有/无拼写错误的多个输入术语</li></ol><p id="9d84" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">拆分错误、连接错误、替换错误、换位错误、删除错误和插入错误可以混合在同一个单词中。</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nz"><img src="../Images/0831ba152fdbf6b398d1dd658fc07ece.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1PjI__iLpKGS8ggS1_XOkg.png"/></div></div></figure><p id="73aa" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated">那么，<strong class="ld ir">符号之间有什么区别呢。单词切分</strong>和<strong class="ld ir">符号拼写。LookupCompound </strong>，和什么相似？</p><ul class=""><li id="f5ba" class="mm mn iq ld b le mg li mh lm mo lq mp lu mq ly mr ms mt mu bi translated"><strong class="ld ir">两个</strong>都可以在多词输入字符串中插入和删除空格，以及纠正拼写错误。</li><li id="209d" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated"><strong class="ld ir"> LookupCompound </strong>可以<strong class="ld ir">只在令牌</strong>中插入一个空格(由现有空格分隔的字符串片段)。它用于单词分段文本的拼写纠正，但可以修复偶尔丢失的空格。因为每个标记只有一个空格，所以要生成和评估的变量较少。因此，它更快，并且校正的质量通常更好。</li><li id="dc24" class="mm mn iq ld b le mv li mw lm mx lq my lu mz ly mr ms mt mu bi translated"><strong class="ld ir">分词</strong>可以<strong class="ld ir">在一个令牌中插入任意多的空格</strong>。因此，它也适用于没有任何空间的长字符串。缺点是速度和校正质量较慢，因为存在更多的潜在变量，需要生成、评估和从中选择。</li></ul><h1 id="1443" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">应用</h1><p id="a5bb" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">CJK语言文本或嘈杂文本的分词是搜索引擎和<a class="ae kc" href="https://seekstorm.com" rel="noopener ugc nofollow" target="_blank">搜索即服务搜索API </a>的文本处理管道的一部分，在这里你必须每秒索引数千个文档。在分词甚至不是主要处理任务，而只是文本预处理中许多组件之一的情况下，您需要最快的分词算法。</p><h1 id="9720" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">源代码</h1><p id="202f" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="ld ir">分词</strong> <em class="ml">无</em>拼写纠正<strong class="ld ir"> : </strong></p><p id="ee05" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated"><a class="ae kc" href="https://github.com/wolfgarbe/WordSegmentationTM" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">worsegmentationtm</strong></a><strong class="ld ir">和</strong><a class="ae kc" href="https://github.com/wolfgarbe/WordSegmentationDP" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">worsegmentationdp</strong></a><strong class="ld ir"/>并在<a class="ae kc" href="https://github.com/wolfgarbe/WordSegmentationTM" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上发布为<strong class="ld ir">开源</strong>在<strong class="ld ir"> MIT许可下。</strong></p><p id="c601" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated"><strong class="ld ir">分词</strong> <em class="ml">同</em> <strong class="ld ir">拼写更正:</strong></p><p id="66d9" class="pw-post-body-paragraph lb lc iq ld b le mg lg lh li mh lk ll lm mi lo lp lq mj ls lt lu mk lw lx ly ij bi translated"><a class="ae kc" href="https://github.com/wolfgarbe/SymSpell" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">WordSegmentation</strong></a><strong class="ld ir"/>和<a class="ae kc" href="https://github.com/wolfgarbe/SymSpell" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">lookup compound</strong></a><strong class="ld ir"/>是<a class="ae kc" href="https://github.com/wolfgarbe/SymSpell" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir"> SymSpell </strong> </a>库的一部分，在<a class="ae kc" href="https://github.com/wolfgarbe/SymSpell" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上发布为<strong class="ld ir">开源</strong>在<strong class="ld ir"> MIT许可</strong>下。</p></div></div>    
</body>
</html>