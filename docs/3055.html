<html>
<head>
<title>Information Theory of Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络信息论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/information-theory-of-neural-networks-ad4053f8e177?source=collection_archive---------3-----------------------#2018-04-04">https://towardsdatascience.com/information-theory-of-neural-networks-ad4053f8e177?source=collection_archive---------3-----------------------#2018-04-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3ff6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">打开黑盒…稍微</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b391ff5daea528a3c67e187e368b7e51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cdrrLhNd5KrQWv9IS7i8Kw.png"/></div></div></figure><blockquote class="kr"><p id="bf4c" class="ks kt iq bd ku kv kw kx ky kz la lb dk translated">"信息:概率的负倒数值."—克劳德·香农</p></blockquote><p id="2970" class="pw-post-body-paragraph lc ld iq le b lf lg jr lh li lj ju lk ll lm ln lo lp lq lr ls lt lu lv lw lb ij bi translated">这个博客的目的不是理解神经网络背后的数学概念，而是从信息处理的角度来看神经网络。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="f3db" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">编码器-解码器</h1><p id="9baa" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">在我们开始之前:</p><blockquote class="nb nc nd"><p id="52ba" class="lc ld ne le b lf nf jr lh li ng ju lk nh ni ln lo nj nk lr ls nl nm lv lw lb ij bi translated">编码器-解码器不是两个 CNN/rnn 组合在一起！事实上也不一定是神经网络！</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/aedc6ad2faaae89cfafeeac68dfeaa0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ICOovMPf3eR-HscOWjXR6Q.jpeg"/></div></div></figure><p id="3bfe" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">本来，信息论的一个概念。<a class="ae no" href="https://www.cs.toronto.edu/~hinton/science.pdf" rel="noopener ugc nofollow" target="_blank">编码器只是简单地压缩信息，而解码器扩展编码后的信息。</a></p><p id="f135" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">在机器学习的情况下，编码和解码都是完全丢失的过程，即一些信息总是丢失。</p><p id="7694" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">编码器的编码输出被称为<em class="ne">上下文向量</em>，这是解码器的输入。</p><p id="23c2" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">有两种方法可以设置编码器-解码器设置:</p><ol class=""><li id="c9f5" class="np nq iq le b lf nf li ng ll nr lp ns lt nt lb nu nv nw nx bi translated">编码器反函数中的解码器。这样，解码器试图再现原始信息。这是用来消除数据噪声的。这个设置有一个特殊的名字，叫做自动编码器。</li><li id="a892" class="np nq iq le b lf ny li nz ll oa lp ob lt oc lb nu nv nw nx bi translated">编码器是压缩算法，解码器是生成算法。这有助于将上下文从一种格式转换为另一种格式。</li></ol><blockquote class="nb nc nd"><p id="6cb4" class="lc ld ne le b lf nf jr lh li ng ju lk nh ni ln lo nj nk lr ls nl nm lv lw lb ij bi translated">示例应用:</p><p id="0a73" class="lc ld ne le b lf nf jr lh li ng ju lk nh ni ln lo nj nk lr ls nl nm lv lw lb ij bi translated">自动编码器:把英文文本压缩成矢量的编码器。解码器从向量生成原始英文文本。</p><p id="f79d" class="lc ld ne le b lf nf jr lh li ng ju lk nh ni ln lo nj nk lr ls nl nm lv lw lb ij bi translated">编码器-解码器:将英文文本压缩成向量的编码器。解码器从向量生成原文的法语翻译。</p><p id="1ea5" class="lc ld ne le b lf nf jr lh li ng ju lk nh ni ln lo nj nk lr ls nl nm lv lw lb ij bi translated">编码器-解码器:将英文文本压缩成向量的编码器。从文本内容生成图像的解码器。</p></blockquote></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="5d53" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">信息论</h1><p id="65f8" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">现在，如果我说每个神经网络，本身，是一个编码器-解码器设置；对大多数人来说，这听起来很荒谬。</p><p id="ac5a" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated"><strong class="le ir">让我们重新想象一下神经网络。</strong></p><p id="8b0a" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">假设输入层是 X，它们的真实标签/类(存在于训练集中)是 y。现在我们已经知道神经网络找到了 X 和 y 之间的潜在函数。</p><p id="67c2" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated"><em class="ne">所以 X 可以看成是 Y 的高熵分布，高熵是因为 X 包含了 Y 的信息，但它也包含了很多其他信息。</em></p><blockquote class="nb nc nd"><p id="cb19" class="lc ld ne le b lf nf jr lh li ng ju lk nh ni ln lo nj nk lr ls nl nm lv lw lb ij bi translated">示例:</p><p id="6c2f" class="lc ld ne le b lf nf jr lh li ng ju lk nh ni ln lo nj nk lr ls nl nm lv lw lb ij bi translated">“这小子不错。”包含足够的信息来告诉我们它的“积极”情绪。</p><p id="87d6" class="lc ld ne le b lf nf jr lh li ng ju lk nh ni ln lo nj nk lr ls nl nm lv lw lb ij bi translated">酪</p><p id="ebd0" class="lc ld ne le b lf nf jr lh li ng ju lk nh ni ln lo nj nk lr ls nl nm lv lw lb ij bi translated">它还包含以下内容:</p><p id="62f0" class="lc ld ne le b lf nf jr lh li ng ju lk nh ni ln lo nj nk lr ls nl nm lv lw lb ij bi translated">1.是一个特定的男孩</p><p id="39d0" class="lc ld ne le b lf nf jr lh li ng ju lk nh ni ln lo nj nk lr ls nl nm lv lw lb ij bi translated">2.只是一个男孩</p><p id="f255" class="lc ld ne le b lf nf jr lh li ng ju lk nh ni ln lo nj nk lr ls nl nm lv lw lb ij bi translated">3.句子的时态是现在时</p><p id="29e3" class="lc ld ne le b lf nf jr lh li ng ju lk nh ni ln lo nj nk lr ls nl nm lv lw lb ij bi translated">这句话的非熵版本应该是“正的”。是的，这也是输出。我们过一会儿再回到这个话题。</p></blockquote><p id="0f55" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">现在想象每一个隐藏的层作为一个单一的变量 H(因此层将被命名为 H0，H1 …..H(n-1))</p><p id="ce90" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">现在每一层都变成了变量，神经网络变成了<a class="ae no" href="https://brilliant.org/wiki/markov-chains/" rel="noopener ugc nofollow" target="_blank">马尔可夫链</a>。因为每个变量只依赖于前一层。</p><p id="b525" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">所以本质上每一层都形成了一个信息的党派。</p><p id="56b2" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">下面是一个神经网络的可视化马尔可夫链。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/bc161a05f65020a07158bd044bb8a5d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xomkoSgT4-Icsu8NnqU0mg.jpeg"/></div></div></figure><p id="64b3" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">最后一层 Y_ 应该产生最小熵输出(相对于原始标签/类“Y”)。</p><p id="59d5" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated"><em class="ne">这个获取 Y_ 的过程就是在 X 层的信息流经 H 层的时候对其进行挤压，只保留与 Y 最相关的信息，这就是信息瓶颈。</em></p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="5e95" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">交互信息</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/afb8c5581d2d125d52efd88a022f1073.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GXaA5KmLrl4P-FDSJdtrdA.jpeg"/></div></div></figure><p id="bb79" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">I(X，Y) = H(X) — H(X|Y)</p><p id="f246" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">H -&gt;熵</p><p id="b38d" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">h(X)-&gt; X 的熵</p><p id="a5d2" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">H(X|Y) -&gt;给定 Y 的 X 的条件熵</p><p id="f921" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">换句话说，H(X|Y)表示如果 Y 已知，从 X 中去除了多少不确定性。</p><h2 id="d88e" class="od mf iq bd mg oe of dn mk og oh dp mo ll oi oj mq lp ok ol ms lt om on mu oo bi translated">互信息的性质</h2><ol class=""><li id="2935" class="np nq iq le b lf mw li mx ll op lp oq lt or lb nu nv nw nx bi translated">当你沿着马尔可夫链移动时，互信息只会减少</li><li id="aea5" class="np nq iq le b lf ny li nz ll oa lp ob lt oc lb nu nv nw nx bi translated">互信息对于重新参数化是不变的，即在图层中混排值不会改变输出</li></ol></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="5d26" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">重温瓶颈</h1><p id="01a8" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">在神经网络的马尔可夫表示中，每一层都成为信息的一个分区。</p><p id="54f7" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">在信息论中，这些划分被称为相关信息的连续细化。你不必担心细节。</p><p id="5d7c" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">另一种方式是将输入编码和解码成输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/aa229b818e2c2c37ee2e6138bcf9f8a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fyOYhqxXJNrzN7SxSuKiJA.jpeg"/></div></div></figure><p id="2e4a" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">因此，对于足够的隐藏层:</p><ol class=""><li id="b349" class="np nq iq le b lf nf li ng ll nr lp ns lt nt lb nu nv nw nx bi translated">深度神经网络的样本复杂度由最后一个隐层的编码互信息决定</li><li id="d0cd" class="np nq iq le b lf ny li nz ll oa lp ob lt oc lb nu nv nw nx bi translated">精度由解码的最后一个隐藏层的互信息决定</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/4e7f9b05f24cb712c92c451be022f540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*dbVNyK7nacJHhvutuzyEdg.jpeg"/></div></figure><p id="5bc8" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated"><em class="ne">样本复杂度是一个人需要获得一定准确度的样本的数量和种类。</em></p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="bead" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">培训阶段的相互信息</h1><p id="7af7" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">我们计算相互之间的信息</p><ol class=""><li id="a04a" class="np nq iq le b lf nf li ng ll nr lp ns lt nt lb nu nv nw nx bi translated">层和输入</li><li id="3437" class="np nq iq le b lf ny li nz ll oa lp ob lt oc lb nu nv nw nx bi translated">层和输出</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/ce5b1f89b21c0a791531fc2a56944532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1rm8So6vBDdN5oWFJeDrHA.jpeg"/></div></div><figcaption class="ou ov gj gh gi ow ox bd b be z dk">Initial Conditions</figcaption></figure><p id="4afa" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">最初，权重是随机初始化的。因此，关于正确的输出几乎一无所知。对于连续层，关于输入的互信息减少，并且隐藏层中关于输出的信息也很少。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/a75b84700fa9ced308e9810510f00125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-_YbjhJ9h_6sZR1a6Yj5yQ.jpeg"/></div></div><figcaption class="ou ov gj gh gi ow ox bd b be z dk">Compression Phase</figcaption></figure><p id="df05" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">当我们训练神经网络时，图开始向上移动，表示关于输出的信息的获得。</p><p id="e7c3" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">酪</p><p id="386e" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">图也开始向右侧移动，表示后面层中关于输入的信息增加。</p><p id="c74e" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">这是他最长的阶段。这里，图的密度最大，图集中在右上方。这意味着与输出相关的输入信息的压缩。</p><p id="9019" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">这被称为压缩阶段。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/6a52aafad3205b871296960c6ecbb129.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JXNRt_1D2zbXK5l6ejVNnQ.jpeg"/></div></div><figcaption class="ou ov gj gh gi ow ox bd b be z dk">Expansion Phase</figcaption></figure><p id="94f1" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">在压缩阶段之后，曲线开始向顶部移动，但也向左侧移动。</p><p id="c3d8" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">这意味着，在连续的层中，关于输入的信息会丢失，而在最后一层中保留的是关于输出的<em class="ne">最低熵信息</em>。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="38cb" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">虚拟化</h1><p id="8e54" class="pw-post-body-paragraph lc ld iq le b lf mw jr lh li mx ju lk ll my ln lo lp mz lr ls lt na lv lw lb ij bi translated">神经网络的马尔可夫链版本突出了一点，学习是从一层到另一层发生的。图层拥有预测输出所需的所有信息(外加一些噪声)。</p><p id="e74f" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">所以我们用每一层来预测输出。这有助于我们窥视所谓黑盒的分层知识。</p><p id="c395" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">这给了我们一个视角，需要多少层才能对输出做出足够准确的预测。如果在较早的层达到饱和，则该层之后的层可以被修剪/丢弃。</p><p id="b3b0" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">这些层通常有数百或数千维。我们的进化不允许我们想象任何超越三维的事物。所以我们使用降维技术。</p><p id="b2ea" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">有各种方法来执行降维。克里斯托夫·奥拉有一个精彩的博客解释这些方法。我不会深入 t-SNE 的细节，你可以查看这个<a class="ae no" href="https://distill.pub/2016/misread-tsne/" rel="noopener ugc nofollow" target="_blank">博客</a>了解详情。</p><p id="9e49" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">为了保持简洁，SNE 霸王龙试图降低维数，将高维空间的邻居保留在低维空间。因此，这导致了相当准确的 2D 和三维绘图。</p><p id="2591" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">以下是具有两层的语言模型的层图。</p><p id="9572" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">关于情节:</p><ol class=""><li id="3303" class="np nq iq le b lf nf li ng ll nr lp ns lt nt lb nu nv nw nx bi translated">精选 16 个单词</li><li id="56eb" class="np nq iq le b lf ny li nz ll oa lp ob lt oc lb nu nv nw nx bi translated">使用最终的语言模型找到上述 16 个单词的 N 个同义词(2D 的 N=200，3D 的 N=50)</li><li id="c14a" class="np nq iq le b lf ny li nz ll oa lp ob lt oc lb nu nv nw nx bi translated">在每一层找到每个单词的表示向量</li><li id="3fe3" class="np nq iq le b lf ny li nz ll oa lp ob lt oc lb nu nv nw nx bi translated">使用 t-SNE 找出以上所选单词及其同义词的 2D 和三维简化表示</li><li id="d50e" class="np nq iq le b lf ny li nz ll oa lp ob lt oc lb nu nv nw nx bi translated">绘制简化的表示</li></ol><div class="kg kh ki kj gt ab cb"><figure class="oy kk oz pa pb pc pd paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/6f89d37b91c61ba96f68865d5b8c34f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*TkvVA6cxM6rcl0GHGps4ZQ.png"/></div></figure><figure class="oy kk oz pa pb pc pd paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/e88c9c842502d436a51f4879f03ec157.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*7HqPTclRdGWCm2t_49WxZQ.png"/></div><figcaption class="ou ov gj gh gi ow ox bd b be z dk pe di pf pg">2D plots of Layer 1 and Layer 2</figcaption></figure></div><div class="ab cb"><figure class="oy kk oz pa pb pc pd paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/b96d09f1e76571463c727003a8f0a253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*PbY873h94s7r5qKm7GiSiA.png"/></div></figure><figure class="oy kk oz pa pb pc pd paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/820b5ba2db07801f4afed9cb504119de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*9M3jIfQPi2kLhc3IwVAuFg.png"/></div><figcaption class="ou ov gj gh gi ow ox bd b be z dk pe di pf pg">3D plots of Layer 1 and Layer 2</figcaption></figure></div></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="ca69" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">摘要</h1><ol class=""><li id="2d94" class="np nq iq le b lf mw li mx ll op lp oq lt or lb nu nv nw nx bi translated">几乎每个深度神经网络都像一个编码器-解码器</li><li id="c322" class="np nq iq le b lf ny li nz ll oa lp ob lt oc lb nu nv nw nx bi translated">大部分训练时间花在压缩阶段</li><li id="93d7" class="np nq iq le b lf ny li nz ll oa lp ob lt oc lb nu nv nw nx bi translated">层是自下而上学习的</li><li id="970a" class="np nq iq le b lf ny li nz ll oa lp ob lt oc lb nu nv nw nx bi translated">在压缩阶段之后，神经网络对输入的遗忘越多，它就越准确(消除输入中不相关的部分)。</li></ol><p id="3f75" class="pw-post-body-paragraph lc ld iq le b lf nf jr lh li ng ju lk ll ni ln lo lp nk lr ls lt nm lv lw lb ij bi translated">我已经把数学排除在这个博客之外了。如果你对信息论、博弈论、学习理论等数学足够熟悉，那么请观看 Mastero <strong class="le ir"> Naftali Tishby </strong>的<a class="ae no" href="https://www.youtube.com/watch?v=bLqJHjXihK8&amp;t=856s" rel="noopener ugc nofollow" target="_blank"> <strong class="le ir">视频</strong> </a>。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><blockquote class="kr"><p id="eae5" class="ks kt iq bd ku kv ph pi pj pk pl lb dk translated">谢谢\(-_- )/</p></blockquote></div></div>    
</body>
</html>