<html>
<head>
<title>Object Detection using Google AI Open Images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用谷歌人工智能开放图像的对象检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/object-detection-using-google-ai-open-images-4c908cad4a54?source=collection_archive---------4-----------------------#2018-12-14">https://towardsdatascience.com/object-detection-using-google-ai-open-images-4c908cad4a54?source=collection_archive---------4-----------------------#2018-12-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1403" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">学会打造自己的自动驾驶汽车！！！….开玩笑</h2></div><p id="107d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">由</em> <a class="lc ld ep" href="https://medium.com/u/8eac85bcc326?source=post_page-----4c908cad4a54--------------------------------" rel="noopener" target="_blank">阿丁德拉班迪</a><em class="lb"/><a class="lc ld ep" href="https://medium.com/u/15110fd89b3a?source=post_page-----4c908cad4a54--------------------------------" rel="noopener" target="_blank">艾利森布朗</a><em class="lb"/><a class="lc ld ep" href="https://medium.com/u/140754e8ee4d?source=post_page-----4c908cad4a54--------------------------------" rel="noopener" target="_blank">萨加尔查达</a><a class="lc ld ep" href="https://medium.com/u/4dc229ed7053?source=post_page-----4c908cad4a54--------------------------------" rel="noopener" target="_blank">阿米当</a><a class="ae le" href="https://www.linkedin.com/in/zhaoshun-jason-su/" rel="noopener ugc nofollow" target="_blank">杰森苏</a></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/f304a7df691356ca44cbc590fe466d05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ACc03086R6H_LyLydy8Z4g.jpeg"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk"><a class="ae le" href="https://www.tripsavvy.com/driving-californias-scenic-highway-one-1473971" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="77c8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你上一次只用脸登录手机是什么时候？或者点击了一张与一些朋友的自拍，并使用了 Snapchat 滤镜，在你的脸上放置了一些花哨的狗耳朵？你知道吗，这些很酷的功能是由一个奇特的神经网络实现的，它不仅可以识别照片中有一张脸，还可以检测耳朵应该去哪里。从某种意义上说，你的手机可以“看到”你，它甚至知道你长什么样！</p><p id="4af1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">帮助计算机‘看’的技术叫做“<strong class="kh ir">计算机视觉”</strong>。近年来，由于计算能力的爆炸使得深度学习模型更快更可行，计算机视觉应用变得越来越普遍。许多公司，如亚马逊、谷歌、特斯拉、脸书和微软，都在大力投资这项技术及其应用。</p><h1 id="3a24" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">计算机视觉任务</h1><p id="2c95" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">我们关注两个主要的计算机视觉任务——图像分类和目标检测。</p><ol class=""><li id="ee95" class="ms mt iq kh b ki kj kl km ko mu ks mv kw mw la mx my mz na bi translated"><strong class="kh ir">图像分类</strong>专注于将图像分组到预定义的类别中。为了实现这一点，我们需要有我们感兴趣的类的多个图像，并训练计算机将像素数字转换为符号。这只是说电脑看到猫的照片，说里面有猫。</li><li id="ad85" class="ms mt iq kh b ki nb kl nc ko nd ks ne kw nf la mx my mz na bi translated"><strong class="kh ir">物体检测</strong>利用图像分类器来计算出图像中存在什么以及在哪里。通过使用卷积神经网络(CNN ),这些任务变得更加容易，这使得在图像的一次通过中检测多个类别成为可能。</li></ol><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ng"><img src="../Images/5f6ca4e5f67f2a1876c683e2a7f01de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ly6JczJzzpIxQl1S"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">For more details on the difference in such tasks, please reference the following <a class="ae le" rel="noopener" target="_blank" href="/evolution-of-object-detection-and-localization-algorithms-e241021d8bad">article</a>.</figcaption></figure><h1 id="9c83" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">计算机视觉很酷！</h1><p id="d9bc" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">认识到未来许多有趣的数据科学应用将涉及图像工作，我和我的初露头角的数据科学家团队决定在 Kaggle 上举办的<a class="ae le" href="https://www.kaggle.com/c/google-ai-open-images-object-detection-track" rel="noopener ugc nofollow" target="_blank">谷歌人工智能开放图像挑战赛</a>上一试身手。我们认为这是接触神经网络和卷积的绝佳机会，有可能给我们的教授和同学留下深刻印象。这个挑战为我们提供了 170 万张<strong class="kh ir"> </strong>图片，带有 500 个<strong class="kh ir"> </strong>对象类的<strong class="kh ir"/>1200 万个<strong class="kh ir"> </strong>包围盒标注(它们相对于图片的 X 和 Y 坐标)。你可以在这里找到数据<a class="ae le" href="https://www.figure-eight.com/dataset/open-images-annotated-with-bounding-boxes/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="75ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们强烈推荐任何想了解 CNN 的人去读吴恩达的关于卷积神经网络的课程。</p><h1 id="c28e" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">弄脏我们的手！</h1><p id="ac46" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated"><strong class="kh ir">探索性数据分析</strong> —与所有数据分析一样，我们开始探索我们拥有哪些图像以及我们需要检测的对象类型。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nh"><img src="../Images/0ee1499cdfe15ab9501cc1c05dd56e72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eH16__PHvmrH_UR1"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Frequency of Classes in the Training Dataset</figcaption></figure><p id="effe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">快速浏览训练图像可以发现，就出现的次数而言，某些物体比其他物体更具存在感。上图显示了前 43 名班级的分布情况。很明显，这是一个巨大的差距，需要以某种方式解决。为了节省时间和金钱(GPU 成本很高:( )我们选择了前面提到的 43 个对象类和包含这些对象的大约 300K 图像的子集。对于训练数据中的每个对象类，我们有大约 400 张图像。</p><h2 id="99ca" class="ni lw iq bd lx nj nk dn mb nl nm dp mf ko nn no mh ks np nq mj kw nr ns ml nt bi translated">选择对象检测算法</h2><p id="4f5f" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">我们考虑了各种算法，如 VGG、盗梦空间，但最终<strong class="kh ir">选择了 YOLO 算法</strong>，因为它的速度、计算能力和大量在线文章可以指导我们完成这个过程。面对计算和时间的限制，我们做了两个关键的决定-</p><ol class=""><li id="ac60" class="ms mt iq kh b ki kj kl km ko mu ks mv kw mw la mx my mz na bi translated">使用被训练来识别某些物体的 YOLO v2 模型。</li><li id="6b51" class="ms mt iq kh b ki nb kl nc ko nd ks ne kw nf la mx my mz na bi translated">利用迁移学习来训练最后一个卷积层，以识别以前未见过的对象，如吉他、房子、男人/女人、鸟等。</li></ol><h2 id="e8b2" class="ni lw iq bd lx nj nk dn mb nl nm dp mf ko nn no mh ks np nq mj kw nr ns ml nt bi translated">对 YOLO 的投入</h2><p id="c00c" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">YOLO 算法需要一些特定的输入-</p><ol class=""><li id="af0e" class="ms mt iq kh b ki kj kl km ko mu ks mv kw mw la mx my mz na bi translated"><strong class="kh ir">输入图像尺寸</strong> — YOLO 网络设计用于特定的输入图像尺寸。我们发送了尺寸为 608 * 608 的图像。</li><li id="7206" class="ms mt iq kh b ki nb kl nc ko nd ks ne kw nf la mx my mz na bi translated">班级数量 — 43。这是定义 YOLO 输出尺寸所必需的。</li><li id="baaf" class="ms mt iq kh b ki nb kl nc ko nd ks ne kw nf la mx my mz na bi translated"><strong class="kh ir">锚箱— </strong>要使用的锚箱的数量和尺寸。</li><li id="e26e" class="ms mt iq kh b ki nb kl nc ko nd ks ne kw nf la mx my mz na bi translated"><strong class="kh ir">置信度和 IoU 阈值</strong> —定义选择哪些锚框以及如何在锚框之间挑选的阈值。</li><li id="4e55" class="ms mt iq kh b ki nb kl nc ko nd ks ne kw nf la mx my mz na bi translated"><strong class="kh ir">带有边框信息的图像名称</strong>——对于每张图像，我们需要以如下所示的特定格式向 YOLO 提供其中的内容</li></ol><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nu"><img src="../Images/b31282ccdf130517893eb0dc7faa4336.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4LCkDjRARFBtYmalX5bq8w.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Sample input for YOLO</figcaption></figure><p id="d605" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">下面是 YOLO 输入的代码片段</em></p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="nv nw l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Inputs into YOLO</figcaption></figure><h2 id="9bb8" class="ni lw iq bd lx nj nk dn mb nl nm dp mf ko nn no mh ks np nq mj kw nr ns ml nt bi translated">YOLO v2 架构</h2><p id="ef5a" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">该架构如下所示——它有 23 个卷积层，每个卷积层都有自己的批量归一化、漏 RELU 激活和最大池。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nx"><img src="../Images/239a5811b1840b3f4fc861b0f5690ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*c07LAfYZrIvYa1Bw"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Representation of the actual YOLO v2 architecture.</figcaption></figure><p id="0e9a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些层试图从图像中提取多个重要特征，以便可以检测到各种类别。为了对象检测的目的，YOLO 算法将输入图像分成 19*19 的网格，每个网格具有 5 个不同的锚框。然后，它尝试检测每个网格单元中的类，并将一个对象分配给每个网格单元的 5 个锚定框之一。锚定框的形状不同，旨在为每个网格单元捕捉不同形状的对象。</p><p id="8904" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">YOLO 算法为每个定义的锚定框输出一个矩阵(如下所示)</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/8754e0bf3d8a9ac23b37ac0cc299a325.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/0*IEhgzOTyuvilAirO"/></div></figure><p id="60d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们必须为 43 个类训练算法，我们得到的输出维数为:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nz"><img src="../Images/2d1de97af21e226669edd25dcf426c4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y1E8dZjHJWnEmLSeDVejaQ.png"/></div></div></figure><p id="941b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些矩阵给我们观察每个锚盒中的对象的概率，以及该对象是什么类的概率。为了过滤掉不具有任何类或者具有与一些其他框相同的对象的锚框，我们使用两个阈值— <strong class="kh ir"> IoU 阈值</strong>来过滤掉捕获相同对象的锚框，以及<strong class="kh ir">置信度阈值</strong>来过滤掉不包含任何具有高置信度的类的框。</p><p id="253d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">下面是 YOLO v2 架构最后几层的示意图:</em></p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="nv nw l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Last few layers of YOLO v2 architecture (Only for illustration purposes)</figcaption></figure><h2 id="54db" class="ni lw iq bd lx nj nk dn mb nl nm dp mf ko nn no mh ks np nq mj kw nr ns ml nt bi translated">迁移学习</h2><p id="f4d6" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">转移学习是指获得一个已经训练好的神经网络来分类图像，并将其用于我们的特定目的。这节省了我们的计算时间，因为我们不需要训练大量的权重——例如，我们使用的 YOLO v2 模型有大约 5000 万个权重——在我们使用的谷歌云实例上，训练可能需要 4-5 天。</p><p id="1596" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了成功实施迁移学习，我们必须对我们的模型进行一些更新:</p><ul class=""><li id="9cf9" class="ms mt iq kh b ki kj kl km ko mu ks mv kw mw la oa my mz na bi translated"><strong class="kh ir">输入图像尺寸— </strong>我们下载的型号使用的输入图像尺寸为 416*416。由于我们训练的一些对象非常小——鸟、鞋——我们不想把输入图像挤压得太厉害。为此，我们使用大小为 608*608 的输入图像。</li><li id="97ce" class="ms mt iq kh b ki nb kl nc ko nd ks ne kw nf la oa my mz na bi translated"><strong class="kh ir">网格大小— </strong>我们更改了网格大小的尺寸，以便它将图像划分为 19*19 个网格单元，而不是我们下载的模型的默认 13*13。</li><li id="e026" class="ms mt iq kh b ki nb kl nc ko nd ks ne kw nf la oa my mz na bi translated"><strong class="kh ir">输出层— </strong>由于我们在不同数量的类别 43 和 80 上进行训练，而原始模型是在这些类别上进行训练的，因此输出层被更改为输出矩阵维度，如上所述。</li></ul><p id="fa24" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们重新初始化了 YOLO 的最后一个卷积层的权重，以在我们的数据集上对其进行训练，最终帮助我们识别唯一的类。下面是相同的代码片段-</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="nv nw l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Re-initializing the last convolution layer of YOLO</figcaption></figure><h1 id="35eb" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">价值函数</h1><p id="4ba1" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">在任何目标检测问题中，我们都希望在一幅图像中用一个<strong class="kh ir">高置信度</strong>来识别位于右侧<strong class="kh ir">位置</strong>的<strong class="kh ir">右侧目标</strong>。<a class="ae le" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">成本函数</a>有 3 个主要组成部分:</p><ol class=""><li id="45d3" class="ms mt iq kh b ki kj kl km ko mu ks mv kw mw la mx my mz na bi translated"><strong class="kh ir">分类损失:</strong>如果检测到物体，是类别条件概率的平方误差。因此，损失函数仅在对象存在于网格单元中时惩罚分类错误。</li><li id="465a" class="ms mt iq kh b ki nb kl nc ko nd ks ne kw nf la mx my mz na bi translated"><strong class="kh ir">定位损失:</strong>它是预测的边界框位置和尺寸与地面真实框的平方误差，如果这些框负责检测物体的话。为了补偿边界框坐标预测的损失，我们使用一个正则化参数(ƛcoord).此外，为了确保较大框中的小偏差没有较小框中的小偏差重要，该算法使用边界框宽度和高度的平方根。</li><li id="6e14" class="ms mt iq kh b ki nb kl nc ko nd ks ne kw nf la mx my mz na bi translated"><strong class="kh ir">置信度损失:</strong>是包围盒置信度得分的平方误差。大多数盒子不负责检测物体，因此该等式被分成两部分，一部分用于检测物体的盒子，另一部分用于其余的盒子。正则项λnoobj(默认值:0.5)应用于后一部分，以降低未检测到对象的框的权重。</li></ol><p id="057c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于成本函数的详细信息，请随意参考 YOlO <a class="ae le" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">的原文</a>。</p><p id="4bdb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">YOLO 的妙处在于，它使用的误差易于使用优化函数进行优化，如随机梯度下降(SGD)、带动量的 SGD 或 Adam 等。下面的代码片段显示了我们用于优化成本函数的参数。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="nv nw l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Training algorithm for YOLO (Adam optimizer)</figcaption></figure><h1 id="ea0d" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">输出精度-平均精度(地图得分):</h1><p id="7886" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">在对象检测中有许多指标来评估模型，对于我们的项目，我们决定使用 mAP 得分，这是所有 IoU 阈值上不同召回值的最大精度的平均值。为了理解 mAP，我们将快速回顾一下 precision、recall 和 union 上的交集)。</p><h2 id="7ce8" class="ni lw iq bd lx nj nk dn mb nl nm dp mf ko nn no mh ks np nq mj kw nr ns ml nt bi translated">精确度和召回率</h2><p id="f29d" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">精度衡量正确的正面预测的百分比。回忆是所有可能结果中真正肯定的比例。这两个值是反向相关的，并且还依赖于您为模型设置的模型得分阈值(在我们的例子中，它是置信度得分)。它们的数学定义如下:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ob"><img src="../Images/3c2859482a88f2adfd5aa878d543d28e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c7d8kk5P0qXkmNGwMxHfIA.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk"><a class="ae le" href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173" rel="noopener">Source</a></figcaption></figure><h2 id="3e5a" class="ni lw iq bd lx nj nk dn mb nl nm dp mf ko nn no mh ks np nq mj kw nr ns ml nt bi translated">并集上的交集</h2><p id="fab5" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">IoU 衡量两个区域之间有多少重叠，等于重叠面积与并集面积之比。这测量你的预测(从你的物体探测器)与地面事实(真实物体边界)相比有多好。总而言之，mAP 得分是所有 IoU 阈值的平均 AP。</p><h1 id="6ee9" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">结果</h1><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/40604f4087b2033a60d655d0d97ec8ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/0*OrWP3clyl1VlGDvg"/></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/a94fc90651693186353daf8321e086a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/0*2GgkbXusZbKPAeDL"/></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/85c883bd3cb19fd4fb380cfbda288dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/0*xFrZ-9PNypo4qCUW"/></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/7f9cb30a352222ffd4cb8f1ed837e07d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/0*aTuTeAl2S9MpVTCe"/></div></figure><h1 id="27f9" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">结论</h1><p id="8efc" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">物体检测不同于其他计算机视觉任务。您可以使用预先训练的模型，并根据需要进行编辑以满足您的需求。你可能需要 GCP 或另一个允许更高计算能力的平台。数学很难，看别人文章，不及格快。</p><h1 id="c543" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">经验教训</h1><p id="6481" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">一开始，我们发现该模型无法预测许多类别，因为许多类别只有少量训练图像，这导致了不平衡的训练数据集。因此，我们决定只使用最流行的 43 个类，这不是一个完美的方法，但每个类至少有 500 张图片。然而，我们预测的可信度仍然很低。为了解决这个问题，我们选择了包含目标类的图像。</p><p id="c381" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">物体检测是一个非常具有挑战性的话题，但是不要害怕，尽量从各种在线开放资源中学习，比如 Coursera、YouTube 教学视频、GitHub 和 Medium。所有这些免费的智慧可以帮助你在这个令人惊叹的领域取得成功！</p><h1 id="cf16" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">未来工作——继续或改进</h1><ol class=""><li id="1f09" class="ms mt iq kh b ki mn kl mo ko oe ks of kw og la mx my mz na bi translated">在更多类别上训练模型，以检测更多种类的对象。为了达到这个目标，我们需要首先解决不平衡数据的问题。一个潜在的解决方案是，我们可以用这些更稀有的类收集更多的图像。</li></ol><p id="28e0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> a .数据扩充</strong> —稍微改变现有图像以创建新图像</p><p id="b1c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> b .图像复制</strong> —我们可以多次使用相同的图像来训练特定稀有类上的算法</p><p id="d885" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">c . Ensemble</strong>——在流行类上训练一个模型，在稀有类上训练另一个模型，并使用两者的预测。</p><p id="d2d9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.此外，我们可以尝试不同型号的合奏，如 MobileNet、VGG 等。这是也用于对象检测的卷积神经网络算法。</p><p id="35a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你想详细了解我们团队的代码，这里有 GitHub 的链接。请随时提供任何反馈或意见！</p><div class="oh oi gp gr oj ok"><a href="https://github.com/bandiatindra/Object-Detection-Project" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ir gy z fp op fr fs oq fu fw ip bi translated">bandiatindra/物体探测项目</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">通过在 GitHub 上创建一个帐户，为 bandiatindra/Object-Detection 项目开发做出贡献。</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">github.com</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy lp ok"/></div></div></a></div></div></div>    
</body>
</html>