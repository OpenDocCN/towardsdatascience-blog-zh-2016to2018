<html>
<head>
<title>Face2face — A Pix2Pix demo that mimics the facial expression of the German chancellor</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">face 2 face——模仿德国总理面部表情的 Pix2Pix 演示</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/face2face-a-pix2pix-demo-that-mimics-the-facial-expression-of-the-german-chancellor-b6771d65bf66?source=collection_archive---------0-----------------------#2017-07-04">https://towardsdatascience.com/face2face-a-pix2pix-demo-that-mimics-the-facial-expression-of-the-german-chancellor-b6771d65bf66?source=collection_archive---------0-----------------------#2017-07-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="b039" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">受吉恩·科岗工作室的启发，我制作了自己的 face2face 演示，可以在 2017 年德国总理发表新年演讲时将我的网络摄像头图像翻译成她的样子。它还不完美，因为这个模型还有一个问题，例如，学习德国国旗的位置。然而，这个模特已经很好地模仿了她的面部表情，考虑到我在这个项目上的有限时间，我对结果很满意。</p><p id="3a65" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这就是了😃</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi kn"><img src="../Images/51dd7d8468e2ac431de79390ff3acd5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*qtHE9Ru0k4TorxqShyKNmw.gif"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Face2face demo with the original video input.</figcaption></figure><p id="1853" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这也是另一个版本，它把我的面部标志作为输入。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi kn"><img src="../Images/98def98374f4a731f8832e3207e615dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*vl_qYBBIpEVbAvP4b9_Knw.gif"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Face2face demo with the detected facial landmarks.</figcaption></figure><p id="878f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你有兴趣创建自己的演示或者只是自己运行它，<a class="ae kz" href="https://github.com/datitran/face2face-demo" rel="noopener ugc nofollow" target="_blank"> Github repositor </a> y 包含了你需要的一切。当然，你应该继续阅读！😁</p></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><h1 id="2b4a" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">动机</h1><p id="13ab" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">几周前，我参加了在柏林艺术博物馆举行的<a class="ae kz" href="http://genekogan.com/" rel="noopener ugc nofollow" target="_blank">吉恩·科岗</a>关于<a class="ae kz" href="https://phillipi.github.io/pix2pix/" rel="noopener ugc nofollow" target="_blank"> pix2pix </a>和深度生成模型的研讨会。在那里，他展示了几个利用生成模型的艺术项目。他展示的其中一个项目是他自己的，他使用面部追踪器创建了一个能够模仿特朗普的生成模型:</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="f1d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种演示真的让我耳目一新，因为我在工作中通常不会接触这类项目。在这次研讨会后，我决定创建自己的项目，类似于 Gene 用面部追踪器做的项目，但用的是不同的人。</p><h1 id="dc1d" class="lh li iq bd lj lk mm lm ln lo mn lq lr ls mo lu lv lw mp ly lz ma mq mc md me bi translated">生成培训数据</h1><p id="cce1" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">我要做的第一件事是创建数据集。为此，我使用了<a class="ae kz" href="http://dlib.net/face_landmark_detection.py.html" rel="noopener ugc nofollow" target="_blank"> Dlib 的姿势估计器</a>，它可以检测一张脸上的 68 个标志(嘴、眉毛、眼睛等……)以及 OpenCV 来处理视频文件:</p><ul class=""><li id="771e" class="mr ms iq jp b jq jr ju jv jy mt kc mu kg mv kk mw mx my mz bi translated">检测面部标志是两个阶段的过程。首先，使用<a class="ae kz" href="https://github.com/davisking/dlib/blob/master/examples/face_detection_ex.cpp" rel="noopener ugc nofollow" target="_blank">面部检测器</a>来检测面部，然后对检测到的面部应用姿态估计器。</li><li id="76a1" class="mr ms iq jp b jq na ju nb jy nc kc nd kg ne kk mw mx my mz bi translated">姿势估计器是论文的一个实现:<a class="ae kz" href="https://pdfs.semanticscholar.org/d78b/6a5b0dcaa81b1faea5fb0000045a62513567.pdf" rel="noopener ugc nofollow" target="_blank">一毫秒人脸对齐与回归树集合</a>由瓦希德·卡泽米和约瑟芬·苏利文于 2014 年在 CVPR 发表</li><li id="748d" class="mr ms iq jp b jq na ju nb jy nc kc nd kg ne kk mw mx my mz bi translated">我遇到的一个问题是，在我第一次实现时，面部标志检测器非常滞后(非常低的每秒帧数-fps)。我发现输入框太大了。将帧的大小缩小到四分之一会大大提高 fps。在 Satya Mallick 的另一篇博客文章中，他也建议跳过帧，但我没有这样做，因为 fps 现在已经足够好了。但是，我可以在以后尝试这种方法来进一步提高性能。</li><li id="8c0b" class="mr ms iq jp b jq na ju nb jy nc kc nd kg ne kk mw mx my mz bi translated">我在 YouTube 上查找了几个潜在的视频，我可以用它们来创建从采访到名人演讲的数据。最后，我决定用<a class="ae kz" href="https://youtu.be/mJEKql2QV48" rel="noopener ugc nofollow" target="_blank">安格拉·默克尔(德国总理)的 2017 年新年致辞</a>。这个视频特别适合，因为相机的位置是静态的，所以我可以用她的脸和背景的相同位置得到很多图像。</li></ul><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/b355b838effeb1431cb5bb66188acce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*_xJeriKSt-yNBvU_chFOQw.png"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">One sample of the training data.</figcaption></figure><h1 id="e69f" class="lh li iq bd lj lk mm lm ln lo mn lq lr ls mo lu lv lw mp ly lz ma mq mc md me bi translated">训练模型</h1><p id="fc5b" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">幸运的是，在研讨会上，Gene 还指出了一些现有的生成模型的代码库，如 pix2pix。所以我不需要做很多研究。对于模型的训练，我使用了 Daniel Hesse 令人惊奇的<a class="ae kz" href="https://github.com/affinelayer/pix2pix-tensorflow" rel="noopener ugc nofollow" target="_blank"> pix2pix TensorFlow (TF) </a>实现，这是有据可查的。丹尼尔也在他自己的博客上对<a class="ae kz" href="https://affinelayer.com/pix2pix/" rel="noopener ugc nofollow" target="_blank"> pix2pix 做了很好的介绍。如果你没看过，你应该看看！<em class="ng">剧透:</em>还利用了 hello kitty！🐱</a></p><p id="9cc3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">备注:</strong></p><ul class=""><li id="a0ef" class="mr ms iq jp b jq jr ju jv jy mt kc mu kg mv kk mw mx my mz bi translated">pix2pix 的最初实现实际上在 Torch 中，但我更喜欢 TensorFlow。</li><li id="3a54" class="mr ms iq jp b jq na ju nb jy nc kc nd kg ne kk mw mx my mz bi translated">此外，如果你不知道 pix2pix 是什么或者一般的生成模型，你可以考虑 PCA 或自动编码器，它们的主要目标是重建。不过，与这些模型的主要区别在于，生成模型中的“重建”在生成输出数据时会涉及一些随机性。例如，如果您已经了解自动编码器的基本知识，那么<a class="ae kz" href="http://kvfrans.com/variational-autoencoders-explained/" rel="noopener ugc nofollow" target="_blank">variable auto encoder</a>是一个易于理解的生成模型。</li><li id="ef64" class="mr ms iq jp b jq na ju nb jy nc kc nd kg ne kk mw mx my mz bi translated">Gene 也在为 Pix2Pix 编写教程。我认为它还没有完成，但在他的页面上你可以找到很多其他的展示案例，例如贾斯帕·范·洛宁的<a class="ae kz" href="https://jaspervanloenen.com/neural-city/" rel="noopener ugc nofollow" target="_blank">神经城</a>也很酷。</li></ul><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="nh ml l"/></div></figure><ul class=""><li id="40ec" class="mr ms iq jp b jq jr ju jv jy mt kc mu kg mv kk mw mx my mz bi translated">我发现的另一个<a class="ae kz" href="https://hackernoon.com/remastering-classic-films-in-tensorflow-with-pix2pix-f4d551fa0503" rel="noopener ugc nofollow" target="_blank">很酷的应用</a>是 Arthur Juliani 做的，他用 pix2pix 在 TensorFlow 中翻拍经典电影。这是我从他的文章中截取的电影<a class="ae kz" href="http://www.imdb.com/title/tt0047396/?ref_=nv_sr_1" rel="noopener ugc nofollow" target="_blank">后窗</a>的一个经过重新制作的短片:</li></ul><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/39e0b3426e21284f5322e56fe8d4eea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*SdOT0c6sW3gIpdlZVzSmJA.gif"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Top: input video. Middle: pix2pix output. Bottom: original remastered version.</figcaption></figure><p id="6f2e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，在我克隆了 Daniel 的 repo 并用他的助手脚本处理数据后，主要的挑战是实际的训练本身，因为训练模型可能需要 1-8 个小时，这取决于 GPU 和实际设置，如时期数、图像等..关于 CPU 的培训马上被排除，因为这可能需要几个小时。和往常一样，由于我家里没有带 GPU 的机器(我知道是时候投资这样的 machine^^了)，我不得不依赖云服务。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nj"><img src="../Images/e06c6e4da0dfcce65f4a846a97011135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I3_29oYivQ4fGNQZrQYIEQ.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Pix2Pix model graph in TensorFlow.</figcaption></figure><p id="1a24" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通常，我的第一选择是 AWS 和它的 G2 实例，但是这次我使用了<a class="ae kz" href="https://www.floydhub.com/" rel="noopener ugc nofollow" target="_blank"> FloydHub </a>来代替。我必须承认，这很酷。我在《黑客新闻》上读到过，想尝试一下。好的一面是，目前当你在那里注册一个账户，你将获得 100 小时的免费 GPU，这是相当可怕的。他们也有一个非常好的 CLI 工具，比起 GUI 我更喜欢它(但是他们也有一个)。训练包含一个命令行的模型也非常容易。到目前为止，我唯一的批评是，在培训结束后，你不能像在 AWS 上那样 ssh 到容器中。有时候，你只需要改变一件事，而不需要重新上传所有的文件，如果你的文件很大，这尤其令人讨厌。不管怎样，从另一方面来说，你省钱了，但是总有好处和坏处。</p><p id="0d51" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我终于对我生成的 400 张图片进行了实际的训练。就此而言，320 用于训练，其余用于验证。而且我用不同的历元数(5*，200，400)对模型进行了训练。训练时间因设置而异，从 6 分钟到 7 小时以上不等。</p><blockquote class="no np nq"><p id="a937" class="jn jo ng jp b jq jr js jt ju jv jw jx nr jz ka kb ns kd ke kf nt kh ki kj kk ij bi translated">*这更多是出于测试目的。在低位，安格拉·默克尔生成的输出非常像素化和模糊，但你已经可以很好地看到她的表情轮廓。</p></blockquote><p id="1008" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是我做的一个实验的一些结果(纪元 200)。如您所见，鉴别器和生成器的学习过程非常嘈杂:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nu"><img src="../Images/70c4379ca602d0fb1adff8151a5c32c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZxHlq7w07w5xVlVS-He6dg.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Plot for the discriminator and generator loss.</figcaption></figure><p id="8f97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们查看不同步骤的输出摘要，我们可以看到这可能是由标志引起的:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/4f6b29475b0d4d88052b44bb27eb1703.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/1*tGU6i_3QWO1o3YxAEr6uqg.gif"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Output summary for different steps.</figcaption></figure><p id="6c6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">增加时期的数量有助于减少一点点的像素化，但是旗帜的问题仍然存在。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi kn"><img src="../Images/373df426440af8dfbba450c927feacfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*LLaHCy7ke8E1DK_Ax1fQUg.gif"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Detected facial landmarks and the generated output for epoch 400.</figcaption></figure><p id="3656" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有趣的是，我注意到，根据我旋转头部的方式，旗子的位置也会发生变化。我想为了提高我的成绩，我可能需要用更多的数据来训练它。</p><h1 id="a7af" class="lh li iq bd lj lk mm lm ln lo mn lq lr ls mo lu lv lw mp ly lz ma mq mc md me bi translated">为模型服务</h1><p id="5282" class="pw-post-body-paragraph jn jo iq jp b jq mf js jt ju mg jw jx jy mh ka kb kc mi ke kf kg mj ki kj kk ij bi translated">训练完模型后，是时候在此基础上用 OpenCV 构建应用程序了。最大的挑战实际上是如何在应用程序中集成这个模型，因为 Daniel 的实现并不真正适合这个。他的实现对于训练来说是高度优化的，例如他使用<a class="ae kz" href="https://www.tensorflow.org/versions/r0.12/api_docs/python/io_ops/input_pipeline" rel="noopener ugc nofollow" target="_blank">队列读入</a>数据。这对训练真的很好，但我在为模特服务时没有看到这种必要性。此外，如您所知，在 TF 中保存模型时，会创建很多文件，如检查点、图本身的权重和元数据。但是在生产中，我们不需要任何这些元数据文件，因为我们只是希望我们的模型和它的权重很好地打包在一个文件中(如果你想知道如何做到这一点，<a class="ae kz" href="https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc" rel="noopener ugc nofollow" target="_blank"> Morgan Giraud 写了一个关于这个的很好的教程</a>)。因此，我不得不做了相当多的逆向工程，使它更适用于应用程序。</p><p id="2465" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">注:</strong>从 TF 1.0 开始，我们也可以使用<code class="fe nw nx ny nz b"><a class="ae kz" href="https://www.tensorflow.org/api_docs/python/tf/saved_model" rel="noopener ugc nofollow" target="_blank">tf.saved_model</a></code>保存模型。这是使用<a class="ae kz" href="https://tensorflow.github.io/serving/" rel="noopener ugc nofollow" target="_blank"> TensorFlow 服务</a>时的首选方式。但是，这种方法也会创建多个文件。通常，您会得到一个包含模型的图形定义的<code class="fe nw nx ny nz b">save_model.pb</code>和一个保存其权重的<code class="fe nw nx ny nz b">variables</code>文件夹。</p></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><p id="e420" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你喜欢这个职位给我一个❤️.这是我第一个将深度学习用于艺术的项目。我还有一些想法想要解决。敬请关注。在 Medium <a class="kl km ep" href="https://medium.com/u/4ff6d2f67626?source=post_page-----b6771d65bf66--------------------------------" rel="noopener" target="_blank"> Dat Tran </a>或 twitter <a class="ae kz" href="https://twitter.com/datitran" rel="noopener ugc nofollow" target="_blank"> @datitran </a>上关注我，了解我的最新作品。</p></div></div>    
</body>
</html>