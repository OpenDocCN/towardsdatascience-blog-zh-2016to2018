<html>
<head>
<title>The Ultimate NanoBook to understand Deep Learning based Image Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解基于深度学习的图像分类器的终极纳米书</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/https-medium-com-rishabh-grg-the-ultimate-nanobook-to-understand-deep-learning-based-image-classifier-33f43fea8327?source=collection_archive---------13-----------------------#2018-12-15">https://towardsdatascience.com/https-medium-com-rishabh-grg-the-ultimate-nanobook-to-understand-deep-learning-based-image-classifier-33f43fea8327?source=collection_archive---------13-----------------------#2018-12-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a59f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">理解卷积神经网络的指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/a121b187afc446bc34d9c4a8421941a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*fgt_HW_q7IgRHOBUIMlTFg.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 1: The Four Superheroes of Deep Learning, Source: bit.ly/2Re5HNE</figcaption></figure><p id="35b1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在当今世界，我们广泛使用图像。不是吗？你有没有想过<a class="ae ln" href="https://bit.ly/2K46MkU" rel="noopener ugc nofollow" target="_blank">脸书</a>是如何自动检测图像中的人脸，然后像上图一样分辨出图像中的人是谁？当我将指针移近图像中的一张脸时，它会自动说出这个人的名字，不是别人，正是 Yann le Cun(CNN 架构的提出者)。你用过 Snapchat 吗？它首先自动检测您的面部，然后相应地应用您选择的过滤器。苹果的 Face ID 也做了类似的事情，当它发现你的脸在设备前面时，它会自动解锁你的手机。</p><p id="571f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">有很多这样的例子，它们都有一个共同点。<strong class="kt ir">他们在摆弄图像，让机器变得聪明，所以它们会自动从中提取有意义的信息</strong>。太神奇了。不是吗？当我第一次了解这些技术时，我非常好奇它们是如何工作的？说实话，一开始好像很神奇。电脑怎么会自动分辨出这是哥哥的照片不是我的？如果你也发现自己和我在同一页上，那么拿一杯咖啡，因为在这本纳米书中，你会学到很多关于这些事情背后的主要思想的每一个细节，但以一种非常有趣的方式。</p><p id="389e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">有许多任务，如图像分类、目标定位、目标检测、目标分割等等。但是在这篇文章中，我们将主要关注图像分类。</p><p id="18bf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这篇文章分为三个部分。在第一个中，我们将尝试通过跟随我们自己的直觉来建立这个技巧，这将是非常有趣的，也是学习这个魔术的独特方法。在第二部分中，我们将使用 python 实现它，以查看它的运行情况。在第三部分，我们将尝试通过问几个有趣的问题来探索和了解更多信息。</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="537d" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">理解语义差距</h1><p id="3954" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">我们人类是视觉动物，我们用眼睛看到周围的各种物体，但你有没有想过图像对计算机意味着什么？我们感知图像的方式和计算机感知图像的方式有很大的不同。我们特别称这个间隙为<code class="fe ms mt mu mv b">Semantic Gap</code>，下图对此进行了很好的描述。</p><div class="kg kh ki kj gt ab cb"><figure class="mw kk mx my mz na nb paragraph-image"><img src="../Images/db4e2c3517721dd01a9bb369736542e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*h-Yg2-U8Q-R3i_FJephhJg.jpeg"/></figure><figure class="mw kk nc my mz na nb paragraph-image"><img src="../Images/ddc44417be69f033fd7054ed1e4b1c69.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*uMpdmUEsi5Z2B7crcucatA.png"/><figcaption class="kn ko gj gh gi kp kq bd b be z dk nd di ne nf">Fig 2: On the left— What we see using our eyes. On the right — What computer sees, Source: bit.ly/2ga5Cpu</figcaption></figure></div><p id="9d86" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在上面的图片中，很明显，对于计算机来说，图像只不过是一串排列成网格状结构的数字，正式名称为<code class="fe ms mt mu mv b">Array.</code>,然而，我只展示了其中的一小部分。如果您想看到计算机的整个图像，您可以使用下面的代码。</p><pre class="kg kh ki kj gt ng mv nh ni aw nj bi"><span id="2a70" class="nk lw iq mv b gy nl nm l nn no">import os <br/>import cv2 <br/>import numpy as np</span><span id="5905" class="nk lw iq mv b gy np nm l nn no">np.set_printoptions(threshold=np.inf)</span><span id="3196" class="nk lw iq mv b gy np nm l nn no">image = cv2.imread('/cat.jpeg')<br/>print(type(image))<br/>image = cv2.resize(image , (28,28) , cv2.INTER_AREA)<br/>print(image.shape)<br/>#visualizing the seperate channels of image<br/>print(image[: , : , 0]) #prints out Blue channel<br/>print(image[: , : , 1]) #prints out Green channel<br/>print(image[: , : , 2]) #prints out Red channel<br/>#To visualize the whole image with all 3 channels<br/>print(image[: , : , :])</span></pre><p id="bc8b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，你可能会问，我们心爱的电脑是否有办法知道哪个物体出现在画面中。当然有，我们会建造它。我知道，这似乎是一项艰巨的任务，但请忍耐到最后。我们一定会实现的！</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="abf3" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">我们旅程的第一步也是最重要的一步:</h1><p id="f85a" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">正如我以前说过的，我们将简单地问一些问题，这些问题将引导我们建立一个图像分类器。为了简洁起见，我们将称图像分类器为 IC <br/>。现在，我们准备开始我们的旅程。所以让我们问第一个问题:“你能说出下面的图片属于哪一类吗？猫还是狗？</p><div class="kg kh ki kj gt ab cb"><figure class="mw kk nq my mz na nb paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><img src="../Images/8180279c1281562d8d1be7853dd95d8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*h-Yg2-U8Q-R3i_FJephhJg.jpeg"/></div></figure><figure class="mw kk nv my mz na nb paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><img src="../Images/ea143d4dcf7701baff2535c1a5140b7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*5ZmwuwmeaQc6g-kKEE4nFg.jpeg"/></div></figure><figure class="mw kk nw my mz na nb paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><img src="../Images/1ce2f76fec667b5690d5db922d3dbcf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*l3SQFpciSHSN0idiKRMR3A.jpeg"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk nx di ny nf">Fig 3: Images of Cats, Source: bit.ly/2V5RBwK and bit.ly/2Q2OaTG</figcaption></figure></div><p id="bbc2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我相信你说得对！他们属于<code class="fe ms mt mu mv b">Cat</code>的范畴，但是等等，退一步想想你是怎么得出这个结论的？试着想想，你的大脑是怎么做到的？你怎么能如此确定这些照片是猫而不是狗？你知道猫长什么样，你刚才可能做的是<strong class="kt ir">相似性检查</strong>在你无意识地储存在你大脑中的猫脸的粗略图像和这些图像(更具体地说是这些图像的中间部分)之间，如果它返回一个高分。然后你得出结论，这些图像实际上是猫</p><p id="1a11" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，你可能会说“我们不能把同样的机制整合到我们自己的图像分类器中吗？”这似乎是一个很棒的想法，但它带来了两个主要挑战:</p><ol class=""><li id="2f9b" class="nz oa iq kt b ku kv kx ky la ob le oc li od lm oe of og oh bi translated">哪个图像会充当我们已经储存在脑海中的图像(粗糙图像)？</li><li id="5c3c" class="nz oa iq kt b ku oi kx oj la ok le ol li om lm oe of og oh bi translated">我们将如何测量两幅图像之间的相似性？</li></ol><p id="ee9d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们解决第一个问题。在这里，我们主要集中在分类一个图像是猫还是不是猫。那么我们就不能用一张猫脸的形象作为我们的粗糙形象吗？为什么不呢！我们可以使用下面的图像作为粗略的图像，然后我们可以对上面显示猫脸的图像的中间部分进行相似性检查，或者我们可以使用数组中间的像素值</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/5039c94196ceeff66f9f8e44ba74383d.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*fph0T47BA0ge2AYDuqycDw.jpeg"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 4: Face of a cat — Our rough image</figcaption></figure><p id="75c4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，我们的粗糙图像也将像我们的主图像(必须分类)一样，以数字或像素值的形式存储在计算机中。</p><p id="488c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于第二个问题，有没有办法做两幅图像之间的<strong class="kt ir">相似性检查</strong>，换句话说，在<strong class="kt ir">大矩阵</strong>(原始图像)和<strong class="kt ir">小矩阵</strong>(粗糙图像)之间？是啊！我们有东西可以代替我们。在数学上被称为<strong class="kt ir"> Frobenius 内积</strong>。不要害怕。我将解释它是什么和它做什么。</p><p id="3bcd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> Frobenius 内积无非是</strong> <strong class="kt ir">两个数组按元素相乘后再加运算</strong>。假设你有下面两个网格或矩阵，我们称它们为<strong class="kt ir"> A </strong>和<strong class="kt ir"> B </strong>，如下所示，那么弗罗贝纽斯内积(FIP)将计算如下。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/d648d8d0bfd562ca6b4776d5aec2a6fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*oGzPWjH25IlgW669JgRUxA.jpeg"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 5: Frobenius Inner product</figcaption></figure><p id="7d94" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> Frobenius 内积</strong>，总的来说，是你衡量两个事物(矩阵)相似度的方式之一。它只不过是点积的一般化版本，只有一点不同:点积是为向量定义的，内积是为矩阵定义的。它如何准确地测量相似性超出了本指南的范围，但是如果你想详细了解相同的内容，那么请随意查看这篇<a class="ae ln" href="https://betterexplained.com/articles/vector-calculus-understanding-the-dot-product/" rel="noopener ugc nofollow" target="_blank">博客文章</a>和<a class="ae ln" href="http://mathworld.wolfram.com/InnerProduct.html" rel="noopener ugc nofollow" target="_blank">这篇文章</a>以了解内积。</p><p id="2e83" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在我们有了两个图像(粗略的和主要的)和一种方法来衡量两个矩阵或图像之间的相似性。酷！因此，让我们来看看我们的非常简单的管道来将图像分类为猫或非猫，它包括两个步骤:</p><ol class=""><li id="a2ff" class="nz oa iq kt b ku kv kx ky la ob le oc li od lm oe of og oh bi translated">选择需要分类的图像的中间部分</li><li id="e508" class="nz oa iq kt b ku oi kx oj la ok le ol li om lm oe of og oh bi translated">对所选区域和粗略图像进行相似性检查。如果它返回一个高值，那么主图像是一只猫。</li></ol><p id="9bb2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但是这个管道或者我们的图像分类器对下面的图像有效吗？</p><div class="kg kh ki kj gt ab cb"><figure class="mw kk op my mz na nb paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><img src="../Images/e7e5d9da591e1311658a9ebda2e0dd66.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*rETXEEP_kp2xSdByS1iuww.jpeg"/></div></figure><figure class="mw kk oq my mz na nb paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><img src="../Images/d8dcfc8d30c83b475bd9ee3c9873fc16.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*dZOUnu3uzxfZN1vBXS27jQ.jpeg"/></div></figure><figure class="mw kk or my mz na nb paragraph-image"><img src="../Images/0f8a6e1e8128b5d0d818380b50b65342.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*cSwAeuP7J_uSTiH4C6xL0A.jpeg"/><figcaption class="kn ko gj gh gi kp kq bd b be z dk os di ot nf">Fig 6: Images of cats, Source:bit.ly/2DkrlZd</figcaption></figure></div><p id="7463" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">不，一个大大的不！它对这些图片不起作用，因为上面的图片中间没有猫的脸。这将导致两幅图像之间的相似性降低，这意味着<strong class="kt ir">不正确的分类</strong>。<strong class="kt ir"> </strong>我们能解决这个问题吗？是的，我们可以。我们不应该只关注图像的中间部分，我们应该做的是对主图像的每个部分与粗糙图像进行相似性检查，这将使我们的图像分类器<strong class="kt ir">具有全局平移不变性</strong>，这意味着人脸或图案位于图像中的哪个位置并不重要，我们的图像分类器肯定会检测到它。<strong class="kt ir">最后，结果将存储在输出矩阵中，我们必须对主图像的每个部分重复这个过程。请注意，选择主图像的特定部分的方向将是从左到右和从上到下。我们在主图像上移动粗略图像的步数或单位数称为<strong class="kt ir">步距</strong>。现在，让我用下面的图形和伪代码更清楚地说明这一点。</strong></p><pre class="kg kh ki kj gt ng mv nh ni aw nj bi"><span id="b91b" class="nk lw iq mv b gy nl nm l nn no">category = None<br/>for selected_part in (each part of main image):<br/>    1. calculate the similarity between selected part and rough part<br/>    2. if similarity == HIGH:<br/>    3.     category = cat<br/>if category == cat:<br/>     print("Cat is present in image")<br/>else:<br/>     print("Cat is not present in image")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/1e1e37b2cb5b18679443ec8f1c479bdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/1*cFMF_uWgUFdVRMZAZ0Bfzg.gif"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 7: Convolution operation with stride = 1 and no padding | Source : bit.ly/2kDIN1O</figcaption></figure><p id="5827" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">看起来很简单也很有前途。这种在主图像上卷积粗糙图像的特定过程被称为<strong class="kt ir">卷积运算(线性运算)</strong>，它非常直观，因为它只是通过计算两个矩阵之间的 Frobenius 内积来检查主图像中的某些特定模式。这个过程的结果将是另一个矩阵，它存储了粗略图像和我们的主图像的每个部分的相似性分数。</p><p id="7ca4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">你有没有注意到一些非常有趣的事情？我们的输出矩阵或特征图的尺寸将总是小于主图像的原始尺寸。不是吗？如果你不服气，我建议你仔细看看上面的操作。但是，如果您想保留主图像的大小，以便即使在操作之后也能保留主图像的大部分信息，该怎么办呢？有什么办法可以做到呢？是的，有。为了保持输出图像的尺寸，我们简单地用零填充主图像的边界，这个过程被称为<strong class="kt ir">填充。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi ov"><img src="../Images/dc0d1704a3dd2671b76584f098ff96ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sJ_u25GlJzLmmYOt9ZvL2Q.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 8: Zero padding, Source: bit.ly/2rK7fAM</figcaption></figure><p id="88e3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，是时候让我给你介绍一下深度学习社区中已知的这些矩阵的名称了。形式上，粗糙图像被称为<strong class="kt ir">核</strong>、<strong class="kt ir">特征检测器</strong>或<strong class="kt ir">滤波器</strong>，我们正在进行相似性检查的主图像的特定部分被称为<strong class="kt ir">感受野</strong>。输出矩阵被称为<strong class="kt ir">激活或特征图</strong>。</p><p id="5446" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">到目前为止一切顺利。是时候感谢我们自己了，我们已经走了这么远，并建立了一个图像分类器，它可以在单个内核的帮助下检测主图像中的特定模式。让我们再走一步，尝试使我们的图像分类器对各种不同类型的图像更加鲁棒。我们的图像分类器适用于以下图像吗？</p><div class="kg kh ki kj gt ab cb"><figure class="mw kk ow my mz na nb paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><img src="../Images/ffe6b81476202330b7247c415118a5c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*OcBs1XxSElyA5nZc-pVsMQ.jpeg"/></div></figure><figure class="mw kk ox my mz na nb paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><img src="../Images/ffd6acf66b6f2d28d1f816e955ce0a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*IBVPDazZ6bXDth_ZIePsNA.jpeg"/></div></figure><figure class="mw kk oy my mz na nb paragraph-image"><img src="../Images/27b73ef85659e0aff695aac127e61195.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*jP2D_pLiYszWyTwhY2sRtQ.jpeg"/><figcaption class="kn ko gj gh gi kp kq bd b be z dk oz di pa nf">Fig 9: Images of cats, Source:bit.ly/2DkrlZd</figcaption></figure></div><p id="7f12" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">同样，答案是否定的，这背后的原因很简单。上面图片中猫的脸和我们选择作为内核的猫的脸是有区别的。即使图像中某个对象的颜色或形状发生很小的变化，也会改变数组的值。那么，这种差异就足以导致我们的分类器出现不正确的分类。我们必须解决这个问题。我们必须通过使我们的分类器更加健壮来解决这个问题。我们不能只使用一种特定的模式对每张图片进行分类。我们的分类器还应该对猫的脸部轻微旋转、变形或出现任何其他图案的图像进行分类。因此，非常需要<strong class="kt ir">多个内核或特征检测器</strong>,因为我们不能只依赖一个内核。我们应该有不同的内核来识别图像中的不同模式。这样，它将提高我们的图像分类器的性能。</p><p id="1954" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，现在我们将对之前的卷积运算进行修改，它包含以下三个部分:</p><ol class=""><li id="7ac4" class="nz oa iq kt b ku kv kx ky la ob le oc li od lm oe of og oh bi translated">输入体:可以是深度为 3 (RGB)的图像，也可以是之前卷积运算的输出。</li><li id="e6b4" class="nz oa iq kt b ku oi kx oj la ok le ol li om lm oe of og oh bi translated">特征检测器矩阵:这将构成<strong class="kt ir"> K </strong>个内核来检测各种模式。</li><li id="b00b" class="nz oa iq kt b ku oi kx oj la ok le ol li om lm oe of og oh bi translated">特征图:这将是卷积运算的输出，它将简单地存储相似性值。</li></ol><p id="9c6e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">形式上，这三个组件的组合被称为单个<strong class="kt ir">卷积层</strong>。而如果你在琢磨这次卷积运算会怎么做。下面仔细看看。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi pb"><img src="../Images/3cf7049e2e21f0a284b9dd40b42d57ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ciDgQEjViWLnCbmX-EeSrA.gif"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 10: Convolution operation in case of multiple kernels and input depth &gt; 1, Source: bit.ly/2PnUwSr</figcaption></figure><p id="2528" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，我们的图像分类器可以检测图像中任何位置的各种特征。看起来我们做得很好。但是仍然需要改进。我们不能减少工作量吗？注意这里的一件事，我们手动定义我们的特征检测器，这个特定的内核应该检测这个特定的特征。这总是可能的吗？作为人类，我们能够识别构建分类器所需的所有模式吗？这可行吗？答案是不，我们不能。但是，我们能让我们的计算机足够智能来学习特征检测器本身吗？是的，我们可以。我们将利用现代计算机的计算能力，通过机器学习，用大量带有各自标签的图像来训练我们的机器。然后，它将自己识别各种特征检测器，以建立鲁棒的图像分类器。如果这对你来说没有多大意义，我会建议你去看看这个<a class="ae ln" href="https://hackernoon.com/the-simplest-explanation-of-machine-learning-youll-ever-read-bebc0700047c" rel="noopener ugc nofollow" target="_blank">博客</a>。</p><p id="50c5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们一致认为，我们的机器将足够智能，能够自己学习特征检测器，并将能够在图像的任何位置检测多个特征，但这里有一件更重要和关键的事情需要理解:你的机器将像你一样学习这些特征检测器。我这么说是什么意思？还记得你是如何学习英语或其他概念的吗？首先，你已经学习了字母，然后是单词，从这里开始，你可以学习如何收集有意义的单词，也就是句子。我们作为人类以一种分等级的方式学习。机器也将模仿我们的学习方式，它不会直接学习更高层次的特征，如猫的脸、手或人的腿。但首先，它将学习非常低级的特征，如检测图像中的水平边缘、垂直边缘或颜色的特征。从那里，它将学习更高层次的特征，如脸、手和形状。<strong class="kt ir">因此，这清楚地表明我们的图像分类器将不仅具有单个卷积层，而且具有多个卷积层。</strong></p><h1 id="02da" class="lv lw iq bd lx ly pc ma mb mc pd me mf jw pe jx mh jz pf ka mj kc pg kd ml mm bi translated">引入非线性</h1><p id="5324" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">你有没有注意到，随着我们的进步，我们正在推出一个更好的图像分类器版本？在上一节中，我们了解到我们的分类器应该有多个卷积层，以便以分层的方式学习。这意味着我们的分类器的结构如下:</p><pre class="kg kh ki kj gt ng mv nh ni aw nj bi"><span id="cc08" class="nk lw iq mv b gy nl nm l nn no">CONV LAYER--&gt; CONV LAYER--&gt; CONV LAYER --------&gt; CONV LAYER</span></pre><p id="b12b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">召回！我们之前已经了解到<strong class="kt ir">卷积是一种线性运算</strong>,简单来说就是，不管你在分类器中引入多少层，它只会学习线性函数。即使您在网络中添加 100 个层，它们都将作为单个卷积层。是的，<code class="fe ms mt mu mv b">100 Layers == 1 Layer</code>不相信我？好吧。让我借助简单的线性方程来阐述一下。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi ph"><img src="../Images/be88d9e4095b489a76f2c3b9707a760a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FU6oidkCyyjEMN3-cNqe8w.jpeg"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 11: Linear Equations (Captured by me)</figcaption></figure><p id="ae06" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">从“u”到<code class="fe ms mt mu mv b">y</code>的转换由 3 个线性方程完成，但如上图所示，这也可以由一个线性方程<code class="fe ms mt mu mv b">60u+31</code>完成，即<code class="fe ms mt mu mv b">Power(3 linear equations) = Power of 1 linear equation</code>。</p><p id="68e8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，我们必须在卷积层之间引入非线性，原因如下:</p><ol class=""><li id="c841" class="nz oa iq kt b ku kv kx ky la ob le oc li od lm oe of og oh bi translated">学习复杂和非线性函数来分类图像。</li><li id="23d7" class="nz oa iq kt b ku oi kx oj la ok le ol li om lm oe of og oh bi translated">通过保护<code class="fe ms mt mu mv b">100 Layers == 100 Layers</code>使我们的网络更加强大</li></ol><p id="388a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如今在深度学习社区中常见的<strong class="kt ir">非线性函数或激活函数</strong>是<strong class="kt ir"> ReLU(校正线性单元)。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/61feb87faaae38518f47d08b8658e26c.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*njuH4XVXf-l9pR_RorUOrA.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 12: ReLU Function, Source: bit.ly/2uPTxNx</figcaption></figure><p id="7661" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">可以把 ReLU 函数想象成一个接受任何数字的盒子，如果是正数，它将给出输出 0 或数字。下面的代码将使它更加清晰。</p><pre class="kg kh ki kj gt ng mv nh ni aw nj bi"><span id="45c0" class="nk lw iq mv b gy nl nm l nn no">def relu(x):<br/>    if x &lt; 0:<br/>        return 0<br/>    else:<br/>        return x</span></pre><p id="5bb6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，引入激活函数将使我们的图像分类器更加鲁棒，并将帮助它学习非线性决策边界，但这不是强制性的，您应该只使用<strong class="kt ir"> ReLU </strong>作为激活函数。你可以自由使用任何非线性函数。要了解更多关于激活功能的信息，请随意阅读这个<a class="ae ln" href="https://bit.ly/2ntXyn8" rel="noopener ugc nofollow" target="_blank">博客</a>。</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="b175" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">池化:使其更加健壮和高效</h1><p id="bfb2" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">到目前为止，在本指南中，我们更多地关注图像分类器的鲁棒性，但现在，让我们也关注分类器的效率。我们的分类器的输入是一幅图像。对吗？并且它的大小可以根据手头的问题而变化。图像的形状可以是<code class="fe ms mt mu mv b">28x28</code>、<code class="fe ms mt mu mv b">64x64</code>、<code class="fe ms mt mu mv b">128x128</code>、<code class="fe ms mt mu mv b">256x256</code>。图像的尺寸越大，我们的图像分类器中的参数数量就会越多。深度学习社区中众所周知的一个事实是<strong class="kt ir">参数数量越多，你的模型就越容易过度拟合，需要更多的时间来训练</strong>。因此，没有任何方法可以解决这个问题吗？是的，有。我们可以使用称为<strong class="kt ir">池化</strong>的操作来减少输入体积的形状。</p><p id="19cd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">汇集是一种非常简单的操作，它通过用单个值替换非常小的子数组来对输入形状进行子采样或下采样，并且该单个值通常是该小子数组的汇总统计。下图将使其更加清晰。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi pj"><img src="../Images/c7b974cc0b66dffc5ffb901447f43951.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Feiexqhmvh9xMGVVJweXhg.gif"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 13: Pooling operation | Source: stanford.io/2Td4J2d</figcaption></figure><p id="8805" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在上图中，红色的数组是我们的子数组，蓝色的小矩阵是池化操作的输出。用<code class="fe ms mt mu mv b">2x2</code>数组减少<code class="fe ms mt mu mv b">20x20</code>数组将导致进一步卷积运算中参数数量的急剧变化。这个小的子数组被称为<strong class="kt ir">内核</strong>，而<strong class="kt ir">步幅</strong>的概念也适用于此，并在决定您希望内核移动多少步时发挥作用。我们可以使用任何<strong class="kt ir">汇总统计数据</strong>将其替换为一个数字，这可以通过从子数组中取最大值来完成，称为<strong class="kt ir"> Max-Pooling </strong>，或者我们可以取子数组值的平均值，称为<strong class="kt ir"> Average-Pooling </strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/a96180d6a35c9ca83645abbc9052f4de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*AaQqX4542KI_nTSGAepXMw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 14: Various Pooling operations | Source: bit.ly/2K5zlP2</figcaption></figure><p id="a1b5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将利用池操作来缩减输入体积的大小，但是池操作还有一个更有趣的地方。在池中，我们用单个值替换小的子数组，这意味着我们不太关注特征检测器操作的结果的确切位置。相反，我们缩小一点，并要求该子数组的近似结果。这将使我们的图像分类器对<strong class="kt ir">局部平移不变量</strong>具有鲁棒性。下面的例子肯定会有帮助</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi pl"><img src="../Images/1aa7281659b686af66063cd49c5d9942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D8klsdrfCJxjokRs7UtIlw.jpeg"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 15: Images of different faces with different spacing</figcaption></figure><p id="d9a2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">【以上是三张不同的人脸图片，略有不同。如果你仔细注意，两对眼睛之间的间距和眼睛与鼻子之间的间距是不一样的，但我们仍然希望我们的图像分类器能够完美地对所有这些人脸进行分类。但是我们对图像分类器还有什么期望呢？我们希望我们的人脸检测器内核不要关注眼睛和鼻子的确切相对位置，而是检查是否有一只眼睛在左边，一只眼睛在右边，鼻子在中间，后面是嘴唇，如果满足以上所有条件，则将其声明为人脸。这正是池化操作帮助我们实现的目标。因此，池主要执行两个功能，现在您已经知道这两个功能了！</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="30c8" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">我们图像分类器的最后一个组件</h1><p id="146b" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">我们已经完成了最困难的部分。现在，让我们覆盖最简单的一个，使我们的分类器充分工作。它会给我们一个单一的数字，通过它我们可以保证这个图像是一只猫，对吗？但是，你注意到另一件事了吗？到目前为止，我们已经覆盖的所有层的输出都是形状<code class="fe ms mt mu mv b">(width,height,depth)</code>的，但我们感兴趣的是一个单一的数字，它可以告诉我们一只猫是否出现在图像中。那么，我们如何实现这一点呢？我们如何利用存在于先前层的输出体积中的所有神经元，因为这些神经元将告诉我们特定特征是否存在于图像中？我们如何智能地将那些神经元的信息组合起来，得到一个单一的数字？首先，我们必须执行一个名为<code class="fe ms mt mu mv b">Flattening</code>的操作，这样我们就可以智能地利用所有这些神经元来获得这个数字。展平是一个非常简单的操作，它将形状为<code class="fe ms mt mu mv b">(width,height,depth)</code>的输入体积转换为形状为<code class="fe ms mt mu mv b">(K,1)</code>的一维数组，其中<code class="fe ms mt mu mv b">K=width*height*depth</code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi pm"><img src="../Images/f6020617a353b1b3a6d0a3bf7465e18a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Cb1dzBhciwRi9y1-J6qjQ.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 16: Flattening operation | Source: <a class="ae ln" href="https://bit.ly/2AUN6Mq" rel="noopener ugc nofollow" target="_blank">https://bit.ly/2AUN6Mq</a></figcaption></figure><p id="b0b2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，我们有一个一维数组作为图像分类器的输出，到这一部分，我们的分类器充当了<strong class="kt ir">特征提取器</strong>。它从图像中提取出有意义的信息，这将在确定图像中是否存在猫的过程中发挥至关重要的作用。现在，我们必须利用这些有意义的信息来得到一个单一的数字。这将由称为<strong class="kt ir">全连接层</strong>的层来完成，该层将前一层中存在的所有神经元连接到下一层中的所有神经元。</p><p id="92a3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你知道简单的神经网络(没有任何花哨的层)，那么连接多个具有不同数量神经元的全连接层就相当于在基于深度学习的<strong class="kt ir">特征提取器</strong>之上堆叠一个简单的神经网络。不是吗？下图将进一步阐明这个概念，它也将帮助你理解一维数组如何被转换成一个单一的数字。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi pn"><img src="../Images/e07b11cd95f0291e7e57c5d69585dff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xHjXT2Nf5fg_P1Ur3aoZNg.jpeg"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 17: Fully connected layer | Source: <a class="ae ln" href="https://bit.ly/2JaVD0w" rel="noopener ugc nofollow" target="_blank">bit.ly/2JaVD0w</a></figcaption></figure></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><p id="fce7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">恭喜你！借助我们的直觉，我们已经完全开发了成熟的基于深度学习的图像分类器。我们已经介绍了这种基于深度学习的图像分类器的每一层，很明显，在所有这些层中，最重要的一层是<strong class="kt ir">卷积层</strong>，因此这种分类器被称为<strong class="kt ir">卷积神经网络</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi po"><img src="../Images/5dc784af3fad34ed6676ea6ae9bf9ce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*WoangK_xl0m6zYc5oJH_bQ.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 18: Convolutional Neural Network</figcaption></figure><h1 id="34ce" class="lv lw iq bd lx ly pc ma mb mc pd me mf jw pe jx mh jz pf ka mj kc pg kd ml mm bi translated">第 2 部分:卷积神经网络的作用</h1><p id="d90a" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">我们已经非常详细地了解了卷积神经网络。让我们转换一下模式。现在，足够的理论和直觉。让我们实现我们到目前为止学到的所有东西，并建立一个端到端的<strong class="kt ir"> LeNet 架构</strong>，这是第一个由<strong class="kt ir"> Yann LeCun </strong>提出的 CNN 架构。<strong class="kt ir"> </strong>然而，还有许多其他 CNN 架构，你可以在这里阅读<a class="ae ln" href="https://bit.ly/2mWCUw1" rel="noopener ugc nofollow" target="_blank">。LeNet 架构中各层的排列如下:</a></p><pre class="kg kh ki kj gt ng mv nh ni aw nj bi"><span id="cf6f" class="nk lw iq mv b gy nl nm l nn no">Input -&gt; Conv -&gt; Activation -&gt; MaxPooling -&gt; Conv -&gt; Activation -&gt; MaxPooling -&gt; Flattening -&gt; FC -&gt; FC</span></pre><p id="b268" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将使用 CNN 将手写数字分为 10 类中的一类。我们将使用的数据集是<code class="fe ms mt mu mv b">MNIST Database</code>,看起来像</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/e7620493a3de60425cfd89c7d99be1f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*Ft2rLuO82eItlvJn5HOi9A.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 19: MNIST Dataset — Hand Written Digits | Source: <a class="ae ln" href="https://bit.ly/2InhIcI" rel="noopener ugc nofollow" target="_blank">bit.ly/2InhIcI</a></figcaption></figure><p id="b090" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，让我们使用<a class="ae ln" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>在 python 中实现我们到目前为止学到的所有东西。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pq pr l"/></div></figure><p id="d242" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">第 1–10 行:</strong>我们已经导入了所需的类，这些类将进一步用于实现 LeNet 架构。</p><p id="74bf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">第 13 行:</strong>将 MNIST 数据集加载到四个变量<code class="fe ms mt mu mv b">((x_train,y_train), (x_test,y_test))</code></p><p id="530c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">第 15–18 行:</strong>这些行将打印出训练和测试数据集的形状。</p><p id="3818" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">第 21 行和第 22 行:</strong>它们将通过将图像的每个像素的范围从 0–255 转换为 0–1 来标准化输入图像。</p><p id="a570" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">第 24–25 行:</strong>执行<a class="ae ln" href="https://bit.ly/2SNuiG7" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir">一次热编码</strong> </a> <strong class="kt ir"> </strong>，这基本上是为了明确地告诉我们的模型，类之间没有顺序关系。</p><p id="dc87" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">第 27 和 28 行:</strong>它们将把数据(训练和测试)从(data.shape[0]，28，28)整形为(data.shape[0]，28，28，1)，这将用于表示卷积层的输入图像的通道数。</p><p id="5783" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">第 38–49 行:</strong>这些行将定义 LeNet 架构。为了实现这个架构，我们使用了 keras 的<code class="fe ms mt mu mv b">Sequential API</code>(第 38 行)。网络的第一个隐藏层是具有 30 个大小为(5，5)的滤波器的卷积层，接着是<strong class="kt ir"> ReLu </strong>激活层，然后是池大小=(2，2)的<strong class="kt ir"> MaxPooling </strong>层，堆叠在前一层的顶部。这 3 层的组合根据架构再次重复。在三层的这两个块之后，由<strong class="kt ir">展平层</strong>(第 47 行)进行展平操作，然后将两个<strong class="kt ir">全连接层</strong>附着 500 和 10 个神经元(第 48 和 49 行)。</p><p id="728e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">第 52 行和第 53 行:</strong>这些行将定义所需的优化器，该优化器将用于训练 CNN(第 52 行)，而第 53 行将编译定义的模型，其中<code class="fe ms mt mu mv b">categorical crossentropy</code>作为损失函数，<code class="fe ms mt mu mv b">accuracy</code>作为监控度量。</p><p id="75fd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">第 56 行:</strong>它将负责训练我们的 CNN(真实的东西)以 10 为纪元数。</p><p id="4c10" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">第 59–63 行:</strong>这些行将用于评估经过训练的模型，通过计算测试数据集的对数损失值和准确性，帮助我们检查我们的模型在未知数据上的表现。</p><p id="04d9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果您将执行上述代码，您将获得以下输出。</p><pre class="kg kh ki kj gt ng mv nh ni aw nj bi"><span id="25c3" class="nk lw iq mv b gy nl nm l nn no">Accuracy of our model is 97.008%<br/>Log loss value : 0.079</span></pre><p id="e9e3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因为我们的模型的准确度大约等于 98%,并且对数损失值等于 0.079，这是非常好的。</p><p id="7c75" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">希望这一节能让您体会到 python 中卷积神经网络的实现细节。现在，让我们转到这本纳米书最激动人心和最重要的部分，以更好地理解卷积神经网络。</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="d693" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">第 3 节:使用 ConvNets 来更好地理解它们</h1><p id="17ce" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">在深度学习社区中，大多数人将卷积神经网络视为<strong class="kt ir">“黑盒”</strong>，而没有太多关注它们如何做它们所做的事情。在本节中，我们将研究卷积神经网络，并主要关注卷积神经网络的可视化方面。通过研究一些最重要和最有趣的论文，我们将对 CNN 有更多的了解，但同样通过问一些有趣的问题。这一部分将包括深度学习社区中大多数人不经常讨论或不知道的所有细节。希望你也能学到新的东西。你兴奋吗？我知道你是！</p><h1 id="f679" class="lv lw iq bd lx ly pc ma mb mc pd me mf jw pe jx mh jz pf ka mj kc pg kd ml mm bi translated">3.1:可视化中间输出</h1><p id="e14a" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">我们非常清楚，从输入层到输出(softmax)层，卷积神经网络中存在许多层。当我们通过网络前馈一个图像时，如果我们能够可视化每一层所做的转换或每一层的输出，这将是一个有趣的练习。这个练习将帮助我们了解每一层到底对输入图像做了什么。这正是论文作者所做的:<a class="ae ln" href="https://arxiv.org/abs/1506.06579" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir">通过深度可视化理解神经网络</strong> </a>。如果你想看看这个东西的运行，你可以使用他们开发的工具，可以在这里找到<a class="ae ln" href="http://yosinski.com/deepvis" rel="noopener ugc nofollow" target="_blank"/>。</p><div class="kg kh ki kj gt ab cb"><figure class="mw kk ps my mz na nb paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><img src="../Images/28c04be777d00d70e7a2a338ad89ee51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*QCEWKpPM4Zm8MYZnjifYxg.png"/></div></figure><figure class="mw kk ps my mz na nb paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><img src="../Images/efaf4041f64629a38993064440005020.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*bUHwHbeoemYsA-g_lzP9KA.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk pt di pu nf">Fig 20: Output of various intermediate layers | Source: <a class="ae ln" href="https://bit.ly/2PlxOFC" rel="noopener ugc nofollow" target="_blank">video</a>.</figcaption></figure></div></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="9bba" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">3.2:在给定 CNN 代码的情况下重建图像</h1><p id="176b" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">在前面的小节中，我们已经尝试了在图像通过网络时可视化中间层的输出，中间层的输出也称为该特定层的图像的<code class="fe ms mt mu mv b">encoding</code>。但是，除了可视化中间层的输出，是否有可能使用图像的编码来构造原始图像？是的，这是可能的，这在论文中有广泛的论述:<a class="ae ln" href="https://arxiv.org/abs/1412.0035" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir">通过反转</strong> </a>来理解深层意象表征。本文作者通过提出以下问题对包含在中间表征中的视觉信息进行了直接分析:给定图像的编码，在多大程度上可以重建图像本身？</p><p id="d9a5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">他们使用的方法很简单。他们将这个问题设计成一个最优化问题，该问题将计算图像表示的近似逆，使得图像的原始表示(φ(0))和我们试图找出的表示(φ(x))之间的差异或损失应该是最小的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/3f265e8af0123e8c789a3493152c857e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*SFxqYdJiSkU4VJtsn44M0A.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Optimization problem</figcaption></figure><p id="b13a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这里，x 代表我们试图寻找的图像，φ(0)是中间层的输出或图像的编码，φ(x)是我们试图寻找的图像的编码，loss(φ(x)，φ(0))是两种编码的差，λR(x)是正则化项。他们用来测量两种编码之间差异的损失函数是<code class="fe ms mt mu mv b">Euclidean distance</code>,数学上看起来如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/e71eac9a29ede39bcdaea2a430a20288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*h5aqOSCyTmnd2ZGPGJ52vg.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Loss function: Euclidean distance</figcaption></figure><p id="f03e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面是对应于不同层的编码的单个图像的反转表示的图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi px"><img src="../Images/484e6c2c32df378510ea2f3412e07dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MBfPLmxZidc4dZlcqEhfBA.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi py"><img src="../Images/9b84bb02d4c290a49f1d78e662533345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*saoyyh6RIANDoXqZuBpqbg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 21: Result of the inverse of image representations of different layers, Source: <a class="ae ln" href="https://arxiv.org/abs/1412.0035" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><p id="b9c7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">从上面示出的结果中，非常清楚的是，与从远离输入层的层构建的图像相比，使用更靠近输入层的层的编码的图像重建包含更多的视觉信息。</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="e1a5" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">3.3:图像的哪个部分负责分类？</h1><p id="b65b" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">到目前为止，我们已经就图像中对象的存在负责图像分类器的输出这一事实达成一致。例如，图像中猫的脸的存在将负责图像分类器的输出。但是我们怎么能确定呢？如果分类器基于周围环境而不是基于猫的脸的存在将图像分类为<strong class="kt ir">猫</strong>会怎样？主要有两种方法可以确定，第一种是<strong class="kt ir">遮挡实验</strong>，第二种是通过使用<strong class="kt ir">显著图</strong>。</p><h2 id="0670" class="nk lw iq bd lx pz qa dn mb qb qc dp mf la qd qe mh le qf qg mj li qh qi ml qj bi translated">3.3.1:遮挡实验</h2><p id="2191" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">这种方法最早是在马修·泽勒的<a class="ae ln" href="http://arxiv.org/abs/1311.2901" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir">可视化和理解卷积网络</strong> </a>中提出的。在这种方法中，他们用灰色方块遮挡图像的特定部分，然后监控该图像的分类器输出。他们通过从上到下和从左到右滑动灰色框，为图像的每个可能部分完成了上述步骤，与我们在卷积运算中在输入图像上滑动小子阵列的方式相同。</p><p id="d786" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">这个实验有什么值得期待的？</strong></p><p id="cbd6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当我们遮挡图像的<strong class="kt ir">不太重要的</strong>部分时，图像分类器的输出(概率)应该不变，但是当我们用灰色方块替换主要对象时，概率应该显著下降。下面是这个实验结果的图片。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi qk"><img src="../Images/6e606866474291105605b019465a5c22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mVbQ150GGUu1O-5JBZMl-g.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 22: Result of occlusion experiment, Source: <a class="ae ln" href="https://arxiv.org/abs/1311.2901" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><p id="a59c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，我们现在可以确定，图像中对象的存在负责图像分类器的输出。</p><h2 id="e1dd" class="nk lw iq bd lx pz qa dn mb qb qc dp mf la qd qe mh le qf qg mj li qh qi ml qj bi translated">3.3.2:显著性图</h2><p id="ce6a" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">这种方法首次在<a class="ae ln" href="https://arxiv.org/abs/1312.6034" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir">深入卷积网络中引入:可视化图像分类模型和显著性图</strong> </a>。这是一种非常简单的方法，上面提到的论文的作者试图知道图像的哪组像素对于分类的输出是重要的。他们试图观察一个非常简单的事情，即如果我们一个接一个地改变图像的像素值，那么多少和哪个像素组将最大程度地影响该图像的类得分？这背后的原因非常简单，即与对象相对应的像素集比其他像素集更能影响类得分。</p><p id="e678" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">他们已经通过简单地计算图像的输出分数相对于图像的梯度<code class="fe ms mt mu mv b">(Score(Ic))</code>来做到这一点。</p><p id="c084" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> w* = ∂Sc /∂I:类别分数相对于图像的梯度。</strong></p><div class="kg kh ki kj gt ab cb"><figure class="mw kk ql my mz na nb paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><img src="../Images/d69a5439c1ae696b7dd894a1f750c6fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*otfFybHz9ubvg4TR-20lgA.png"/></div></figure><figure class="mw kk qm my mz na nb paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><img src="../Images/c3a80b1000dc93df9ae9fb53cd39a8e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*SzNyR167Dwz9F3dSjuNuWQ.png"/></div></figure></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi qn"><img src="../Images/12874146ff2ea2276629b2c6c3c40c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QI2eo0VMKA-T9LCZ1lYJCA.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 23: Images with their corresponding saliency maps, Source : <a class="ae ln" href="https://arxiv.org/abs/1312.6034" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><p id="93b8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">具有较高梯度值的像素组在上面的图像中被突出显示，这表示这些是图像中最重要的像素，因为这些像素的值的微小移动或改变可以极大地改变类别分数。因此，它清楚地告诉我们，图像中对象的存在是分类器输出的原因。</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><h1 id="ed43" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">3.4:我们的假设对吗？</h1><p id="e236" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">毫无疑问，卷积神经网络的骨干是<strong class="kt ir">卷积层</strong>，它由许多<strong class="kt ir">内核</strong>或<strong class="kt ir">特征检测器</strong>组成，这些检测器寻找图像中某些特征的存在，如果该特征存在于图像中，则反过来用大数字进行响应。这是我们已经在这本纳米书中涉及的内容。</p><p id="950f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们的整个直觉和解释是基于最重要和唯一的假设，即卷积层中存在的子阵列充当特征检测器，但事实真是这样吗？我们如何确定子阵列充当特征检测器？有什么方法可以确定这一点吗？你打赌，有。这将是这本纳米书的一个非常重要的部分，因为它将检验我们关于 ConvNets 的假设，这是整本纳米书的基础。用于确定这一点的方法主要分为两个阵营，一个是以数据集为中心，另一个是以网络为中心。</p><h1 id="066e" class="lv lw iq bd lx ly pc ma mb mc pd me mf jw pe jx mh jz pf ka mj kc pg kd ml mm bi translated">3.4.1:以网络为中心理解 CNN</h1><p id="8b1a" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">与同样需要数据的以数据集为中心的方法不同，以网络为中心的方法只需要经过训练的卷积神经网络。本文首先介绍了一种主要且有效的以网络为中心的方法:<a class="ae ln" href="https://arxiv.org/abs/1506.06579" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir">通过深度可视化理解神经网络</strong> </a>。</p><p id="cff5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这种方法中，该论文的作者做了一件非常简单的事情，即他们试图通过强加一个<strong class="kt ir">优化问题</strong>来可视化激活，该优化问题将试图构建一个输入图像，使得任何层上存在的任何激活的值都应该是最大的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qo"><img src="../Images/d0b4b7e127bbd1cc7d5ff39dfeeb3b4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*z4lfuQDWWgdnSiQBe8kw1w.png"/></div></figure><p id="2ad3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这里，<strong class="kt ir"> x </strong>是要构建的输入图像，<strong class="kt ir"> ai(x) </strong>是第 I 个激活的值，<strong class="kt ir"> Rθ(x) </strong>是正则化项。</p><p id="8c69" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最大化特定激活的值将导致图像(x)的构造，该图像将包含与该激活相关的视觉信息。下面的图片将帮助你将使用这种方法构建的不同层的不同激活的输出可视化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi qp"><img src="../Images/ac3df3c0478f2819b77550701c2e9a03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_E-4vo9ResMWjKOeftrRlw.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi qq"><img src="../Images/450e494b1ed324098716cba765d1c9ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UyN7DKY_ylvCFT17Wfnhrg.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi qr"><img src="../Images/d4bd2478305cf8fb02e9f408c70d1ee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Q9qvkIN-yRbIpLO6t0AIQ.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 24: Visualization of different activations of different layers, Source: <a class="ae ln" href="https://arxiv.org/abs/1506.06579" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><h1 id="e96f" class="lv lw iq bd lx ly pc ma mb mc pd me mf jw pe jx mh jz pf ka mj kc pg kd ml mm bi translated">3.4.2:理解 CNN 的以数据集为中心的方法</h1><p id="9fc6" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">以数据集为中心的方法需要经过训练的卷积神经网络和通过该网络运行的数据(图像)。其中一个主要的以数据集为中心的方法首先是在马修·泽勒的<a class="ae ln" href="http://arxiv.org/abs/1311.2901" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir">可视化和理解卷积网络</strong> </a> <strong class="kt ir">中介绍的。</strong>在这篇论文中，他们介绍了一种可视化技术，揭示了在模型中的任何层激发或激活单个特征图的输入刺激(输入图像的一部分)。为了做到这一点，他们利用了<strong class="kt ir">去进化神经网络</strong>，它可以被认为是一个使用相同组件(池化、非线性)但顺序相反的 convnet 模型。但是你会问 deconvnet 是做什么的，它将如何帮助我们？当输入图像通过网络传递时，我们获得了激活图作为中间层的输出，当我们将 deconvnet 附加到每个激活图时，它将这些活动映射回输入像素空间。将特定激活映射回输入像素空间将在输入像素空间上形成该激活正在寻找的图案。</p><p id="ef38" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">本文中的以下摘录和图表不言自明，有助于您更好地了解 deconvnet 和一般使用的方法。</p><blockquote class="qs qt qu"><p id="1531" class="kr ks qv kt b ku kv jr kw kx ky ju kz qw lb lc ld qx lf lg lh qy lj lk ll lm ij bi translated">为了检查一个 convnet，需要将一个 deconvnet 连接到它的每一层，如图 1(上图)所示，提供一条返回图像像素的连续路径。首先，将输入图像呈现给 convnet，并计算各层的特征。为了检查给定的 convnet 激活，我们将该层中的所有其他激活设置为零，并将特征映射作为输入传递给附加的 deconvnet 层。然后，我们连续地(I)取消 pool，(ii)校正和(iii)过滤，以重建引起所选激活的下层中的活动。然后重复这一过程，直到到达输入像素空间。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qz"><img src="../Images/49f2d92cdc89a56b4053717a8f8522d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*gcxSJfCvmjsiWjLcVmRYLw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 25: Deconvolutional neural network</figcaption></figure><p id="6801" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，让我们看看上述实验的结果，并试图理解不同层的激活在寻找什么。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi ra"><img src="../Images/a9b041a2a7c678cee40cc81583c05d89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k-5K-C8BsbhgnvnKyf3iKA.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 26: Visualizing the activations of Layer 1 and 2</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi rb"><img src="../Images/b31a2c13e14b9cfd079f2c9aa80cdf85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4EQnvcwn0wstnsnJeoboaw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 27: Visualizing the activations of Layer 3</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi rc"><img src="../Images/5389e627f7e3840e45ed18f7ac049593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZaaTCKldq9GGk3GWnW0DwA.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Fig 27: Visualizing the activation of Layer 4 and 5</figcaption></figure><p id="d873" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">从上面显示的图像中可以清楚地看到，靠近输入层的层正在寻找较低级别的特征(如角、边、颜色)，而远离输入层的层正在寻找较高级别的特征(如狗脸、键盘)，从而证实了我们的另一个假设，即卷积神经网络以分层的方式学习。</p><p id="c2ca" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在纳米书籍的最后也是最重要的部分，我们已经研究了卷积神经网络，并通过回顾 CNN 的四篇最重要的论文对它们有了更多的了解，这些论文也考虑了可视化方面。希望你也像喜欢其他部分一样喜欢这个部分。</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><p id="b0f3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我希望你喜欢这本纳米书的每一个部分，如果你从书中学到了什么新东西，那么你可以通过与他人分享来表达你的爱。花了这么多时间来写这么全面的博文，希望我的努力能帮助你们中的一些人理解卷积神经网络。</p><p id="eaae" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">欢迎在<a class="ae ln" href="https://www.linkedin.com/in/rishabhgarg7/" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir"> LinkedIn </strong> </a>上与我联系，在<a class="ae ln" href="https://twitter.com/rishabh_grg" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir"> Twitter </strong> </a>和<a class="ae ln" href="https://www.quora.com/profile/Rishabh-Garg-109" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir"> Quora </strong> </a>上关注我。</p></div></div>    
</body>
</html>