<html>
<head>
<title>An Exploratory Data Analysis on Lower Back Pain</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">下腰痛的探索性数据分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-exploratory-data-analysis-on-lower-back-pain-6283d0b0123?source=collection_archive---------6-----------------------#2018-09-04">https://towardsdatascience.com/an-exploratory-data-analysis-on-lower-back-pain-6283d0b0123?source=collection_archive---------6-----------------------#2018-09-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/445402986f81edbe17bcc886bf0dcc1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tLNMa-ZNJoAnQxI6BhE9ww.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Image collected from <a class="ae kc" href="https://unsplash.com/photos/XNRHhomhRU4" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/XNRHhomhRU4</a></figcaption></figure><p id="175c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://www.healthline.com/health/back-pain" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir">腰痛</strong> </a>，也叫<strong class="kf ir">腰痛</strong>，不是一种病症。这是几种不同类型的医学问题的症状。它通常由下背部的一个或多个部位的问题引起，例如:</p><ul class=""><li id="706c" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">韧带</li><li id="098b" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">肌肉</li><li id="a6f7" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">神经紧张</li><li id="5882" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">构成脊柱的骨结构，称为椎体或椎骨</li></ul><p id="f743" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">腰部疼痛也可能是由于附近器官(如肾脏)的问题引起的。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="2a9b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个<a class="ae kc" href="https://en.wikipedia.org/wiki/Exploratory_data_analysis" rel="noopener ugc nofollow" target="_blank"> EDA </a>中，我将使用下背部疼痛症状<a class="ae kc" href="https://www.kaggle.com/sammy123/lower-back-pain-symptoms-dataset" rel="noopener ugc nofollow" target="_blank">数据集</a>并尝试找出该数据集的有趣见解。我们开始吧！</p><h1 id="323c" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">数据集描述</h1><p id="b89e" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">数据集包含:</p><ul class=""><li id="c86c" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated"><strong class="kf ir"> 310 </strong>观察结果</li><li id="398f" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kf ir"> 12 </strong>特征</li><li id="bc5c" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kf ir"> 1 </strong>标签</li></ul><figure class="mz na nb nc gt jr"><div class="bz fp l di"><div class="nd ne l"/></div></figure><h2 id="948c" class="nf lx iq bd ly ng nh dn mc ni nj dp mg ko nk nl mk ks nm nn mo kw no np ms nq bi translated">导入必要的包:</h2><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="0ac2" class="nf lx iq ns b gy nw nx l ny nz">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="90e9" class="nf lx iq ns b gy oa nx l ny nz">import seaborn as sns<br/>sns.set()<br/>from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder<br/>from sklearn.svm import SVC<br/>from sklearn.naive_bayes import GaussianNB<br/>from sklearn.linear_model import LogisticRegression<br/>from xgboost import XGBClassifier, plot_importance<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score,confusion_matrix</span></pre><p id="0be3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">读取<code class="fe ob oc od ns b">.csv</code>文件:</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="cd3d" class="nf lx iq ns b gy nw nx l ny nz">dataset = pd.read_csv("../input/Dataset_spine.csv")</span></pre><p id="2933" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">查看数据集中的前 5 行:</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="f119" class="nf lx iq ns b gy nw nx l ny nz">dataset.head() # this will return top 5 rows </span></pre><p id="1896" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">移除虚拟列:</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="646d" class="nf lx iq ns b gy nw nx l ny nz"># This command will remove the last column from our dataset.<br/>del dataset["Unnamed: 13"]</span></pre><h2 id="277a" class="nf lx iq bd ly ng nh dn mc ni nj dp mg ko nk nl mk ks nm nn mo kw no np ms nq bi translated">数据集摘要:</h2><p id="14c6" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated"><a class="ae kc" href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html" rel="noopener ugc nofollow" target="_blank"> DataFrame.describe() </a>方法生成描述性统计数据，这些统计数据总结了数据集分布的集中趋势、离散度和形状，不包括<code class="fe ob oc od ns b">NaN</code>值。这个方法告诉我们关于数据集的很多事情。重要的一点是<code class="fe ob oc od ns b">describe()</code>方法只处理数值。它不适用于任何分类值。</p><p id="7ba9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们来理解由<code class="fe ob oc od ns b">describe()</code>方法生成的统计数据:</p><ul class=""><li id="24ba" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated"><code class="fe ob oc od ns b">count</code>告诉我们一个特征中的<code class="fe ob oc od ns b">NoN-empty</code>行数。</li><li id="c707" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><code class="fe ob oc od ns b">mean</code>告诉我们该特征的平均值。</li><li id="f2d6" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><code class="fe ob oc od ns b">std</code>告诉我们该特征的标准偏差值。</li><li id="84e6" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><code class="fe ob oc od ns b">min</code>告诉我们该特性的最小值。</li><li id="566f" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><code class="fe ob oc od ns b">25%</code>、<code class="fe ob oc od ns b">50%</code>和<code class="fe ob oc od ns b">75%</code>是每个特征的百分位数/四分位数。这种四分位数信息有助于我们发现异常值。</li><li id="4327" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><code class="fe ob oc od ns b">max</code>告诉我们该特性的最大值。</li></ul><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="1ef6" class="nf lx iq ns b gy nw nx l ny nz">dataset.describe()</span></pre><figure class="mz na nb nc gt jr"><div class="bz fp l di"><div class="nd ne l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">dataset.describe() method output</figcaption></figure><p id="66b6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">重命名列以增加可读性:</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="6790" class="nf lx iq ns b gy nw nx l ny nz">dataset.rename(columns = {<br/>    "Col1" : "pelvic_incidence", <br/>    "Col2" : "pelvic_tilt",<br/>    "Col3" : "lumbar_lordosis_angle",<br/>    "Col4" : "sacral_slope", <br/>    "Col5" : "pelvic_radius",<br/>    "Col6" : "degree_spondylolisthesis", <br/>    "Col7" : "pelvic_slope",<br/>    "Col8" : "direct_tilt",<br/>    "Col9" : "thoracic_slope", <br/>    "Col10" :"cervical_tilt", <br/>    "Col11" : "sacrum_angle",<br/>    "Col12" : "scoliosis_slope", <br/>    "Class_att" : "class"}, inplace=True)</span></pre><p id="ffa2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html" rel="noopener ugc nofollow" target="_blank"> DataFrame.info() </a>打印关于数据帧的信息，包括<code class="fe ob oc od ns b">index</code> dtype 和<code class="fe ob oc od ns b">column</code>dtype、<code class="fe ob oc od ns b">non-null</code>值和内存使用情况。我们可以使用<code class="fe ob oc od ns b">info()</code>来知道一个数据集是否包含任何缺失值。</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="2cc3" class="nf lx iq ns b gy nw nx l ny nz">dataset.info()</span></pre><h2 id="befe" class="nf lx iq bd ly ng nh dn mc ni nj dp mg ko nk nl mk ks nm nn mo kw no np ms nq bi translated">可视化异常和正常情况的数量:</h2><p id="a9ca" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated"><code class="fe ob oc od ns b">abnormal</code>病例的趋势比<code class="fe ob oc od ns b">normal</code>病例高 2 倍。</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="110d" class="nf lx iq ns b gy nw nx l ny nz">dataset["class"].value_counts().sort_index().plot.bar()</span></pre><figure class="mz na nb nc gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f2c4246a5ef3f28a0eea381fe8cdcf2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*YZcpJuaFxahZsQUhr_8Sow.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">class distribution</figcaption></figure><h2 id="0fd8" class="nf lx iq bd ly ng nh dn mc ni nj dp mg ko nk nl mk ks nm nn mo kw no np ms nq bi translated">检查功能之间的相关性:</h2><p id="a640" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated"><a class="ae kc" href="https://en.wikipedia.org/wiki/Correlation_coefficient" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir">相关系数</strong> </a>是某种相关性的数值度量，表示两个变量之间的统计关系。</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="1189" class="nf lx iq ns b gy nw nx l ny nz">dataset.corr()</span></pre><p id="67dd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可视化与<a class="ae kc" href="https://en.wikipedia.org/wiki/Heat_map" rel="noopener ugc nofollow" target="_blank">热图</a>的关联:</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="149d" class="nf lx iq ns b gy nw nx l ny nz">plt.subplots(figsize=(12,8))<br/>sns.heatmap(dataset.corr())</span></pre><figure class="mz na nb nc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/be1fb3a80e30dc3b098ee39cca1283cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M5ole8YWPcDYYsUoBPMDAg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">correlation between features</figcaption></figure><h2 id="4597" class="nf lx iq bd ly ng nh dn mc ni nj dp mg ko nk nl mk ks nm nn mo kw no np ms nq bi translated">自定义相关图:</h2><p id="155f" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">一个<a class="ae kc" href="https://seaborn.pydata.org/generated/seaborn.pairplot.html" rel="noopener ugc nofollow" target="_blank">对图</a>允许我们看到单个变量的分布和两个变量之间的关系。</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="fb43" class="nf lx iq ns b gy nw nx l ny nz">sns.pairplot(dataset, hue="class")</span></pre><p id="477b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下图中，很多事情都在发生。让我们试着理解结对情节。在结对情节中，我们主要需要了解两件事。一个是特征的<strong class="kf ir">分布，另一个是一个特征与所有其他特征</strong>之间的<strong class="kf ir">关系。如果我们看对角线，我们可以看到每个特征的分布。让我们考虑一下<code class="fe ob oc od ns b">first row X first column</code>，这条对角线向我们展示了<code class="fe ob oc od ns b">pelvic_incidence</code>的分布。同样，如果我们观察<code class="fe ob oc od ns b">second row X second column</code>对角线，我们可以看到<code class="fe ob oc od ns b">pelvic_tilt</code>的分布。除对角线以外的所有单元格都显示了一个要素与另一个要素之间的关系。让我们考虑一下<code class="fe ob oc od ns b">first row X second column</code>，这里我们可以说明<code class="fe ob oc od ns b">pelvic_incidence</code>和<code class="fe ob oc od ns b">pelvic_tilt</code>之间的关系。</strong></p><figure class="mz na nb nc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi og"><img src="../Images/cd362052ab5eaee9c0308f75ad322fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o8UYdiaC4eU_c6padpGMIw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">custom correlogram</figcaption></figure><h2 id="c8a9" class="nf lx iq bd ly ng nh dn mc ni nj dp mg ko nk nl mk ks nm nn mo kw no np ms nq bi translated">使用直方图可视化要素:</h2><p id="8974" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">一个<strong class="kf ir"> </strong> <a class="ae kc" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir">直方图</strong> </a>是显示频率分布最常用的图形。</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="0f5d" class="nf lx iq ns b gy nw nx l ny nz">dataset.hist(figsize=(15,12),bins = 20, color="#007959AA")<br/>plt.title("Features Distribution")<br/>plt.show()</span></pre><figure class="mz na nb nc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oh"><img src="../Images/e45133d5938dedbd2f7f93c988557b20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mbFCkbQ2xHYF39ysNpHhrA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">features histogram</figcaption></figure><h2 id="d2cb" class="nf lx iq bd ly ng nh dn mc ni nj dp mg ko nk nl mk ks nm nn mo kw no np ms nq bi translated">检测和移除异常值</h2><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="9ed5" class="nf lx iq ns b gy nw nx l ny nz">plt.subplots(figsize=(15,6))<br/>dataset.boxplot(patch_artist=True, sym=”k.”)<br/>plt.xticks(rotation=90)</span></pre><figure class="mz na nb nc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oi"><img src="../Images/62be5f1e0d50a86630a2d386f7339969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pl2nFp7vFB_h931ZSNU2Cw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Detect outliers using boxplot</figcaption></figure><p id="2b12" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">移除异常值:</strong></p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="b470" class="nf lx iq ns b gy nw nx l ny nz"># we use tukey method to remove outliers.<br/># whiskers are set at 1.5 times Interquartile Range (IQR)</span><span id="9c05" class="nf lx iq ns b gy oa nx l ny nz">def  remove_outlier(feature):<br/>    first_q = np.percentile(X[feature], 25)<br/>    third_q = np.percentile(X[feature], 75)<br/>    IQR = third_q - first_q<br/>    IQR *= 1.5</span><span id="2fa8" class="nf lx iq ns b gy oa nx l ny nz">    minimum = first_q - IQR # the acceptable minimum value<br/>    maximum = third_q + IQR # the acceptable maximum value<br/>    <br/>    mean = X[feature].mean()</span><span id="0fd2" class="nf lx iq ns b gy oa nx l ny nz">    """<br/>    # any value beyond the acceptance range are considered<br/>    as outliers. </span><span id="439f" class="nf lx iq ns b gy oa nx l ny nz">    # we replace the outliers with the mean value of that <br/>      feature.<br/>    """</span><span id="f29b" class="nf lx iq ns b gy oa nx l ny nz">    X.loc[X[feature] &lt; minimum, feature] = mean <br/>    X.loc[X[feature] &gt; maximum, feature] = mean</span><span id="ae4f" class="nf lx iq ns b gy oa nx l ny nz"><br/># taking all the columns except the last one<br/># last column is the label</span><span id="a053" class="nf lx iq ns b gy oa nx l ny nz">X = dataset.iloc[:, :-1]</span><span id="39f1" class="nf lx iq ns b gy oa nx l ny nz">for i in range(len(X.columns)): <br/>        remove_outlier(X.columns[i])</span></pre><p id="28c5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">移除异常值后:</p><figure class="mz na nb nc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oj"><img src="../Images/e2bfdb890e35b7b25a4e3d8d54f456a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-bc1vi1lER1rJYGCvui6eA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">features distribution after removing outliers</figcaption></figure><h2 id="b083" class="nf lx iq bd ly ng nh dn mc ni nj dp mg ko nk nl mk ks nm nn mo kw no np ms nq bi translated">特征缩放:</h2><p id="7384" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated"><a class="ae kc" href="http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html" rel="noopener ugc nofollow" target="_blank">特征缩放</a>尽管标准化(或 Z 分数归一化)对于许多机器学习算法来说可能是一个重要的预处理步骤。我们的数据集包含在量级、单位和范围上差异很大的要素。但由于大多数机器学习算法在计算中使用两个数据点之间的欧几里德距离，这将产生一个问题。为了避免这种影响，我们需要将所有的特征放在相同的量级上。这可以通过<a class="ae kc" href="https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e" rel="noopener">特征缩放</a>来实现。</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="29f6" class="nf lx iq ns b gy nw nx l ny nz">scaler = MinMaxScaler()<br/>scaled_data = scaler.fit_transform(X)<br/>scaled_df = pd.DataFrame(data = scaled_data, columns = X.columns)<br/>scaled_df.head()</span></pre><figure class="mz na nb nc gt jr"><div class="bz fp l di"><div class="nd ne l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">dataset head after feature scaling</figcaption></figure><h2 id="49e7" class="nf lx iq bd ly ng nh dn mc ni nj dp mg ko nk nl mk ks nm nn mo kw no np ms nq bi translated">标签编码:</h2><p id="d2d5" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">像<a class="ae kc" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>这样的算法只能将数值作为它们的预测变量。因此，我们需要编码我们的分类值。来自<code class="fe ob oc od ns b">sklearn.preprocessing</code>包的<a class="ae kc" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html" rel="noopener ugc nofollow" target="_blank"> LabelEncoder </a>对值在<code class="fe ob oc od ns b">0</code>和<code class="fe ob oc od ns b">n_classes-1</code>之间的标签进行编码。</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="e8e4" class="nf lx iq ns b gy nw nx l ny nz">label = dataset["class"]</span><span id="1efe" class="nf lx iq ns b gy oa nx l ny nz">encoder = LabelEncoder()<br/>label = encoder.fit_transform(label)</span></pre></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h2 id="b22f" class="nf lx iq bd ly ng nh dn mc ni nj dp mg ko nk nl mk ks nm nn mo kw no np ms nq bi translated">模型培训和评估:</h2><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="abf9" class="nf lx iq ns b gy nw nx l ny nz">X = scaled_df<br/>y = label</span><span id="8f0d" class="nf lx iq ns b gy oa nx l ny nz">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)</span><span id="e512" class="nf lx iq ns b gy oa nx l ny nz">clf_gnb = GaussianNB()<br/>pred_gnb = clf_gnb.fit(X_train, y_train).predict(X_test)<br/>accuracy_score(pred_gnb, y_test)</span><span id="333d" class="nf lx iq ns b gy oa nx l ny nz"># Out []: 0.8085106382978723</span><span id="1be8" class="nf lx iq ns b gy oa nx l ny nz">clf_svc = SVC(kernel="linear")<br/>pred_svc = clf_svc.fit(X_train, y_train).predict(X_test)<br/>accuracy_score(pred_svc, y_test)</span><span id="e923" class="nf lx iq ns b gy oa nx l ny nz"># Out []: 0.7872340425531915</span><span id="eeec" class="nf lx iq ns b gy oa nx l ny nz">clf_xgb =  XGBClassifier()<br/>pred_xgb = clf_xgb.fit(X_train, y_train).predict(X_test)<br/>accuracy_score(pred_xgb, y_test)</span><span id="5750" class="nf lx iq ns b gy oa nx l ny nz"># Out []: 0.8297872340425532</span></pre><h2 id="af65" class="nf lx iq bd ly ng nh dn mc ni nj dp mg ko nk nl mk ks nm nn mo kw no np ms nq bi translated">功能重要性:</h2><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="ba8c" class="nf lx iq ns b gy nw nx l ny nz">fig, ax = plt.subplots(figsize=(12, 6))<br/>plot_importance(clf_xgb, ax=ax)</span></pre><figure class="mz na nb nc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ok"><img src="../Images/11a99ea04368e4133cb91dc153677e1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4q1ag7U_-OlZk0lrFLEjHg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">feature importance</figcaption></figure><h2 id="e401" class="nf lx iq bd ly ng nh dn mc ni nj dp mg ko nk nl mk ks nm nn mo kw no np ms nq bi translated">边缘地块</h2><p id="b428" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">边际图<a class="ae kc" href="https://python-graph-gallery.com/82-marginal-plot-with-seaborn/" rel="noopener ugc nofollow" target="_blank">允许我们研究两个数字变量之间的关系。中间的图表显示了它们的相关性。</a></p><p id="d80c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们想象一下<code class="fe ob oc od ns b">degree_spondylolisthesis</code>和<code class="fe ob oc od ns b">class</code>之间的关系:</p><pre class="mz na nb nc gt nr ns nt nu aw nv bi"><span id="55b1" class="nf lx iq ns b gy nw nx l ny nz">sns.set(style="white", color_codes=True)<br/>sns.jointplot(x=X["degree_spondylolisthesis"], y=label, kind='kde', color="skyblue")</span></pre><figure class="mz na nb nc gt jr gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/676d3d3b9c361e448ecc688448d4346f.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*JCa4VaVnA48J4_YRHE4Cnw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Marginal plot between <code class="fe ob oc od ns b">degree_spondylolisthesis and class</code></figcaption></figure><p id="7157" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就这些。感谢阅读。:)</p><p id="d3c9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完整代码请访问<a class="ae kc" href="https://www.kaggle.com/nasirislamsujan/exploratory-data-analysis-lower-back-pain?scriptVersionId=5480406" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>或<a class="ae kc" href="https://colab.research.google.com/drive/1m4ZOmii2y9W8YfsWow7wLN3K8Ia84j9Y" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>。</p><p id="ac29" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你喜欢这篇文章，然后给👏鼓掌。编码快乐！</p></div></div>    
</body>
</html>