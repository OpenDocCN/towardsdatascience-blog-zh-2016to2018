<html>
<head>
<title>Math Behind Reinforcement Learning, the Easy Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习背后的数学，简单的方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/math-behind-reinforcement-learning-the-easy-way-1b7ed0c030f4?source=collection_archive---------2-----------------------#2018-08-02">https://towardsdatascience.com/math-behind-reinforcement-learning-the-easy-way-1b7ed0c030f4?source=collection_archive---------2-----------------------#2018-08-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/1220777412d6201c04a2da1076b7a229.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FPOScqIaUZc8xJTa"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@jeshoots?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">JESHOOTS.COM</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4589" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更新:学习和练习强化学习的最佳方式是去<a class="ae kf" href="http://rl-lab.com" rel="noopener ugc nofollow" target="_blank">http://rl-lab.com</a></p><p id="a870" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看看这个等式:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi le"><img src="../Images/dc68c80adc6e5280a364b9c5a47d25c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XBua6P6Rw1w4FRmsh0e7ZQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Value function of Reinforcement Learning</figcaption></figure><p id="0c41" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果它没有吓倒你，那么你是一个数学通，阅读这篇文章是没有意义的:)</p><p id="7f55" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章不是关于教授强化学习(RL)的，而是关于解释它背后的数学原理。因此，它假设你已经知道什么是 RL，但在理解数学方程时有一些困难。</p><p id="154d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你不知道 RL，你最好在回到本文之前阅读一下。</p><p id="3de7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将一步一步地探究上述等式是如何以及为什么会出现的。</p><h2 id="fa8d" class="lj lk it bd ll lm ln dn lo lp lq dp lr kr ls lt lu kv lv lw lx kz ly lz ma mb bi translated">状态和奖励</h2><p id="78f3" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">让我们考虑一系列的国家 S1，S2，…，Sn 每个国家都有某种奖励 R1，R2，…，Rn。我们知道一个代理人(例如:机器人)的工作是最大化其总报酬。这意味着它将经过提供最大奖励的州。</p><p id="ce07" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设代理人在 S1 州立大学，应该有办法让它知道什么是最大化其回报的最佳途径。考虑到代理人没有看到其紧邻国家以外的地方。</p><p id="4c09" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为此，除了每个状态<strong class="ki iu"> <em class="mh"> s </em> </strong>的奖励之外，我们还将存储另一个值 V，它代表每个状态所连接的其他状态的奖励。<br/>例如，V1 代表与 S1 相连的所有州的总奖励。奖励 R1 不是 V1 的一部分。但是 S2 的 R2 是 S1 的 V1 的一部分。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mi"><img src="../Images/ad3f0dda76edc711a967793ee2ff3485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cRZKGnHxPgQHb2xsETPNlg.png"/></div></div></figure><p id="b971" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这样，通过简单地查看下一个状态，代理将知道后面是什么。</p><p id="fe36" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">存储在状态<strong class="ki iu"> <em class="mh"> s </em> </strong>的值 V(s)是从一个称为“值函数”的函数中计算出来的。价值函数计算未来的回报。<br/>注意，最终状态(也称为终端状态)没有值 V (V = 0 ),因为没有未来状态和未来奖励。</p><p id="113b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">价值函数在计算未来奖励时也使用折旧。<br/>这与金融业的情况类似，你两年后获得的 1000 美元不如你今天获得的 1000 美元有价值。为了表达这个想法，我们将 1000 美元乘以某个折扣因子𝛄 (0≤𝛄 ≤1)的幂<strong class="ki iu"> <em class="mh"> t </em> </strong>其中<strong class="ki iu"> <em class="mh"> t </em> </strong>是收到付款之前的时间步数。</p><p id="34c3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，如果您预计两年后有 1000 美元，贴现因子为 0.9，那么这 1000 美元的今天价值就是 1000 * 0.9 = 810 美元</p><p id="aa6b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为什么这在 RL 中很重要？</p><p id="dda4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设最终状态 S(n)具有 R(n) = 1，并且所有中间状态 S(i)具有 R(i)= 0，将每个状态上的 R(n)乘以𝛄的幂<strong class="ki iu"> <em class="mh"> t </em> </strong>将给出比前一状态更低的 v。这将创建一个从起点到终点递增 V 的序列，这构成了对代理的一个提示，即哪个方向的回报最大化。</p><p id="08e3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，到目前为止，我们已经确定每个状态都有一个奖励 R(可能为零)和一个代表未来奖励的值 V。换句话说，V(s)是来自其他州的未来回报的函数。</p><p id="a55d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数学上它被写成</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/bc4fdc30873690f878d04d842f31e95f.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*3p2itZ5GsTvDSb_0pqZrRg.png"/></div></figure><p id="859f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中 St 是所有与 s 直接或间接相连的状态。</p><p id="586c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，用下一个状态来表示 V(s)比仅用回报 R 来表示更容易，这具有当我们知道下一个状态的 V 时计算当前状态的 V 的优点，而不是对所有未来状态的所有回报求和。</p><p id="5a33" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">公式变成<strong class="ki iu"><em class="mh">v(s)= r+𝛄* v(s’)</em></strong></p><p id="f4e0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">到目前为止，我们已经假设所有的状态都是按顺序连接的，但是很少出现这种情况，每个状态都可以连接到代理可能移动到的多个其他状态。</p><p id="706a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设 S1 连接到 S2、S3、S4 和 S5，S1 的 V 值应该反映这种情况。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/f87bbbf6d060b21e75299454f793f439.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*dmI7stmF2CfbiUfXZGQ8lA.png"/></div></figure><p id="c48b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"><em class="mh">v(S1)=(R2+R3+R4+r5+𝛄*(v2+v3+v4+V5))/4</em>。</strong> <br/>我们这里说的是 V(S1)是它所连接的状态的所有值的平均值。请记住，每个 V 都包含未来回报的值，因此通过对邻居进行平均，我们还可以了解他们之后会发生什么。</p><p id="4fd0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个公式表明，我们可以从 S1 到 S2、S3、S4 和 S5，而没有任何对任何特定州的偏好，然而这是不准确的，因为我们知道有一定的概率去每个相邻州，并且这些概率可能不相同。例如，可能有 50%的机会去 S2，30%的机会去 S3，10%的机会去 S4 和 S5。<br/>让我们分别称这些概率为 p2，p3，p4，p5。</p><p id="d3e4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">于是 V(S1)就变成了<strong class="ki iu"><em class="mh">v(S1)= p2 * R2+P3 * R3+P4 * R4+P5 * r5+𝛄*(p2 * v2+P3 * v3+P4 * v4+P5 * V5)</em></strong>。不出所料，这些概率被称为转移概率，因为它们表示从一种状态转移到另一种状态的可能性。</p><p id="a18a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们也可以把它们表示为一个矩阵<strong class="ki iu"> <em class="mh"> P </em> </strong>其中<strong class="ki iu"> <em class="mh"> Pij </em> </strong>是从一个状态<strong class="ki iu"> <em class="mh"> i </em> </strong>到一个状态<strong class="ki iu"> <em class="mh"> j </em> </strong>的跃迁概率。当没有可能的转换时，<strong class="ki iu"> <em class="mh"> Pij </em> </strong>将为零。</p><p id="3927" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们把公式安排得更有感染力一点:<br/><strong class="ki iu">v(S1)= p2 *(R2+𝛄*v2)+p3*(r3+𝛄*v3)+p4(r4+𝛄*v4)+P5 *(r5+𝛄*v5).</strong></p><p id="8eee" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">或者采用以下形式:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ml"><img src="../Images/6fa00c503742f5b65b9631c958eb83b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BNdq7JdWEUqTjeTs8q_IUA.png"/></div></div></figure><p id="e7a1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="ki iu"> <em class="mh"> P(Sk|S) </em> </strong>是到达状态<strong class="ki iu"> <em class="mh"> Sk </em> </strong>已知我们处于<strong class="ki iu"> <em class="mh"> S1 </em> </strong>的概率。</p><p id="f9f4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在一般形式下，我们可以把它写成:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mm"><img src="../Images/6e749ab8cf197a877df0fb27dac1c193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Im9k-zqolKX3st3Uxon56w.png"/></div></div></figure><p id="9b8b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意<strong class="ki iu"> <em class="mh"> k </em> </strong>经过所有状态，这意味着我们在<strong class="ki iu">对</strong>所有状态求和！如果你感到惊讶，不要。正如我们之前所说，转移概率是一个矩阵，它给出了从一个状态转移到另一个状态的概率。因为不是所有的状态都与其他状态相连，所以这个矩阵是稀疏的，包含许多零。</p><p id="748e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">示例让我们考虑前面的示例，其中 s 1 连接到 S2、S3、S4 和 S5，还让我们假设州的总数是 100，所以我们有 S1 到 s 100。做<strong class="ki iu"><em class="mh">∑(p(sk | s)*(r(k)+𝛄* v(sk))</em></strong>其中<strong class="ki iu"> <em class="mh"> k </em> </strong>从 2 到 100，与做 sum 从<strong class="ki iu"> <em class="mh"> k = 2 到 5 </em> </strong>是一样的，因为对于所有的<strong class="ki iu"> <em class="mh"> k ≥ 6 P(Sk|S)=0。</em> </strong></p><h2 id="7693" class="lj lk it bd ll lm ln dn lo lp lq dp lr kr ls lt lu kv lv lw lx kz ly lz ma mb bi translated">随机奖励</h2><p id="8086" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">现在深呼吸，因为我们将增加一层新的复杂性！<br/>奖励本身是不确定的，这意味着你不能假设<strong class="ki iu"> R </strong>在每个状态下都是精确已知的。事实上，它是概率性的，可以取不同的值。</p><p id="ea70" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了说明这一事实，假设你是一名射手，你正在瞄准一个目标，我们将假设只有三种状态，S1(瞄准)、S2(击中)、S3(未中)。你肯定知道目标是一串同心圆，你击中的内圈越多，你得到的奖励就越多。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/d01e43319b3f1242bb2e23df6eb11f76.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*qqPf0k4naDxPV2X_3vWhzQ.png"/></div></figure><p id="fcbb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设你是一个相当好的弓箭手，你有 80%的机会击中目标，所以你有 20%的风险错过它。击中中心的奖励是 100，外圈是 50，最外圈奖励是 10。击中中心的概率是 10%，击中外圈的概率是 30%，击中最外侧的概率是 60%。最后，我们还将假设完全错过目标将导致-50。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mo"><img src="../Images/484c2563362194def465129877d997e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*faRAVkapkq1IEYryH1RMPA.png"/></div></div></figure><p id="2ad2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以状态 S1 的值将是:<br/><strong class="ki iu"><em class="mh">v(S1)= . 8 *(0.1 * 100+. 3 * 50+. 6 * 10+𝛄* v(S2))+. 2 *(1 *-50+𝛄* v(S3))</em></strong></p><p id="c899" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于 S2 和 S3 是终态，那么 V(S1)和 V(S2)都是零，但上面提到它们是为了不断提醒一般公式。<br/>你可以清楚地看到，每个状态下的奖励都乘以各自的概率，然后求和，例如:(<strong class="ki iu"><em class="mh">. 1 * 100+. 3 * 50+. 6 * 10)</em></strong>和<strong class="ki iu"> <em class="mh"> (1*-50) </em> </strong>，每个状态都乘以导致它的转移概率例如:<strong class="ki iu"><em class="mh">. 8 *(1 * 100+. 3 * 50+. 6 * 10+𝛄* v(S2))</em></strong></p><p id="e5d8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面我们可以推导出通式:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mp"><img src="../Images/8438c8ff4acb42165b57f4665f72a220.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bh_thYhxQ-PJ0l-uET9mAw.png"/></div></div></figure><p id="f756" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">值得澄清的是，表达式<strong class="ki iu"> <em class="mh"> p(si，rj | s)</em></strong>读作从状态<strong class="ki iu"> <em class="mh"> s </em> </strong>过渡到<strong class="ki iu"> <em class="mh"> si </em> </strong>并带有奖励<strong class="ki iu"> <em class="mh"> rj </em> </strong>的概率。<br/>例如<strong class="ki iu"> <em class="mh"> p(S2，100|S1) </em> </strong>读作假设我们在 S1 状态，奖励 100(击中中心)去 S2(击中状态)的概率。答案是. 8 * .1 = .08 (8%)。</p><p id="2f23" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你还没有注意到，这构成了初始公式的第二部分(在页面的顶部)。<br/>只需将<strong class="ki iu"> S </strong>中的<strong class="ki iu"> <em class="mh"> si、rj </em> </strong>替换为<em class="mh">S’</em>，将<strong class="ki iu"> R </strong>中的<strong class="ki iu"> <em class="mh"> r </em> </strong>替换为<strong class="ki iu">R</strong>即可得到:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mq"><img src="../Images/f6d6d27b68aecdfd7dc3991dc2cd1406.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bI8-Lll93bgQb-TPiZ6iXA.png"/></div></div></figure><p id="1a3f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> S </strong>是所有状态的集合<strong class="ki iu"> R </strong>是所有奖励的集合。<br/>我们接下来将讨论行动<strong class="ki iu"><em class="mh"/></strong>和政策<strong class="ki iu"> 𝜋 </strong>。</p><h2 id="400b" class="lj lk it bd ll lm ln dn lo lp lq dp lr kr ls lt lu kv lv lw lx kz ly lz ma mb bi translated">行动和政策</h2><p id="f8e2" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">到目前为止，我们已经说过，我们是随机地在状态之间转换的。例如，我们有 80%的机会从 S1 转移到 S2，20%的机会从 S1 转移到 S3。但是我们没有说这是怎么做到的！是什么引发了这种转变？答案很简单，就是代理在某个状态下做的某个动作。这个动作可能不是唯一的，在给定的状态下可能有几个可用的动作。到目前为止，我们假设有一个被称为“前进”或“做某事”的隐含动作，它以一定的概率将我们从一个状态带到另一个状态。</p><p id="8932" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">似乎随机的奖励是不够的，而且行动是不确定的！你无法保证如果你执行一项行动，你会 100%成功。<br/>我们以弓箭手为例，你的目标是击中正中，得到 100 分，那么你瞄准正中，射出这一箭。然而，有很多因素可能会导致你错过。可能是你不够专注，或者是你放箭的时候手在抖，或者是风太大了。所有这些因素都会影响箭的轨迹。所以你可能会击中中心，或者其中一个外圈，或者你可能会完全错过目标！</p><p id="9ffb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们考虑另一个例子，你在网格上控制一个机器人。假设你用遥控器命令机器人前进。<br/>动作是“向前移动”，预期状态是机器人前面的方块。然而，遥控器可能坏了，或者可能有一些干扰，或者机器人轮子的位置不正确，机器人不是向前移动，而是向右、向左或向后移动。</p><p id="7ddf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">底线，在执行动作<strong class="ki iu"><em class="mh"/></strong>后，从状态<strong class="ki iu"> <em class="mh"> s </em> </strong>到状态<strong class="ki iu"><em class="mh">s’</em></strong>并获得奖励<strong class="ki iu"> <em class="mh"> r </em> </strong>的概率不是 100%。这就是为什么我们写<strong class="ki iu"> <em class="mh"> p(s '，r|s，a) </em> </strong>这是在给定一个状态<strong class="ki iu"><em class="mh">s</em><strong class="ki iu"><em class="mh">a</em> </strong>和一个动作</strong>的情况下，跃迁到状态<strong class="ki iu"> <em class="mh"> s' </em> </strong>的概率。</p><p id="ec7a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如前所述，每个状态都有几种可用的操作。例如，机器人可能在每个状态(或方块)中都有“向前”、“向左”、“向右”、“向后”。一个猎人在狩猎一个猎物时可能会有不同的动作，比如“开枪”、“射箭”或“投掷长矛”，这些动作中的每一个都会有不同的奖励。指示在特定状态下使用哪个动作的策略被称为策略𝜋。</p><p id="e5d2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你猜怎么着！<br/>也是概率性的！我知道生活很艰难:)</p><p id="1b50" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以一个猎人有一些概率使用他的枪，另一个概率使用他的弓，第三个概率使用他的矛。对于一个机器人来说也是一样，它有一定的概率去“前进”，“向左”，“向右”，“向后”。</p><p id="02ec" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">像往常一样，我们通过对所有这些可能性进行平均，在 V(s)中对此进行量化。<br/>给出以下内容:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mr"><img src="../Images/e4da8bab8640e33cbaa9104b8a8e7d7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KWkN1GJ9LBTjGMFwzUW63g.png"/></div></div></figure><p id="4358" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"><em class="mh">【𝜋(a|s】</em></strong>是使用动作<strong class="ki iu"> <em class="mh"> a </em> </strong>遵循策略<strong class="ki iu"> 𝜋 </strong>假设我们处于状态<strong class="ki iu"> <em class="mh"> s. <br/> V𝜋 (s) </em> </strong>是应用<strong class="ki iu"><em class="mh"/>t22】策略<strong class="ki iu"> 𝜋.时状态<strong class="ki iu"> <em class="mh"> s </em> </strong>的值<br/> <em class="mh"> f(a，s，r) </em> </strong>在这里是我们在<strong class="ki iu">随机回报</strong>部分建立的价值函数 V(s)的简写，另外使用<strong class="ki iu"> <em class="mh"> p(s '，r|s，a) </em> </strong>以反映对动作<strong class="ki iu"><em class="mh"/></strong>的依赖。<br/>使用<strong class="ki iu"> <em class="mh"> f(a，s，r) </em> </strong>只是为了减轻复杂性，强调<strong class="ki iu"><em class="mh">【𝜋(a|s】</em></strong>的作用。</strong></p><p id="b86b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以最后在展开<strong class="ki iu"> <em class="mh"> f(a，s，r) </em> </strong>之后，我们得到了这篇冗长文章的主题的初始公式:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi le"><img src="../Images/dc68c80adc6e5280a364b9c5a47d25c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XBua6P6Rw1w4FRmsh0e7ZQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Value function of Reinforcement Learning</figcaption></figure><h2 id="7f2d" class="lj lk it bd ll lm ln dn lo lp lq dp lr kr ls lt lu kv lv lw lx kz ly lz ma mb bi translated">结论</h2><p id="6279" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">希望这篇文章能够揭开强化学习中价值函数背后的数学之谜。</p><p id="4216" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为本文的一个收获，您可以将状态<strong class="ki iu"> <em class="mh"> s </em> </strong>的价值函数 V(s)理解为在该状态<strong class="ki iu"> <em class="mh"> s </em> </strong>(由于某种策略<strong class="ki iu"><em class="mh">【𝜋</em></strong>)下可用的动作所提供的平均奖励，其中每个动作都有一定的概率将代理转换到状态<strong class="ki iu"><em class="mh"/></strong>T74】</p><h2 id="a54b" class="lj lk it bd ll lm ln dn lo lp lq dp lr kr ls lt lu kv lv lw lx kz ly lz ma mb bi translated">相关文章</h2><ul class=""><li id="9b13" class="ms mt it ki b kj mc kn md kr mu kv mv kz mw ld mx my mz na bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/understanding-reinforcement-learning-math-for-developers-b538b6ef921a">理解强化学习数学，面向开发者</a></li><li id="5186" class="ms mt it ki b kj nb kn nc kr nd kv ne kz nf ld mx my mz na bi translated"><a class="ae kf" href="https://medium.com/@zsalloum/revisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182" rel="noopener">面向开发人员的强化学习政策</a></li><li id="1e29" class="ms mt it ki b kj nb kn nc kr nd kv ne kz nf ld mx my mz na bi translated"><a class="ae kf" href="https://medium.com/p/9350e1523031" rel="noopener"> Q vs V 在强化学习中，最简单的方法</a></li><li id="0eaa" class="ms mt it ki b kj nb kn nc kr nd kv ne kz nf ld mx my mz na bi translated"><a class="ae kf" href="https://medium.com/p/1b7ed0c030f4" rel="noopener">数学背后的强化学习，最简单的方法</a></li><li id="bf04" class="ms mt it ki b kj nb kn nc kr nd kv ne kz nf ld mx my mz na bi translated"><a class="ae kf" href="https://medium.com/@zsalloum/dynamic-programming-in-reinforcement-learning-the-easy-way-359c7791d0ac" rel="noopener">动态编程在强化学习中的简便方法</a></li><li id="31c6" class="ms mt it ki b kj nb kn nc kr nd kv ne kz nf ld mx my mz na bi translated"><a class="ae kf" href="https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511" rel="noopener">蒙特卡洛强化学习中的简单方法</a></li><li id="6b89" class="ms mt it ki b kj nb kn nc kr nd kv ne kz nf ld mx my mz na bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce"> TD 在强化学习中，最简单的方法</a></li></ul></div></div>    
</body>
</html>