<html>
<head>
<title>Graph Representation Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图形表示学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graph-representation-learning-dd64106c9763?source=collection_archive---------3-----------------------#2017-12-13">https://towardsdatascience.com/graph-representation-learning-dd64106c9763?source=collection_archive---------3-----------------------#2017-12-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/001ce783e427847006bbafd432552bb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4VCJP-XMwB1zm6QAKTeIog.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Graph captured on the <a class="ae kf" href="https://marco-brambilla.com/2017/08/31/analysis-of-user-behaviour-and-social-media-content-for-art-and-culture-events/" rel="noopener ugc nofollow" target="_blank">Floating Piers study</a> conducted in our data science lab.</figcaption></figure><p id="9f31" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在使用复杂信息的任何科学和工业领域，图模型普遍用于描述信息。</p><p id="bdd4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在图中需要解决的经典问题有:节点分类、链接预测、社区检测等等。</p><p id="bb3b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当网络庞大时，分析它们会变得很有挑战性。一般来说，机器学习技术在网络上效果不佳。</p><p id="0c61" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，考虑到实际的网络结构，要点变成<strong class="ki iu">学习网络中节点的特征。</strong>这些被称为特征表示或嵌入。因此，任何节点都被描述为一个向量。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi le"><img src="../Images/38b3262bcd8d73445b341df7d1435654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vo59kkSP6Qy5Gi2M8mkjCw.jpeg"/></div></div></figure><h2 id="a84f" class="lj lk it bd ll lm ln dn lo lp lq dp lr kr ls lt lu kv lv lw lx kz ly lz ma mb bi translated">目标是将每个节点映射到一个低维空间，同时保留大部分网络信息。</h2><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mc"><img src="../Images/5a1f367e6cf1222f8dece46583ee0f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qN3yM2wdvQISya0rjoEk8Q.jpeg"/></div></div></figure><p id="01ad" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基于这种低维表示，我可以进行任何下游分析。</p><p id="d3d8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">主要的挑战是，如果与图像、音频或文本相比，网络的结构非常不规则。图像可以被看作是刚性的图形网格。运行机器学习更容易。</p><p id="ae68" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相反，图是非欧几里得的:节点的数量是任意的，它们的连接也是任意的。例如，如何将它输入神经网络？这就是为什么我们需要图形嵌入。</p><h1 id="5ff6" class="md lk it bd ll me mf mg lo mh mi mj lr mk ml mm lu mn mo mp lx mq mr ms ma mt bi translated">node2vec:无监督特征学习</h1><p id="49e4" class="pw-post-body-paragraph kg kh it ki b kj mu kl km kn mv kp kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">主要思想是找到保持相似性的维度为<em class="mz"> d </em>的节点的嵌入。目标也是嵌入使得附近的节点靠得很近。我需要计算一个节点的邻域，然后运行一个最大化问题，以一种<em class="mz"> f(u) </em>预测邻域<em class="mz"> N(u) </em>的方式计算节点<em class="mz"> u </em>的特征学习。这可以通过 softmax 函数来实现，假设节点的因式分解(独立性)。</p><p id="4a2d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">邻域的定义可以基于 BFS(宽度优先)或 DFS(深度优先)策略。BFS 提供网络的局部微观视图，而 DFS 提供网络的宏观视图。一个聪明的解决方案是 BFS 和 DFS 的插值。这是一个二阶随机游走，因为我们记得最后访问的节点，并且在每一步我们都要决定是返回最后访问的节点还是继续前往新的节点。本质上，在每一步，我都有三个选择:</p><ul class=""><li id="ef50" class="na nb it ki b kj kk kn ko kr nc kv nd kz ne ld nf ng nh ni bi translated">节点回到更靠近原节点的位置<em class="mz"> u </em></li><li id="cb79" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">与当前节点距离相同的节点相对于<em class="mz"> u </em></li><li id="73a6" class="na nb it ki b kj nj kn nk kr nl kv nm kz nn ld nf ng nh ni bi translated">距离节点<em class="mz">更远的节点 u </em></li></ul><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi no"><img src="../Images/b68c3b72d67fd33dafdf621a4adf58e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d5Dl9SyxIP3euOA4b4egHg.jpeg"/></div></div></figure><p id="f1f6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我可以设置概率<em class="mz"> p </em>(返回)和<em class="mz"> q </em>(离开)来决定每一步是离 u 远还是近。然后我可以运行随机梯度下降(线性时间复杂度)算法来寻找最佳路径。</p><p id="ce61" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基于我如何设置 p 和 q 的值，我可以得到对网络完全不同的解释:随着更大的<em class="mz"> q </em>，我倾向于检测节点的“角色”，随着更小的<em class="mz"> q </em>，我倾向于检测节点的“接近度”。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi np"><img src="../Images/aebb7e24a7be50f6f05c13c708274618.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3eyEleKDqYM1FYKpv9-27g.jpeg"/></div></div></figure><p id="8300" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一种独立于任务的特征学习方法，在某种意义上，它不依赖于你对结果的使用。复杂度是线性的。网络的规模。</p><h1 id="70a5" class="md lk it bd ll me mf mg lo mh mi mj lr mk ml mm lu mn mo mp lx mq mr ms ma mt bi translated">GraphSAGE:监督特征学习</h1><p id="c008" class="pw-post-body-paragraph kg kh it ki b kj mu kl km kn mv kp kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">相反，这种方法是受卷积神经网络的启发，通过将它们从简单网格(图像网格)的处理推广到一般图形。但是如何推广卷积呢？</p><p id="3f23" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以将图像视为一个图表，其中每个像素都连接(并以某种方式影响)附近的像素。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nq"><img src="../Images/9c0016e15d6b0274e4a70023c74cde9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lrc4w4HyTk21W3VK7ztgOw.jpeg"/></div></div></figure><p id="a2f4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一种选择是使用图的邻接矩阵(加上节点的可能特征),并将其输入神经网络。问题:输入的数量与网络的规模成线性关系(巨大)。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nr"><img src="../Images/a0146b54f55d9ba39bd1db79e1185b7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v_IdxvGqo3bX8S3cVCqCfA.jpeg"/></div></div></figure><p id="2d9d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个改进是使用子图(例如，定义节点邻域的一片邻近节点)。这个邻域定义了一个计算图，我用它来传播和转换所考虑节点的所有邻居的信息(即属性)。本质上，对于每个节点，计算图是一个不同的神经网络，我可以运行它来学习参数。我有许多神经网络，它们使用相同的一组参数，所以这就像在进行监督学习。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ns"><img src="../Images/c7e53d1b1119aa176d0e2fc3ed938af6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R1InDXfXWSkuz8qfPJh_ew.jpeg"/></div></div></figure><p id="d42e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如下所述，整个框架可以方便地以 MapReduce 方式执行，因为这自动避免了计算任务的重复。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/daf53ce427ab6f2e88871ea9f893ac6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_-uM_AwxZVNsUdsjM-iH7g.jpeg"/></div></div></figure><p id="8328" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本质上，我们定义了图的卷积网络概念。</p><p id="4e06" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在 Pinterest 中的实验已经成功地在 3 个 BLN 节点和 17 个 BLN 边上运行。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/6661b8b33dbc9e6a3ff724b9f7388c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ylwz_RPNHSnTVPwrU7NK5A.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Graph captured on the <a class="ae kf" href="https://marco-brambilla.com/2017/08/31/analysis-of-user-behaviour-and-social-media-content-for-art-and-culture-events/" rel="noopener ugc nofollow" target="_blank">Floating Piers study</a> conducted in our data science lab.</figcaption></figure></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><p id="c96a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇报道是关于 2017 年 12 月在波士顿举行的第四届 IEEE BigData 会议上由<a class="ae kf" href="https://cs.stanford.edu/people/jure/" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu"> Jure Leskovec </strong> </a>(与斯坦福大学和 Pinterest)发表的主题演讲。</p><p id="a3af" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">演讲的完整幻灯片<a class="ae kf" href="http://i.stanford.edu/~jure/pub/talks2/graphsage-ieee_bigdata-dec17a.pdf" rel="noopener ugc nofollow" target="_blank">可以在 Jure 的主页上找到:</a></p><figure class="lf lg lh li gt ju"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="511c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="http://i.stanford.edu/~jure/pub/talks2/graphsage-ieee_bigdata-dec17a.pdf" rel="noopener ugc nofollow" target="_blank">http://I . Stanford . edu/~ jure/pub/talks 2/graph sage-IEEE _ big data-dec 17a . pdf</a></p></div></div>    
</body>
</html>