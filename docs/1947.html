<html>
<head>
<title>Principal Component Analysis- Intro</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析-简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-intro-61f236064b38?source=collection_archive---------0-----------------------#2017-11-21">https://towardsdatascience.com/principal-component-analysis-intro-61f236064b38?source=collection_archive---------0-----------------------#2017-11-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/8d3b38f476c1b410475eb58c69c0c4fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cTec2mXxUcoyDoTk13vevg.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="ebb6" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">可变缩减技术</h2></div><blockquote class="kq kr ks"><p id="b091" class="kt ku kv kw b kx ky kc kz la lb kf lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">变数太多？你应该使用所有可能的变量来生成模型吗？</p></blockquote><p id="2c46" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">为了处理“维数灾难”并避免高维空间中的过拟合等问题，使用了主成分分析等方法。</p><p id="1945" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">PCA是一种通过从大量数据中提取重要变量来减少数据中变量数量的方法。它减少了数据的维度，目的是保留尽可能多的信息。换句话说，这种方法将高度相关的变量组合在一起，形成一个较小数量的人工变量集，称为“主成分”，它解释了数据中的大多数差异。</p><p id="54c4" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">让我们深入了解PCA是如何在幕后实现的。</p><p id="f743" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated"><strong class="kw jc"> <em class="kv">开始</em> </strong>通过从每个数据点中减去平均值来标准化预测值。对预测值进行归一化非常重要，因为原始预测值可能处于不同的等级，并且可能对方差产生显著影响。结果如表2所示，平均值为零。</p><figure class="lu lv lw lx gt is gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/32b2251b471abe4d6a26d89168c70e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*yffTLw7LRoDgJPzSB-aL1A.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Normalized Data</figcaption></figure><p id="1d92" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated"><strong class="kw jc">接下来</strong>，计算数据的协方差矩阵，该矩阵将测量两个预测器如何一起移动。它是在两个预测值之间测量的，但如果您有三维数据(x，x1，x2)，则测量x x1，x x2，x1 x2之间的协方差。供参考的协方差公式为:</p><figure class="lu lv lw lx gt is gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/9e75fb3c05b6c4625032b954a35bb778.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*vYlPM9CL1aGpO8VNPs1Zmg.png"/></div></figure><p id="8b48" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">在我们的例子中，协方差矩阵如下所示:</p><figure class="lu lv lw lx gt is gh gi paragraph-image"><div class="gh gi md"><img src="../Images/50ff704557adb02d64b2e1491bbc3354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*lSF7qhTkw4O-JsN4YgYqvw.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Covariance Matrix</figcaption></figure><p id="8a1c" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated"><strong class="kw jc">现在</strong>，计算上述矩阵的特征值和特征向量。这有助于发现数据中的潜在模式。在我们的例子中，它大约是:</p><figure class="lu lv lw lx gt is gh gi paragraph-image"><div class="gh gi me"><img src="../Images/3e50f0a822a9d9c2542e265de47ee606.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*4W39v2-lwPlUFpQrs-0OZg.png"/></div><figcaption class="ly lz gj gh gi ma mb bd b be z dk">Eigen Value and Vector</figcaption></figure><p id="c002" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated">我们快到了:)。执行重定向。为了将数据转换成新的轴，将原始数据乘以特征向量，这表示新轴的方向。请注意，您可以选择省略较小的特征向量，或者两者都使用。此外，根据哪一组占了95%或更多差异，决定保留多少组特性。</p><p id="cef4" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated"><strong class="kw jc">最后，</strong>从上述步骤中计算出的分数可被绘制出来，并输入预测模型。绘图让我们感觉到两个变量有多接近/高度相关。我们没有使用原始数据来绘制X和Y轴(这并不能告诉我们点与点之间的关系),而是绘制转换后的数据(使用特征向量),这些数据可以发现模式并显示点之间的关系。</p><p id="28d2" class="pw-post-body-paragraph kt ku jb kw b kx ky kc kz la lb kf lc lq le lf lg lr li lj lk ls lm ln lo lp ij bi translated"><em class="kv">尾注</em>:很容易将<em class="kv"> PCA </em>与<em class="kv">因子分析</em>混淆，但这两种方法在概念上是有区别的。我将在下一篇文章中详细介绍<em class="kv">因子分析</em>以及它与<em class="kv"> PCA </em>的不同之处..敬请关注。</p></div></div>    
</body>
</html>