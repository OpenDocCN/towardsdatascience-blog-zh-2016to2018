<html>
<head>
<title>PCA vs Autoencoders</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PCA与自动编码器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pca-vs-autoencoders-1ba08362f450?source=collection_archive---------1-----------------------#2018-04-23">https://towardsdatascience.com/pca-vs-autoencoders-1ba08362f450?source=collection_archive---------1-----------------------#2018-04-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/b6e56b82eed21db62ae269c323698ef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ren766bdY7fBeOBFs3g2CA.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Behind Shinjuku Station, Tokyo.</figcaption></figure><p id="c070" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在本教程中，我将解释PCA和自动编码器(AE)之间的关系。我假设你对什么是PCA和AE有一个基本的了解，但是如果你不熟悉PCA或自动编码器，请阅读[1，2]。</p><p id="ef28" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">关于使用线性激活自动编码器(AE)来近似主成分分析(PCA)，已经写了很多。从数学的角度来看，最小化PCA中的重建误差与AE [3]相同。</p><p id="00d9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">然而，为什么要把自己局限于线性变换呢？神经网络非常灵活，因此我们可以通过使用非线性激活函数引入非线性[4]。此外，随着特征数量的增加，与AE相比，PCA将导致更慢的处理。我们的假设是，声发射跨越的子空间将类似于主成分分析发现的子空间[5]。</p><p id="e70d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在这项研究中，我们将看到PCA、线性和非线性自动编码器之间的异同。请注意，非线性AE将是非线性的，除非输入数据被线性跨越。</p></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="8ebc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">首先，让我们加载虹膜数据集，并在[0，1]之间缩放。</p><figure class="lk ll lm ln gt ju"><div class="bz fp l di"><div class="lo lp l"/></div></figure><p id="90ba" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们创建一个函数，根据原始标签绘制数据。</p><figure class="lk ll lm ln gt ju"><div class="bz fp l di"><div class="lo lp l"/></div></figure><p id="b063" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">然后，我们使用数据集来拟合主成分分析，并绘制前两个主成分分析图。我们可以看到出现了两个大斑点，使用目标标签，我们可以看到这三个集群是如何包含在这两个斑点中的。</p><figure class="lk ll lm ln gt ju"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="lk ll lm ln gt ju gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/10d9fef84313f34ff64f795a20a4987a.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*6Wa6LT6rwNnE9gKfo_wE-g.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">A plot of PC1 against PC2.</figcaption></figure><p id="0c29" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们的第一个网络是一个线性AE，有3层(编码、隐藏和解码)，编码和解码层有“线性激活”，隐藏层有两个神经元。本质上，这种结构通过将隐藏层中的数据从四个特征减少到两个特征来近似PCA。如您所见，该模型收敛得非常好，我们的验证损失已降至零。在通过隐藏层传递训练数据之后，我们得到两个新的向量，并且通过将它们彼此相对绘制，我们清楚地形成了类似于PCA的斑点和集群。</p><figure class="lk ll lm ln gt ju"><div class="bz fp l di"><div class="lo lp l"/></div></figure><div class="lk ll lm ln gt ab cb"><figure class="lr ju ls lt lu lv lw paragraph-image"><img src="../Images/532fbdab5e749669fab5cbb732503c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*LdgHuUbeXGK5HnfJrygX8Q.png"/></figure><figure class="lr ju lx lt lu lv lw paragraph-image"><img src="../Images/5eb3b3a79314cae4b95fd061a7fa1265.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*vulCDE_8f_g8UuZrP59ekQ.png"/><figcaption class="kb kc gj gh gi kd ke bd b be z dk ly di lz ma">Left: the linear AE train and validation loss. Right: A plot of AE1 against AE2.</figcaption></figure></div><p id="ac34" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们创建第二个AE，这一次我们将用一个sigmoid代替两个线性激活函数。这种网络结构可以被认为是具有非线性变换的PCA，并且类似于上面的网络结构，它收敛到局部最小值，并且我们可以绘制得到的密集向量。</p><figure class="lk ll lm ln gt ju"><div class="bz fp l di"><div class="lo lp l"/></div></figure><div class="lk ll lm ln gt ab cb"><figure class="lr ju mb lt lu lv lw paragraph-image"><img src="../Images/6a0e07b2fee4cfef71c23bf24d30f951.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*3p0pz3UnopaeWK7uFLLnYQ.png"/></figure><figure class="lr ju mc lt lu lv lw paragraph-image"><img src="../Images/5fd908d4ca31f55e236360315a3f9b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*qqIG5_XLySrEMVpWFXx0vg.png"/><figcaption class="kb kc gj gh gi kd ke bd b be z dk md di me ma">Left: the sigmoid-based AE train and validation loss. Right: A plot of AE1 against AE2.</figcaption></figure></div><p id="c167" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们的最后一个AE使用了带有L1正则化的relu激活。换句话说，我们希望通过使用具有受限表示的非线性AE来近似PCA[2]。类似于先前的网络，它收敛到局部最小值，并且两个密集向量显示包含在两个斑点中的三个集群。</p><figure class="lk ll lm ln gt ju"><div class="bz fp l di"><div class="lo lp l"/></div></figure><div class="lk ll lm ln gt ab cb"><figure class="lr ju mb lt lu lv lw paragraph-image"><img src="../Images/4ad73c54b174bc69d3b2edd6848a5504.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*kGqZHFQ_Qk3Z7l8e6aJBKw.png"/></figure><figure class="lr ju mc lt lu lv lw paragraph-image"><img src="../Images/0ae8290d4cfb00e18b25831b1151a0b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*lt_KR5fT1tiwRETJxVhWJQ.png"/><figcaption class="kb kc gj gh gi kd ke bd b be z dk md di me ma">Left: the relu-based AE train and validation loss. Right: A plot of AE1 against AE2.</figcaption></figure></div><p id="7bf2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在训练所有3个自动编码器并通过隐藏层推送我们的训练数据后，我们比较前2个PC和AE的密集特征。我们可以清楚地看到，在所有模型中，这些向量中的数字并不相同。通过绘制每两个向量并观察得到的集群，这一点非常清楚。接下来，我们将比较具有2个和3个聚类的简单KMEANS如何对数据进行分类。</p><figure class="lk ll lm ln gt ju"><div class="bz fp l di"><div class="lo lp l"/></div></figure><figure class="lk ll lm ln gt ju"><div class="bz fp l di"><div class="lo lp l"/></div></figure><div class="lk ll lm ln gt ab cb"><figure class="lr ju mf lt lu lv lw paragraph-image"><img src="../Images/10d9fef84313f34ff64f795a20a4987a.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*6Wa6LT6rwNnE9gKfo_wE-g.png"/></figure><figure class="lr ju mf lt lu lv lw paragraph-image"><img src="../Images/5eb3b3a79314cae4b95fd061a7fa1265.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*vulCDE_8f_g8UuZrP59ekQ.png"/></figure></div><div class="ab cb"><figure class="lr ju mf lt lu lv lw paragraph-image"><img src="../Images/5fd908d4ca31f55e236360315a3f9b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*qqIG5_XLySrEMVpWFXx0vg.png"/></figure><figure class="lr ju mf lt lu lv lw paragraph-image"><img src="../Images/0ae8290d4cfb00e18b25831b1151a0b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*lt_KR5fT1tiwRETJxVhWJQ.png"/><figcaption class="kb kc gj gh gi kd ke bd b be z dk ly di lz ma">A comparison of all four plots.</figcaption></figure></div><p id="733b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们想知道，用KMEANS (k=2 &amp; 3)聚类的每个结果是否可以在所有模型中相似地标记数据，即使密集范围非常不同。以下是训练集中样本的打印分类向量和指标。打印输出和度量显示，使用由不同算法创建的不同密集表示，可以找到两个相同或三个非常相似的集群。我们可以看到，当对两个斑点进行聚类时，度量分数基本相同，并且当对三个已知的花类进行聚类时，度量数字非常接近。请注意，由于神经网络的随机性，可能会出现微小的差异。</p><figure class="lk ll lm ln gt ju"><div class="bz fp l di"><div class="lo lp l"/></div></figure><p id="461e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我要感谢Natanel Davidovits和Gal Yona的宝贵评论、校对和评论。</p><p id="3a34" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">Ori Cohen在机器学习、脑机接口和神经生物学领域获得了计算机科学博士学位。</p><p id="56a9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">参考资料:</p><p id="ac39" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">[1] <a class="ae mg" href="https://blog.keras.io/building-autoencoders-in-keras.html" rel="noopener ugc nofollow" target="_blank">在keras建造AE</a></p><p id="467b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">[2] <a class="ae mg" href="http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html" rel="noopener ugc nofollow" target="_blank">使用Iris分三步进行PCA</a></p><p id="4747" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">[3] <a class="ae mg" href="https://www.cs.toronto.edu/~urtasun/courses/CSC411/14_pca.pdf" rel="noopener ugc nofollow" target="_blank"> CSC 411:第14讲:主成分分析&amp;自动编码器</a>，第16页。</p><p id="5114" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">[4] <a class="ae mg" href="https://lazyprogrammer.me/a-tutorial-on-autoencoders/" rel="noopener ugc nofollow" target="_blank">深度学习自动编码器教程</a></p><p id="c58e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">[5]<a class="ae mg" href="https://stats.stackexchange.com/questions/120080/whatre-the-differences-between-pca-and-autoencoder" rel="noopener ugc nofollow" target="_blank">PCA和自动编码器</a>有什么区别</p></div></div>    
</body>
</html>