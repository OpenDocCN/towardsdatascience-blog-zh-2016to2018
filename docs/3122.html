<html>
<head>
<title>MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MobileNetV2:反向残差和线性瓶颈</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mobilenetv2-inverted-residuals-and-linear-bottlenecks-8a4362f4ffd5?source=collection_archive---------0-----------------------#2018-04-11">https://towardsdatascience.com/mobilenetv2-inverted-residuals-and-linear-bottlenecks-8a4362f4ffd5?source=collection_archive---------0-----------------------#2018-04-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="ac46" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2017年4月，谷歌的一组研究人员发表了一篇论文，介绍了一种针对移动设备优化的神经网络架构。他们努力寻求一种模型，在保持参数和数学运算尽可能低的同时提供高精度。为了给智能手机带来深度神经网络，这是非常必要的。</p><p id="4c86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">被称为<a class="ae kl" href="https://arxiv.org/abs/1704.04861" rel="noopener ugc nofollow" target="_blank"> MobileNet </a>的架构围绕着使用深度方向可分离卷积的思想，它由一个深度方向卷积和一个点方向卷积组成。如果你对这个操作的细节有点模糊，请随意查看我的另一篇<a class="ae kl" rel="noopener" target="_blank" href="/types-of-convolutions-in-deep-learning-717013397f4d">文章</a>，它详细解释了这个概念。<a class="ae kl" href="https://arxiv.org/abs/1801.04381" rel="noopener ugc nofollow" target="_blank"> MobileNetV2 </a>用两个主要想法扩展了它的前身。</p><h1 id="bd4d" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">反向残差</h1><p id="5c7d" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">残余块用跳过连接来连接卷积块的开始和结束。通过添加这两种状态，网络有机会访问在卷积块中未被修改的早期激活。事实证明，这种方法对于构建深度网络至关重要。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/743e3c114e7cad022fe39be01150e5f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*5Jdh_PDTXp0uhF8c79TEsQ.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">A residual block connects wide layers with a skip connection while layers in between are narrow</figcaption></figure><p id="c5c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当稍微靠近观察跳过连接时，我们注意到<a class="ae kl" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">原始残差块</a>遵循关于通道数量的宽- &gt;窄- &gt;宽方法。输入具有大量通道，这些通道通过廉价的1x1卷积进行压缩。这样，下面的3×3卷积的参数就少得多。为了最终增加输入和输出，使用另一个1x1卷积再次增加通道的数量。在喀拉斯，它看起来像这样:</p><pre class="lq lr ls lt gt mb mc md me aw mf bi"><span id="8ba0" class="mg kn iq mc b gy mh mi l mj mk">def residual_block(x, squeeze=16, expand=64):<br/>  m = Conv2D(squeeze, (1,1), activation='relu')(x)<br/>  m = Conv2D(squeeze, (3,3), activation='relu')(m)<br/>  m = Conv2D(expand, (1,1), activation='relu')(m)<br/>  return Add()([m, x])</span></pre><p id="2eb3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一方面，MobileNetV2遵循窄-&gt;宽-&gt;窄的方法。第一步使用1×1卷积来加宽网络，因为接下来的3×3深度方向卷积已经大大减少了参数的数量。之后，另一个1x1卷积挤压网络，以便匹配初始的信道数量。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/f35f27d282aad604a4742752e74a8789.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*BaxdP8RS5x_EVMNJSd1Urg.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">An inverted residual block connects narrow layers with a skip connection while layers in between are wide</figcaption></figure><p id="f891" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在喀拉斯，它看起来像这样:</p><pre class="lq lr ls lt gt mb mc md me aw mf bi"><span id="37c9" class="mg kn iq mc b gy mh mi l mj mk">def inverted_residual_block(x, expand=64, squeeze=16):<br/>  m = Conv2D(expand, (1,1), activation='relu')(x)<br/>  m = DepthwiseConv2D((3,3), activation='relu')(m)<br/>  m = Conv2D(squeeze, (1,1), activation='relu')(m)<br/>  return Add()([m, x])</span></pre><p id="970a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作者将这种想法描述为反向剩余块，因为在网络的狭窄部分之间存在跳跃连接，这与原始剩余连接的工作方式相反。当您运行上面的两个代码片段时，您会注意到反向块的参数要少得多。</p><h1 id="ef77" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">线性瓶颈</h1><p id="9657" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">我们在神经网络中使用非线性激活函数的原因是多个矩阵乘法不能简化为单个数值运算。它允许我们建立多层神经网络。同时，通常在神经网络中使用的激活函数ReLU丢弃小于0的值。这种信息丢失可以通过增加信道数量来解决，以便增加网络的容量。</p><p id="6ccc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于反向残差块，我们做相反的事情，并挤压跳跃连接所链接的层。这损害了网络的性能。作者引入了线性瓶颈的概念，其中剩余块的最后卷积在被添加到初始激活之前具有线性输出。将它写入代码非常简单，因为我们只需丢弃卷积模块的最后一个激活函数:</p><pre class="lq lr ls lt gt mb mc md me aw mf bi"><span id="b7f8" class="mg kn iq mc b gy mh mi l mj mk">def inverted_linear_residual_block(x, expand=64, squeeze=16):<br/>  m = Conv2D(expand, (1,1), activation='relu')(x)<br/>  m = DepthwiseConv2D((3,3),  activation='relu')(m)<br/>  m = Conv2D(squeeze, (1,1))(m)<br/>  return Add()([m, x])</span></pre><h1 id="f179" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">ReLU6</h1><p id="2abb" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">上面的片段显示了一个卷积块的结构，它包含了反向残差和线性瓶颈。如果您想尽可能地匹配MobileNetV2，您还需要另外两个部分。第一个方面只是在每个卷积层后面增加了批量归一化，这是你现在可能已经习惯的。</p><p id="e269" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二个附加项不太常见。作者使用ReLU6而不是ReLU，这将激活值限制在最大值…嗯…6。只要在0到6之间，激活就是线性的。</p><pre class="lq lr ls lt gt mb mc md me aw mf bi"><span id="3a63" class="mg kn iq mc b gy mh mi l mj mk">def relu(x):<br/>  return max(0, x)</span><span id="a438" class="mg kn iq mc b gy mm mi l mj mk">def relu6(x):<br/>  return min(max(0, x), 6)</span></pre><p id="a0e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这在你处理定点推理时很有帮助。它将小数点左边的信息限制为3位，这意味着我们可以保证小数点右边的精度。这也在最初的MobileNet论文中使用。最后一个构建块看起来像这样:</p><pre class="lq lr ls lt gt mb mc md me aw mf bi"><span id="00a3" class="mg kn iq mc b gy mh mi l mj mk">def bottleneck_block(x, expand=64, squeeze=16):<br/>  m = Conv2D(expand, (1,1))(x)<br/>  m = BatchNormalization()(m)<br/>  m = Activation('relu6')(m)<br/>  m = DepthwiseConv2D((3,3))(m)<br/>  m = BatchNormalization()(m)<br/>  m = Activation('relu6')(m)<br/>  m = Conv2D(squeeze, (1,1))(m)<br/>  m = BatchNormalization()(m)<br/>  return Add()([m, x])</span></pre><h1 id="242b" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">建筑</h1><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/28591677afeb42b649540c0b9fbd4dc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*f29Ku3IxE5POo6zq0KG88w.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">The MobileNetV2 architecture</figcaption></figure><p id="265f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们已经了解了MobileNetV2的构建块，我们可以看一下整个架构。在表格中，您可以看到瓶颈块是如何排列的。<em class="mo"> t </em>代表通道的膨胀率。如您所见，他们使用了系数6，而不是我们示例中的系数4。<em class="mo"> c </em>表示输入通道的数量，<em class="mo"> n </em>表示该块重复的频率。最后,<em class="mo"> s </em>告诉我们块的第一次重复是否为下采样过程使用了步幅2。总而言之，这是一个非常简单和常见的卷积块集合。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mp"><img src="../Images/3c8e22da5896eb0037f8f793c791575d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GDu8Hqek2EkkH6wC4MLUDA.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Performance of MobileNetV2 and other architectures on ImageNet</figcaption></figure><h1 id="4bc2" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">总结想法</h1><p id="2791" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">我特别高兴的是MobileNetV2提供了与NASNet类似的参数效率。NASNet是几项图像识别任务的最新技术。它的构建模块相当复杂，这使得它为什么工作得这么好变得相当不直观。NASNet的构建模块不是由人类设计的，而是由另一个神经网络设计的。引入一个简单的架构，比如MobileNetV2，它表现出了相当的效率，这让我更加相信下一个大的架构可能也是由人类设计的。</p></div></div>    
</body>
</html>