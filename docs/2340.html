<html>
<head>
<title>Introduction to Various Reinforcement Learning Algorithms. Part I (Q-Learning, SARSA, DQN, DDPG)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">各种强化学习算法介绍。第一部分(Q-Learning，SARSA，DQN，DDPG)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287?source=collection_archive---------0-----------------------#2018-01-12">https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287?source=collection_archive---------0-----------------------#2018-01-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/21dc95432419ae4cece199f50ce9acd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MiN803ThUoqdCKwklZ8wwA.png"/></div></div></figure><p id="9b35" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">强化学习(RL)是指一种机器学习方法，在这种方法中，智能体在下一个时间步接收延迟奖励，以评估其先前的行为。它主要用于游戏(如雅达利、马里奥)，性能与人类相当甚至超过人类。最近，随着算法随着神经网络的结合而发展，它能够解决更复杂的任务，例如钟摆问题:</p><figure class="kz la lb lc gt ju"><div class="bz fp l di"><div class="ld le l"/></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Deep Deterministic Policy Gradient (DDPG) Pendulum OpenAI Gym using Tensorflow</figcaption></figure><p id="03ce" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">虽然有大量的RL算法，但似乎没有对它们中的每一个进行全面的比较。在决定将哪些算法应用于特定任务时，我很难抉择。本文旨在通过简要讨论RL设置来解决这个问题，并介绍一些著名的算法。</p></div><div class="ab cl lj lk hx ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="im in io ip iq"><h1 id="f2b4" class="lq lr it bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">1.强化学习101</h1><p id="4a6e" class="pw-post-body-paragraph kb kc it kd b ke mo kg kh ki mp kk kl km mq ko kp kq mr ks kt ku ms kw kx ky im bi translated">通常，RL设置由两个组件组成，代理和环境。</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mt"><img src="../Images/1e2eeb182bd8f19863e45b814ef54024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c3pEt4pFk0Mx684DDVsW-w.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Reinforcement Learning Illustration (<a class="ae mu" href="https://i.stack.imgur.com/eoeSq.png" rel="noopener ugc nofollow" target="_blank">https://i.stack.imgur.com/eoeSq.png</a>)</figcaption></figure><p id="0540" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">那么环境是指智能体所作用的对象(例如Atari游戏中的游戏本身)，而智能体则代表RL算法。该环境首先向代理发送一个状态，然后代理根据它的知识采取行动来响应该状态。之后，环境发送一对下一个状态和奖励给代理。代理将使用环境返回的奖励更新其知识，以评估其最后的动作。循环继续，直到环境发送一个终端状态，结束于epset。</p><p id="7da0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">大多数RL算法都遵循这种模式。在下面的段落中，我将简要地谈谈RL中使用的一些术语，以便于我们在下一节中进行讨论。</p><h2 id="22a9" class="mv lr it bd ls mw mx dn lw my mz dp ma km na nb me kq nc nd mi ku ne nf mm ng bi translated">定义</h2><ol class=""><li id="3410" class="nh ni it kd b ke mo ki mp km nj kq nk ku nl ky nm nn no np bi translated">动作(A):代理可以采取的所有可能的动作</li><li id="0a6f" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">状态:环境返回的当前情况。</li><li id="2c79" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">奖励(R):从环境中发送回来的对上次行为进行评估的即时回报。</li><li id="2e8a" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">策略(π):代理用来根据当前状态确定下一步行动的策略。</li><li id="15cb" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">价值(V):相对于短期回报R. <em class="nv"> Vπ(s) </em>贴现的预期长期回报定义为当前状态下的预期长期回报。</li><li id="33fc" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated">Q-value或action-value (Q): Q-value类似于value，除了它需要一个额外的参数，即当前动作<em class="nv"> a </em>。<em class="nv"> Qπ(s，a) </em>指长期返回当前状态<em class="nv"> s </em>，在策略π下采取行动<em class="nv"> a </em>。</li></ol><h2 id="d5e7" class="mv lr it bd ls mw mx dn lw my mz dp ma km na nb me kq nc nd mi ku ne nf mm ng bi translated">无模型vs .基于模型</h2><p id="6a6f" class="pw-post-body-paragraph kb kc it kd b ke mo kg kh ki mp kk kl km mq ko kp kq mr ks kt ku ms kw kx ky im bi translated">该模型代表对环境动态的模拟。即，模型学习从当前状态s <em class="nv"> 0 </em>和动作a到下一状态s <em class="nv"> 1 </em>的转换概率<em class="nv"> T(s1|(s0，a)) </em>。如果成功学习了转移概率，代理将知道在给定当前状态和动作的情况下进入特定状态的可能性有多大。然而，随着状态空间和动作空间的增长，基于模型的算法变得不切实际。</p><p id="91f4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">另一方面，无模型算法依靠试错法来更新其知识。因此，它不需要空间来存储所有状态和动作的组合。下一节讨论的所有算法都属于这一类。</p><h2 id="e4db" class="mv lr it bd ls mw mx dn lw my mz dp ma km na nb me kq nc nd mi ku ne nf mm ng bi translated">保单与非保单</h2><p id="04af" class="pw-post-body-paragraph kb kc it kd b ke mo kg kh ki mp kk kl km mq ko kp kq mr ks kt ku ms kw kx ky im bi translated">策略上的代理基于其从当前策略导出的当前动作a学习该值，而其策略外的对应部分基于从另一个策略获得的动作a*学习该值。在Q-learning中，这样的策略就是贪婪策略。(我们将在Q-learning和SARSA中详细讨论这一点)</p><h1 id="9ddf" class="lq lr it bd ls lt nw lv lw lx nx lz ma mb ny md me mf nz mh mi mj oa ml mm mn bi translated">2.各种算法的说明</h1><h2 id="88c8" class="mv lr it bd ls mw mx dn lw my mz dp ma km na nb me kq nc nd mi ku ne nf mm ng bi translated">2.1 Q-学习</h2><p id="34bd" class="pw-post-body-paragraph kb kc it kd b ke mo kg kh ki mp kk kl km mq ko kp kq mr ks kt ku ms kw kx ky im bi translated">Q-Learning是一种基于著名的贝尔曼方程的非策略、无模型RL算法:</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/8c6ab3e499f8b3a25590944359017c34.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*JPn8KZr7yxbQdPcr90qheA.png"/></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Bellman Equation (<a class="ae mu" href="https://zhuanlan.zhihu.com/p/21378532?refer=intelligentunit" rel="noopener ugc nofollow" target="_blank">https://zhuanlan.zhihu.com/p/21378532?refer=intelligentunit</a>)</figcaption></figure><p id="0be2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">上式中的e指的是期望值，而ƛ指的是折现因子。我们可以把它改写成Q值的形式:</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/cd3dcc74aec2b9940bff230c0583b6fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*kwLmPgagp0o31nD8PmRjmg.png"/></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Bellman Equation In Q-value Form (<a class="ae mu" href="https://zhuanlan.zhihu.com/p/21378532?refer=intelligentunit" rel="noopener ugc nofollow" target="_blank">https://zhuanlan.zhihu.com/p/21378532?refer=intelligentunit</a>)</figcaption></figure><p id="6af4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">用Q*表示的最佳Q值可以表示为:</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/99729178ed1a1e0bce0bd30c75c303d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*vA87bBl9ZKfsEa3W--1L6Q.png"/></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Optimal Q-value (<a class="ae mu" href="https://zhuanlan.zhihu.com/p/21378532?refer=intelligentunit" rel="noopener ugc nofollow" target="_blank">https://zhuanlan.zhihu.com/p/21378532?refer=intelligentunit</a>)</figcaption></figure><p id="3eb5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">目标是最大化Q值。在深入研究优化Q值的方法之前，我想先讨论两种与Q学习密切相关的值更新方法。</p><p id="a92c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">策略迭代</strong></p><p id="73c3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">策略迭代在策略评估和策略改进之间循环。</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/e843bbee09d09e75a0011073ab531b6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PkQDd3IdcDXI4vUVwMAqKA.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Policy Iteration (<a class="ae mu" href="http://blog.csdn.net/songrotek/article/details/51378582" rel="noopener ugc nofollow" target="_blank">http://blog.csdn.net/songrotek/article/details/51378582</a>)</figcaption></figure><p id="018b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">策略评估使用从上次策略改进中获得的贪婪策略来估计值函数V。另一方面，策略改进用使每个状态的V最大化的动作来更新策略。更新方程基于贝尔曼方程。它不断迭代直到收敛。</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi of"><img src="../Images/5b34900ba1ad52ade9e189979c32bf0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XuyjK3QfRqV04y--hYK87w.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Pseudo Code For Policy Iteration (<a class="ae mu" href="http://blog.csdn.net/songrotek/article/details/51378582" rel="noopener ugc nofollow" target="_blank">http://blog.csdn.net/songrotek/article/details/51378582</a>)</figcaption></figure><p id="32bd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">值迭代</strong></p><p id="c987" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">值迭代只包含一个分量。它基于最佳贝尔曼方程更新值函数V。</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi og"><img src="../Images/ccfc2164142656ceb3fc387d7a90e2dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rWapNlIa0C1bXV8RgaTEmA.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Optimal Bellman Equation (<a class="ae mu" href="http://blog.csdn.net/songrotek/article/details/51378582" rel="noopener ugc nofollow" target="_blank">http://blog.csdn.net/songrotek/article/details/51378582</a>)</figcaption></figure><figure class="kz la lb lc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oh"><img src="../Images/062bd2c041740dc918a5715eceafa236.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S-a_T-k5hXYhinq9758xCQ.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Pseudo Code For Value Iteration (<a class="ae mu" href="http://blog.csdn.net/songrotek/article/details/51378582" rel="noopener ugc nofollow" target="_blank">http://blog.csdn.net/songrotek/article/details/51378582</a>)</figcaption></figure><p id="7d76" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在迭代收敛之后，通过对所有状态应用argument-max函数，直接导出最优策略。</p><p id="3b2c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">注意，这两种方法需要转移概率<em class="nv"> p </em>的知识，表明它是基于模型的算法。然而，正如我前面提到的，基于模型的算法存在可扩展性问题。那么Q-learning是如何解决这个问题的呢？</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/759f461afd716fb1316c45774121b6e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n9yjEWqBVZ0jw2bff9hRBw.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Q-Learning Update Equation (<a class="ae mu" href="https://www.quora.com/What-is-the-difference-between-Q-learning-and-SARSA-learning" rel="noopener ugc nofollow" target="_blank">https://www.quora.com/What-is-the-difference-between-Q-learning-and-SARSA-learning</a>)</figcaption></figure><p id="7b4e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">α指的是学习率(即我们接近目标的速度)。Q-learning背后的思想高度依赖于价值迭代。然而，更新等式被替换为上述公式。因此，我们不再需要担心转移概率。</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/20a4dcd85e7be34a11d72dfdc7495d4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*B8tGarFYboV9maL93sF45Q.png"/></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Q-learning Pseudo Code (<a class="ae mu" href="https://martin-thoma.com/images/2016/07/q-learning.png" rel="noopener ugc nofollow" target="_blank">https://martin-thoma.com/images/2016/07/q-learning.png</a>)</figcaption></figure><p id="c983" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">注意，选择下一个动作<em class="nv">a’</em>来最大化下一个状态的Q值，而不是遵循当前策略。因此，Q-learning属于非政策范畴。</p><h2 id="a0d2" class="mv lr it bd ls mw mx dn lw my mz dp ma km na nb me kq nc nd mi ku ne nf mm ng bi translated">2.2国家-行动-奖励-国家-行动</h2><p id="fef1" class="pw-post-body-paragraph kb kc it kd b ke mo kg kh ki mp kk kl km mq ko kp kq mr ks kt ku ms kw kx ky im bi translated">SARSA非常类似于Q-learning。SARSA和Q-learning之间的关键区别在于，SARSA是一种基于策略的算法。这意味着SARSA基于由当前策略而不是贪婪策略执行的动作来学习Q值。</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ok"><img src="../Images/d67e618d65342a8d9bda7159920dd51d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DVtlBC0pNsW6LbDM25y7qw.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">SARSA Update Equation (<a class="ae mu" href="https://www.quora.com/What-is-the-difference-between-Q-learning-and-SARSA-learning" rel="noopener ugc nofollow" target="_blank">https://www.quora.com/What-is-the-difference-between-Q-learning-and-SARSA-learning</a>)</figcaption></figure><p id="46b2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">动作a(t+1)是在当前策略下在下一个状态s(t+1)中执行的动作。</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/9d009526556aa36ff4ca3e17e45ee5fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*NdEQk3LeJfkzImOiQij_NA.png"/></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">SARSA Pseudo Code (<a class="ae mu" href="https://martin-thoma.com/images/2016/07/sarsa-lambda.png" rel="noopener ugc nofollow" target="_blank">https://martin-thoma.com/images/2016/07/sarsa-lambda.png</a>)</figcaption></figure><p id="fe37" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">从上面的伪代码中，您可能会注意到执行了两个操作选择，它们总是遵循当前策略。相比之下，Q-learning对下一个动作没有约束，只要它最大化下一个状态的Q值。因此，SARSA是一个基于策略的算法。</p><h2 id="aed8" class="mv lr it bd ls mw mx dn lw my mz dp ma km na nb me kq nc nd mi ku ne nf mm ng bi translated">2.3深Q网(DQN)</h2><p id="373d" class="pw-post-body-paragraph kb kc it kd b ke mo kg kh ki mp kk kl km mq ko kp kq mr ks kt ku ms kw kx ky im bi translated">虽然Q-learning是一个非常强大的算法，但它的主要缺点是缺乏通用性。如果您将Q-learning视为更新二维数组(动作空间*状态空间)中的数字，实际上，它类似于动态编程。这表明对于Q学习代理以前没有见过的状态，它不知道采取哪种动作。换句话说，Q-learning agent不具备估计未知状态值的能力。为了处理这个问题，DQN通过引入神经网络摆脱了二维数组。</p><p id="eeed" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">DQN利用神经网络来估计Q值函数。网络的输入是电流，而输出是每个动作的相应Q值。</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oh"><img src="../Images/531148e4a88456fb398fe2bf26a20a3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4antxYinbORGPNUElrzOUA.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">DQN Atari Example (<a class="ae mu" href="https://zhuanlan.zhihu.com/p/25239682" rel="noopener ugc nofollow" target="_blank">https://zhuanlan.zhihu.com/p/25239682</a>)</figcaption></figure><p id="ae46" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">2013年，DeepMind将DQN应用于<a class="ae mu" href="https://arxiv.org/pdf/1312.5602.pdf" rel="noopener ugc nofollow" target="_blank">雅达利游戏</a>，如上图所示。输入是当前游戏情况的原始图像。它经历了几层，包括卷积层以及全连接层。输出是代理可以采取的每个操作的Q值。</p><p id="fbb7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">问题归结为:<strong class="kd iu">我们如何训练网络？</strong></p><p id="a150" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">答案是我们根据Q学习更新方程来训练网络。回想一下，Q学习的目标Q值是:</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/a2a3ea79f5d09f9e5d44f936c39036a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*VcgBin7pa2eERUxjVwvg1Q.png"/></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Target Q-value (<a class="ae mu" href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" rel="noopener ugc nofollow" target="_blank">https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf</a>)</figcaption></figure><p id="710b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ϕ相当于状态s，而𝜽代表神经网络中的参数，这不在我们讨论的范围内。因此，网络的损失函数被定义为目标Q值和从网络输出的Q值之间的平方误差。</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi om"><img src="../Images/80c676faf19ab16636a0c6b55908607e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nb61CxDTTAWR1EJnbCl1cA.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">DQN Pseudo Code (<a class="ae mu" href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" rel="noopener ugc nofollow" target="_blank">https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf</a>)</figcaption></figure><p id="defb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">另外两个技巧对训练DQN也很重要:</p><ol class=""><li id="a7f4" class="nh ni it kd b ke kf ki kj km on kq oo ku op ky nm nn no np bi translated"><strong class="kd iu">经验回放</strong>:由于典型RL设置中的训练样本高度相关，数据效率较低，这将导致网络更难收敛。解决样本分布问题的一种方法是采用经验回放。本质上，样本转换被存储，然后从“转换池”中随机选择以更新知识。</li><li id="5ad4" class="nh ni it kd b ke nq ki nr km ns kq nt ku nu ky nm nn no np bi translated"><strong class="kd iu">分离目标网络</strong>:目标Q网络与估值的结构相同。每C步，根据上面的伪代码，将目标网络重置为另一个。因此，波动变得不那么剧烈，导致更稳定的训练。</li></ol><h2 id="8e03" class="mv lr it bd ls mw mx dn lw my mz dp ma km na nb me kq nc nd mi ku ne nf mm ng bi translated">2.4深度确定性政策梯度(DDPG)</h2><p id="f7c1" class="pw-post-body-paragraph kb kc it kd b ke mo kg kh ki mp kk kl km mq ko kp kq mr ks kt ku ms kw kx ky im bi translated">虽然DQN在更高维度的问题上取得了巨大的成功，比如雅达利游戏，但是动作空间仍然是离散的。然而，许多感兴趣的任务，尤其是物理控制任务，动作空间是连续的。如果你把行动空间划分得太细，你最终会有一个太大的行动空间。例如，假设自由随机系统的程度是10。对于每个度数，你把空间分成4个部分。你最终有4个⁰ =1048576个动作。这么大的行动空间也极难收敛。</p><p id="1ad4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">DDPG依靠演员和评论家两个同名元素的演员-评论家架构。参与者用于调整策略功能的参数𝜽，即决定特定状态的最佳动作。</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oq"><img src="../Images/318894048b67975b135cc1b26eb5c439.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*py-aIXySIL28u_1_cRrHqg.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Policy Function (<a class="ae mu" href="https://zhuanlan.zhihu.com/p/25239682" rel="noopener ugc nofollow" target="_blank">https://zhuanlan.zhihu.com/p/25239682</a>)</figcaption></figure><p id="7e2b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一个批评家被用来评估由行动者根据时间差异(TD)误差估计的策略函数。</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div class="gh gi or"><img src="../Images/e9fcd9697f49ad9af344664960421437.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*-LcAiv5h_LEVdqIwkPNaUA.png"/></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Temporal Difference Error (<a class="ae mu" href="http://proceedings.mlr.press/v32/silver14.pdf" rel="noopener ugc nofollow" target="_blank">http://proceedings.mlr.press/v32/silver14.pdf</a>)</figcaption></figure><p id="5e30" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里，小写的<em class="nv"> v </em>表示参与者已经决定的策略。看起来眼熟吗？是啊！它看起来就像Q学习更新方程！TD学习是一种学习如何根据给定状态的未来值来预测值的方法。Q学习是用于学习Q值的TD学习的特定类型。</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi os"><img src="../Images/73c382065dc870fc46db15ecd4a8e25c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IgGdMLe12MeWoQNkDhm0mg.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Actor-critic Architecture (<a class="ae mu" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><p id="096d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">DDPG还借鉴了DQN <strong class="kd iu">的<strong class="kd iu">经验重演</strong>和<strong class="kd iu">分众目标网</strong>的理念。DDPG的另一个问题是它很少探索行动。对此的解决方案是在参数空间或动作空间上添加噪声。</strong></p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/86403e6fb58c4cbe17662c225fd9c773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*kQOZZfjgTMg7fiqNOXTsOg.png"/></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">Action Noise (left), Parameter Noise (right) (<a class="ae mu" href="https://blog.openai.com/better-exploration-with-parameter-noise/" rel="noopener ugc nofollow" target="_blank">https://blog.openai.com/better-exploration-with-parameter-noise/</a>)</figcaption></figure><p id="fbe4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">OpenAI写的这篇<a class="ae mu" href="https://blog.openai.com/better-exploration-with-parameter-noise/" rel="noopener ugc nofollow" target="_blank">文章</a>声称在参数空间上加比在动作空间上加好。一种常用的噪声是<a class="ae mu" href="http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab" rel="noopener ugc nofollow" target="_blank">奥恩斯坦-乌伦贝克随机过程</a>。</p><figure class="kz la lb lc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ou"><img src="../Images/3c4a42291b045aa770759e5bf5757e79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qV8STzz6mEYIKjOXyibtrQ.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">DDPG Pseudo Code (<a class="ae mu" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.02971.pdf</a>)</figcaption></figure><h1 id="bbd0" class="lq lr it bd ls lt nw lv lw lx nx lz ma mb ny md me mf nz mh mi mj oa ml mm mn bi translated">3.结论</h1><p id="1385" class="pw-post-body-paragraph kb kc it kd b ke mo kg kh ki mp kk kl km mq ko kp kq mr ks kt ku ms kw kx ky im bi translated">我已经讨论了Q-learning、SARSA、DQN和DDPG的一些基本概念。在下一篇文章中，我将继续讨论其他最新的强化学习算法，包括NAF、A3C等。最后，我将简单地比较一下我所讨论的每一种算法。如果你对这篇文章有任何问题，请不要犹豫，在下面留下你的评论，或者在twitter上关注我。</p></div></div>    
</body>
</html>