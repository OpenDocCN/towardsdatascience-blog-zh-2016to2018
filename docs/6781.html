<html>
<head>
<title>Supercharging word vectors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">增压词向量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/supercharging-word-vectors-be80ee5513d?source=collection_archive---------13-----------------------#2018-12-31">https://towardsdatascience.com/supercharging-word-vectors-be80ee5513d?source=collection_archive---------13-----------------------#2018-12-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="58b5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个在你的 NLP 项目中提升快速文本和其他单词向量的简单技术</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7e446c90b7797f726d2ba76ecc34072a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r6i9eoa09l_Yvwi0C3qwtw.jpeg"/></div></div></figure><p id="ffe2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在过去的几年中，单词向量在创建单词间语义联系的能力方面具有变革性。现在，将这些输入到深度学习模型中进行分类或情感分析等任务已经成为常态。</p><p id="bd06" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">尽管如此，我一直对基于词频分析的简单能力印象深刻。<em class="lq">术语频率逆文档频率</em> (TF-IDF)可以很容易地向非技术受众解释，结果也很容易解释。</p><blockquote class="lr ls lt"><p id="91e7" class="ku kv lq kw b kx ky ju kz la lb jx lc lu le lf lg lv li lj lk lw lm ln lo lp im bi translated">本文探讨了如何将 TF-IDF 与单词向量相结合，从而得到既易于解释又能捕捉语言中存在的微妙语义关系的输出。</p></blockquote><p id="89e2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">虽然互联网上充斥着使用 TF-IDF ( <a class="ae lx" href="https://medium.freecodecamp.org/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3" rel="noopener ugc nofollow" target="_blank">见此处</a>)和词向量方法(<a class="ae lx" rel="noopener" target="_blank" href="/word-to-vectors-natural-language-processing-b253dd0b0817">见此处</a>)进行“词袋”分析的例子，但这些几乎总是被视为相互排斥的技术。本文探讨了如何将 TF-IDF 与单词向量相结合，从而得到既易于解释又能捕捉语言中存在的微妙语义关系的输出。使用 fastText 方法创建单词向量，我们还将能够创建一个模型，该模型可以处理词汇表之外的单词，并对拼写错误和打字错误具有鲁棒性。</p><h2 id="d917" class="ly lz it bd ma mb mc dn md me mf dp mg ld mh mi mj lh mk ml mm ll mn mo mp mq bi translated">快速文本单词向量</h2><p id="f605" class="pw-post-body-paragraph ku kv it kw b kx mr ju kz la ms jx lc ld mt lf lg lh mu lj lk ll mv ln lo lp im bi translated">本文假设读者对单词向量有一定的了解，但是值得一提的是 fastText，以及它与更广为人知的 word2vec 创建单词向量表示的方法有何不同。FastText 由脸书开发，2017 年开源了一个稳定版本。</p><p id="efc0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">fastText 和 word2vec 之间最明显的区别是 fastText 使用 n-gram 字符拆分单词。例如，“林肯郡”(英国的一个郡)将被分为:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="e32f" class="ly lz it mx b gy nb nc l nd ne">Lin, inc, nco, col, oln, lns, nsh, shi, hir, ire</span></pre><p id="3695" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中 n=3。这种方法是对 word2vec 的重大改进，原因有二:</p><ol class=""><li id="5c44" class="nf ng it kw b kx ky la lb ld nh lh ni ll nj lp nk nl nm nn bi translated">推断词汇外单词的能力。例如，上述模型会理解“兰开夏郡”(也是英国的一个县)与林肯郡相关，因为这两个词之间有“shire”(或“shi”、“hir”和“ire”)的重叠。</li><li id="eaf7" class="nf ng it kw b kx no la np ld nq lh nr ll ns lp nk nl nm nn bi translated">对拼写错误和错别字的鲁棒性。很容易看出，相同的字符级建模也意味着 fastText 足够健壮，可以处理拼写变化。这在分析社交媒体内容时特别有用。</li></ol><p id="c180" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">关于 fastText 如何工作的详细回顾可以点击<a class="ae lx" rel="noopener" target="_blank" href="/fasttext-under-the-hood-11efc57b2b3">这里</a>查看。</p><h2 id="77af" class="ly lz it bd ma mb mc dn md me mf dp mg ld mh mi mj lh mk ml mm ll mn mo mp mq bi translated">给我看看代码！</h2><p id="3f56" class="pw-post-body-paragraph ku kv it kw b kx mr ju kz la ms jx lc ld mt lf lg lh mu lj lk ll mv ln lo lp im bi translated">本文的其余部分将介绍一个简单的例子，该例子将在一系列文档上训练一个 fastText 模型，将 TF-IDF 应用于这些向量，并使用它来执行进一步的分析。</p><p id="45c8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">有问题的文件是公司提交的现代奴隶制声明，以解释它们正在内部和供应链中采取的消除现代奴隶制的步骤。下面的文章展示了在分析之前如何清理这些数据:</p><div class="nt nu gp gr nv nw"><a rel="noopener follow" target="_blank" href="/clean-your-data-with-unsupervised-machine-learning-8491af733595"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">使用无监督的机器学习清理您的数据</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">清理数据不一定是痛苦的！这篇文章是一个如何使用无监督机器学习的快速例子…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok ks nw"/></div></div></a></div><p id="219e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你想继续下去，可以在这里找到包含所有代码的 colab 笔记本。</p><p id="4ee6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">第一步。标记文本并创建短语</strong></p><p id="3a84" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将使用 spaCy 将每个文档分割成一个单词列表(标记化)。我们还将通过删除停用词、标点符号并使用 Gensim 库转换为小写来清理数据:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="e52f" class="ly lz it mx b gy nb nc l nd ne">#The dataframe is called 'combined' it has a column called 'text' containing the text data for each company</span><span id="aa89" class="ly lz it mx b gy ol nc l nd ne">#creates a list of documents with a list of words inside:<br/>text = []<br/>for i in combined.text.values:<br/>  doc = nlp(remove_stopwords(strip_punctuation(strip_non_alphanum(str(i).lower()))))<br/>  tokens = [token.text for token in doc]<br/>  text.append(tokens)</span></pre><p id="7c16" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后我们将共同的术语粘在一起。例如，当每个文件都提到现代奴隶制时，将这两个词合并成一个短语“现代 _ 奴隶制”是很有用的。Gensim 库使这变得简单，代码如下:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="3c7f" class="ly lz it mx b gy nb nc l nd ne">common_terms = ["of", "with", "without", "and", "or", "the", "a"]<br/># Create the relevant phrases from the list of sentences:<br/>phrases = Phrases(text, common_terms=common_terms, threshold = 10, min_count=5)<br/># The Phraser object is used from now on to transform sentences<br/>bigram = Phraser(phrases)<br/># Applying the Phraser to transform our sentences is simply<br/>tokens = list(bigram[text])</span></pre><p id="ffc2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们现在有一个文档列表，每个文档包含一个单词列表。常用短语已被组合成词。</p><p id="943b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">第二步。训练快速文本模型</strong></p><p id="b325" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这只需要使用 Gensim 库的 fastText 模型的一行代码。您可以设置 n 元大小(“窗口”)以及向量的维度大小(“大小”)。这里，我们创建了 100 维的向量，并使用了 3:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="4c2c" class="ly lz it mx b gy nb nc l nd ne">model = FastText(tokens, size=100, window=3, min_count=1, iter=10, sorted_vocab=1)</span></pre><p id="4fb9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以使用以下内容来检查模型:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="2331" class="ly lz it mx b gy nb nc l nd ne">similarities = model.wv.most_similar(restrict_vocab=10000, positive=['contract'])</span></pre><p id="e190" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这将返回:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="965b" class="ly lz it mx b gy nb nc l nd ne">subcontract   0.9493274688720703 <br/>sub_contract  0.9349175095558167 <br/>contractual   0.9346154928207397 <br/>contracts     0.9312876462936401 <br/>contractor    0.9068889617919922</span></pre><p id="61ad" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了展示 fastText 如何处理不在词汇表中的单词，我们可以试着将 contract 拼错为'<strong class="kw iu">contract</strong>'。从下面，我们可以看到模型没有问题解释这个词，即使它从来没有见过！</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="e13c" class="ly lz it mx b gy nb nc l nd ne">contract      0.9389102458953857 <br/>contracts     0.9058693051338196 <br/>contrary      0.9027011394500732 <br/>contractor    0.8995087742805481 <br/>contractual   0.885408341884613</span></pre><p id="ab55" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">第三步。将 TF-IDF 应用于矢量</strong></p><p id="2b7a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这一步首先使用 Scikit 为每个文档创建 TF-IDF 信息。然后，它将权重应用于文档中的每个单词向量，然后对整个文档进行平均:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="9f97" class="ly lz it mx b gy nb nc l nd ne">#TF-IDF <br/>text = []<br/>for i in tqdm(tokens):<br/>  string = ' '.join(i)<br/>  text.append(string)<br/>tf_idf_vect = TfidfVectorizer(stop_words=None)<br/>final_tf_idf = tf_idf_vect.fit_transform(text)<br/>tfidf_feat = tf_idf_vect.get_feature_names()</span><span id="faa6" class="ly lz it mx b gy ol nc l nd ne">#Applying TF-IDF scores to the model vectors</span><span id="7622" class="ly lz it mx b gy ol nc l nd ne">tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list<br/>row=0;<br/>errors=0<br/>for sent in tqdm(tokens): # for each review/sentence<br/>    sent_vec = np.zeros(100) # as word vectors are of zero length<br/>    weight_sum =0; # num of words with a valid vector in the sentence/review<br/>    for word in sent: # for each word in a review/sentence<br/>        try:<br/>            vec = model.wv[word]<br/>            # obtain the tf_idfidf of a word in a sentence/review<br/>            tfidf = final_tf_idf [row, tfidf_feat.index(word)]<br/>            sent_vec += (vec * tfidf)<br/>            weight_sum += tfidf<br/>        except:<br/>            errors =+1<br/>            pass<br/>    sent_vec /= weight_sum<br/>    #print(np.isnan(np.sum(sent_vec)))</span><span id="301b" class="ly lz it mx b gy ol nc l nd ne">tfidf_sent_vectors.append(sent_vec)<br/>    row += 1<br/>print('errors noted: '+str(errors))</span></pre><p id="9d1b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们的语料库中的每个文档都有了超级 TF-IDF 单词向量！是时候测试一下了…</p><h2 id="c14f" class="ly lz it bd ma mb mc dn md me mf dp mg ld mh mi mj lh mk ml mm ll mn mo mp mq bi translated">结果</h2><p id="91f8" class="pw-post-body-paragraph ku kv it kw b kx mr ju kz la ms jx lc ld mt lf lg lh mu lj lk ll mv ln lo lp im bi translated">现在我们有了与 TF-IDF 权重相结合的快速文本向量，使用余弦相似性来检测结果就很容易了。下面的代码根据查询字符串找到一个匹配的公司，然后根据它们的 Modern Slavery returns 生成一个最相似的公司列表。</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="49ca" class="ly lz it mx b gy nb nc l nd ne">compName = 'Deloitte'</span><span id="8f01" class="ly lz it mx b gy ol nc l nd ne">query = [combined.loc[combined.Company.str.contains(compName)].iloc[0]['FT_tfidf']]<br/>query = np.array(list(query))<br/>query = np.nan_to_num(query)</span><span id="657f" class="ly lz it mx b gy ol nc l nd ne">vectors = np.array(list(combined.FT_tfidf.values))<br/>vectors = np.nan_to_num(vectors)</span><span id="a37f" class="ly lz it mx b gy ol nc l nd ne">cosine_similarities = pd.Series(cosine_similarity(query, vectors).flatten())</span><span id="998b" class="ly lz it mx b gy ol nc l nd ne">for i,j in cosine_similarities.nlargest(10).iteritems():<br/>  print(str(i) + '-' + combined.Company.iloc[i] + " " + str(j))</span></pre><p id="8605" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里的例子是专业服务公司德勤。从下面的结果我们可以看到，最相似的回报来自相关公司(即德勤自己的另一个财政年度的回报，以及 DLA 律师事务所和安永会计师事务所，另一家“四大”会计师事务所):</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="7767" class="ly lz it mx b gy nb nc l nd ne">Deloitte LLP                  1.0000000000000002 <br/>Deloitte LLP                  0.949480726712043 <br/>DLA Piper International LLP   0.8635928917765654 <br/>Travers Smith LLP             0.8495683187822698 <br/>EBSCO International Inc.      0.8405915834557236 <br/>Vink UK                       0.8356471225683573 <br/>Ernst &amp; Young LLP             0.8345225966043321 <br/>IFS UK Ltd                    0.8288755547154663 <br/>First Central Insurance       0.8279939308519769 <br/>TalkTalk Telecom Group PLC    0.8260778120395709</span></pre><p id="fd2a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">希望这篇文章展示了将单词向量与 TF-IDF 技术结合起来可以产生强大的结果，并且也易于解释和说明。在此演练中生成的文档向量现在可以用于文本分类、聚类以及进一步分析(例如，发现公司回报如何随时间或跨行业变化的趋势)。</p></div></div>    
</body>
</html>