<html>
<head>
<title>Getting Data ready for modelling: Feature engineering, Feature Selection, Dimension Reduction (Part two)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为建模准备数据:特征工程、特征选择、降维(下)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-data-ready-for-modelling-feature-engineering-feature-selection-dimension-reduction-39dfa267b95a?source=collection_archive---------4-----------------------#2018-12-13">https://towardsdatascience.com/getting-data-ready-for-modelling-feature-engineering-feature-selection-dimension-reduction-39dfa267b95a?source=collection_archive---------4-----------------------#2018-12-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div class="gh gi io"><img src="../Images/f4925bf06ca11f36fe9972b4d6c989bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*3e-wlJFTAbL3YRYV.jpg"/></div></figure><div class=""/><p id="670e" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这是为建模准备数据系列的第二部分。如果您还没有阅读第 1 部分，那么我建议您先浏览一遍。因为特征工程通常是第一步。</p><div class="ip iq gp gr ir ks"><a href="https://medium.com/@desardaakash/getting-data-ready-for-modelling-feature-engineering-feature-selection-dimension-reduction-77f2b9fadc0b" rel="noopener follow" target="_blank"><div class="kt ab fo"><div class="ku ab kv cl cj kw"><h2 class="bd iy gy z fp kx fr fs ky fu fw iw bi translated">为建模准备数据:特征工程、特征选择、降维…</h2><div class="kz l"><h3 class="bd b gy z fp kx fr fs ky fu fw dk translated">特征工程，特征选择，降维</h3></div><div class="la l"><p class="bd b dl z fp kx fr fs ky fu fw dk translated">…特征工程、特征选择、尺寸 Reductionmedium.com</p></div></div><div class="lb l"><div class="lc l ld le lf lb lg it ks"/></div></div></a></div><p id="5925" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">一旦有了足够的、更少的或没有丢失的数据或异常值，接下来就是特征选择或特征提取(这两者通常做相同的工作，可以互换使用)。通常有两种方法:</p><ol class=""><li id="14f3" class="lh li ix jw b jx jy kb kc kf lj kj lk kn ll kr lm ln lo lp bi translated"><strong class="jw iy">特征提取/选择</strong></li><li id="f1e8" class="lh li ix jw b jx lq kb lr kf ls kj lt kn lu kr lm ln lo lp bi translated"><strong class="jw iy">维度缩减或特征缩减</strong></li></ol><p id="decb" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们一个一个地，一步一步地解决它们。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="df75" class="mc md ix bd me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz bi translated">第 2 部分:特征提取/选择</h1><h2 id="c21e" class="na md ix bd me nb nc dn mi nd ne dp mm kf nf ng mq kj nh ni mu kn nj nk my nl bi translated">那么什么是特征选择呢？特征提取？他们的区别？</h2><p id="1801" class="pw-post-body-paragraph ju jv ix jw b jx nm jz ka kb nn kd ke kf no kh ki kj np kl km kn nq kp kq kr ij bi translated">→在机器学习和统计学中，特征选择也称为变量选择，是选择相关特征(变量、预测器)的子集用于模型构建的过程。</p><p id="4118" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→特征提取用于创建一个新的、更小的特征集，该特征集仍能捕获大部分有用信息。</p><p id="d3c2" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→同样，特征选择保留原始特征的子集，而特征提取创建新的特征。</p><p id="8fb6" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw iy">特征选择/提取的重要性</strong></p><p id="b42e" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→当特征数量非常大时，这变得更加重要。</p><p id="8c43" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→你不需要使用你所掌握的每一个特性来创建一个算法。</p><p id="7d72" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→你可以通过只输入那些真正重要的特征来帮助你的算法。</p><p id="1cea" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw iy">为什么要使用特征选择？</strong></p><p id="f41e" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→它使机器学习算法能够更快地训练，降低复杂性，并使其更容易解释。</p><p id="8236" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→如果选择了正确的子集，它会提高模型的准确性。</p><p id="9a02" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→减少过度拟合。</p><blockquote class="nr ns nt"><p id="2307" class="ju jv nu jw b jx jy jz ka kb kc kd ke nv kg kh ki nw kk kl km nx ko kp kq kr ij bi translated">它可以大致分为两种技术(尽管这不是“T8”的唯一方法)</p></blockquote><p id="cbd2" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">I .单变量特征选择</p><p id="f9f9" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">二。多元特征选择</p><p id="72f5" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi ny translated"><span class="l nz oa ob bm oc od oe of og di"> U </span>访问每一个特征，并根据目标检查其重要性。为了实现单变量特征选择，你应该掌握一些技巧。</p><p id="6e1a" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→如果你有适当的<strong class="jw iy">领域知识</strong>并且相信你的判断，那么总是从这一步开始。分析所有的特征，去掉所有不需要的。是的，这是费时费力的一步，但是，嘿，你更相信谁呢，<strong class="jw iy">“机器还是你自己”</strong></p><p id="fdb7" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→ <strong class="jw iy">检查所有特征的方差</strong>(是的，永远令人困惑的偏差-方差权衡:)。这里的经验法则是设置一个阈值(假设一个特征的方差为 0，意味着它对每个样本都具有相同的值，因此这样的特征不会给模型带来任何预测能力)并相应地移除特征。</p><p id="9838" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→ <strong class="jw iy">皮尔森相关性的使用:</strong>这可能是三种技术中最适用的一种。如果你不知道或者对它感到困惑，那么先看看这篇<a class="ae oh" href="https://www.analyticsvidhya.com/blog/2015/06/correlation-common-questions/" rel="noopener ugc nofollow" target="_blank">文章</a>。</p><ul class=""><li id="afcb" class="lh li ix jw b jx jy kb kc kf lj kj lk kn ll kr oi ln lo lp bi translated">因此，简而言之，它给了我们目标变量和特征之间的相互依赖性。</li></ul><figure class="ok ol om on gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3fa986fc2a0f6f5e2f359c807eeaf4c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*h6qUH598sZlKhOa_D3vuFw.jpeg"/></div><figcaption class="oo op gj gh gi oq or bd b be z dk">Thump Rule To analyse Pearson Correlation</figcaption></figure><ul class=""><li id="fd79" class="lh li ix jw b jx jy kb kc kf lj kj lk kn ll kr oi ln lo lp bi translated">使用皮尔逊相关性的经验法则:</li></ul><p id="a66c" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">I .仅选择与目标变量的中度至强关系。(见上图)。</p><p id="4b12" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">二。当两个特征本身与目标变量有很强的相互关系时，选择其中任何一个(选择两个都不会增加任何值)。使用'<a class="ae oh" href="https://seaborn.pydata.org/generated/seaborn.heatmap.html" rel="noopener ugc nofollow" target="_blank"><strong class="jw iy">seaborn . heat map()</strong></a><strong class="jw iy">'</strong>进行可视化和挑选，很有帮助。</p><p id="d304" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">三。这里有一个陷阱😢。它最适用于线性数据，不适用于非线性数据(所以请避免使用它)。</p><p id="17ed" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi ny translated"><span class="l nz oa ob bm oc od oe of og di"> M </span>所以用外行的话来说就是一次选择多个特性。</p><p id="0d37" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw iy">多元特征选择大致分为三类:</strong></p><figure class="ok ol om on gt is gh gi paragraph-image"><div class="gh gi os"><img src="../Images/09a350ba34ee9bc6e3dadc57ce602dbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/0*2OZwUmZETN9TBPZn.jpg"/></div></figure><p id="7251" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们来看看它们(我们将讨论每个类别中最广泛使用的技术)</p><h1 id="68d9" class="mc md ix bd me mf ot mh mi mj ou ml mm mn ov mp mq mr ow mt mu mv ox mx my mz bi translated">过滤方法:</h1><p id="42ab" class="pw-post-body-paragraph ju jv ix jw b jx nm jz ka kb nn kd ke kf no kh ki kj np kl km kn nq kp kq kr ij bi translated">→过滤方法通常用作预处理步骤。特征的选择独立于任何机器学习算法。</p><p id="8c83" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→过滤方法对特征进行排序。等级表示每个特征对于分类的“有用”程度。一旦计算出这个排序，就创建了由最好的 N 个特征组成的特征集。</p><p id="c9d8" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→根据其在各种统计测试中的得分选择特征，以确定其与结果变量的相关性。(这里的相关性是一个主观术语)。</p><figure class="ok ol om on gt is gh gi paragraph-image"><div role="button" tabindex="0" class="oz pa di pb bf pc"><div class="gh gi oy"><img src="../Images/35151f9c0f8accafdd9b6fef5ef793bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fefKZY27vG0VWMkw.png"/></div></div><figcaption class="oo op gj gh gi oq or bd b be z dk">Blueprint of Filter Method</figcaption></figure><ol class=""><li id="1323" class="lh li ix jw b jx jy kb kc kf lj kj lk kn ll kr lm ln lo lp bi translated"><strong class="jw iy">皮尔逊相关:</strong>哦对！皮尔逊相关是过滤方法。我们已经讨论过了。</li><li id="d6b9" class="lh li ix jw b jx lq kb lr kf ls kj lt kn lu kr lm ln lo lp bi translated"><strong class="jw iy">方差</strong> <strong class="jw iy">阈值:</strong>这一点我们也已经讨论过了。</li><li id="5d13" class="lh li ix jw b jx lq kb lr kf ls kj lt kn lu kr lm ln lo lp bi translated"><strong class="jw iy">线性判别分析:</strong>目标是将数据集投影到一个具有良好类别可分性的低维空间，以避免过拟合(<strong class="jw iy"><em class="nu"/></strong>)【维数灾难】，同时降低计算成本。</li></ol><p id="ab0b" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→没有进入数学领域，LDA 将所有的高维变量(我们不能绘制和分析)放到 2D 图上&amp;同时这样做去除了无用的特征。</p><p id="2f57" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→ LDA 也是的<strong class="jw iy"> <em class="nu">监督降维</em> </strong>技术，更像是<strong class="jw iy">特征提取</strong>而不是<strong class="jw iy">选择</strong>(因为它通过降低变量的维度来创建一种新的变量)。所以它只对带标签的数据有效。</p><p id="f891" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→最大化类别间的<em class="nu">可分性</em>。(技术术语太多，对。不要担心看视频)。</p><figure class="ok ol om on gt is"><div class="bz fp l di"><div class="pd pe l"/></div><figcaption class="oo op gj gh gi oq or bd b be z dk">Creator: Josh Starmer</figcaption></figure><p id="02a2" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw iy">其他:</strong></p><p id="3680" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw iy"> ANOVA: </strong>方差分析除了使用一个或多个分类独立特征和一个连续相关特征进行操作之外，它类似于 LDA。它提供了几个组的平均值是否相等的统计检验。</p><p id="4714" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw iy">卡方:</strong>这是一种应用于分类特征组的统计检验，利用它们的频率分布来评估它们之间相关或关联的可能性。</p><blockquote class="nr ns nt"><p id="0fe5" class="ju jv nu jw b jx jy jz ka kb kc kd ke nv kg kh ki nw kk kl km nx ko kp kq kr ij bi translated"><em class="ix">需要记住的一点是，过滤方法不能去除多重共线性。因此，在为数据训练模型之前，还必须处理要素的多重共线性。</em></p></blockquote><figure class="ok ol om on gt is gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/2020c3875029f8e772f6088556ce2beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/0*TD6Tf326AV9N9dCY.png"/></div><figcaption class="oo op gj gh gi oq or bd b be z dk">What to choose when</figcaption></figure><h1 id="b870" class="mc md ix bd me mf ot mh mi mj ou ml mm mn ov mp mq mr ow mt mu mv ox mx my mz bi translated">包装方法:</h1><figure class="ok ol om on gt is gh gi paragraph-image"><div role="button" tabindex="0" class="oz pa di pb bf pc"><div class="gh gi pg"><img src="../Images/b7b6beeadfa7187f9c60438a9e862546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZK0ElqCL-bE3i_Oj.png"/></div></div><figcaption class="oo op gj gh gi oq or bd b be z dk">Blueprint Of Wrapper Method</figcaption></figure><p id="fd7e" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→根据我们从之前的模型中得出的推论，我们决定在您的子集中添加或删除特征。</p><p id="6be3" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→包装方法之所以被称为包装方法，是因为它们将分类器包装在特征选择算法中。通常选择一组特征；这种设置的效率是确定的；进行一些扰动以改变原始集合，并评估新集合的效率。</p><p id="012a" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→这种方法的问题是特征空间很大，查看每个可能的组合需要大量的时间和计算。</p><p id="d3c4" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→问题本质上归结为一个搜索问题。这些方法通常在计算上非常昂贵。</p><ol class=""><li id="3e85" class="lh li ix jw b jx jy kb kc kf lj kj lk kn ll kr lm ln lo lp bi translated"><strong class="jw iy">正向选择:</strong>正向选择是一种迭代方法，我们从模型中没有特征开始。在每一次迭代中，我们不断地添加最能改进我们模型的特性，直到添加一个新变量不能改进模型的性能。</li><li id="07a8" class="lh li ix jw b jx lq kb lr kf ls kj lt kn lu kr lm ln lo lp bi translated"><strong class="jw iy">向后消除:</strong>在向后消除中，我们从所有特征开始，并在每次迭代中移除最不重要的特征，这提高了模型的性能。我们重复这一过程，直到在特征的移除上没有观察到改进。</li><li id="4167" class="lh li ix jw b jx lq kb lr kf ls kj lt kn lu kr lm ln lo lp bi translated"><a class="ae oh" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jw iy">【递归特征消除(RFE) </strong> </a> <strong class="jw iy"> : </strong> It <strong class="jw iy"> </strong>的工作原理是递归地删除属性，并在那些保留的属性上建立模型。它使用外部估计器为要素分配权重(例如，线性模型的系数)，以确定哪些属性(以及属性组合)对预测目标属性的贡献最大。</li></ol><p id="a84e" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→这是一种贪婪优化算法，旨在找到性能最佳的特征子集。</p><p id="2400" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→它重复创建模型，并在每次迭代中保留最佳或最差的性能特征。</p><p id="011c" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→它用剩下的特征构造下一个模型，直到所有的特征都用完。然后，它根据要素被消除的顺序对其进行排序。</p><pre class="ok ol om on gt ph pi pj pk aw pl bi"><span id="d028" class="na md ix pi b gy pm pn l po pp"># Recursive Feature Elimination<br/>from sklearn.feature_selection import RFE<br/>from sklearn.linear_model import LinearRegression</span><span id="25cf" class="na md ix pi b gy pq pn l po pp"># create a base classifier used to evaluate a subset of attributes<br/>model = LinearRegression()</span><span id="8f9e" class="na md ix pi b gy pq pn l po pp">X, y = iowa.iloc[:,:-1], iowa.iloc[:,-1]<br/># create the RFE model and select 3 attributes<br/>rfe = RFE(model, 10)<br/>rfe = rfe.fit(X, y)</span><span id="6f3b" class="na md ix pi b gy pq pn l po pp"># summarize the selection of the attributes<br/>print(rfe.support_)<br/>print(rfe.ranking_)<br/><strong class="pi iy"><em class="nu">Output:<br/></em></strong>[False False  True  True False False False False False False False False<br/> False False False  True  True  True False  True  True  True  True False<br/>  True False False False False False False False False False]<br/>[16 24  1  1  4  9 18 13 14 15 11  6  7 12 10  1  1  1  2  1  1  1  1  5  1<br/> 23 17 20 22 19  8 21 25  3]</span></pre><p id="be88" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→下面是上面例子中发生的情况，</p><p id="df1d" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">I . '<strong class="jw iy">rfe . support _【T9]'按顺序给出了与特性相关的结果(显然是基于所选型号和需求编号)。</strong></p><p id="e4e4" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">二。<strong class="jw iy"> rfe.ranking_ </strong>'分别给出所有特性的等级。当你需要的功能比输入给'<strong class="jw iy"> n_features_to_select' </strong>(在上面的例子中是 10 个)的功能更多时，这真的很方便。所以，你可以设置一个阈值&amp;分别选择它上面的所有特征。</p><p id="6490" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><a class="ae oh" href="http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/" rel="noopener ugc nofollow" target="_blank"> <strong class="jw iy"> 4。顺序特征选择器</strong> </a> <strong class="jw iy"> : </strong>顺序特征选择算法是一类贪婪搜索算法，用于将初始的<em class="nu"> d </em>维特征空间缩减为<em class="nu"> k </em>维特征子空间(其中<em class="nu"> k &lt; d) </em>。</p><p id="5d46" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→逐步推进特性选择从评估每个单独的特性开始，并选择能够产生最佳性能选定算法模型的特性。</p><p id="b17b" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→后退功能选择密切相关，正如您可能已经猜到的，它从整个功能集开始，并从那里后退，删除功能以找到预定义大小的最佳子集。</p><p id="6f19" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw iy"> <em class="nu"> →什么叫“最好？”</em>T29】</strong></p><p id="fa11" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这完全取决于定义的评估标准(AUC、预测准确性、RMSE 等。).接下来，评估所选特征和后续特征的所有可能组合，并且选择第二特征，等等，直到选择了所需的预定数量的特征。</p><p id="dbd4" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→简而言之，SFA 根据分类器性能一次删除或添加一个特征，直到达到所需大小的特征子集<em class="nu"> k </em>。</p><blockquote class="nr ns nt"><p id="fcce" class="ju jv nu jw b jx jy jz ka kb kc kd ke nv kg kh ki nw kk kl km nx ko kp kq kr ij bi translated">注意:我建议你访问官方文档，通过例子了解更多细节</p></blockquote><div class="ip iq gp gr ir ks"><a href="http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/" rel="noopener  ugc nofollow" target="_blank"><div class="kt ab fo"><div class="ku ab kv cl cj kw"><h2 class="bd iy gy z fp kx fr fs ky fu fw iw bi translated">顺序特征选择器— mlxtend</h2><div class="kz l"><h3 class="bd b gy z fp kx fr fs ky fu fw dk translated">包含日常数据科学任务的有用工具和扩展的库。</h3></div><div class="la l"><p class="bd b dl z fp kx fr fs ky fu fw dk translated">rasbt.github.io</p></div></div></div></a></div><h1 id="6afd" class="mc md ix bd me mf ot mh mi mj ou ml mm mn ov mp mq mr ow mt mu mv ox mx my mz bi translated">嵌入式方法:</h1><figure class="ok ol om on gt is gh gi paragraph-image"><div role="button" tabindex="0" class="oz pa di pb bf pc"><div class="gh gi pr"><img src="../Images/75906ebdeddda42b94e3fda0ececdaf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pUMX6L3I6Y1n6S3C.png"/></div></div><figcaption class="oo op gj gh gi oq or bd b be z dk">Blueprint of Embbeded Methods</figcaption></figure><p id="5150" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→嵌入式方法结合了过滤器和包装器方法的特性。它是由具有内置特征选择方法的算法实现的。</p><p id="4aa1" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→因此，这不是任何类型的特殊特征选择或提取技术，它们也有助于避免过度拟合。</p><ol class=""><li id="d3ba" class="lh li ix jw b jx jy kb kc kf lj kj lk kn ll kr lm ln lo lp bi translated">线性回归中的套索正则化</li><li id="1e35" class="lh li ix jw b jx lq kb lr kf ls kj lt kn lu kr lm ln lo lp bi translated">在随机森林中选择 k-best</li><li id="e76e" class="lh li ix jw b jx lq kb lr kf ls kj lt kn lu kr lm ln lo lp bi translated">梯度推进机</li></ol><h1 id="1d81" class="mc md ix bd me mf ot mh mi mj ou ml mm mn ov mp mq mr ow mt mu mv ox mx my mz bi translated">过滤器和包装器方法之间的区别</h1><figure class="ok ol om on gt is gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/99f929ebe244079521c89ab2ab535c96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*gBJoWHzuNiRAB9M3oDPeNQ.png"/></div></figure></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="13bd" class="mc md ix bd me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz bi translated">第 3 部分:降维</h1><p id="f469" class="pw-post-body-paragraph ju jv ix jw b jx nm jz ka kb nn kd ke kf no kh ki kj np kl km kn nq kp kq kr ij bi translated"><strong class="jw iy">所以，还是从同一个问题开始，什么是降维？</strong></p><p id="2d73" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">简单来说，就是将一个初始的<em class="nu"> d </em>维特征空间缩减为一个<em class="nu"> k </em>维特征子空间(其中<em class="nu">k&lt;d】</em>)。</p><p id="fe7c" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw iy">那么特征选择&amp;提取又是为什么呢？</strong></p><p id="43e0" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">某种程度上，是的(但只是俗人术语<em class="nu"/>’)。为了理解这一点，我们必须深入研究。</p><blockquote class="nr ns nt"><p id="2a95" class="ju jv nu jw b jx jy jz ka kb kc kd ke nv kg kh ki nw kk kl km nx ko kp kq kr ij bi translated">在机器学习中，<strong class="jw iy">维度</strong>简单来说就是指你的数据集中的<strong class="jw iy">个特征</strong>(即输入变量)的数量。当要素的数量相对于数据集中的观察值数量非常大时，<em class="ix">某些</em>算法很难训练出有效的模型。这被称为“维数灾难”，它尤其与依赖于距离计算的聚类算法相关。</p></blockquote><p id="b51c" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">(Quora 的一位用户为维度诅咒提供了一个很好的类比，看看吧)</p><p id="9203" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">因此，当你拥有 100 个甚至 1000 个特性时，你只有一个选择<strong class="jw iy">降维。</strong>让我们讨论两种非常健壮和流行的技术。</p><ol class=""><li id="ca92" class="lh li ix jw b jx jy kb kc kf lj kj lk kn ll kr lm ln lo lp bi translated"><strong class="jw iy">线性判别分析(LDA): </strong>是的，与过滤方法(如上所述)一起，它也被用作降维技术。</li></ol><p id="9b29" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→当特征被标记时，我们在监督学习中使用 LDA。</p><p id="b644" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→请努力理解 LDA(如果您还没有理解的话)。</p><p id="2354" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw iy"> 2。主成分分析(PCA): </strong>主成分分析的主要目的是对数据进行分析，以识别模式并找出模式，从而以最小的信息损失降低数据集的维度。</p><p id="203a" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→ PCA 将尝试通过探索数据的一个特征如何用其他特征来表示来降低维数(线性相关性)。相反，特征选择会考虑目标。</p><p id="810e" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→ PCA 最适用于 3 维或更高维的数据集。因为随着维度的增加，从产生的数据云中做出解释变得越来越困难。</p><p id="4994" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">(PCA 有点复杂，没错。在这里解释会让这篇已经很长的博客更无聊。所以用这两个极好来源来理解，</p><p id="f0b2" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><a class="ae oh" rel="noopener" target="_blank" href="/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c">一、主成分分析一站式服务</a></p><p id="cc89" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">二。Josh Starmer 的视频解释(来自 StatQuest 的同一个人)</p><figure class="ok ol om on gt is"><div class="bz fp l di"><div class="pd pe l"/></div><figcaption class="oo op gj gh gi oq or bd b be z dk">Creator: Josh Starmer</figcaption></figure></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><p id="dc58" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi ny translated"><span class="l nz oa ob bm oc od oe of og di"> W </span>说唱起来:但不是通过包装方法(咄…😜).这是系列的结尾。最后(但不是列表)分钟提示:</p><p id="2cd8" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">永远不要忽视特征工程和特征选择，一切都要依靠算法。</p><p id="c579" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">二。在尾注上，我分享了两个非常有用并且非常棒的工具(记住我的话，这会对你有很大帮助)</p><p id="0575" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→ <a class="ae oh" rel="noopener" target="_blank" href="/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0"> <strong class="jw iy"> <em class="nu">特征选择器</em> </strong> </a> <strong class="jw iy"> </strong>(这得感谢<a class="pt pu ep" href="https://medium.com/u/e2f299e30cb9?source=post_page-----39dfa267b95a--------------------------------" rel="noopener" target="_blank">威廉·科尔森</a>)</p><p id="51e3" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">→ <a class="ae oh" href="https://www.featuretools.com" rel="noopener ugc nofollow" target="_blank"> <strong class="jw iy"> <em class="nu">特征工具</em> </strong> </a>(或<a class="ae oh" href="https://www.analyticsvidhya.com/blog/2018/08/guide-automated-feature-engineering-featuretools-python/" rel="noopener ugc nofollow" target="_blank">本</a>)</p></div></div>    
</body>
</html>