<html>
<head>
<title>Kernel PCA vs PCA vs ICA in Tensorflow/sklearn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Tensorflow/sklearn 中的核 PCA vs PCA vs ICA</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/kernel-pca-vs-pca-vs-ica-in-tensorflow-sklearn-60e17eb15a64?source=collection_archive---------9-----------------------#2018-09-10">https://towardsdatascience.com/kernel-pca-vs-pca-vs-ica-in-tensorflow-sklearn-60e17eb15a64?source=collection_archive---------9-----------------------#2018-09-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/f39e81f4f2bbaed254b14a46b3c0c77c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*UNVEOn9Uuu5W-MtnxahTMA.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/trapcode-xponentialdesign-trapcodetao-UOqSr1m4isHaE" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="9004" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">主成分分析对给定的数据执行线性变换，然而，许多真实世界的数据不是线性可分的。那么，我们能否利用更高维度的优势，同时又不大幅增加所需的计算能力呢？</p><blockquote class="kx ky kz"><p id="cc9d" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated"><strong class="kb ir">请注意，这个帖子是给未来的自己看的，用来回顾和复习这个帖子上的材料。</strong>(还有自学)</p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="4faa" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">讲座:内核主成分分析</strong></p><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">PPT from this <a class="ae jy" href="http://www.cs.haifa.ac.il/~rita/uml_course/lectures/KPCA.pdf" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="e97a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">根据上面的 PPT，我会做一些对我有帮助的简短笔记。</p><div class="ll lm ln lo gt ab cb"><figure class="lr jr ls lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/8404f962f4a1e5075059eed8eceaa046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*G3xkYabVftHOP8UZwnkBdQ.png"/></div></figure><figure class="lr jr mb lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/6d69ae0b7acf486750b8d88db2e181ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*pZ7rLf0-ax6_HbQJYs19Yg.png"/></div></figure></div><p id="e15a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><a class="ae jy" href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory" rel="noopener ugc nofollow" target="_blank">VAP Nik–Chervonenkis theor</a>y 告诉我们，如果我们将数据投影到一个更高维的空间，它会为我们提供更好的分类能力。(左图示例。)这可能类似于神经网络总体所做的，随着深度的增加，更多的抽象特征被提取，并且具有更好的特征来执行分类。</p><div class="ll lm ln lo gt ab cb"><figure class="lr jr mc lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/4000e8275958988b9b090d5f8ca135c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*T5nv7b7Ah9PtxsEtp7VWjQ.png"/></div></figure><figure class="lr jr md lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/d0068bf9cf1d2b5daeca8e732698fe97.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*edaJcjlkgU0udaDdPyA3Ew.png"/></div></figure><figure class="lr jr me lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/a568a2c53f4c8adafa9494df5bf1c1e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*I-PPWPMeKazAK1hYPfkX9A.png"/></div></figure></div><p id="e063" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">内核技巧，一种在不牺牲太多计算时间的情况下将原始数据投影到更高维度的方法。(非线性特征映射)。和矩阵形式来归一化特征空间。</p><div class="ll lm ln lo gt ab cb"><figure class="lr jr mf lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/c64054b8ca0ee553892fda950ae81bed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*1oGSqObQ3FeRqldBHExAiQ.png"/></div></figure><figure class="lr jr mg lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/1deca29016fb3459d3d5ea153400854b.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*LXGs1GgTfJmGAneFNS1YQw.png"/></div></figure></div><p id="89b1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">有效使用 KPCA 的例子，见上文。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="3713" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">KPCA 的不同使用案例</strong></p><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-137" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="https://www.frontiersin.org/articles/10.3389/fnsys.2012.00074/full" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="781f" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在第一篇论文中，作者使用 KPCA 作为预处理步骤，作为特征变换的手段，并与最小二乘支持向量机配对，对 DNA 微阵列进行分类。(微阵列数据具有高维度，因此在执行分类之前执行维度缩减技术是一个好主意。)在第二篇论文中，使用 KPCA 从功能性磁共振图像(fMRI)中提取特征，以对注意力缺陷多动障碍(ADHD)执行自动诊断。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="4770" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">tensor flow 中的 KPCA (RBF)层</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mh"><img src="../Images/e4da543adc8499be1e634f2d1fe59396.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M5u1NjggWWDOoROqdVjmHg.png"/></div></div></figure><p id="be01" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">可以像上面那样实现一个简单的前馈操作，在撰写本文时，我不会对输入数据实现反向传播。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="5949" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> KPCA vs PCA vs ICA </strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mi"><img src="../Images/79f0a2cb0b0fd0f7d691c75e946ab321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EeZacTcHaTz38xQmg-t-fQ.png"/></div></div></figure><p id="3b20" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">让我们从简单的开始，我们有一个线性不可分的 2D 数据点，现在为了验证我们的实现正在工作，让我们使用每个 KPCA、PCA 和 ICA 将我们的数据投影到二维空间中。</p><div class="ll lm ln lo gt ab cb"><figure class="lr jr mj lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/71d80ce45d765c234ec251a7286b6505.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*GKabnDS01LTUgCQMcRrgFw.png"/></div></figure><figure class="lr jr mk lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/1b9e244b06e9379b7941c7e2d87269bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*68W2QDk7QjI24yZTrBQS1w.png"/></div></figure><figure class="lr jr mk lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/1d3ffc3f0c958b0d776043518a3f3622.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*sADSymv_bGfdejvaEVhJaw.png"/></div></figure></div><p id="f901" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →使用 KPCA 的投影<br/> <strong class="kb ir">中图</strong> →使用 PCA 的投影<br/> <strong class="kb ir">右图→ </strong>使用 ICA 的投影</p><p id="271a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从上面的例子我们可以看到，我们的实现工作正常，我们的数据现在是线性可分的。但是为了让事情变得更有趣，让我们看看这些方法在组织病理学图像上的效果。我正在使用来自<a class="ae jy" href="https://zenodo.org/record/1205024#.W5bbVOhKiUk" rel="noopener ugc nofollow" target="_blank">骨髓活检组织病理学数据(HistBMP)的数据集。</a></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi ml"><img src="../Images/32bcec8871e5077b75e65208c049eb62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ttHfT1a0lAbNy09e3YYU8w.png"/></div></div></figure><p id="83c3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如上所述，每幅图像都是 28*28 的灰度图像，我们将通过将 1000 幅图像压缩成 100 幅来找到特征图像。</p><div class="ll lm ln lo gt ab cb"><figure class="lr jr mm lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/5cf786ba42cccedd06a034ace75d18ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*xCV8vi1EvXd4bzSTdj75qg.png"/></div></figure><figure class="lr jr mm lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/9a9ef63605b4fb73f917551ad53b407c.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*OB-yhn_a7QNeRZaHqIqUTw.png"/></div></figure><figure class="lr jr mm lt lu lv lw paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><img src="../Images/a0594f19f756241adadff602b7c48c09.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*d2FbNItjOHcqGr2QnXnzrw.png"/></div></figure></div><p id="a2ff" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">左图</strong> →使用 KPCA <br/>的投影<strong class="kb ir">中图</strong> →使用 PCA <br/>的投影<strong class="kb ir">右图→ </strong>使用 ICA 的投影</p><p id="0630" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">总的来说，我们可以看到 PCA 试图捕捉全局变化，ICA 试图捕捉局部变化。但 KPCA 似乎首先捕捉到了全球变化，但当我们到达特征图像的下部时，我们可以看到它正在捕捉局部变化。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="5170" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">代码</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mn"><img src="../Images/fd4f56920ba9b9f182818a2ff0c2539f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zukv2k1LF2PkDHrMlWsIow.png"/></div></div></figure><p id="6e24" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><em class="la">对于 Google Colab，您需要一个 Google 帐户来查看代码，而且您不能在 Google Colab 中运行只读脚本，因此请在您的操场上创建一个副本。最后，我永远不会请求允许访问你在 Google Drive 上的文件，仅供参考。编码快乐！</em></p><p id="3a81" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">要访问这篇文章的代码，请点击<a class="ae jy" href="https://colab.research.google.com/drive/1n-RW3kPHKExZNS_d7imsbjuwzr06qclU" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="ff1c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">遗言</strong></p><p id="7eaa" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">请注意，对于距离矩阵，我从这个<a class="ae jy" href="https://medium.com/dataholiks-distillery/l2-distance-matrix-vectorization-trick-26aa3247ac6c" rel="noopener">网站</a>借用了非循环形式，整体实现从 Sebastian Raschka 的“<a class="ae jy" href="https://sebastianraschka.com/Articles/2014_kernel_pca.html" rel="noopener ugc nofollow" target="_blank">核技巧和通过 RBF 核 PCA 的非线性维度缩减</a>”借用。</p><p id="b546" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我一直想知道如何绘制每个特征值的方差，这是一篇很好的文章<a class="ae jy" href="https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html" rel="noopener ugc nofollow" target="_blank">解释了其中的诀窍。</a></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mo"><img src="../Images/0976d48c0592f20750c549d0e4594bc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aXEohONuzQf59WGNOGylKQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image from this <a class="ae jy" href="https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="e5ab" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这也是我发现的一个有趣的视频。</p><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="mp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">video from this <a class="ae jy" href="https://www.youtube.com/watch?v=3k9hwRCcT30" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="986e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后，有趣的是，主成分分析/ KPCA 受到方差膨胀和缺乏可推广性的影响，下面的论文提出了一个解决问题的方法。</p><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="http://jmlr.csail.mit.edu/papers/v12/abrahamsen11a.html" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="113b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你想看我所有写作的列表，请在这里查看我的网站。</p><p id="002b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的 twitter 上关注我<a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>，访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或者我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我还实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文</a> t。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="0dc2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="0200" class="mq mr iq kb b kc kd kg kh kk ms ko mt ks mu kw mv mw mx my bi translated">主成分分析。(2015).塞巴斯蒂安·拉什卡博士。2018 年 9 月 7 日检索，来自<a class="ae jy" href="https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html" rel="noopener ugc nofollow" target="_blank">https://sebastianraschka . com/Articles/2015 _ PCA _ in _ 3 _ steps . html</a></li><li id="c869" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">关于特征缩放和规范化。(2014).塞巴斯蒂安·拉什卡博士。检索于 2018 年 9 月 7 日，来自<a class="ae jy" href="https://sebastianraschka.com/Articles/2014_about_feature_scaling.html" rel="noopener ugc nofollow" target="_blank">https://sebastianraschka . com/Articles/2014 _ about _ feature _ scaling . html</a></li><li id="1b23" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">关于特征缩放和规范化。(2014).塞巴斯蒂安·拉什卡博士。检索于 2018 年 9 月 7 日，来自<a class="ae jy" href="https://sebastianraschka.com/Articles/2014_about_feature_scaling.html" rel="noopener ugc nofollow" target="_blank">https://sebastianraschka . com/Articles/2014 _ about _ feature _ scaling . html</a></li><li id="cc77" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">实施主成分分析(PCA)。(2014).塞巴斯蒂安·拉什卡博士。2018 年 9 月 7 日检索，来自<a class="ae jy" href="https://sebastianraschka.com/Articles/2014_pca_step_by_step.html" rel="noopener ugc nofollow" target="_blank">https://sebastianraschka . com/Articles/2014 _ PCA _ step _ by _ step . html</a></li><li id="8595" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">tf.ones | TensorFlow。(2018).张量流。检索于 2018 年 9 月 10 日，来自<a class="ae jy" href="https://www.tensorflow.org/api_docs/python/tf/ones" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/ones</a></li><li id="460a" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">距离矩阵矢量化技巧-流形博客-中。(2016).中等。检索于 2018 年 9 月 10 日，来自<a class="ae jy" href="https://medium.com/dataholiks-distillery/l2-distance-matrix-vectorization-trick-26aa3247ac6c" rel="noopener">https://medium . com/data holiks-distillery/L2-distance-matrix-vectorization-trick-26aa 3247 ac6c</a></li><li id="f883" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">matplotlib，P. (2018 年)。用 matplotlib 同时绘制两个直方图。堆栈溢出。检索于 2018 年 9 月 10 日，来自<a class="ae jy" href="https://stackoverflow.com/questions/6871201/plot-two-histograms-at-the-same-time-with-matplotlib" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/6871201/plot-two-histograms-at-same-time-with-matplotlib</a></li><li id="628a" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">tf.self _ 共轭 _eig | TensorFlow。(2018).张量流。检索于 2018 年 9 月 10 日，来自<a class="ae jy" href="https://www.tensorflow.org/api_docs/python/tf/self_adjoint_eig" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/self _ agreement _ EIG</a></li><li id="8421" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">核技巧和基于 RBF 核 PCA 的非线性降维。(2014).塞巴斯蒂安·拉什卡博士。检索于 2018 年 9 月 10 日，来自 https://sebastianraschka.com/Articles/2014_kernel_pca.html<a class="ae jy" href="https://sebastianraschka.com/Articles/2014_kernel_pca.html" rel="noopener ugc nofollow" target="_blank"/></li><li id="51f8" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">VAP Nik-Chervonenkis 理论。(2018).En.wikipedia.org。检索于 2018 年 9 月 10 日，来自<a class="ae jy" href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/VAP Nik % E2 % 80% 93c hervonenkis _ theory</a></li><li id="8da5" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">Sidhu、n . as garian、r . Greiner 和 m . Brown(2012 年)。核主成分分析在基于 fMRI 的 ADHD 诊断中的降维作用。系统神经科学前沿，6。doi:10.3389/fnsys</li><li id="36ea" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">m .托马斯、k .布拉班特和 b .穆尔(2014 年)。核主成分分析的新带宽选择准则:降维和分类问题的方法。BMC 生物信息学，15(1)，137。doi:10.1186/1471–2105–15–137</li><li id="dbe2" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">t .亚伯拉罕森和 l .汉森(2011 年)。高维核主成分分析中方差膨胀的一种解决方法。机器学习研究杂志，12 期(6 月)，2027–2044。从 http://jmlr.csail.mit.edu/papers/v12/abrahamsen11a.html<a class="ae jy" href="http://jmlr.csail.mit.edu/papers/v12/abrahamsen11a.html" rel="noopener ugc nofollow" target="_blank">取回</a></li><li id="7e47" class="mq mr iq kb b kc mz kg na kk nb ko nc ks nd kw mv mw mx my bi translated">j . tomczak(2018 年)。骨髓活检(HistBMP)的组织病理学数据。芝诺多。检索于 2018 年 9 月 10 日，来自<a class="ae jy" href="https://zenodo.org/record/1205024#.W5bcCOhKiUm" rel="noopener ugc nofollow" target="_blank">https://zenodo.org/record/1205024#.W5bcCOhKiUm</a></li></ol></div></div>    
</body>
</html>