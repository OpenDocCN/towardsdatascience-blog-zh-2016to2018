<html>
<head>
<title>[Lecture] Evolution: from vanilla RNN to GRU &amp; LSTMs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[讲座]进化:从香草 RNN 到 GRU 和 LSTMs</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lecture-evolution-from-vanilla-rnn-to-gru-lstms-58688f1da83a?source=collection_archive---------3-----------------------#2017-08-21">https://towardsdatascience.com/lecture-evolution-from-vanilla-rnn-to-gru-lstms-58688f1da83a?source=collection_archive---------3-----------------------#2017-08-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/779ea9a007288e3d30605647cd2f7f0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W51n2gqbRiAxIeL4E7phrw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">RNNs evolution</figcaption></figure><p id="9712" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如今，递归神经网络无处不在。像谷歌、百度这样的巨头在生产中广泛使用它们来做机器翻译、语音识别以及许多其他任务。实际上，NLP 相关任务中的几乎所有最新技术成果都是通过利用 RNNs 实现的。</p><p id="7bac" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">随着 TensorFlow 等出色的深度学习框架的兴起，构建 LSTM 和其他类型的递归网络比以往任何时候都更容易。人们很容易将它们视为一个黑盒。</p><p id="2123" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们觉得现代 RNNs 背后的直觉是至关重要的。仅仅通过看方程，很难很好地理解 GRU 和 LSTM 网络。事实上，LSTM 网络是香草 RNN 与特定问题斗争的结果。因此，希望理解这些问题和解决它们的方法会使 GRU，LSTM 方程变得更加透明和直观。</p><p id="5f7e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现代 rnn 背后的想法真的很美。在今天的讲座“进化:从香草 RNN 到 GRU &amp; LSTMs”中，我们将讨论它们！</p><p id="aaa1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是幻灯片的<a class="ae la" href="https://goo.gl/XodLUU" rel="noopener ugc nofollow" target="_blank">链接。</a></p><figure class="lb lc ld le gt jr"><div class="bz fp l di"><div class="lf lg l"/></div></figure><p id="0ef3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">非常尊重 R2RT 的博客文章:<a class="ae la" href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html" rel="noopener ugc nofollow" target="_blank">书面记忆:理解、衍生和延伸 LSTM </a>。我们的讲座受到他们工作的强烈启发。</p></div></div>    
</body>
</html>