<html>
<head>
<title>Intuitive Guide to Latent Dirichlet Allocation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">潜在狄利克雷分配直观指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158?source=collection_archive---------1-----------------------#2018-08-23">https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158?source=collection_archive---------1-----------------------#2018-08-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2a94" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/light-on-math" rel="noopener" target="_blank">点亮数学机器学习</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/91194810478499bf8b769440ce1877bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BBJhh7x3g8Va2LaORmo_wA.jpeg"/></div></div></figure><p id="9e0e" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">主题建模是指确定最能描述一组文档的主题的任务。这些主题只会在主题建模过程中出现(因此称为潜在)。其中一种流行的主题建模技术被称为<em class="li">潜在狄利克雷分配</em> (LDA)。虽然名字很拗口，但背后的概念非常简单。</p><p id="5d58" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">简而言之，LDA 设想了一组固定的主题。每个主题代表一组单词。LDA 的目标是以某种方式将所有文档映射到主题，使得每个文档中的单词大部分被那些虚构的主题捕获。我们将系统地学习这种方法，直到你能足够舒服地自己使用这种方法。</p><p id="5e61" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">这是数学机器学习系列<strong class="km jd"> <em class="li">之光 A-Z </em> </strong>的第四篇博文。你可以在下面的信中找到以前的博客文章。</p><p id="98d5" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><strong class="km jd">A B</strong><a class="ae lj" href="http://www.thushv.com/computer_vision/light-on-math-machine-learning-intuitive-guide-to-convolution-neural-networks/" rel="noopener ugc nofollow" target="_blank"><strong class="km jd">C</strong></a><strong class="km jd"/><a class="ae lj" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-understanding-decision-trees-adb2165ccab7"><strong class="km jd">D</strong></a><strong class="km jd">E F G H I J</strong><a class="ae lj" href="http://www.thushv.com/machine-learning/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence/" rel="noopener ugc nofollow" target="_blank"><strong class="km jd">K</strong></a><strong class="km jd">L</strong><a class="ae lj" href="https://thushv89.medium.com/light-on-math-ml-intuitive-guide-to-matrix-factorization-bee5af0c01aa" rel="noopener"><strong class="km jd">M</strong></a>*<strong class="km jd"/><a class="ae lj" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee"><strong class="km jd">N</strong></a><strong class="km jd">O P Q R S T U V</strong></p><p id="7082" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><strong class="km jd"/>🔈🔥<strong class="km jd">最新文章</strong>🔥🔈<strong class="km jd"/>:<a class="ae lj" href="https://medium.com/p/bee5af0c01aa" rel="noopener">M—矩阵分解</a></p><p id="f9aa" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">*表示中等付费墙后面的文章。</p><h1 id="b965" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">为什么要主题建模？</h1><p id="27ac" class="pw-post-body-paragraph kk kl it km b kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld mm lf lg lh im bi translated">主题建模在现实世界中有哪些用途？历史学家可以使用 LDA 通过分析基于年份的文本来确定历史上的重要事件。基于网络的图书馆可以使用 LDA<em class="li">根据你过去的阅读推荐书籍</em>。新闻提供者可以使用主题建模来<em class="li">快速理解文章或者聚集相似的文章</em>。另一个有趣的应用是<em class="li">图像的无监督聚类</em>，其中每个图像都被视为类似于一个文档。</p><h1 id="f788" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">这篇文章有什么独特之处？这是海里的另一条鱼吗？</h1><p id="1670" class="pw-post-body-paragraph kk kl it km b kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld mm lf lg lh im bi translated">简而言之，答案是否定的！我浏览了许多不同的文章。而且有很多很棒的文章/视频给人直觉。然而，他们中的大多数人仅仅停留在回答这样的问题上:</p><ul class=""><li id="bca5" class="mn mo it km b kn ko kr ks kv mp kz mq ld mr lh ms mt mu mv bi translated">LDA 背后的直觉是什么？</li><li id="467e" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated">什么是狄利克雷分布？</li></ul><p id="ec74" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">我确实谈到了这一点，但我不认为我们应该就此止步。这些模型的训练方式是我在阅读的许多文章中发现缺失的一个关键部分。所以我试着回答更多的问题，比如:</p><ul class=""><li id="cdad" class="mn mo it km b kn ko kr ks kv mp kz mq ld mr lh ms mt mu mv bi translated">我们想解决的数学实体是什么？</li><li id="f35b" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated">我们如何解决这个问题？</li></ul><h1 id="6c3f" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">LDA 背后的大理念是什么？</h1><p id="36ff" class="pw-post-body-paragraph kk kl it km b kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld mm lf lg lh im bi translated">一旦你理解了这个大概念，我想它会帮助你理解为什么 LDA 的机制是这样的。所以现在开始。</p><blockquote class="nb"><p id="1858" class="nc nd it bd ne nf ng nh ni nj nk lh dk translated">每个文档可以通过主题的分布来描述，每个主题可以通过词的分布来描述</p></blockquote><p id="f151" class="pw-post-body-paragraph kk kl it km b kn nl kp kq kr nm kt ku kv nn kx ky kz no lb lc ld np lf lg lh im bi translated">但是我们为什么要用这个想法呢？我们通过一个例子来想象一下。</p><h1 id="87a1" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">通俗地说就是 LDA</h1><figure class="nr ns nt nu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nq"><img src="../Images/597c041fcefc3dc3c434a0b7423b5253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dbjL9SvCRtXsjKZYhbnj8w.jpeg"/></div></div></figure><p id="3eb6" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">假设您有一组 1000 个单词(即所有文档中最常见的 1000 个单词)，并且您有 1000 个文档。假设每个文档中平均出现 500 个这样的单词。怎么才能了解每个文档属于什么类别？一种方法是根据单词在文档中的出现，通过线索将每个文档与每个单词连接起来。类似下面的东西。</p><figure class="nr ns nt nu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nv"><img src="../Images/f3976472b35b5e9a64a8e78a2ae21fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QQTk2TGyzhakGh0lZ9P03w.jpeg"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">Modeling documents just with words. You can see that we can’t really infer any useful information due to the large amount of connections</figcaption></figure><p id="e4a7" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">然后当你看到一些文档与同一套单词相关联时。你知道他们讨论同样的话题。然后你可以阅读其中的一份文件，知道所有这些文件的内容。但要做到这一点，你没有足够的线程。为此，您将需要大约 500*1000=500，000 个线程。但我们生活在 2100 年，我们已经耗尽了制造螺纹的所有资源，所以它们非常昂贵，你只能负担得起 10，000 根螺纹。你如何解决这个问题？</p><h1 id="5c2d" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">深入减少螺纹！</h1><p id="486c" class="pw-post-body-paragraph kk kl it km b kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld mm lf lg lh im bi translated">我们可以通过引入潜在(即隐藏)层来解决这个问题。假设我们知道文档中出现的 10 个主题。但是这些话题是不被观察的，我们只观察单词和文档，因此话题是潜在的。我们希望利用这些信息来减少线程的数量。然后你可以做的是，根据单词在主题中的位置将单词与主题联系起来，然后根据每个文档涉及的主题将主题与文档联系起来。</p><p id="6f4a" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">现在假设你得到的每个文档有大约 5 个主题，每个主题涉及 500 个单词。也就是说，我们需要 1000*5 个线程将文档连接到主题，10*500 个线程将主题连接到单词，总计 10000 个线程。</p><figure class="nr ns nt nu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi oa"><img src="../Images/69bd8f4793003e7a36e57cef2e6df9e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2uj6t3gNv76SpHrWf5-z-A.jpeg"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">Words are modeled by a set of topics and documents are modeled by a set of topics. The relationships are clearer than the first example because there’s a fewer connections than the first example.</figcaption></figure><p id="8bfe" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><strong class="km jd">注</strong>:我这里用的话题(“动物”、“运动”、“科技”)都是虚构的。在真解中，你不会有这样的题目而是类似<em class="li">(0.3 *猫，0.4 *狗，0.2 *忠，0.1 *恶)</em>代表题目“动物”的东西。也就是说，如前所述，每个文档都是单词的分布。</p><h1 id="c0d3" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">一个不同的观点:LDA 想象文档是如何生成的？</h1><p id="9d4a" class="pw-post-body-paragraph kk kl it km b kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld mm lf lg lh im bi translated">为了给正在发生的事情提供更多的上下文，LDA 假设您看到的任何文档背后都有以下生成过程。为了简单起见，让我们假设我们正在生成一个包含 5 个单词的文档。但是同样的过程可以推广到每个都有<em class="li"> N </em>个单词的<em class="li"> M </em>个文档。标题很好地解释了这里发生了什么。所以我就不重申了。</p><figure class="nr ns nt nu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ob"><img src="../Images/95bc946c2ad33c0d07ec9eb1b20c8a24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fCc0JT3W-1ViYyw0hJ7rdA.jpeg"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">How a document is generated. First α (alpha) organise the ground θ (theta) and then you go and pick a ball from θ. Based on what you pick, you’re sent to ground β (beta). β is organised by η (Eta). Now you pick a word from β and put it into the document. You iterate this process 5 times to get 5 words out.</figcaption></figure><p id="9d4e" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">这个图像描绘了一个已经学习过的 LDA 系统的样子。但是要达到这个阶段，你必须回答几个问题，例如:</p><ul class=""><li id="1dfd" class="mn mo it km b kn ko kr ks kv mp kz mq ld mr lh ms mt mu mv bi translated">我们如何知道文档中有多少主题？</li><li id="2b3b" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated">你可以看到，我们的场地已经有了一个很好的结构，可以帮助我们生成合理的文件，因为组织者已经确保了场地的适当设计。我们如何找到如此优秀的组织者？</li></ul><p id="9c69" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">这将在接下来的几节中回答。此外，我们将从这一点开始变得有点技术性。所以系好安全带！</p><p id="1b06" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><strong class="km jd">注意</strong> : LDA 不关心文档中单词的顺序。通常，LDA 使用<a class="ae lj" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">词袋</a>特征表示来表示一个文档。这是有道理的，因为，如果我拿一份文件，把这些单词混在一起，然后给你，你仍然可以猜出文件中讨论了什么样的主题。</p><h1 id="f986" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">变得有点数学化…</h1><p id="30a5" class="pw-post-body-paragraph kk kl it km b kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld mm lf lg lh im bi translated">在深入细节之前。让我们弄清楚一些事情，比如符号和定义。</p><h2 id="7100" class="oc ll it bd lm od oe dn lq of og dp lu kv oh oi ly kz oj ok mc ld ol om mg iz bi translated">定义和符号</h2><ul class=""><li id="7d1d" class="mn mo it km b kn mi kr mj kv on kz oo ld op lh ms mt mu mv bi translated">k —文档所属主题的数量(固定数量)</li><li id="0cb7" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated">v——词汇量</li><li id="845d" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated">M —文件数量</li><li id="acaf" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated">N —每个文档中的字数</li><li id="6828" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated"><em class="li"> w </em> —文档中的一个单词。这被表示为大小为<em class="li"> V </em>的一个热编码向量(即<em class="li"> V </em> —词汇大小)</li><li id="4e55" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated"><strong class="km jd"> <em class="li"> w </em> </strong>(粗体<em class="li"> w </em>):代表一个文档(即<em class="li"> N </em>字的矢量<em class="li">w</em>s)</li><li id="cdb6" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated"><em class="li"> D — </em>语料库，收集了<em class="li"> M </em>的文档</li><li id="8443" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated">z —一组<em class="li"> k </em>主题中的一个主题。一个话题就是一个分布词。例如，它可能是，<em class="li">动物= (0.3 猫，0.4 狗，0 人工智能，0.2 忠诚，0.1 邪恶)</em></li></ul><h2 id="dc35" class="oc ll it bd lm od oe dn lq of og dp lu kv oh oi ly kz oj ok mc ld ol om mg iz bi translated">更精确地定义文档生成</h2><p id="5aa3" class="pw-post-body-paragraph kk kl it km b kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld mm lf lg lh im bi translated">首先让我们把上面关于生成文档的基础例子，放到一个适当的数学绘图中。</p><figure class="nr ns nt nu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi oq"><img src="../Images/3b95b00ebcdbb6c5140efa93dacb1ea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WGsNq9tjIwhGMCEthvAmVw.jpeg"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">Graphical model of the LDA. Here I mark the shapes of the all the possible variables (both observed and hidden). But remember that θ, z, and β are distributions, not deterministic values</figcaption></figure><p id="2dc9" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">让我们破译这是什么意思。我们有一个单一的α值(即底θ的组织者)来定义θ；文档的主题分布将是这样的。我们有<em class="li"> M </em>个文档，每个文档都有一些θ分布。现在为了更清楚地理解事情，眯起你的眼睛，让那个<em class="li"> M </em>盘子消失(假设只有一份文件)，哇！</p><p id="fd00" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">现在，单个文档有<em class="li"> N </em>个单词，每个单词由一个主题生成。你生成了<em class="li"> N </em>个主题来填充单词。这 N 个字还是占位符。</p><p id="078b" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">现在顶板开始工作了。基于η，β具有某种分布(即，准确地说是狄利克雷分布——即将讨论),并且根据该分布，β为每个主题生成 k 个单独的单词。现在，根据占位符所代表的主题，为每个占位符(在 N 个占位符的集合中)填入一个单词。</p><p id="2467" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">Viola，你现在有一份有 N 个字的文件了！</p><h2 id="06a0" class="oc ll it bd lm od oe dn lq of og dp lu kv oh oi ly kz oj ok mc ld ol om mg iz bi translated">为什么α和η是常数？</h2><p id="4bd7" class="pw-post-body-paragraph kk kl it km b kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld mm lf lg lh im bi translated">α和η在上图中显示为常数。但实际上比这更复杂。例如，α对于每个文档都有一个主题分布(θ ground 对于每个文档)。理想情况下，一个(<em class="li">M×K)</em>形状的矩阵。而η对每个题目都有一个参数向量。η的形状为<em class="li"> (k x V) </em>。在上图中，常数实际上代表矩阵，是通过将矩阵中的单个值复制到每个单元格中而形成的。</p><h2 id="79c8" class="oc ll it bd lm od oe dn lq of og dp lu kv oh oi ly kz oj ok mc ld ol om mg iz bi translated">让我们更详细地理解θ和β</h2><p id="027b" class="pw-post-body-paragraph kk kl it km b kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld mm lf lg lh im bi translated">θ是一个随机矩阵，其中θ <em class="li"> (i，j) </em>表示第<em class="li"> i </em>个文档包含属于第<em class="li"> j </em>个主题的单词的概率。如果你看看上面例子中的地面θ是什么样子，你可以看到球被很好地放置在角落而不是中间。拥有这样一个属性的好处是，我们产生的单词可能属于一个主题，就像现实世界中的文档一样。这是通过将θ建模为<a class="ae lj" href="https://en.wikipedia.org/wiki/Dirichlet_distribution" rel="noopener ugc nofollow" target="_blank">狄利克雷分布</a>而产生的性质。类似地，β(i，j)表示第<em class="li"> i </em>个主题包含第<em class="li"> j </em>个单词的概率。而β也是狄利克雷分布。下面，我提供一个快速的迂回来理解狄利克雷分布。</p><h2 id="9da3" class="oc ll it bd lm od oe dn lq of og dp lu kv oh oi ly kz oj ok mc ld ol om mg iz bi translated">快速迂回:理解狄利克雷分布</h2><p id="4680" class="pw-post-body-paragraph kk kl it km b kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld mm lf lg lh im bi translated">狄利克雷分布是<a class="ae lj" href="https://en.wikipedia.org/wiki/Beta_distribution" rel="noopener ugc nofollow" target="_blank">贝塔分布</a>的多元推广。这里我们讨论一个三维问题的例子，其中α中有 3 个参数影响θ的形状(即分布)。对于 N 维狄利克雷分布，有一个长度为 N 的向量，为α。你可以看到θ的形状随着α值的不同而变化。例如，您可以看到顶部中间的图显示了与θ ground 相似的形状。</p><p id="fa5e" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">主要要点如下:</p><blockquote class="nb"><p id="9ec0" class="nc nd it bd ne nf ng nh ni nj nk lh dk translated">较大的α值将分布推向三角形的中间，而较小的α值将分布推向角落。</p></blockquote><figure class="os ot ou ov ow kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi or"><img src="../Images/635d9e4d6209e9537e3f6854aecb757c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3oOHy1tUfUT9Z379Alb9nA.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">How the distribution of θ changes with different α values</figcaption></figure><h1 id="babe" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">我们如何学习 LDA？</h1><p id="d4cd" class="pw-post-body-paragraph kk kl it km b kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld mm lf lg lh im bi translated">我们仍然没有回答真正的问题是，我们如何知道精确的α和η值？在此之前，让我列出我们需要找到的潜在变量。</p><ul class=""><li id="985c" class="mn mo it km b kn ko kr ks kv mp kz mq ld mr lh ms mt mu mv bi translated">α-分布相关参数，控制语料库中所有文档的主题分布情况</li><li id="fc1d" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated">θ —随机矩阵，其中θ(i，j)表示第 I 个文档包含第 j 个主题的概率</li><li id="675e" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated">η —分布相关参数，控制每个主题中单词的分布情况</li><li id="ce7a" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated">β —一个随机矩阵，其中β(i，j)表示第 I 个主题包含第 j 个单词的概率。</li></ul><h2 id="a7f5" class="oc ll it bd lm od oe dn lq of og dp lu kv oh oi ly kz oj ok mc ld ol om mg iz bi translated">阐明我们需要学习什么</h2><p id="f87c" class="pw-post-body-paragraph kk kl it km b kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld mm lf lg lh im bi translated">如果要我用数学的方式来表述我感兴趣的是什么，那么如下所示:</p><figure class="nr ns nt nu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ox"><img src="../Images/894113c1ec8e45ed3a32a3e75b9624df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OGKyyIsHLgZk0ky4cT83Pg.png"/></div></div></figure><p id="27ad" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">它看起来很吓人，但包含了一个简单的信息。这基本上是在说，</p><p id="fe4c" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">我有一组 M 个文档，每个文档有 N 个单词，其中每个单词由一组 K 个主题中的一个主题生成。我在寻找联合后验概率:</p><ul class=""><li id="d5cc" class="mn mo it km b kn ko kr ks kv mp kz mq ld mr lh ms mt mu mv bi translated">θ —主题分布，每个文档一个主题，</li><li id="d811" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated"><strong class="km jd"> z </strong> —每个文档的 N 个主题，</li><li id="a020" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated">β —词的分布，每个主题一个词，</li></ul><p id="5c8f" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">鉴于，</p><ul class=""><li id="43e2" class="mn mo it km b kn ko kr ks kv mp kz mq ld mr lh ms mt mu mv bi translated">D —我们拥有的所有数据(即，修正数据)，</li></ul><p id="c74d" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">并使用参数，</p><ul class=""><li id="1482" class="mn mo it km b kn ko kr ks kv mp kz mq ld mr lh ms mt mu mv bi translated">α-每个文档的参数向量(文档-主题分布)</li><li id="7a07" class="mn mo it km b kn mw kr mx kv my kz mz ld na lh ms mt mu mv bi translated">η —每个主题的参数向量(主题—单词分布)</li></ul><p id="a422" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">但是我们不能很好地计算这个，因为这个实体很难处理。我们如何解决这个问题？</p><h1 id="e3cc" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">我该如何解决这个问题？救援的变分推理</h1><p id="8eec" class="pw-post-body-paragraph kk kl it km b kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld mm lf lg lh im bi translated">有很多方法可以解决这个问题。但是在这篇文章中，我将集中讨论变分推理。我们上面讨论的概率是一个非常棘手的后验概率(这意味着我们不能在纸上计算，也没有很好的方程)。所以我们要用一些已知的概率分布来近似这个值，这个概率分布和真实的后验概率非常接近。这就是变分推理背后的思想。</p><p id="ee6e" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">这样做的方法是最小化近似和真实后验之间的<a class="ae lj" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8"> KL 发散</a>作为优化问题。同样，我不打算详细讨论，因为这超出了讨论范围。</p><p id="d4b8" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">但是我们将快速查看一下优化问题</p><figure class="nr ns nt nu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi oy"><img src="../Images/fdea5c0c47b43e28f3239f08e1aef1ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*llcMrijX1Ln3-B3_rK4gDA.png"/></div></div></figure><p id="f762" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">γ、ϕ和λ分别代表我们用来逼近θ、<strong class="km jd"> z </strong>和β的自由变分参数。这里 D(q||p)代表<em class="li"> q </em>和<em class="li"> p </em>之间的 KL 散度。并且通过改变γ,ϕ和λ，我们得到不同的<em class="li"> q </em>分布，其与真实后验<em class="li"> p </em>具有不同的距离。我们的目标是找到γ*、ϕ*和λ*，使近似值<em class="li"> q </em>和真实后验值<em class="li"> p </em>之间的 KL 散度最小化。</p><p id="1d88" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">一切都很好地定义了，它只是一个迭代求解上述优化问题的问题，直到解决方案收敛。一旦有了γ*、ϕ*和λ*，你就有了最终 LDA 模型所需的一切。</p><h1 id="3993" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">包裹</h1><p id="9fb2" class="pw-post-body-paragraph kk kl it km b kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld mm lf lg lh im bi translated">本文讨论了潜在狄利克雷分配(LDA)问题。LDA 是一种强大的方法，它允许识别文档中的主题并将文档映射到这些主题。LDA 有许多用途，例如向顾客推荐书籍。</p><p id="f3c5" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">我们通过一个连接线程的例子看了 LDA 是如何工作的。然后，我们看到了基于 LDA 如何想象文档生成的不同视角。最后我们进入了模型的训练。在这篇文章中，我们讨论了 LDA 背后的大量数学知识，同时保持了数学的轻松。我们看了一下狄利克雷分布是什么样子，我们感兴趣的概率分布是什么(即后验分布)，以及我们如何使用变分推理来解决这个问题。</p><p id="848f" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">我将发布一个关于如何使用 LDA 进行主题建模的教程，包括一些很酷的分析，作为另一个教程。干杯。</p></div><div class="ab cl oz pa hx pb" role="separator"><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe"/></div><div class="im in io ip iq"><p id="87d8" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">如果你喜欢我分享的关于数据科学和机器学习的故事，考虑成为会员吧！</p><div class="pg ph gp gr pi pj"><a href="https://thushv89.medium.com/membership" rel="noopener follow" target="_blank"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd jd gy z fp po fr fs pp fu fw jc bi translated">通过我的推荐链接加入媒体</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">thushv89.medium.com</p></div></div><div class="ps l"><div class="pt l pu pv pw ps px ki pj"/></div></div></a></div></div><div class="ab cl oz pa hx pb" role="separator"><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe"/></div><div class="im in io ip iq"><h1 id="d966" class="lk ll it bd lm ln py lp lq lr pz lt lu lv qa lx ly lz qb mb mc md qc mf mg mh bi translated">新的！加入我的新 YouTube 频道</h1><figure class="nr ns nt nu gt kd gh gi paragraph-image"><a href="https://www.youtube.com/channel/UC1HkxV8PtmWRyQ39MfzmtGA/"><div class="gh gi qd"><img src="../Images/9440b177b85aecfa0260c0a3fe873625.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/0*9xvdM2wjiJbra26G.png"/></div></a></figure><p id="dba3" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">如果你渴望看到我关于各种机器学习/深度学习主题的视频，请确保加入<a class="ae lj" href="https://www.youtube.com/channel/UC1HkxV8PtmWRyQ39MfzmtGA/" rel="noopener ugc nofollow" target="_blank"> DeepLearningHero </a>。</p><h1 id="f298" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">参考</h1><p id="b96d" class="pw-post-body-paragraph kk kl it km b kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld mm lf lg lh im bi translated">这里有一些有用的参考资料，有助于理解 LDA 是否有什么不清楚的地方。</p><p id="0c11" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">大卫·布莱教授的原始论文</p><p id="1a5e" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><a class="ae lj" href="https://www.youtube.com/watch?v=3mHy4OSyRf0&amp;t=1058s" rel="noopener ugc nofollow" target="_blank">直观的视频，解释 LDA 背后的基本理念</a></p><p id="0dd3" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">大卫·布雷教授的演讲</p></div></div>    
</body>
</html>