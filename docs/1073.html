<html>
<head>
<title>Tensor2Tensor and One Model to Learn them all</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Tensor2Tensor和一个模型来学习所有内容</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tensor2tensor-and-one-model-to-learn-them-all-7ef3f9b61ba4?source=collection_archive---------9-----------------------#2017-07-26">https://towardsdatascience.com/tensor2tensor-and-one-model-to-learn-them-all-7ef3f9b61ba4?source=collection_archive---------9-----------------------#2017-07-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c07b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最近谷歌宣布发布<a class="ae kl" href="https://github.com/tensorflow/tensor2tensor" rel="noopener ugc nofollow" target="_blank"> Tensor2Tensor </a>库。T2T的主要目的是通过向每个人提供最先进的模型来加速深度学习研究。现在，它主要集中在NLP问题和一般的序列到序列。然而，它是建立在Tensorflow之上的，Tensor2Tensor的模块化结构允许开发人员和研究人员以简单的方式添加新模型。</p><p id="84b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个python库为数据集生成、训练和超参数调整提供了简单的API。它包含:</p><ul class=""><li id="024a" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">流行的序列数据集生成工具</li><li id="c1ae" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">不同输入/输出模式的工作模型:符号、图像、音频、标签</li><li id="9334" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">模型及其优化器的预调超参数集</li><li id="98c6" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">具有简单CLI界面的训练器</li><li id="96e6" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">像ByteNet、MultiModel( <a class="ae kl" href="https://arxiv.org/pdf/1706.05137.pdf" rel="noopener ugc nofollow" target="_blank"> paper </a>)、SliceNet、Transformer( <a class="ae kl" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"> paper </a>)和Xception这样的SOTA模型</li></ul><p id="4ed8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是这一个值得特别注意:</p><h1 id="3d64" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">多模态建筑</h1><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/f9ad4ed1c73bdae1fedd5675f6b27089.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9IVp7Bkj8Pq-prYO.jpg"/></div></div></figure><p id="2e40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个网络最吸引人的特点是它可以学会同时解决一系列不同的问题。研究人员训练同一个神经网络来进行图像分类、图像字幕、文本解析、语音识别、英语德语和英语法语翻译。它在每一个方面都做得很好！</p><p id="2d2d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这在迁移学习中是一个巨大的成功，在不久的将来，我们可能会看到很多论文和对这个架构的改进。</p><p id="ab23" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Transformer也很酷，它使用注意力机制而不是递归层，并在WMT英语-&gt;德语翻译基准测试中获得SOTA。与此同时，相对于竞争对手，它只需要1%到10%的计算能力。</p><p id="0e5d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">事实上，我相信多式联运网络有一个非常光明的未来。也许会成为DL最大的突破之一。</p><blockquote class="mk ml mm"><p id="0b36" class="jn jo mn jp b jq jr js jt ju jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj kk ij bi translated">最初发表于<a class="ae kl" href="http://cognitivechaos.com/tensor2tensor-and-one-model-to-learn-them-all/" rel="noopener ugc nofollow" target="_blank">CognitiveChaos.com</a></p></blockquote></div></div>    
</body>
</html>