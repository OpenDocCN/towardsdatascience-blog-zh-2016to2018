<html>
<head>
<title>Introduction to Word Embedding and Word2Vec</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入和 Word2Vec 简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa?source=collection_archive---------0-----------------------#2018-09-01">https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa?source=collection_archive---------0-----------------------#2018-09-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="5faa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">单词嵌入是最流行的文档词汇表示之一。它能够捕捉文档中单词的上下文、语义和句法相似性、与其他单词的关系等。</p><p id="2242" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">到底什么是单词嵌入？不严格地说，它们是特定单词的矢量表示。说到这里，接下来是我们如何生成它们？更重要的是，他们如何捕捉上下文？</p><p id="a3c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Word2Vec 是使用浅层神经网络学习单词嵌入的最流行的技术之一。它是由谷歌的托马斯·米科洛夫于 2013 年开发的。</p><p id="1e47" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们一部分一部分地解决这个问题。</p><p id="9715" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">我们为什么需要它们？</strong></p><p id="63f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑下面类似的句子:<em class="km">过得愉快</em>和<em class="km">过得愉快。它们几乎没有什么不同的意思。如果我们构造一个穷举词汇表(姑且称之为 V)，那就有 V = {Have，a，good，great，day}。</em></p><p id="23d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们为 V 中的每个单词创建一个独热码编码向量。我们的独热码编码向量的长度将等于 V (=5)的大小。除了索引处代表词汇表中相应单词的元素之外，我们将有一个零向量。这个特殊的元素就是。下面的编码可以更好地解释这一点。</p><p id="848b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Have = [1，0，0，0，0]`；a=[0，1，0，0，0]`；good=[0，0，1，0，0]`；great=[0，0，0，1，0]`；day=[0，0，0，0，1]`(`代表转置)</p><p id="f450" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们试图将这些编码可视化，我们可以想象一个 5 维空间，其中每个单词占据一个维度，与其余维度无关(没有沿其他维度的投影)。这意味着“好”和“伟大”就像“天”和“有”一样不同，这是不正确的。</p><p id="919e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的目标是让具有相似上下文的单词占据相近的空间位置。从数学上来说，这些向量之间的角度余弦应该接近 1，即角度接近 0。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi kn"><img src="../Images/4221286213aba7f40b765c5f80b0d547.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/0*XMW5mf81LSHodnTi.png"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk"><a class="ae kl" href="http://i0.wp.com/techinpink.com/wp-content/uploads/2017/07/cosine.png" rel="noopener ugc nofollow" target="_blank">Google Images</a></figcaption></figure><p id="3327" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">产生<em class="km">分布式表示</em>的想法来了。直观地说，我们引入了一个单词对另一个单词的一些<em class="km">依赖性</em>。该单词的上下文中的单词将获得这种<em class="km">依赖性的更大份额。</em>在一个热编码表示中，所有的字都是彼此独立的<em class="km"/><em class="km">，</em>如前所述。</p><p id="6f41" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">word 2 vec 是如何工作的？</strong></p><p id="6aa0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Word2Vec 是一种构建这种嵌入的方法。它可以使用两种方法获得(都涉及神经网络):Skip Gram 和 Common Bag Of Words (CBOW)</p><p id="6b40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="km"> CBOW 模型:</em> </strong>该方法以每个单词的上下文为输入，尝试预测上下文对应的单词。考虑我们的例子:<em class="km">祝你有美好的一天。</em></p><p id="c76a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让神经网络的输入是单词，<em class="km">棒极了。</em>注意，这里我们试图使用单个上下文输入单词<em class="km">来预测一个目标单词(<em class="km"> d </em> ay <em class="km"> ) </em>。</em>更具体地说，我们使用输入单词的一个热编码，并测量与目标单词的一个热编码相比的输出误差(<em class="km"> d </em> ay)。<em class="km"> </em>在预测目标词的过程中，我们学习目标词的向量表示。</p><p id="f10b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们更深入地看看实际的架构。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi kz"><img src="../Images/9ebd4d9437101d0424501fc1ac47b089.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3DFDpaXoglalyB4c.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk"><a class="ae kl" href="https://i.stack.imgur.com/sAvR9.png" rel="noopener ugc nofollow" target="_blank">CBOW Model</a></figcaption></figure><p id="9809" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输入或上下文单词是大小为 V 的一个热编码向量。隐藏层包含 N 个神经元，输出也是长度为 V 的向量，其元素是 softmax 值。</p><p id="1ff8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们把图中的术语对了:<br/> <em class="km"> - Wvn 是将输入 x 映射到隐藏层的权重矩阵(V*N 维矩阵)<br/> </em> - <em class="km"> W`nv 是将隐藏层输出映射到最终输出层的权重矩阵(N*V 维矩阵)</em></p><p id="fd35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我不会涉足数学。我们就能知道发生了什么。</p><p id="4d7a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">隐藏层神经元只是将输入的加权和复制到下一层。没有类似乙状结肠、tanh 或 ReLU 的激活。唯一的非线性是输出层中的 softmax 计算。</p><p id="483b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是，上述模型使用单个上下文单词来预测目标。我们可以使用多个上下文单词来做同样的事情。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi le"><img src="../Images/e8632655b599e43ea56c2df046f2ffa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/0*CCsrTAjN80MqswXG"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk"><a class="ae kl" href="http://www.stokastik.in/understanding-word-vectors-and-word2vec/" rel="noopener ugc nofollow" target="_blank">Google images</a></figcaption></figure><p id="bee3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以上模型取 C 上下文单词。当<em class="km"> Wvn </em>用于计算隐藏层输入时，我们取所有这些 C 上下文单词输入的平均值。</p><p id="ae5e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我们已经看到了单词表示是如何使用上下文单词生成的。但是我们还有一种方法可以做到。我们可以使用目标单词(我们想要生成它的表示)来预测上下文，在这个过程中，我们产生表示。另一个变体，称为跳过克模型做这个。</p><p id="151e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">跳格模型:</strong></p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi kz"><img src="../Images/5293c993388b5dc14b9c5c778d0b4c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Ta3qx5CQsrJloyCA.png"/></div></div></figure><p id="12bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这看起来像是多上下文 CBOW 模型被翻转了。在某种程度上，这是真的。</p><p id="afb0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们把目标词输入网络。模型输出 C 个概率分布。这是什么意思？</p><p id="842d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于每个上下文位置，我们得到 V 个概率的 C 个概率分布，每个单词一个。</p><p id="4eb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这两种情况下，网络都使用反向传播来学习。详细的数学可以在这里找到<a class="ae kl" href="https://arxiv.org/pdf/1411.2738.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="e171" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">谁赢了？</p><p id="7bb5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">两者各有利弊。根据 Mikolov 的说法，Skip Gram 可以很好地处理少量数据，并被发现可以很好地表示罕见的单词。</p><p id="7a14" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一方面，CBOW 更快，对更频繁的单词有更好的表示。</p><p id="3b3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">前方有什么？</p><p id="da1f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的解释是一个非常基本的解释。它只是让您对什么是单词嵌入以及 Word2Vec 如何工作有一个高层次的概念。</p><p id="8fd4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还有很多事情要做。例如，为了提高算法的计算效率，使用了分层 Softmax 和 Skip-Gram 负采样等技巧。所有这些都可以在这里找到。</p><p id="2fd4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读！我已经开始了我的个人博客，我不打算在媒体上写更多令人惊叹的文章。订阅<a class="ae kl" href="https://thenlp.space/" rel="noopener ugc nofollow" target="_blank">的帮助空间</a>，支持我的博客</p></div></div>    
</body>
</html>