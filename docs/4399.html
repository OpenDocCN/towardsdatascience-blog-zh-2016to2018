<html>
<head>
<title>[ Paper Summary ] Horovod: fast and easy distributed deep learning in TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">【论文摘要】horo VOD:tensor flow 中快速简单的分布式深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-summary-horovod-fast-and-easy-distributed-deep-learning-in-tensorflow-5be535c748d1?source=collection_archive---------14-----------------------#2018-08-13">https://towardsdatascience.com/paper-summary-horovod-fast-and-easy-distributed-deep-learning-in-tensorflow-5be535c748d1?source=collection_archive---------14-----------------------#2018-08-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/4f3024cab1f84f519f7ed320053b5eb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/1*Tjb2HLBtAoTm6riHLyQKXA.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">GIF from this <a class="ae jy" href="https://giphy.com/gifs/channel-7Ub36ixBfRcsw" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="7cc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我想知道更多关于 Tensorflow 中的分布式学习，它是如何完成的以及背后的细节。</p><blockquote class="kx ky kz"><p id="0349" class="jz ka la kb b kc kd ke kf kg kh ki kj lb kl km kn lc kp kq kr ld kt ku kv kw ij bi translated"><strong class="kb ir">请注意，这篇帖子是给未来的自己看的，回顾这篇论文上的材料，而不是从头再看一遍。</strong></p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><figure class="ll lm ln lo gt jr"><div class="bz fp l di"><div class="lp lq l"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Paper from this <a class="ae jy" href="https://arxiv.org/pdf/1802.05799.pdf" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="8a56" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">摘要</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/7dd02fe9be5fbcc991d92eb5a721ea18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qyJKjTxzKG8aIuiyYVGJGQ.png"/></div></div></figure><p id="661c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">训练深度神经网络是一个占用大量计算内存的过程。如果我们可以使用多个图形处理器，那么训练速度会更快。然而，有两个问题，GPU 的内部通信和用户必须修改他们的代码，以兼容分布式训练。本文的作者介绍了 Horovod，它可以自动处理这些问题。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="7643" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">简介</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/a016bb2e86da1b544fa8e04980a70fd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vXrXvRMmmHKqnszGnryoYg.png"/></div></div></figure><p id="8147" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">近年来，深度学习已经在各地应用，以创造有意义的体验。优步在他们的许多产品中使用张量流。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="7168" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">走向分布式</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lx"><img src="../Images/7371d74dc1ae19137c2875f99500ef72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*etn0p3_se4JnuT2wr3cWhw.png"/></div></div></figure><p id="000a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">随着模型规模的增加，所需的计算成本也在增加。为了解决这个问题，优步转向了分布式培训，在尝试了标准的 tensorflow 分布式软件包后，他们显然需要做出一些调整。tensorflow 中的标准分布式库很难调试，扩展性也不好。(如上图所示)</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="9363" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">利用不同类型的算法</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ly"><img src="../Images/8ee14974e11cf7040f6fea7fb670c63f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qzkfF7qmPxUVVcu9jJuT9g.png"/></div></div></figure><p id="bb90" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">首先，他们尝试了脸书的方法，其中结合了数据并行的原则和创新的学习率调整技术。(如上图。)这种方法的一般方法如下所示。</p><div class="ll lm ln lo gt ab cb"><figure class="lz jr ma mb mc md me paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/7f5be9001deffdde0a5d811a30e4e990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*CjSrHj7vsLF_vCFCvAr3zA.png"/></div></figure><figure class="lz jr mf mb mc md me paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><img src="../Images/abffd81fe75ce905c1104cb1f6fcc206.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*cEHYzmBZBzi-0K-V2XRLvQ.png"/></div></figure></div><p id="a900" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在这种方法中，每个进程都有两个潜在的角色，要么成为工作节点(读取数据，并计算梯度)，要么成为参数服务器(平均梯度)。这种方法是可行的，但是它有两个潜在的问题，1)确定工人与参数服务器的正确比例，以及 2)处理增加的 TensorFlow 程序复杂性。在 2017 年初，一种称为 ring-allreduce 的算法对梯度进行平均，并将这些梯度传递给所有节点，(该方法如何运行的视觉效果可以在下面看到，详细信息请阅读论文。)已被提出。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mg"><img src="../Images/92425573ff0a2b2da12a19a3ec060a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zFb-6geYvT-b5kcI0xJVEg.png"/></div></div></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="42a8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">介绍 Horovod /通过 Horovod 分配您的培训工作</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mh"><img src="../Images/2487155c3e86ef48a562ca6de7cabc73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H5sc1t5Bs0RINllG_5C9Cg.png"/></div></div></figure><p id="d82c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">Horovod 这个名字来自传统的俄罗斯民间舞蹈，表演者手挽手围成一圈跳舞，类似于分布式训练的工作方式。并且本文作者已经实现了 tensorflow 版本的 ring-allreduce 算法，具体可以看下面。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mi"><img src="../Images/facf6ef72d515fdaffa6734807089aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*slG_6WZw_RLBB-I9_5qAtg.png"/></div></div></figure><p id="755e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">正如上图所示，使用 Horovod 非常简单，只需要几行额外的代码。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="f4bd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> Horovod 时间轴/张量融合</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mj"><img src="../Images/95c4e35e83a17197770cd78d223b3d62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QNAuuMaq68DcZD6ZjphDZw.png"/></div></div></figure><p id="e542" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在谷歌 chrome 的帮助下，每个用户都可以轻松地跟踪培训的进度，这可以准确地显示每个节点在整个培训工作的每个时间点都做了什么。</p><p id="5ed5" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">此外，作者注意到在训练时，有许多微小的缩减操作。如果张量足够大，ring-allreduce 可以以最佳方式利用网络，但如果张量非常小，则不能有效或快速地工作，因此作者提出了张量融合，一种将张量融合在一起的算法。通过这种方法，作者能够观察到 65%的改善。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mk"><img src="../Images/c6433b9733ecd10a9b4d81ed01100d6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8O4ZF-Kykk4D1GxjA-Z3YQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Step by Step of Tensor Fusion.</figcaption></figure></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="db54" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir"> Horovod 基准/后续步骤</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ml"><img src="../Images/9a40ab60ef826912f2d428bc1d0d244d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mQDouYKtKHvJp_oGeNKMOw.png"/></div></div></figure><p id="8f38" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">当作者比较标准库和 Horovod 之间的情况时，他们能够观察到性能的异常提高，如上所示。</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mm"><img src="../Images/f6d52360097bf565a972fa36c5294170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*po7jeJtY2F12YKZFr4wpqg.png"/></div></div></figure><p id="0a4e" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">使用远程直接内存访问(RDMA)而不是 TCP，作者还注意到性能的巨大提高。如果随着模型中参数数量的增加，它变得更加有效。最后，作者注意到了改进的空间，例如 1)使安装 MPI 变得更容易，2)收集和分享关于调整分布式深度学习的模型参数的学习，以及 3)添加非常大的模型的示例。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="eb0c" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">最后的话</strong></p><p id="23dd" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这是一个令人兴奋的消息，我希望有一天能够使用这个框架。</p><p id="2bd6" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果发现任何错误，请发电子邮件到 jae.duk.seo@gmail.com 给我，如果你希望看到我所有写作的列表，请<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">在这里查看我的网站</a>。</p><p id="ccc9" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">同时，在我的 twitter <a class="ae jy" href="https://twitter.com/JaeDukSeo" rel="noopener ugc nofollow" target="_blank">这里</a>关注我，并访问<a class="ae jy" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">我的网站</a>，或我的<a class="ae jy" href="https://www.youtube.com/c/JaeDukSeo" rel="noopener ugc nofollow" target="_blank"> Youtube 频道</a>了解更多内容。我也实现了<a class="ae jy" href="https://medium.com/@SeoJaeDuk/wide-residual-networks-with-interactive-code-5e190f8f25ec" rel="noopener">广残网，请点击这里查看博文 pos </a> t。</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="1501" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">参考</strong></p><ol class=""><li id="4b11" class="mn mo iq kb b kc kd kg kh kk mp ko mq ks mr kw ms mt mu mv bi translated">阿·谢尔盖耶夫和米·德尔·巴尔索(2018 年)。horo VOD:tensor flow 中快速简单的分布式深度学习。Arxiv.org。检索于 2018 年 8 月 13 日，来自<a class="ae jy" href="https://arxiv.org/abs/1802.05799" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1802.05799</a></li></ol></div></div>    
</body>
</html>