<html>
<head>
<title>The magic of LSTM neural networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTM神经网络的魔力</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-magic-of-lstm-neural-networks-1e4a1f7d81c0?source=collection_archive---------8-----------------------#2018-02-02">https://towardsdatascience.com/the-magic-of-lstm-neural-networks-1e4a1f7d81c0?source=collection_archive---------8-----------------------#2018-02-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/771222f368dc164d6ee210655eeea8b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*68FuTsfvDH-sOVTpBv5BKw.jpeg"/></div></div></figure><p id="0671" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank"> LSTM神经网络</a>，代表<strong class="kd iu">L</strong>ONG<strong class="kd iu">S</strong>short-<strong class="kd iu">T</strong>erm<strong class="kd iu">M</strong>emory，是一种特殊类型的递归神经网络，最近在机器学习社区得到了很多关注。</p><p id="f34c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">简单地说，LSTM网络有一些内部的<strong class="kd iu">上下文状态单元</strong>充当长期或短期记忆单元。<br/>LSTM网络的输出由这些单元的状态<strong class="kd iu">调制</strong>。当我们需要神经网络的预测依赖于输入的<strong class="kd iu">历史背景</strong>而不是仅仅依赖于最后的输入时，这是一个非常重要的特性。</p><p id="9b73" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">举个简单的例子，假设我们要预测以下序列的下一个数字:6 -&gt; 7 -&gt; 8 -&gt;？。我们希望下一个输出是<strong class="kd iu"> 9 </strong> (x+1)。但是，如果我们提供这个序列:2 - &gt; 4 - &gt; 8 - &gt;？，我们想得到<strong class="kd iu"> 16 </strong> (2x)。<br/>虽然在这两种情况下，当前的最后输入是数字<strong class="kd iu"> 8 </strong>，但是预测结果应该是不同的(当我们考虑先前值的上下文信息而不仅仅是最后一个时)。</p><p id="7d5a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">LSTM网络通过整合一个允许信息从一个步骤流向下一个步骤的<em class="la">回路</em>来管理保持输入的上下文信息。这些循环使得递归神经网络看起来很神奇。但是如果我们想一想，当你在读这篇文章的时候，你是在理解前面的单词的基础上理解每个单词的。你不会扔掉所有东西，从每一个单词开始思考。类似地，LSTM预测总是受到网络输入的过去经验的制约。</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="ab gu cl lf"><img src="../Images/ffba2a153afe05cba594d19fa94e3662.png" data-original-src="https://miro.medium.com/v2/format:webp/1*NKhwsOYNUT5xU7Pyf6Znhg.png"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk">LSTM loop unrolled</figcaption></figure><p id="5c90" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">另一方面，时间过得越久，下一个输出就越不可能依赖于一个非常旧的输入。这个时间依赖距离本身也是要学习的上下文信息。LSTM网络通过“忘记门”权重来学习何时记住何时忘记。简单地说，如果遗忘门只是0.9的乘法因子，在10个时间步长内，该因子变成:0。⁹ ⁰=0.348(或65%的信息被遗忘)，以及30步内- &gt; 0.04 (96%被遗忘)。</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="ab gu cl lf"><img src="../Images/81e79561626e23336b2c5ef6a28f3d3e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*MxcWRNyZLPBSgXQhieTyJg.png"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk">LSTM internal wiring showing the forget gate</figcaption></figure><p id="0d07" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这篇文章中，我不会过多地谈论LSTM的技术细节。这篇博文很好地解释了他们的机制。相反，我将在这里介绍LSTM的不同应用汇编。</p><p id="ea43" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">生成这样的文本可以转化为LSTM任务，其中每个字母的生成都要考虑所有先前生成的字母。事实上，<a class="ae kz" href="http://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/" rel="noopener ugc nofollow" target="_blank">教一个LSTM网络字母表</a>可以成为第一个学习如何让LSTM学习东西的编程例子。<a class="ae kz" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">安德烈·卡帕西</a>，展示了LSTM根据你输入的训练集生成文本的不同例子。以下是一些自动生成的文本:</p><p id="d9c9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">莎翁:</strong></p><pre class="lb lc ld le gt lk ll lm ln aw lo bi"><span id="9420" class="lp lq it ll b gy lr ls l lt lu">PANDARUS:<br/>Alas, I think he shall be come approached and the day<br/>When little srain would be attain’d into being never fed,<br/>And who is but a chain and subjects of his death,<br/>I should not sleep.</span><span id="74ff" class="lp lq it ll b gy lv ls l lt lu">Second Senator:<br/>They are away this miseries, produced upon my soul,<br/>Breaking and strongly should be buried, when I perish<br/>The earth and thoughts of many states.</span></pre><p id="aca2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">维基百科</strong>:</p><pre class="lb lc ld le gt lk ll lm ln aw lo bi"><span id="5ecb" class="lp lq it ll b gy lr ls l lt lu">Naturalism and decision for the majority of Arab countries’ capitalide was grounded<br/>by the Irish language by [[John Clair]], [[An Imperial Japanese Revolt]], associated<br/>with Guangzham’s sovereignty. His generals were the powerful ruler of the Portugal<br/>in the [[Protestant Immineners]], which could be said to be directly in Cantonese<br/>Communication, which followed a ceremony and set inspired prison, training. The<br/>emperor travelled back to [[Antioch, Perth, October 25|21]] to note, the Kingdom<br/>of Costa Rica, unsuccessful fashioned the [[Thrales]], [[Cynth’s Dajoard]], known<br/>in western [[Scotland]], near Italy to the conquest of India with the conflict.</span></pre><p id="67c9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> XML: </strong></p><pre class="lb lc ld le gt lk ll lm ln aw lo bi"><span id="c76d" class="lp lq it ll b gy lr ls l lt lu">&lt;revision&gt;<br/> &lt;id&gt;15900676&lt;/id&gt;<br/> &lt;timestamp&gt;2002–08–03T18:14:12Z&lt;/timestamp&gt;<br/> &lt;contributor&gt;<br/> &lt;username&gt;Paris&lt;/username&gt;<br/> &lt;id&gt;23&lt;/id&gt;<br/> &lt;/contributor&gt;<br/> &lt;minor /&gt;<br/> &lt;comment&gt;Automated conversion&lt;/comment&gt;<br/> &lt;/revision&gt;</span></pre><p id="70ff" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">奥巴马:</strong></p><pre class="lb lc ld le gt lk ll lm ln aw lo bi"><span id="9af3" class="lp lq it ll b gy lr ls l lt lu">SEED: War on terror<br/>Good everybody. Thank you very much. God bless the United States of America, and has already began with the world’s gathering their health insurance.<br/>It’s about hard-earned for our efforts that are not continued.<br/>We are all the assumptionion to the streets of the Americas that we are still for everybody and destruction.<br/>We are doing a lot of this.<br/>I know that someone would be prefered to their children to take a million insurance company. We’re watching their people and continued to find ourselves with Republicans — to give up on these challenges and despite the challenges of our country. In the last two years, we must recognise that our borders have access from the world. We’re continuing that this day of things that the United States will clean up it´s allies and prosperity to stand up enough to be a sanctions that we made their faith, and for the country of the Internet to Osama bin Laden.<br/>Thank you. God bless you. Good morning, everybody. And May God loss man. Thank you very much. Thank you very much, everybody.</span></pre><p id="c4c7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这是Alex Graves制作的一个动画，展示了一个LSTM网络在现场进行手写识别:</p><ul class=""><li id="a246" class="lw lx it kd b ke kf ki kj km ly kq lz ku ma ky mb mc md me bi translated">第1行:显示被识别的字母(网络的<strong class="kd iu">输出</strong></li><li id="dc56" class="lw lx it kd b ke mf ki mg km mh kq mi ku mj ky mb mc md me bi translated">第2行:显示存储单元的状态(注意当一个字符被识别时它们是如何复位的)</li><li id="d366" class="lw lx it kd b ke mf ki mg km mh kq mi ku mj ky mb mc md me bi translated">第3行:显示了正在被LSTM分析的书写内容(网络的<strong class="kd iu">输入</strong></li><li id="4df9" class="lw lx it kd b ke mf ki mg km mh kq mi ku mj ky mb mc md me bi translated">第4行:显示从最活跃的字符反向传播到输入的梯度。这体现了<strong class="kd iu">忘记</strong>的效果。</li></ul><p id="9763" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">作为一个反向实验，这里有一些由LSTM生成的笔迹。</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="ab gu cl lf"><img src="../Images/a973949703aa3d0ea269f04772b6ee71.png" data-original-src="https://miro.medium.com/v2/format:webp/1*hrbOCSHaCFGrQkc8A1i3Sg.jpeg"/></div></figure><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="ab gu cl lf"><img src="../Images/8a237ed3bc85ec350bb982096e9417f7.png" data-original-src="https://miro.medium.com/v2/format:webp/1*6YwqrScyczEaG0l05G4-_A.jpeg"/></div></figure><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="ab gu cl lf"><img src="../Images/7525ee21b456fe319f97cee55817fe86.png" data-original-src="https://miro.medium.com/v2/format:webp/1*uNDIdhlXi1t0vWvROtslng.jpeg"/></div></figure><p id="d01a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于现场演示，并自动生成一个LSTM-'手'写文本自己，访问<a class="ae kz" href="http://www.cs.toronto.edu/~graves/handwriting.html" rel="noopener ugc nofollow" target="_blank">这一页</a>。</p><p id="edec" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因为音乐，就像文本一样，是一系列音符(而不是字符)，它也可以由LSTM通过考虑先前演奏的音符(或音符的组合)来生成。在这里你可以找到一个关于如何在midi文件上训练LSTM的有趣解释。否则，您可以欣赏以下生成的音乐(来自古典音乐训练集):</p><p id="a3a4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">语言翻译可以看作是<a class="ae kz" href="https://www.tensorflow.org/tutorials/seq2seq" rel="noopener ugc nofollow" target="_blank">序列到序列</a>的映射。一组研究人员与英伟达合作，公布了如何驯服LSTM完成这项任务的细节(<a class="ae kz" href="https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/" rel="noopener ugc nofollow" target="_blank">第一部分</a>、<a class="ae kz" href="https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/" rel="noopener ugc nofollow" target="_blank">第二部分</a>、<a class="ae kz" href="https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/" rel="noopener ugc nofollow" target="_blank">第三部分</a>)。<br/>简而言之，他们创建了一个神经网络，其中一个编码器将文本压缩为更高抽象的矢量表示，一个解码器将其解码回目标语言。</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="ab gu cl lf"><img src="../Images/3f58043d7e124d7c34a84ebb2515e9eb.png" data-original-src="https://miro.medium.com/v2/format:webp/1*JoG7WmQOSgjscLtOhB_H5A.png"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk">Machine translation encoder/decoder architecture</figcaption></figure><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="ab gu cl lf"><img src="../Images/622e449615ed6f652b942774fad2e9ae.png" data-original-src="https://miro.medium.com/v2/format:webp/1*SiOIKWyN-D3viccI8RmaNg.png"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk">English to french translation by NVidia</figcaption></figure><p id="4492" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最后，LSTM网络最令人印象深刻的用途是从输入图像生成描述图像内容的文本标题。微软研究院在这方面取得了很大进展。以下是一些他们的结果的示例演示:</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="ab gu cl lf"><img src="../Images/ea6a6754593fa8c53e9629a6db7b09f9.png" data-original-src="https://miro.medium.com/v2/format:webp/1*r2fC_4ahhW-CEiCXXgHv4A.png"/></div></figure><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="ab gu cl lf"><img src="../Images/35285a7391d621f97178e10897bbd1d4.png" data-original-src="https://miro.medium.com/v2/format:webp/1*7-3Mb2E96ul52R3uDxZlXg.png"/></div></figure><p id="dbc5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你可以在这里自己尝试一下他们的在线演示:<a class="ae kz" href="https://www.captionbot.ai" rel="noopener ugc nofollow" target="_blank">https://www . caption bot . ai</a></p><p id="4056" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">玩得开心！</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="c79b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="la">原载于2018年2月2日</em><a class="ae kz" href="https://medium.com/datathings/the-magic-of-lstm-neural-networks-6775e8b540cd" rel="noopener"><em class="la">medium.com</em></a><em class="la">。</em></p></div></div>    
</body>
</html>