<html>
<head>
<title>First Thoughts on Kaggle</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于卡格尔的初步想法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/first-thoughts-on-kaggle-326a6c4dc005?source=collection_archive---------6-----------------------#2017-07-12">https://towardsdatascience.com/first-thoughts-on-kaggle-326a6c4dc005?source=collection_archive---------6-----------------------#2017-07-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="78e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大约两周前，我报名参加了我的第一个Kaggle竞赛，即<a class="ae kl" href="https://www.kaggle.com/c/mercedes-benz-greener-manufacturing" rel="noopener ugc nofollow" target="_blank">梅赛德斯-奔驰绿色制造竞赛</a>。我的代码可以在<a class="ae kl" href="https://github.com/yangalexandery/kaggle-mercedes-benz" rel="noopener ugc nofollow" target="_blank">这里</a>找到，我尝试的日志可以在<a class="ae kl" href="https://medium.com/@alex_yang/first-attempts-at-kaggle-bf6f8e778357" rel="noopener">这里</a>找到。以下是我对Kaggle的第一印象:</p><h2 id="fcd7" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">学习曲线</h2><p id="cb72" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">进入大赛，我唯一的机器学习知识来自麻省理工的入门课(6.036)。因此，我惊喜地发现，卡格尔竞赛对那些之前经验很少的人来说是完全可以参加的。这很大程度上是由于Kaggle的内核，它允许更有经验的Kaggle用户与其他人公开分享他们的代码。<strong class="jp ir">内核甚至允许那些完全不熟悉机器学习的人在排名中具有竞争力— </strong>通过简单地复制代码，任何人都可以取得与Kaggle老手不相上下的结果。</p><p id="9dbf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我发现，在我尝试改进内核代码的过程中，我收到了关于机器学习中多个概念的简要介绍，其中许多概念我希望在未来写得更透彻:</p><ul class=""><li id="2be9" class="lk ll iq jp b jq jr ju jv jy lm kc ln kg lo kk lp lq lr ls bi translated">梯度增强树</li><li id="8741" class="lk ll iq jp b jq lt ju lu jy lv kc lw kg lx kk lp lq lr ls bi translated">超参数调谐</li><li id="ba68" class="lk ll iq jp b jq lt ju lu jy lv kc lw kg lx kk lp lq lr ls bi translated">降维:主成分分析，独立分量分析，tSNE，逻辑主成分分析，TSVD，GRP，SRP</li><li id="45b7" class="lk ll iq jp b jq lt ju lu jy lv kc lw kg lx kk lp lq lr ls bi translated">过度拟合、K倍交叉验证、非折叠预测</li><li id="3512" class="lk ll iq jp b jq lt ju lu jy lv kc lw kg lx kk lp lq lr ls bi translated">集合、堆叠和平均</li><li id="801e" class="lk ll iq jp b jq lt ju lu jy lv kc lw kg lx kk lp lq lr ls bi translated">Sklearn型号:套索、ElasticNet等。</li><li id="a0f6" class="lk ll iq jp b jq lt ju lu jy lv kc lw kg lx kk lp lq lr ls bi translated">基本特征选择和特征工程</li><li id="16a2" class="lk ll iq jp b jq lt ju lu jy lv kc lw kg lx kk lp lq lr ls bi translated">可能性编码(赛后)</li></ul><p id="14d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我看来，这个比赛是一个快速“边做边学”的好方法。很难在网上找到任何其他像Kaggle这样有助于学习数据科学概念的资源。</p><h2 id="ada3" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">排行榜的不可预测性</h2><p id="df37" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">从最终排名的<a class="ae kl" href="https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/leaderboard" rel="noopener ugc nofollow" target="_blank">大规模调整</a>可以看出，公共排行榜对于预测私人排行榜来说完全不可靠:几乎所有在比赛中领先的人最终都下降了数百个排名。然而，即使是交叉验证也被证明是无用的:我的最终模型，用5重CV评估，并不比我在第四天制作的严重过度拟合的XGBoost模型表现得更好。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/e427ca29da4a25fcb45d42b88a8494e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Ej4DbJq4x-rYjiHemQsRLg.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Public and Private LB scores for my 36 submissions — a few past models made it over the bronze cutoff!</figcaption></figure><p id="1fa8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，事实证明有<a class="ae kl" href="https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion/36136" rel="noopener ugc nofollow" target="_blank">可靠的方法</a>来测试一个模型——尽管在大多数情况下，参赛者(包括我)对评估他们模型的表现不够彻底。</p><h2 id="87f2" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">Kaggle的竞赛社区</h2><p id="6ac3" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">对于Kaggle的社区，我只有积极的话要说。用户提交的关于比赛的内核和线程大大鼓励了参赛者之间的合作。在比赛期间，许多用户在公开论坛上一起工作，改进彼此的模型并讨论数据集的属性。本次比赛是参赛者共同努力的众多成果之一，通过排行榜探测，从测试数据集中发现了32个y值。</p><h2 id="f59e" class="km kn iq bd ko kp kq dn kr ks kt dp ku jy kv kw kx kc ky kz la kg lb lc ld le bi translated">结论</h2><p id="fbd1" class="pw-post-body-paragraph jn jo iq jp b jq lf js jt ju lg jw jx jy lh ka kb kc li ke kf kg lj ki kj kk ij bi translated">我发现参加这次比赛非常愉快！虽然我的最终排名(~ 1400名)有点令人失望，但比赛非常有趣，让我学到了很多东西，我计划将来在Kaggle上更加活跃。如果您有任何反馈，请告诉我——也许更多关于Kaggle的报道将很快出现。:)</p></div></div>    
</body>
</html>