<html>
<head>
<title>Finding the Gradient of a Vector Function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">求向量函数的梯度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-d002440227fb?source=collection_archive---------2-----------------------#2018-10-20">https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-d002440227fb?source=collection_archive---------2-----------------------#2018-10-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="16ff" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一步一步:神经网络背后的数学</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ad912818cf03e1cd4528409ef67e7aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jNzYlfISJ8hDdsAqLzO03g.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Title image: <a class="ae kv" href="https://pixabay.com/en/matrix-communication-software-pc-2953869/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="e139" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<a class="ae kv" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9">第一部分</a>中，我们被给了一个问题:计算这个损失函数的梯度:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/2a6a7e17495d3b8a7ea7ea8d431ebf73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qb-m0gLn8X-aWnrJMqiNhA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 1: Loss function</figcaption></figure><p id="9c59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了求梯度，我们必须求函数的导数。在<a class="ae kv" href="https://medium.com/@reina.wang/step-by-step-the-math-behind-neural-networks-ac15e178bbd" rel="noopener">第二部分</a>中，我们学习了如何计算函数对每个变量的偏导数。然而，这个损失函数中的大部分变量都是向量。当神经网络处理大量数据时，能够找到向量变量的偏导数尤其重要。向量和矩阵运算是一种简单的方法来表示有这么多数据的运算。确切地说，你怎样才能找到一个向量函数的梯度呢？</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="3ff8" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">标量函数的梯度</h2><p id="c6e9" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">假设我们有一个函数，<em class="my"> f(x，y) = 3x y </em>。我们的偏导数是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/72988280f47494e29f9740f350e8d424.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*6y4KzfIE7LGoStKnzgqN9Q.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 2: Partial derivatives</figcaption></figure><p id="5d8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们把这些分音组织成一个水平向量，我们得到<em class="my"> f(x，y) </em>的<strong class="ky ir">梯度</strong>，或者<em class="my"> ∇ f(x，y) </em>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/30d769f4029f187aa3ab1ae96d7605e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GQzXwr2z_rlxaDiRQdstyQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 3: Gradient of f(x,y)</figcaption></figure><p id="c4e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="my"> 6yx </em>是<em class="my"> f(x，y) </em>相对于<em class="my"> x </em>的变化，而<em class="my"> 3x </em>是<em class="my"> f(x，y) </em>相对于<em class="my"> y </em>的变化。</p><p id="63cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们有两个函数时会发生什么？让我们添加另一个函数，<em class="my"> g(x，y) = 2x+y⁸ </em>。偏导数是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/6084da363627541f42c98eda7094cfa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*3finunH3mDDKToAqeLLeuw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 4: Partials for g(x,y)</figcaption></figure><p id="4d02" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以 g(x，y)的梯度是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/5b8e67ff5e8afbc7f4e1b61928515d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yuUNxw9JHzLEUGpp9zUgEQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 5: Gradient of g(x,y)</figcaption></figure></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="ce5c" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">表示函数</h2><p id="7437" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">当我们有一个带有多个参数的函数时，用一种更简单的方式来表示它们通常是有用的。我们可以将函数的多个参数组合成一个向量自变量，<strong class="ky ir"> x </strong>，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/fba7fe1930b812123d1863ca9d168b1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*t8BCoLs37tMYj_Su3CVbuw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 6: Vector <strong class="bd ne">x</strong></figcaption></figure><p id="d1b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，<em class="my"> f(x，y，z) </em>会变成<em class="my"> f(x₁,x₂,x₃) </em>从而变成<em class="my"> f( </em> <strong class="ky ir"> x </strong> <em class="my">)。</em></p><p id="1458" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们也可以将多个函数组合成一个向量，就像这样:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/4ef7cb10b436184a4888dbb77e353d60.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*FASQPuQjdcKuxyQSDaB1TQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 7: Vector <strong class="bd ne">y</strong></figcaption></figure><p id="8ebd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，<strong class="ky ir"> y=f(x) </strong>其中<strong class="ky ir"> f(x) </strong>是【f₁( <strong class="ky ir"> x </strong>、f₂( <strong class="ky ir"> x </strong>、f₃( <strong class="ky ir"> x </strong> )…fn( <strong class="ky ir"> x </strong>)】的向量</p><p id="f5f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于我们前面的两个函数的例子，<em class="my"> f(x，y)f(</em><strong class="ky ir">x</strong><em class="my">)</em>和<em class="my"> g(x，y)g(</em><strong class="ky ir">x</strong><em class="my">)。</em>此处，向量<strong class="ky ir">x</strong>=【x₁，x₂】，其中<em class="my"> x₁=x </em>，以及<em class="my"> x₂=y </em>。为了进一步简化，我们可以将我们的函数组合起来:[f( <strong class="ky ir"> x </strong>)、g( <strong class="ky ir"> x </strong> )] = [f₁( <strong class="ky ir"> x </strong>)、f₂(<strong class="ky ir">x</strong>)]=<strong class="ky ir">f(x)= y .</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/db11b4f32183523c9b912fe4d7769070.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*jYIV37fXa8NO4mR7T1icAg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 8: Equations within vector function <strong class="bd ne">y</strong></figcaption></figure><p id="89a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通常，函数的数量和变量的数量是相同的，因此每个变量都有一个解。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="8dc6" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">向量函数的梯度</h2><p id="b058" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">现在有了两个函数，怎么求两个函数的梯度呢？如果我们把它们的梯度组织成一个单一的矩阵，我们就从向量演算进入了矩阵演算。这个矩阵以及具有多个变量的多个函数的梯度的组织被称为<strong class="ky ir">雅可比矩阵</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/0268a39b589017ffc1a8ef6d3d4fd34a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*orpueIzJ5nMP6OyMC16WKw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 9: The Jacobian</figcaption></figure><p id="d642" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有多种方法来表示雅可比矩阵。这种布局，我们垂直堆叠渐变，被称为<strong class="ky ir">分子布局</strong>，但其他论文将使用<strong class="ky ir">分母布局</strong>，它只是对角翻转:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/46157508cb0af5846d7f90901bad6326.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*n7KxMlRpxborHJLQ4_Q_Ug.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 10: Denominator layout of the Jacobian</figcaption></figure></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="7669" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">恒等函数的梯度</h2><p id="3a79" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">我们取恒等式函数，<strong class="ky ir"> y = f(x) = x </strong>，其中<em class="my">fi(</em><strong class="ky ir">x</strong><em class="my">)= Xi</em>，求其梯度:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/6ff984b7712a75ecf12efbcba6f1095e.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*fXkgcg6lQJwoH4o9flk61A.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 11: Identity function</figcaption></figure><p id="5a8d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们创建之前的雅可比矩阵一样，我们可以找到每个标量函数的梯度，并将它们垂直堆叠起来，以创建恒等函数的雅可比矩阵:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/70727ad55bb1a950b15c34081dbb1078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TIx0x4sPBdpRjDn_bLba6Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 12: Jacobian of the identity function</figcaption></figure><p id="d416" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然是恒等函数，f₁( <strong class="ky ir"> x </strong> ) = x₁，f₂( <strong class="ky ir"> x </strong> ) = x₂，以此类推。因此，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/7ef8f650935b956cde81a6856d27de4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TgvIIfexl4vkIV5aRXq5vA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 13: Jacobian of the identity function</figcaption></figure><p id="7fe2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">函数对不在函数中的变量的偏导数为零。比如 2x 对 y 的偏导数是 0。换句话说，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/d14d6ceb598c4d466ea107d6a06ddfab.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*Q_WMa0-Ena6xEGjuDl1ktw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 14: The partial derivative of a function with respect to a variable that’s not in the function is zero</figcaption></figure><p id="17f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，不在雅可比对角线上的一切都变为零。同时，任何变量对自身的偏导数都是 1。例如，<em class="my"> x </em>相对于<em class="my"> x </em>的偏导数为 1。因此，雅可比就变成了:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/03d862e087795d43c5e6b039ac345e97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k2iadb4fzOPHOvzdxZWhlw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 15: Jacobian of the identity function</figcaption></figure></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="e4f6" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">元素式向量函数组合的梯度</h2><p id="8508" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated"><strong class="ky ir">基于元素的二元运算符</strong>是连续应用运算符的运算(如加法<strong class="ky ir"> w </strong> + <strong class="ky ir"> x </strong>或<strong class="ky ir"> w </strong> &gt; <strong class="ky ir"> x </strong>，从两个向量的第一项开始获得第一项输出，然后从两个向量的第二项获得第二项输出……依此类推。</p><p id="9d0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文用以下符号表示元素式二元运算:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ce3a540742414754ac500c6bf6b48f35.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*uFqj45Sgg0Ksg5585hdyvQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 16: Element-wise binary operation with f(x) and g(x)</figcaption></figure><p id="2e21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，◯指的是任何元素操作符(比如+)，而不是函数的组合。</p><p id="c65b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么如何找到两个向量的元素运算的梯度呢？</p><p id="ce85" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们有两组函数，我们需要两个雅可比矩阵，一个表示相对于<strong class="ky ir"> x </strong>的梯度，一个表示相对于<strong class="ky ir"> w </strong>的梯度:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/20623afae8e85bff76eee0d8461f71f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dPR77IzJGqi3-iN1UlPMTQ.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/1da1ad821a21264f46c8aa84edf8e022.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HfEFVWfghglQJeqEdY0XPQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 17: Jacobian with respect to <strong class="bd ne">w</strong> and <strong class="bd ne">x</strong></figcaption></figure><p id="4c24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要的大多数算术运算都很简单，所以<strong class="ky ir"> f(w) </strong>通常就是向量<strong class="ky ir"> w </strong>。换句话说，<em class="my"> fi(wi) = wi </em>。例如，操作<strong class="ky ir"> w+x </strong>符合这一类别，因为它可以表示为<strong class="ky ir"> f(w)+g(x) </strong>，其中<em class="my"> fi(wi) + gi(xi) = wi +xi。</em></p><p id="5f6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，两个雅可比矩阵中的每个元素都简化为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/cc6e1511cd058dbc63376eb5c6be5386.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5jEJ2RCEyIcDqHNAYAkJ-Q.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/2692ec647e3e495d9f7197bf99f25419.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*agU3vpCQEMu9C7T3JQYsIg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 18: Elements in the Jacobian</figcaption></figure><p id="9ad8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在对角线上，i=j，所以偏导数存在一个值。然而，离开对角线，i≠j，所以偏导数变为零:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/485dc121a9d02f3527837d67016227b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qx2Jf96tKcO8lfYgdlTQ1w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 19: Diagonal jacobian</figcaption></figure><p id="e4e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以更简洁地表示为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/a5cd6ccde8464411f09da38b1f05e3f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jp9UvrvxoXTfx_G88o9ZuQ.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/dd46fe2337a7082e31785c47b2d1b6d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tYHMJDPLnozhFJvIa9GmnA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 20: The Jacobian with respect to <strong class="bd ne">w</strong> and <strong class="bd ne">x</strong></figcaption></figure><p id="40d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们试着求函数<strong class="ky ir"> w+x </strong>的梯度。我们知道对角线以外的一切都是 0。对角线上相对于<strong class="ky ir"> w </strong>和<strong class="ky ir"> x </strong>的分数值为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/a0885a07ce30ab448130b42c0475130a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N5Gsv1-Mw2wpeJy5n08B7Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 21: Partials with respect to <strong class="bd ne">w </strong>and<strong class="bd ne"> x</strong></figcaption></figure><p id="c57a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以两个雅可比行列式的对角线都是 1。这看起来很熟悉…这是单位矩阵！</p><p id="f26f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们用乘法来试试:<strong class="ky ir"> w*x </strong>。对角线上相对于<strong class="ky ir"> w </strong>和<strong class="ky ir"> x </strong>的分数值为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/dc0372c910a37ca765f16965f08a7f88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*74a7eA8lp-bYgFgXwmMdyw.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/d9eaa9f36096482e6c175d5f60661c68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8k3Oe2Jjmr_0uuG4ZY2Evw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 22: Partials with respect to <strong class="bd ne">w</strong> and <strong class="bd ne">x</strong></figcaption></figure><p id="da0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，<strong class="ky ir"> w*x </strong>相对于<strong class="ky ir"> w </strong>的梯度为<em class="my">diag(</em><strong class="ky ir">x</strong><em class="my">)</em>，而<strong class="ky ir"> w*x </strong>相对于<strong class="ky ir"> x </strong>的梯度为<em class="my"> diag( </em> <strong class="ky ir"> w </strong> <em class="my">)。</em></p><p id="5c3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用同样的步骤做减法和除法，我们可以总结出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/debf87c06b23675227068300405ac48c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*xCVZVGToQwmD5XLIavg4Kw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 23: Gradients of common element-wise binary operations</figcaption></figure></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="2065" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">向量和的梯度</h2><p id="552c" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">深度学习中最常见的运算之一就是求和运算。如何求函数<em class="my">y = sum(</em><strong class="ky ir">x</strong><em class="my">)</em>的梯度？</p><p id="266f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="my">y = sum(</em><strong class="ky ir">)x</strong><em class="my">)</em>也可以表示为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/606502146517f1fe21edbf28ea2ff551.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*jnL1rbSZJYfWt8XbiluKlw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 24: y=sum(<strong class="bd ne">x</strong>)</figcaption></figure><p id="add8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，梯度可以表示为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/349c94b56d6e8495dc9ecd2f7eae71b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*sJKa2mB13D4aae8kzUi4JA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 25: Gradient of y=sum(<strong class="bd ne">x</strong>)</figcaption></figure><p id="305f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为函数对不在函数中的变量的偏导数为零，所以可以进一步简化为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/867b40e7923fe020aabeb6d1766e6ba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*FUfX5ptvqu6bdrfxpMA2jA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 26: Gradient of y=sum(<strong class="bd ne">x</strong>)</figcaption></figure><p id="acc7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，结果是一个水平向量。</p><p id="1528" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="my">y = sum(</em><strong class="ky ir">x</strong><em class="my">z)</em>的梯度呢？唯一的区别是我们用常数 z 乘以每个偏导数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/864e33c4147444a945b99f7522257b98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*gpPxyab-opjyh9z3VpxWZg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 27: Gradient of y=sum(<strong class="bd ne">x</strong>z) with respect to <strong class="bd ne">x</strong></figcaption></figure><p id="bee4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然这是相对于<strong class="ky ir"> x </strong>的导数，但是相对于标量<em class="my"> z </em>的导数仅仅是一个数字:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/27c814e832d765a7fe0a32bc00f18c48.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*6Z9KZGAEiNZHVKpSr59N7Q.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 28: Gradient of y=sum(<strong class="bd ne">x</strong>z) with respect to z</figcaption></figure></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="cf7d" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">链式法则向量函数组合的梯度</h2><p id="b543" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">在<a class="ae kv" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-ac15e178bbd">第二部分</a>中，我们学习了多变量链式法则。然而，这只对标量有效。让我们看看如何将它整合到矢量计算中！</p><p id="cd8b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们取一个向量函数，<strong class="ky ir"> y </strong> = <strong class="ky ir"> f </strong> <em class="my"> (x) </em>，求它的梯度。让我们将该函数定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/2887a076287d0735fd2a47bec0970be9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*7KNDmR23jfAToYfRW7smmQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 29: <strong class="bd ne">y</strong> = <strong class="bd ne">f</strong>(x)</figcaption></figure><p id="7c01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="my"> f₁(x) </em>和<em class="my"> f₂(x) </em>都是复合函数。让我们为<em class="my"> f₁(x) </em>和<em class="my"> f₂(x) </em>引入中间变量，并重写我们的函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/70f6ea14984d29bc11d034c8d21c28c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*IRXpVyevjsjtjCXN8QIBDg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 30: <strong class="bd ne">y</strong> = <strong class="bd ne">f</strong>(<strong class="bd ne">g</strong>(x))</figcaption></figure><p id="7188" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以使用多变量链式法则来计算矢量<strong class="ky ir"> y </strong>的导数。简单计算<em class="my"> f₁(x) </em>和<em class="my"> f₂(x) </em>的导数，并将它们一个放在另一个上面:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/e1bf6beac4a94f968846883a14ba64b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pqss8u5shhJNnTZmAaGsqg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 31: Gradient of <strong class="bd ne">y</strong> = <strong class="bd ne">f</strong>(<strong class="bd ne">g</strong>(x))</figcaption></figure><p id="6233" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">瞧啊。我们有梯度。然而，我们已经用标量规则得出了我们的解决方案，仅仅是将数字组合成一个向量。有没有办法表示向量的多变量链式法则？</p><p id="ec3a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们的梯度计算如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/99af596ca1eff760339c1a7edfbe6bd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*bWl1RAsyGPndYZy5lEspwA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 32: Gradient of <strong class="bd ne">y</strong> = <strong class="bd ne">f</strong>(<strong class="bd ne">g</strong>(x))</figcaption></figure><p id="2482" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，<em class="my"> f₁(x) </em>和<em class="my"> f₂(x) </em>的梯度的第一项都包括<em class="my"> g₁ </em>对<em class="my"> x </em>的部分，而<em class="my"> f₁(x) </em>和<em class="my"> f₂(x) </em>的梯度的第二项都包括<em class="my"> g₂ </em>对<em class="my"> x </em>的部分。这就跟矩阵乘法一样！因此，我们可以将其表示为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/caad4e26f449e9895cc070fccc7ca768.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*QBS8OmFor721K56gDFVccw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 33: Vector representation of the gradient of <strong class="bd ne">y</strong> = <strong class="bd ne">f</strong>(<strong class="bd ne">g</strong>(x))</figcaption></figure><p id="d58f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们测试一下向量链规则的新表示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/69aea2ecb2a43a24965f59ba6f74749a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*Ii7YstM69tpjl8SzrGuqnw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 34: Vector chain rule</figcaption></figure><p id="7f27" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们得到与标量方法相同的答案！如果我们有一个向量参数<strong class="ky ir"> x </strong>而不是单个参数<em class="my"> x </em>，我们只需要稍微改变一下规则就可以得到完整的向量链规则:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/57628a0d80804769e57bc8d9f6796667.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*1Mik5JSVIALJq4oEe1-z8A.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 35: Vector chain rule</figcaption></figure><p id="d103" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/46787aae6b424b1775c18bff72d1abe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*roLEuJ0Lz3DlKvhYZxWZoQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 36: Vector chain rule</figcaption></figure><p id="842b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们上面的例子中，<em class="my"> f </em>纯粹是<em class="my"> g </em>的函数；即<em class="my"> fi </em>是<em class="my"> gi </em>而不是<em class="my"> gj </em>的函数(每个函数<em class="my"> f </em>恰好匹配 1 个函数<em class="my"> g </em> ) <em class="my">)。</em>在这种情况下，对角线以外的一切都变为零，并且:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/b89f3eb6f226264b5268e9c2b246033f.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*iMuLsTSbVg7WYIRYQCv31g.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 37: Special case of vector chain rule</figcaption></figure></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="03c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们有了我们开始系列时找到的神经网络梯度的所有部分:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/783a1a4c34359336b611cccaa8c35e88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qKN80Wu9Iw65KZJv66Cr8Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 38: Cost function</figcaption></figure><p id="e72f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查看<a class="ae kv" rel="noopener" target="_blank" href="/calculating-gradient-descent-manually-6d9bee09aa0b">第 4 部分</a>了解如何计算其导数！</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="541b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您还没有，请阅读第 1 和第 2 部分:</p><ul class=""><li id="06f9" class="ol om iq ky b kz la lc ld lf on lj oo ln op lr oq or os ot bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9">第 1 部分:简介</a></li><li id="161e" class="ol om iq ky b kz ou lc ov lf ow lj ox ln oy lr oq or os ot bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/step-by-step-the-math-behind-neural-networks-ac15e178bbd">第二部分:偏导数</a></li></ul><p id="aa16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">大结局请看<a class="ae kv" rel="noopener" target="_blank" href="/calculating-gradient-descent-manually-6d9bee09aa0b">第四部</a>！</p><p id="2076" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">点击下载<a class="ae kv" href="https://arxiv.org/abs/1802.01528" rel="noopener ugc nofollow" target="_blank">原文。</a></p><p id="606c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你喜欢这篇文章，别忘了留下一些掌声！如果您有任何问题或建议，请在下面留下您的评论:)</p></div></div>    
</body>
</html>