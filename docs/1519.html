<html>
<head>
<title>The Netrrobility is a Newsigation of Exactual¹</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可靠性是精确的新定义</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-netrrobility-is-a-newsigation-of-exactual-%C2%B9-c7a62010b6af?source=collection_archive---------11-----------------------#2017-09-13">https://towardsdatascience.com/the-netrrobility-is-a-newsigation-of-exactual-%C2%B9-c7a62010b6af?source=collection_archive---------11-----------------------#2017-09-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3338" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个机器人写了这个标题。它是由递归神经网络生成的。我用互联网档案馆的回溯机抓取了Medium的<a class="ae kl" href="https://medium.com/topic/popular" rel="noopener">头条页面</a>上过去一个月所有热门内容<a class="ae kl" href="https://web.archive.org/web/*/https://medium.com/topic/popular" rel="noopener ugc nofollow">的标题。关于WaybackMachine的API的更多信息可以在</a><a class="ae kl" href="https://archive.org/help/wayback_api.php" rel="noopener ugc nofollow" target="_blank">这里</a>找到，我收集媒体标题的Python代码<a class="ae kl" href="https://gist.github.com/allisonmorgan/9c0744edaa02fecef27286e49c4cae21" rel="noopener ugc nofollow" target="_blank">这里</a>找到，生成标题的完整列表<a class="ae kl" href="https://gist.github.com/allisonmorgan/696bd8590853b89af1df14dbaaff6728" rel="noopener ugc nofollow" target="_blank">这里</a>找到。(手指交叉这篇博文迅速走红🤞)</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="kr ks l"/></div></figure><p id="0e34" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">受<a class="ae kl" href="https://qz.com/1025049/a-computer-used-artificial-intelligence-to-create-new-dog-names/" rel="noopener ugc nofollow" target="_blank">一些</a> <a class="ae kl" href="https://arstechnica.com/information-technology/2017/05/an-ai-invented-a-bunch-of-new-paint-colors-that-are-hilariously-wrong/" rel="noopener ugc nofollow" target="_blank">伟大的</a> <a class="ae kl" href="http://www.slate.com/blogs/future_tense/2017/05/09/an_a_i_created_new_dungeons_and_dragons_spells.html" rel="noopener ugc nofollow" target="_blank">例子</a>的启发，我开始自己学习如何做这件事。这篇博客文章将详细介绍所需的软件，提供如何使用代码的说明，并展示一个使用来自<a class="ae kl" href="http://dblp.uni-trier.de" rel="noopener ugc nofollow" target="_blank"> DBLP </a>的计算机科学出版物标题的例子，这是一个在线计算机科学参考书目。TL；DR —跳到“有趣的东西”来检查生成的标题。</p><h1 id="e6c0" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">本质的</h1><p id="bf38" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">如果你想在家里尝试这个，你需要安装<a class="ae kl" href="https://github.com/jcjohnson/torch-rnn" rel="noopener ugc nofollow" target="_blank"> torch-rnn </a>(改编自<a class="ae kl" href="https://github.com/karpathy/char-rnn" rel="noopener ugc nofollow" target="_blank"> char-rnn </a>)。设置可能需要一些工作。我强烈推荐Jeffrey Thompson的<a class="ae kl" href="https://www.jeffreythompson.org/blog/2016/03/25/torch-rnn-mac-install/" rel="noopener ugc nofollow" target="_blank">教程</a>在Mac上安装。torch-rnn和char-rnn都实现了<a class="ae kl" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归神经网络</a> (RNNs)，给定训练数据(一组字符序列)学习生成文本，一个字符接一个字符。安德烈·卡帕西在这里有一篇非常详细的博客文章。</p><p id="8e3e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在设置好torch-rnn(上面教程中的步骤1-5)之后，我从DBLP出版物索引下载了最新的dblp.xml.gz文件。计算机科学出版物的这个数据集将用于训练我们的rnn。DBLP将其数据打包成一个巨大的XML文件。因为我们只需要这些出版物的原始标题，所以我们需要做一些数据预处理来抓取相关的标题。</p><p id="6081" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，我将从三个特定的计算机科学会议中选择会议录。通过这种方式，我们可以跨研究主题比较和对比生成的标题。我选择了<a class="ae kl" href="https://en.wikipedia.org/wiki/SIGKDD" rel="noopener ugc nofollow" target="_blank">知识发现与数据挖掘会议</a>(KDD)<a class="ae kl" href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="noopener ugc nofollow" target="_blank">神经信息处理系统会议</a>(NIPS)<a class="ae kl" href="https://en.wikipedia.org/wiki/Conference_on_Human_Factors_in_Computing_Systems" rel="noopener ugc nofollow" target="_blank">计算系统中人的因素会议</a>(池)。</p><p id="1b04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了得到这些会议记录的标题，我写了<a class="ae kl" href="https://gist.github.com/allisonmorgan/8a973a86e9bd353e57e44377e1bcc3c1" rel="noopener ugc nofollow" target="_blank">这个Go脚本</a>。如果你想运行它，你需要<a class="ae kl" href="https://golang.org/doc/install" rel="noopener ugc nofollow" target="_blank">安装Go </a>和<a class="ae kl" href="https://golang.org/doc/articles/go_command.html#tmp_3" rel="noopener ugc nofollow" target="_blank"> go获取</a>在<a class="ae kl" href="https://gist.github.com/allisonmorgan/8a973a86e9bd353e57e44377e1bcc3c1#file-dblp-go-L3-L11" rel="noopener ugc nofollow" target="_blank">序言</a>中的包。该脚本接受会议的命令行参数:</p><pre class="km kn ko kp gt lw lx ly lz aw ma bi"><span id="53ea" class="mb ku iq lx b gy mc md l me mf">&gt;&gt;&gt; go run dblp.go "chi"<br/>2017/09/12 20:01:34 Reading in DBLP gzipped file<br/>2017/09/12 20:01:34 Decoding all proceedings from file<br/>2017/09/12 20:03:23 Done decoding<br/>2017/09/12 20:03:23 Number of CHI titles: 13885 Average title length: 67.01498019445445<br/>2017/09/12 20:03:23 Output data to chi.txt</span></pre><p id="adb6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，确保您的<code class="fe mg mh mi lx b">chi.txt</code>文件位于您的本地副本<a class="ae kl" href="https://github.com/jcjohnson/torch-rnn" rel="noopener ugc nofollow" target="_blank"> torch-rnn </a>的数据文件夹中。(以下步骤对应于Jeffrey Thompson教程中的步骤6–8。)要为RNN准备数据，请运行:</p><pre class="km kn ko kp gt lw lx ly lz aw ma bi"><span id="26f1" class="mb ku iq lx b gy mc md l me mf">python scripts/preprocess.py \<br/>--input_txt data/chi.txt \<br/>--output_h5 data/chi.h5 \<br/>--output_json data/chi.json</span></pre><p id="77c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，要训练您的神经网络，运行如下内容:</p><pre class="km kn ko kp gt lw lx ly lz aw ma bi"><span id="6cde" class="mb ku iq lx b gy mc md l me mf">th train.lua -input_h5 data/chi.h5 -input_json data/chi.json -seq_length 60 -batch_size 1 -gpu -1</span></pre><p id="9f92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(还有更多<a class="ae kl" href="https://github.com/jcjohnson/torch-rnn/blob/master/doc/flags.md" rel="noopener ugc nofollow" target="_blank">可以指定的</a>命令行标志。)这一步会花很长时间，而且“<a class="ae kl" href="https://www.jeffreythompson.org/blog/2016/03/25/torch-rnn-mac-install/" rel="noopener ugc nofollow" target="_blank">你的电脑会变得很热</a>”但是一旦这些都完成了，你就可以从你的RNN取样了:</p><pre class="km kn ko kp gt lw lx ly lz aw ma bi"><span id="9194" class="mb ku iq lx b gy mc md l me mf">th sample.lua -checkpoint cv/checkpoint_581150.t7 -length 2000 -gpu -1 -temperature 0.1 &gt; data/chi_sample.txt</span></pre><p id="c790" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mg mh mi lx b">length</code>标志对应于你想要输出多少字符，而<code class="fe mg mh mi lx b">temperature</code>标志指定了你希望你的神经网络输出在0-1范围内有多新颖。更高的温度会产生很多废话，更低的温度会产生很多重复。</p><h1 id="95f0" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">有趣的东西</h1><p id="52be" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">好了，现在让我们考虑一下我们貌似合理的KDD，日本和中国的头衔。请记住，当从我们的递归神经网络中采样时，我们可以指定我们希望输出有多新颖。我将展示这里选择的三个会议的两个极端。</p><p id="c3f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">KDD是关于知识发现和数据挖掘的<a class="ae kl" href="https://en.wikipedia.org/wiki/SIGKDD" rel="noopener ugc nofollow" target="_blank">会议</a>，如果我们要求我们的RNN生成一个可预测的标题(<code class="fe mg mh mi lx b">temperature = 0.1</code>，那么结果是相当可预测的:</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="kr ks l"/></div></figure><p id="33b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意许多对<a class="ae kl" href="https://en.wikipedia.org/wiki/Data_mining" rel="noopener ugc nofollow" target="_blank">数据挖掘</a>、<a class="ae kl" href="https://en.wikipedia.org/wiki/Distributed_computing" rel="noopener ugc nofollow" target="_blank">分布式计算</a>和<a class="ae kl" href="https://en.wikipedia.org/wiki/Sequential_pattern_mining" rel="noopener ugc nofollow" target="_blank">顺序挖掘</a>的引用，这是该研究领域的典型。如果我们给我们的RNN更多的创作自由，一些古怪的标题就会产生:</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="kr ks l"/></div></figure><p id="bade" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里有一些看似可信的KDD头衔的大样本，分别是低创造力的T8和高创造力的T10。</p><p id="ea02" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，NIPS上覆盖的研究主题，<a class="ae kl" href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="noopener ugc nofollow" target="_blank">关于神经信息处理系统的会议</a>，包括机器学习(特别是神经网络)和人工智能。以下是一些低创造力的标题:</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="kr ks l"/></div></figure><p id="d63d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们所料，出现了许多对<a class="ae kl" href="https://en.wikipedia.org/wiki/Artificial_neuron" rel="noopener ugc nofollow" target="_blank">神经元</a>和<a class="ae kl" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">随机梯度下降</a>的引用。同样，如果我们增加<code class="fe mg mh mi lx b">temperature</code>，我们可以产生一些非常古怪的标题:</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="kr ks l"/></div></figure><p id="3e79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">点击这里查看貌似合理的NIPS头衔的完整样本:高创造力<a class="ae kl" href="https://gist.github.com/allisonmorgan/7d864a4cb902870b8eda56ead4e726c8" rel="noopener ugc nofollow" target="_blank">和低创造力</a><a class="ae kl" href="https://gist.github.com/allisonmorgan/b01de66dfae540c2b7bdee37342513ca" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="357f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，CHI，<a class="ae kl" href="https://en.wikipedia.org/wiki/Conference_on_Human_Factors_in_Computing_Systems" rel="noopener ugc nofollow" target="_blank">计算系统中人的因素会议</a>，是一个关于人机交互的会议。第一，低创意标题:</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="kr ks l"/></div></figure><p id="8f85" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同样，这些标题有一定的意义。注意互动的许多表象，“的研究”和“的效果”。(后两者仅从<a class="ae kl" href="http://dblp.uni-trier.de/db/conf/chi/chi2017.html" rel="noopener ugc nofollow" target="_blank"> CHI 2017 </a>就返回了18个标题。)和现在更有创意的标题:</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="kr ks l"/></div></figure><p id="8c89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里可以找到关于<a class="ae kl" href="https://gist.github.com/allisonmorgan/ec8d5fec3f30bfd57f7b40db994c6189" rel="noopener ugc nofollow" target="_blank">低创造力</a>和<a class="ae kl" href="https://gist.github.com/allisonmorgan/6c5aba17084c2d4b314b5f9ca09946e1" rel="noopener ugc nofollow" target="_blank">高创造力</a>似是而非的CHI标题的完整样本。</p><p id="14e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一般来说，这个练习证实了如果你给你的机器学习算法提供有偏见的数据，你会得到有偏见的结果。在这里，我们使用这些有偏见的结果来比较和对比不同会议的标题风格。我希望这个例子是有趣的，并且激励你去尝试torch-rnn。</p><h1 id="e6a9" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">脚注</h1><p id="07b1" class="pw-post-body-paragraph jn jo iq jp b jq lr js jt ju ls jw jx jy lt ka kb kc lu ke kf kg lv ki kj kk ij bi translated">1:其他更合适的标题可能是:“数据科学家学会了一个很酷的技巧，”或者“内容战略家讨厌这个女人。”我曾经写过关于clickbait的文章；你可以在这里阅读《T4》。谁知道呢，你可能会用一个简单的技巧快速减肥。</p><p id="cfb1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2: <a class="ae kl" href="https://gist.github.com/allisonmorgan/696bd8590853b89af1df14dbaaff6728#file-medium-txt-L218" rel="noopener ugc nofollow" target="_blank">这个</a>对于一台机器来说，独立编写是一个特别可怕的条目。</p></div></div>    
</body>
</html>