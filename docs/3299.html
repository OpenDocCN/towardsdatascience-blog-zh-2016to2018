<html>
<head>
<title>Elbow Clustering for Artificial Intelligence</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能中的肘聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/elbow-clustering-for-artificial-intelligence-be9c641d9cf8?source=collection_archive---------1-----------------------#2018-04-30">https://towardsdatascience.com/elbow-clustering-for-artificial-intelligence-be9c641d9cf8?source=collection_archive---------1-----------------------#2018-04-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/4b12a081d160bc108b966165ad5513b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tlING0Kju2M__CW6REFwVA.png"/></div></div></figure><p id="ecfd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">聚类是将一堆未分类的东西(你的数据集)根据相似性分成几堆(即聚类)的过程。这是一个无人监督的过程(不需要训练的例子)。也许最常见的方法是使用 k-means(T1)，我将介绍一种不同的方法，使用高斯混合模型(GMM)和一些其他的东西(T3)。</p><p id="4cd8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们今天的例子是在一个数据集中分离两种类型的图片，在你的数据集中，你实际上没有一个标签或者一个分类器。正如我们将要看到的，只要看看结果，你就能明白它的工作原理。首先，我想说明这种方法并不总是有效，因此需要一些微调。</p><p id="687c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在你的人工智能项目的每个主要步骤中，你需要记住的第一件事是<strong class="ka ir"> <em class="kx">用你的眼睛</em> </strong>看你的数据。我再怎么强调都不为过，如果你看不到 k 均值<a class="ae kw" href="https://codeahoy.com/2017/02/19/cluster-analysis-using-k-means-explained/" rel="noopener ugc nofollow" target="_blank">在做什么，它可能会愚蠢地出错</a>。</p><p id="1739" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从我在关于<a class="ae kw" rel="noopener" target="_blank" href="/image-datasets-for-artificial-intelligence-bbb12615edd7">图像数据集集合</a>的文章中停止的地方继续，让我们使用 k-means 和 GMM 通过<a class="ae kw" href="https://en.wikipedia.org/wiki/Perceptual_hashing" rel="noopener ugc nofollow" target="_blank">感知哈希</a>来进行图像聚类。如果您只是想找到哪些图片与其他图片相似，在 imagehash 代码中有一个<a class="ae kw" href="https://github.com/JohannesBuchner/imagehash/blob/master/find_similar_images.py" rel="noopener ugc nofollow" target="_blank">示例可以做到这一点</a>。我们在这里尝试做的更多一些:我们想要将数据分成类别(即使我们不知道类别是什么)。<a class="ae kw" href="https://github.com/zubairr/image-clustering-phash" rel="noopener ugc nofollow" target="_blank">在 GitHub 上有一个这种类型的库的例子</a>，它只被许可用于学术用途。</p><p id="0f06" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">不管我们选择什么样的库，我们都会立即遇到一个问题:我们最终想要多少个集群？这不是<a class="ae kw" href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank">聚类库</a>给你的。你需要决定，决定集群的数量应该基于一些逻辑。我们应该使用……<a class="ae kw" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)" rel="noopener ugc nofollow" target="_blank"><strong class="ka ir">肘法！</strong> </a></p><p id="86de" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">肘法对于一个简单的想法来说是一个奇怪的名字。不断添加集群，直到你看到收益递减，然后停止。对于 k-means，这意味着从 2 个均值开始，然后是 3 个均值，以此类推，直到 k。GMM 也是同样的想法。当我们在解释方差与聚类数的关系图中看到一个拐点时，我们后退并选择看到拐点的聚类数。在下图中，您可以看到前两个点是手臂，4 处的点是肘部，前臂是 5 到 8 处的点。</p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div class="gh gi ky"><img src="../Images/8c4bad095edd9bc842c5072b8d773f50.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*oin436ttEbEg2q0x.JPG"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Graph of explained variance vs cluster count, where the red circle shows the elbow. (<a class="ae kw" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)" rel="noopener ugc nofollow" target="_blank">from wikipedia</a>)</figcaption></figure><p id="f9be" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我知道:我们计算机人现在应该停止给东西命名。<a class="ae kw" href="https://en.wikipedia.org/wiki/Turing_machine" rel="noopener ugc nofollow" target="_blank">图灵机</a>，<a class="ae kw" href="https://en.wikipedia.org/wiki/Round-robin_scheduling" rel="noopener ugc nofollow" target="_blank">循环赛</a>，<a class="ae kw" href="https://en.wikipedia.org/wiki/Karnaugh_map" rel="noopener ugc nofollow" target="_blank"> k-maps </a>，就是不停。</p><p id="ef07" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">参见“<a class="ae kw" href="https://books.google.ca/books?id=_plGDwAAQBAJ&amp;printsec=frontcover#v=onepage&amp;q&amp;f=false" rel="noopener ugc nofollow" target="_blank"> Python Machine Learning </a>”第 148 页上的图，了解数据集的一些组件如何比其他组件包含更多差异(PCA)。在早期停止期间，当决定何时停止训练时，肘方法同样有效，但是在这种情况下，数据通常更嘈杂。基本上，当看起来不值得努力时，我们就停下来。</p><p id="9563" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为什么这个肘法的东西不太为人所知？嗯，我想这有点主观，所以那些想知道确切数字的人会感到奇怪。此外，当我们对事物进行聚类时，我们往往会想到一些聚类，这通常是出于愚蠢的原因，但这是事实。例如，我更喜欢有一些我小小的大脑理解的集群(例如，一个小数字)，而不是数学告诉我真正需要的数字。此外，我也遇到过集群数量受到硬件限制等外部因素限制的情况。可能有 299，945 种交易模式类型，但如果我在 GPU 上的时间有限空间有限的神经网络只能在有用的时间内分析 2，048 种模式类型，那么我想我们会有 2，048 个聚类。在<a class="ae kw" href="https://pdfs.semanticscholar.org/5771/aa21b2e151f3d93ba0a5f12d023a0bfcf28b.pdf" rel="noopener ugc nofollow" target="_blank">研究文献中有许多使用肘法</a>的例子。在我们的例子中，我们会看到肘法并不是唯一的游戏。</p><p id="6afb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们继续做我们来这里要做的事情:使用感知哈希用肘方法聚类图片！让我们对来自<a class="ae kw" href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/" rel="noopener ugc nofollow" target="_blank">标准图像数据集</a>的图像进行排序，看看我们最终得到了哪些子类型。这里的目标是对图像<strong class="ka ir"> <em class="kx">进行排序，而不使用</em> </strong>使用基础事实标签来进行排序(即聚类)。我们要用 k-means 做的事情是无人监督的。它只是看着图片，把图片整理成一堆堆。使用带有标签的数据集的原因只是为了帮助在排序过程对数据集所做的事情之后进行可视化。实际上，你会用眼睛看每一组中的图片，并对它的作用有一个直观的感觉。</p><p id="5ae7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我们今天的例子中，我们将使用 Caltech101 数据集的“Faces”和“airplanes”子文件夹。我们把这些图片像一副扑克牌一样混在一起，然后把它们堆成一大堆。现在想象这是一些图像抓取作业的输出。</p><figure class="kz la lb lc gt jr"><div class="bz fp l di"><div class="lh li l"/></div></figure><p id="f042" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于上一篇文章中的叶数据集，我们最终得到的是 4102 行× 2 列，它确切地告诉我们每个图像的散列是什么。大约 20%的原始 5000+图像不会生成散列，通常是因为文件在抓取过程中不知何故被损坏。让我们更好地理解刚刚发生了什么…</p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/b3b4db3f52c715db8bd1209e50a0cf89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*_MFThBXR72SZYhwlS6hYpQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Screenshot of the dataframe printing out after the hashing is completed.</figcaption></figure><p id="55e0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们需要解开这个鬼鬼祟祟的东西，我们称之为哈希。我们可以通过减去两个不同图像的哈希来计算图像之间的差异，但此时会出现一些奇怪的情况。尽管我们能够对这个散列值使用减法运算符，但它实际上并不做减法。通过简单地增加散列而不是减少来证明这一点，它将不会如此优雅地编译失败。那么，我们在输出端得到的这个哈希是什么呢？嗯，它是每张图片的布尔值字段。很明显，hash 字符串中的字符是十六进制的，所以打印一个元素可以揭示隐藏在这个漂亮字符串下面的 hash 的真实性质:它是一串二进制数。</p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lk"><img src="../Images/f2eda18c0db839633f131892ecd29cf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UWa0v8jvQ22gpPYQRw9-Xg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">The binary result of the hash is hiding inside the hex string….</figcaption></figure><p id="811e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">那么，当我们“比较”散列时，减法运算到底在做什么呢？它执行 XOR 运算，并计算不相同的位数。换句话说，图像之间的“差异”被量化为两个图像的散列中的差异数量的计数。我们可以使用这个方便的小脚本来证明这一点…</p><figure class="kz la lb lc gt jr"><div class="bz fp l di"><div class="lh li l"/></div></figure><p id="9659" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">运行该脚本时，我们会看到以下输出:</p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/448364add36e547a3dc95ab21b71c03d.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*a7yCoc2KOrb5M3UGGb-r4g.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Counting the bits that didn’t match gives the same result as “subtracting” hashes.</figcaption></figure><p id="995f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以，这是相当卑鄙的，但现在我们知道，散列之间的距离不是一个<a class="ae kw" href="https://en.wiktionary.org/wiki/orderable" rel="noopener ugc nofollow" target="_blank">可排序的</a>属性。它根据我们使用哪两张图像作为输入而变化。我们真正想说的是，我们计算海明距离。在 scipy land，这个指标在<a class="ae kw" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html" rel="noopener ugc nofollow" target="_blank">距离指标列表</a>中排名第 8。</p><p id="9b94" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，进行聚类！我们的散列字符串中有 4 位十六进制字符，每个散列中有 16 个字符。这给了我们 64 个维度来比较图像。我们需要更新我们的 pandas 数据帧，以包括 64 个列(每个哈希特性一个)。</p><figure class="kz la lb lc gt jr"><div class="bz fp l di"><div class="lh li l"/></div></figure><p id="0c83" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们最终得到了我们想要的 64 个新列。</p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lm"><img src="../Images/2c682704224fc79aec8046c62aadd870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OVx4l1Pm3LAkWrov8FThjg.png"/></div></div></figure><p id="00ea" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用来自<a class="ae kw" href="https://datascience.stackexchange.com/questions/6508/k-means-incoherent-behaviour-choosing-k-with-elbow-method-bic-variance-explain" rel="noopener ugc nofollow" target="_blank">这篇文章</a>的代码，我们可以获得完整图像数据集的一些漂亮的图表，包括解释方差的百分比。</p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/b9fb9d062e48d68c2fdb1474b9f9d02f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*qArTOy9kI84b5hG53ScFYQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">This figure is for hashed generated by pHash. It shows the increase in explained variance as we add clusters.</figcaption></figure><p id="200a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们在上图中看到，在大约 8 或 9 个集群时，我们开始失去动力。此外，由于模型无法解释大多数方差，因此它不是一个非常有用的预测器。</p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lo"><img src="../Images/cfd306631d8af7de8f242a088a231f55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*ooCE_CRQqcORFFDbjwBr4Q.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">This figure is for hashed generated by the average hash. It shows the increase in explained variance as we add clusters.</figcaption></figure><p id="3aa7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">类似于感知哈希图，我们在上图中看到，在大约 8 或 9 个集群时，我们开始失去动力。与上图中的感知哈希相比，这种方法可以解释更多的差异(顶行是 40%，而不是 14%)。在这个图中，我们还看到了一个更好的弯头。它仍然不是一个超级有用的预测工具，但比以前好了。</p><p id="2dea" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们有了一些答案，这意味着什么呢？每一堆都聚集了哪些图像？嗯，只有在一些受限的情况下，集群才能很好地工作。让我们看看在文章开始时混合在一起的两种图像(飞机和面孔)，看看聚类是否能够将这两种图像类型分开。让我们比较一下 GMM 解混和 k-means，看看对于这个问题哪个是更好的无监督“解混器”。</p><p id="bcb1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，这是 K-means 的代码:</p><figure class="kz la lb lc gt jr"><div class="bz fp l di"><div class="lh li l"/></div></figure><p id="3373" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">结果看起来像这样:</p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lp"><img src="../Images/ac8db1816504ab5a94174b5528b80a68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3Rgzws2toETPokcdejd9g.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Cluster 0 is red, and cluster 1 is green. In the chart above, we can see that counting up all of the images containing faces, 90.8% ended up in cluster 0. Similarly 62.6% of the images with faces in them ended up in cluster 0. This is not a nice outcome.</figcaption></figure><p id="c872" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有 800 张飞机照片和 435 张人脸照片。</p><p id="a7fd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">正如我们在上面的饼图中所看到的，在两个集群中，飞机和人脸没有很好的分离。我们可以选择两个集群，因为我们知道有两个东西可以分成两堆。这两个类的内容最终大部分集中在 0 类中。在 GMM，我们看到了一个不同的故事。</p><p id="9934" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们先看看 GMM 的代码，然后看看结果。</p><figure class="kz la lb lc gt jr"><div class="bz fp l di"><div class="lh li l"/></div></figure><p id="d93b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在是几个运行示例的结果:</p><figure class="kz la lb lc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lq"><img src="../Images/84b2ac90a73f9cf39e2195a0a00f353b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D-HnrZEAdaWF539irUVMQw.png"/></div></div></figure><figure class="kz la lb lc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lr"><img src="../Images/8060bd8911be0d583b13b832f5acaba7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*klMfSFG38HeqD6ZIQej8ww.png"/></div></div></figure><p id="7b32" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以这些结果要好得多。GMM 把这些文件分成一个文件夹，里面大部分是人脸，另一个文件夹里大部分是飞机。</p><p id="3f3b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以通过运行几次 GMM 来挑选结果，以查看何时分割看起来不错(通过对结果图像数据集的视觉检查)。我们还可以标记数据的一些子集，使用上面的代码，我们只需查看饼状图就可以了解拆分是如何进行的。这种方法比成熟的图像分类器需要更少的数据。还有 50 种其他的方法可以做到这一点，但是把这个算作第 51 种方法。</p><p id="b102" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">克隆它！！</strong></p><p id="f82e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我已经为你提供了代码，希望这能帮助你更好地处理你收集的图片。在很多课程中，这变得非常混乱，但是我有一个建议。经常发生的情况是，在很多类中，垃圾图像和重复图像聚集在一起，帮助你删除垃圾图像。然后，您可以重新集群，看看会发生什么。K-means 有时会起作用，在这种情况下，GMM 稍微好一点。世事难料。基本上，尝试很多东西，你会马上知道它什么时候起作用。</p><p id="3113" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，总之，感知哈希可以用来将图像压缩到一组二进制维度，k-means、GMM 或其他无监督的方法可以根据这些维度将图像分类。</p><p id="e0b9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你喜欢这篇关于用 k-means 和 elbow 方法对图像进行聚类的文章，看看我过去读过的一些文章，比如“<a class="ae kw" href="https://medium.com/towards-data-science/how-to-price-an-ai-project-f7270cb630a4" rel="noopener">如何给一个人工智能项目定价</a>”和“<a class="ae kw" href="https://medium.com/towards-data-science/why-hire-an-ai-consultant-50e155e17b39" rel="noopener">如何雇佣一个人工智能顾问</a>”除了与商业相关的文章，我还准备了一些关于寻求采用深度机器学习的公司所面临的其他问题的文章，如“<a class="ae kw" href="https://medium.com/@lemaysolutions/locked-in-a-box-machine-learning-without-cloud-or-apis-76cc54e391c8" rel="noopener">没有云和 API 的机器学习</a></p><p id="bb44" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">编码快乐！</p><p id="e8c7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">-丹尼尔<br/> <a class="ae kw" href="mailto:daniel@lemay.ai" rel="noopener ugc nofollow" target="_blank">丹尼尔@lemay.ai </a> ←打个招呼。<br/><a class="ae kw" href="https://lemay.ai" rel="noopener ugc nofollow" target="_blank">LEMAY . AI</a><br/>1(855)LEMAY-AI</p><p id="8427" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您可能喜欢的其他文章:</p><ul class=""><li id="167e" class="ls lt iq ka b kb kc kf kg kj lu kn lv kr lw kv lx ly lz ma bi translated"><a class="ae kw" rel="noopener" target="_blank" href="/artificial-intelligence-and-bad-data-fbf2564c541a">人工智能和不良数据</a></li><li id="d59c" class="ls lt iq ka b kb mb kf mc kj md kn me kr mf kv lx ly lz ma bi translated"><a class="ae kw" rel="noopener" target="_blank" href="/artificial-intelligence-hyperparameters-48fa29daa516">人工智能:超参数</a></li><li id="4172" class="ls lt iq ka b kb mb kf mc kj md kn me kr mf kv lx ly lz ma bi translated"><a class="ae kw" href="https://medium.com/towards-data-science/artificial-intelligence-get-your-users-to-label-your-data-b5fa7c0c9e00" rel="noopener">人工智能:让你的用户给你的数据贴上标签</a></li></ul></div></div>    
</body>
</html>