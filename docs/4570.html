<html>
<head>
<title>Coding Deep Learning for Beginners — Linear Regression (Part 3): Training with Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">初学者的深度学习编码——线性回归(第三部分):梯度下降训练</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/coding-deep-learning-for-beginners-linear-regression-gradient-descent-fcd5e0fc077d?source=collection_archive---------6-----------------------#2018-08-23">https://towardsdatascience.com/coding-deep-learning-for-beginners-linear-regression-gradient-descent-fcd5e0fc077d?source=collection_archive---------6-----------------------#2018-08-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div class="gh gi io"><img src="../Images/a4d8ee0f6d5ebb8239cb9e8103f1fa71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*mElyetzsTIJrNnKI8kTkCw.jpeg"/></div><figcaption class="iv iw gj gh gi ix iy bd b be z dk">Source: <a class="ae iz" href="https://reconsider.news/2018/05/09/ai-researchers-allege-machine-learning-alchemy/" rel="noopener ugc nofollow" target="_blank">https://reconsider.news/2018/05/09/ai-researchers-allege-machine-learning-alchemy/</a></figcaption></figure><div class=""/><p id="4974" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这是系列“<strong class="kb jd">初学者深度学习编码</strong>”的第 5 篇文章。你可以在这里找到所有文章、<em class="kx">的<em class="kx">链接，以及第一篇文章</em>  <em class="kx">底部关于下一篇文章</em> <a class="ae iz" href="https://medium.com/@krzyk.kamil/coding-deep-learning-for-beginners-start-a84da8cb5044" rel="noopener"> <em class="kx">预计发布日期的一般信息。</em>它们也可以在我的</a><a class="ae iz" href="https://github.com/FisherKK/F1sherKK-MyRoadToAI" rel="noopener ugc nofollow" target="_blank">开源文件夹— <strong class="kb jd"> MyRoadToAI </strong> </a>中找到，还有一些迷你项目、演示、教程和链接。</em></p><h1 id="f111" class="ky kz jc bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">目标</h1><p id="cfb8" class="pw-post-body-paragraph jz ka jc kb b kc lw ke kf kg lx ki kj kk ly km kn ko lz kq kr ks ma ku kv kw ij bi translated">在本文中，我将<strong class="kb jd">解释用梯度下降</strong>训练机器学习算法的概念。大多数监督算法都在利用它——尤其是所有的神经网络。这是一个至关重要的话题，也是开始机器学习的人需要克服的最大障碍之一。这是因为它是基于微积分的<strong class="kb jd">，有些人在学校没有学过，或者他们的知识生锈了。</strong></p><p id="4205" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">但是不要担心，即使你对数学不熟悉或者根本不懂微积分，你仍然可以理解它，学习它，并使用它。我会告诉你怎么做！</p><h1 id="62f9" class="ky kz jc bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">靠蛮力训练</h1><p id="c63e" class="pw-post-body-paragraph jz ka jc kb b kc lw ke kf kg lx ki kj kk ly km kn ko lz kq kr ks ma ku kv kw ij bi translated">由于上一篇文章中介绍的成本函数，已经可以训练模型来预测克拉科夫公寓的价格。<strong class="kb jd">模型只使用了公寓的尺寸</strong>，所以它的形式并不太复杂:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/14a4c0f688289d9e429d59eab029b315.png" data-original-src="https://miro.medium.com/v2/resize:fit:260/format:webp/0*NOG_xxqN9f3-dKws.png"/></div></figure><p id="2f00" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">其中:</p><ul class=""><li id="9df1" class="mg mh jc kb b kc kd kg kh kk mi ko mj ks mk kw ml mm mn mo bi translated"><em class="kx"> ŷ </em> —预测房价，</li><li id="1acd" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated"><em class="kx">x</em>—户型，</li><li id="6aba" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated"><em class="kx"> w </em> —分配给公寓大小的重量，</li><li id="b7f2" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated"><em class="kx"> b </em> —偏置。</li></ul><p id="23f2" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">下标“0”代表特征在数据集<em class="kx"> X </em>的每个矢量中的位置，为了提高可读性，省略了该下标。</p><p id="ff5f" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">训练过程的目标是<strong class="kb jd">找到<em class="kx"> w </em>和<em class="kx"> b </em>参数的组合，为此成本函数返回尽可能小的误差</strong>。</p><p id="e0c6" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">为了提高模型性能，会做一些非常幼稚的事情。在许多<strong class="kb jd">迭代</strong>中，参数将通过一个小的固定<strong class="kb jd">步长</strong>改变，并且不知道参数值是否应该改变以及在哪个<strong class="kb jd">方向</strong>上改变。因此，将计算所有可能组合的成本值，以找到最佳修改，这就是为什么它被称为“暴力训练”<strong class="kb jd">在每次训练迭代中，可以对参数执行三个动作</strong>:<strong class="kb jd">增加、减少、不变。</strong>有<strong class="kb jd">两个参数</strong>，这意味着每次迭代将有<strong class="kb jd"> 3 = 9 组参数来检查</strong>，并对其进行预测，计算成本。模型将使用成本最低的设置进行更新。</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mu"><img src="../Images/a905283c6f25359cdd327a28eda7ac9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HOc6GI6YxSalGPj4JXYmRQ.png"/></div></div><figcaption class="iv iw gj gh gi ix iy bd b be z dk">Training with Brute Force — diagram showing how to train model with two parameters (w and b) without using Gradient Descent algorithm.</figcaption></figure><p id="0e07" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">让我们实现它，看看结果。大多数操作已经在上一篇文章中编码了，因此只剩下训练循环需要实现。</p><figure class="mc md me mf gt is"><div class="bz fp l di"><div class="mz na l"/></div><figcaption class="iv iw gj gh gi ix iy bd b be z dk">Full code available under this <a class="ae iz" href="https://gist.github.com/FisherKK/7c19abce0bab8c2a90384f632cc71c52#file-a5_brute_force_training_full-py" rel="noopener ugc nofollow" target="_blank">link</a>.</figcaption></figure><p id="7b49" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">只有两个参数，代码已经足够复杂，可以省略一些部分来提高可读性。隐藏部分的实现与图中所示非常相似——唯一的区别是数学运算符从<code class="fe nb nc nd ne b">+=</code>变为<code class="fe nb nc nd ne b">-=</code>,或者有时根本不使用。</p><p id="949e" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">代码片段以从<code class="fe nb nc nd ne b">copy</code>模块导入<code class="fe nb nc nd ne b">deepcopy</code>函数开始。需要复制存储在<code class="fe nb nc nd ne b">model_parameters</code>字典中的模型参数及其内容。在任何训练发生之前，函数<code class="fe nb nc nd ne b">predict</code>和<code class="fe nb nc nd ne b">mse</code>使用<code class="fe nb nc nd ne b">model_parameters</code>通过计算其初始误差来测量模型的潜力。</p><p id="3117" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">然后<code class="fe nb nc nd ne b">model_parameters</code>被复制九次。每个副本都以不同的方式(加法、减法、无变化)被<code class="fe nb nc nd ne b">step</code>值修改，然后用于预测。根据<code class="fe nb nc nd ne b">candidate_pred</code>的正确性，副本具有作为<code class="fe nb nc nd ne b">candidate_error</code>变量分配给它的成本值。误差和参数存储在<code class="fe nb nc nd ne b">candidates</code>和<code class="fe nb nc nd ne b">errors</code>列表中。在计算了每个候选项并测量了它们的性能后，使用成本最低的一组参数来代替<code class="fe nb nc nd ne b">model_parameters</code>。它发生<code class="fe nb nc nd ne b">iterations</code>次，默认设置为 100。请注意，函数<code class="fe nb nc nd ne b">train</code>没有返回任何结果，并且<code class="fe nb nc nd ne b">model_parameters</code>被就地修改。</p><p id="61e1" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在所有这些之后，可以对加载的数据和初始化的模型参数使用<code class="fe nb nc nd ne b">train</code>函数，以便找到将误差最小化的值:</p><figure class="mc md me mf gt is"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="cee4" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">可以将简单的<code class="fe nb nc nd ne b">print</code>函数添加到<code class="fe nb nc nd ne b">train</code>函数体中，并显示训练过程如何改变模型参数(如果您以前没有这样做，请查看此<a class="ae iz" href="https://gist.github.com/FisherKK/7c19abce0bab8c2a90384f632cc71c52#file-a5_brute_force_training_full-py" rel="noopener ugc nofollow" target="_blank">链接</a>以获得完整代码):</p><pre class="mc md me mf gt nf ne ng nh aw ni bi"><span id="cb58" class="nj kz jc ne b gy nk nl l nm nn">Initial state:<br/> - error: [75870.4884482]<br/> - parameters: {'w': array([0.]), 'b': 0.0}<br/><br/>Iteration 0:<br/> - error: [73814.2609893]<br/> - parameters: {'w': array([0.1]), 'b': 0.1}<br/><br/>Iteration 20:<br/> - error: [38764.28631114]<br/> - parameters: {'w': array([2.1]), 'b': 2.1}<br/><br/>Iteration 40:<br/> - error: [15284.92972772]<br/> - parameters: {'w': array([4.1]), 'b': 4.1}<br/><br/>Iteration 60:<br/> - error: [3376.19123904]<br/> - parameters: {'w': array([6.1]), 'b': 6.1}<br/><br/>Iteration 80:<br/> - error: [1753.32046443]<br/> - parameters: {'w': array([7.1]), 'b': 8.1}<br/><br/>Final state:<br/> - error: [1741.85716443]<br/> - parameters: {'w': array([7.1]), 'b': 10.0}</span></pre><p id="ccb2" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb jd">误差随着每次迭代而减小。</strong>这是因为所有的训练样本都在每次迭代中使用——但是在以后的文章中会有更多的介绍。另一个重要的观察结果是训练结束后<strong class="kb jd">误差值不等于零。</strong>仅仅用线性函数是不可能遍历所有的点的。</p><p id="026e" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后，可以显示训练的结果，并查看模型如何能够估算公寓价格:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi no"><img src="../Images/230960adf6a9664c724ed621c0394c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d4PEErEmrocYq6piRqRH6Q.png"/></div></div><figcaption class="iv iw gj gh gi ix iy bd b be z dk">Model projection after training. Code used to create the chart is available <a class="ae iz" href="https://gist.github.com/FisherKK/55e4a1d52df1951fdb2ba4a7ba89ffa2" rel="noopener ugc nofollow" target="_blank">here</a>.</figcaption></figure><h2 id="b283" class="nj kz jc bd la np nq dn le nr ns dp li kk nt nu lm ko nv nw lq ks nx ny lu nz bi translated">结论</h2><p id="1ab9" class="pw-post-body-paragraph jz ka jc kb b kc lw ke kf kg lx ki kj kk ly km kn ko lz kq kr ks ma ku kv kw ij bi translated">训练这样一个简单的模型并找到可以在给定数据上表现良好的参数，不需要任何复杂的技术。但是<strong class="kb jd">目前的方法有很多局限性</strong>:</p><ul class=""><li id="26fb" class="mg mh jc kb b kc kd kg kh kk mi ko mj ks mk kw ml mm mn mo bi translated">一大堆<strong class="kb jd">样板代码</strong>。</li><li id="c5bb" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated"><strong class="kb jd">在每次迭代中需要测试太多的组合。这个模型只使用了两个参数。想象一下在像神经网络这样的模型中会发生什么，其中参数的数量有时以百万计——3⁰⁰⁰⁰⁰⁰不是一个好数字…</strong></li><li id="42a6" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated"><strong class="kb jd">改变参数的步骤</strong>对于每个参数是固定的<strong class="kb jd">。<strong class="kb jd">其值的范围也是未知的，</strong>有时 0.1 可能太大，有时可能太小。</strong></li><li id="791b" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated"><strong class="kb jd">每个参数应该改变的方向是未知的，</strong>这就是为什么必须尝试所有可能的改变。它需要大量的计算能力，并且非常慢。</li></ul><h1 id="8e74" class="ky kz jc bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">衍生工具的作用</h1><p id="22f2" class="pw-post-body-paragraph jz ka jc kb b kc lw ke kf kg lx ki kj kk ly km kn ko lz kq kr ks ma ku kv kw ij bi translated">矛盾的是，当涉及到重用已经发明的涉及梯度下降的机器学习概念时，理解<strong class="kb jd">哪些导数可以用于比能够计算它</strong>更重要。<a class="ae iz" href="https://twitter.com/iamtrask?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor" rel="noopener ugc nofollow" target="_blank"> Andrew Trask </a>在他的书<a class="ae iz" href="https://www.manning.com/books/grokking-deep-learning" rel="noopener ugc nofollow" target="_blank">“探索深度学习”</a>中提供了非常好的关于什么是衍生品以及它如何运作的直觉。本文的解释将使用类似的推理。</p><h2 id="285b" class="nj kz jc bd la np nq dn le nr ns dp li kk nt nu lm ko nv nw lq ks nx ny lu nz bi translated">直觉</h2><p id="2a6c" class="pw-post-body-paragraph jz ka jc kb b kc lw ke kf kg lx ki kj kk ly km kn ko lz kq kr ks ma ku kv kw ij bi translated">想象有两根杆子伸出墙外。让我们将它们命名为杆 A 和杆 b。杆 A 比杆 b 短两倍。为了了解杆之间的关系，进行了两个实验:</p><ul class=""><li id="dd5d" class="mg mh jc kb b kc kd kg kh kk mi ko mj ks mk kw ml mm mn mo bi translated">首先，杆 A 被推入墙中。可以观察到的是，木棒 B 的长度随着木棒 A 一起缩短，但是缩短了两倍。</li><li id="8253" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated">其次，杆 A 被拉出墙壁，其长度增加。结果，杆 B 也伸长了，但速度快了两倍。</li></ul><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi oa"><img src="../Images/2c7e7b1d889b329d1ee0849671139f11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wZBqJNnBz-uWNri-OEQ58A.png"/></div></div></figure><p id="ff27" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在让我们试着<strong class="kb jd">用数学方程的形式描述两杆之间的关系。</strong>在两种情况下，都可以观察到杆 B 的长度总是杆 A 的两倍:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/336da539b32d612943a8f8847253b9d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/format:webp/0*8VLtrIGz7KmsMIlr.png"/></div></figure><p id="a017" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">其中:</p><ul class=""><li id="b16c" class="mg mh jc kb b kc kd kg kh kk mi ko mj ks mk kw ml mm mn mo bi translated"><em class="kx"> l_A </em> —杆 A 的长度，</li><li id="e711" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated"><em class="kx"> l_B </em> —杆 B 的长度</li></ul><p id="e6df" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">稍等片刻，想想<strong class="kb jd">数字 2 在这个等式中的作用。</strong>是<em class="kx"> l_B </em>相对于<em class="kx"> l_A </em>的导数，可以写成:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/36352ebfaf04eeb8bceb5550cd0297cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:140/format:webp/0*qmh867qSAf5ChwtD.png"/></div></figure><p id="7bca" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从中可以学到什么？</p><ul class=""><li id="b83e" class="mg mh jc kb b kc kd kg kh kk mi ko mj ks mk kw ml mm mn mo bi translated"><strong class="kb jd">导数描述了一个变量被修改时，另一个变量是如何变化的。</strong>那么在这种情况下，当杆 A 的长度被修改时，杆 B 的长度以什么因子延伸或收缩。</li><li id="05a1" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated"><strong class="kb jd">计算两个变量之间的导数。</strong></li><li id="9e73" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated"><strong class="kb jd">当导数为正值时，两个值的变化方向相同。如果杆 A 延伸，那么杆 B 也延伸，因为 2 是正数。反之亦然，<strong class="kb jd">如果导数为负值，则值向相反方向变化。</strong></strong></li></ul><p id="4a26" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">请记住:</p><blockquote class="od oe of"><p id="47b0" class="jz ka kx kb b kc kd ke kf kg kh ki kj og kl km kn oh kp kq kr oi kt ku kv kw ij bi translated">对于可微函数，有可能找到两个变量之间的关系——当一个变量通过使用导数被修改时，一个变量改变了多少以及在什么方向上改变。</p></blockquote><p id="40b4" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">考虑到之前关于“暴力学习”的问题，导数有一些有趣的性质可以用来解决这些问题。</p><h1 id="c590" class="ky kz jc bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated"><strong class="ak">梯度下降</strong></h1><p id="3924" class="pw-post-body-paragraph jz ka jc kb b kc lw ke kf kg lx ki kj kk ly km kn ko lz kq kr ks ma ku kv kw ij bi translated">求 <a class="ae iz" href="https://en.wikipedia.org/wiki/Convex_function" rel="noopener ugc nofollow" target="_blank"> <strong class="kb jd">凸函数</strong> </a> <strong class="kb jd">最小值的一种迭代优化<strong class="kb jd">算法。</strong>它基于微积分— <strong class="kb jd">依赖于一阶导数</strong>的性质来寻找在什么<strong class="kb jd">方向</strong>和用什么<strong class="kb jd">幅度系数来修改函数。</strong></strong></p><p id="58d7" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在之前的文章中提到，用于测量机器学习模型性能的<strong class="kb jd">成本函数</strong>需要是可微分的。如果不是这种情况，那么梯度下降算法不能应用。</p><p id="f1f7" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">让我们以均方误差函数为例，它被广泛用作回归模型的成本函数。</p><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/535947e512e5400293e0c0d253ae92b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/0*3tUgqNjtX102ruKT.png"/></div></figure><ul class=""><li id="7431" class="mg mh jc kb b kc kd kg kh kk mi ko mj ks mk kw ml mm mn mo bi translated">i —样品指数，</li><li id="2662" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated">ŷ —预测值，</li><li id="aff8" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated">y —期望值，</li><li id="eba7" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated">m —数据集中的样本数量。</li></ul><p id="248a" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">预测值<em class="kx"> ŷ </em>可以用公寓价格近似模型的公式代替。让我们也稍微调整一下使用的命名法，如<strong class="kb jd">用大写字母<em class="kx"> J </em> </strong> <em class="kx">来命名机器学习模型的错误非常流行。</em></p><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/7409e77060ebbe16f212f2f3cf4d0403.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/0*3boprC82I9Aq2DS4.png"/></div></figure><p id="c2a8" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在很容易看出<strong class="kb jd">误差值取决于<em class="kx"> w </em>和<em class="kx"> b </em>系数</strong>。参数<strong class="kb jd"> <em class="kx"> m </em>，<em class="kx"> x </em>，<em class="kx"> y </em>可以被视为常量，其值是已知的，并由训练模型的数据集</strong>确定。</p><p id="5b5a" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">已经说过，计算任何两个函数参数之间的导数是可能的，并且它将提供当一个参数改变时另一个参数改变的信息。<em class="kx"> J 的派生结果:</em></p><ul class=""><li id="8b11" class="mg mh jc kb b kc kd kg kh kk mi ko mj ks mk kw ml mm mn mo bi translated">关于参数<em class="kx"> w </em>提供了如何调整参数<em class="kx"> w </em>的值以最小化或最大化<em class="kx"> J、</em>的值的信息</li></ul><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/b531b0b16064ad0123b8a8df723bdf3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/0*7Hv7sFAy5xOPdIQN.png"/></div></figure><ul class=""><li id="0ba2" class="mg mh jc kb b kc kd kg kh kk mi ko mj ks mk kw ml mm mn mo bi translated">关于参数<em class="kx"> b </em>提供了如何调整参数<em class="kx"> b </em>的值以最小化或最大化<em class="kx"> J. </em>的值的信息</li></ul><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi om"><img src="../Images/36c4ca2cfe6025df0e31dff86730fec9.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*2f2pJmQ3fQHrZ0GV.png"/></div></figure><p id="8358" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb jd">如果模型有更多的参数，那么需要计算更多的导数。这就是微积分派上用场的时刻。在大多数情况下，使用梯度下降算法不需要数学。<strong class="kb jd">机器学习、详细计算或代码实现中使用的各种函数的衍生物</strong> <a class="ae iz" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank"> <strong class="kb jd">在 web </strong> </a> <strong class="kb jd">中全局呈现。</strong></strong></p><p id="6821" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">首先，在网上查找答案是很好的。使用微积分的能力肯定会导致更好地理解各种机器学习机制是如何工作的。它还将提供检查其他人的代码的能力。</p><p id="bff1" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果你想知道这两个导数是如何计算出来的，那么试试这篇博文或者看看伊恩·古德菲勒的书《深度学习》的第 106 页(T2)。</p><h2 id="83a2" class="nj kz jc bd la np nq dn le nr ns dp li kk nt nu lm ko nv nw lq ks nx ny lu nz bi translated">几何解释</h2><p id="d6ac" class="pw-post-body-paragraph jz ka jc kb b kc lw ke kf kg lx ki kj kk ly km kn ko lz kq kr ks ma ku kv kw ij bi translated">可以考虑在给定点处曲线切线的<strong class="kb jd">斜率的导数。“梯度”一词的同义词是“坡度”</strong></p><p id="f34a" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">模型参数值通常在开始时是随机的。它们的值<strong class="kb jd">在<strong class="kb jd">误差曲线</strong>(对于一个参数的模型)<strong class="kb jd">误差曲面</strong>(对于两个参数的模型)或<strong class="kb jd">误差超平面</strong>(对于两个以上的参数)上强加点</strong>的位置。梯度下降算法的目标是<strong class="kb jd">找到参数值，因此该点总是出现在最低区域</strong>，因为那里的误差值最低。</p><p id="30e2" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">以下是展示单参数模型如何根据随机生成的数据进行迭代改进的可视化效果。有两种情况:</p><ul class=""><li id="6398" class="mg mh jc kb b kc kd kg kh kk mi ko mj ks mk kw ml mm mn mo bi translated">随机化的权重值太小，需要增加，</li></ul><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi on"><img src="../Images/45422d5b73a326c3e24eb91026a0dd93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*iWRgqnhWm5CBt-0Pi6hzbg.gif"/></div></div></figure><ul class=""><li id="a88c" class="mg mh jc kb b kc kd kg kh kk mi ko mj ks mk kw ml mm mn mo bi translated">随机权重值太大，需要减小。</li></ul><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi on"><img src="../Images/7e61f15f05e5d80c21a3b8fa30dee7e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*rEXxyndPpGuOwe-lBCOTow.gif"/></div></div></figure><p id="7e27" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">有一些重要的观察结果:</p><ul class=""><li id="a74b" class="mg mh jc kb b kc kd kg kh kk mi ko mj ks mk kw ml mm mn mo bi translated">请注意，<strong class="kb jd">权重值太小，则导数为负</strong>，而<strong class="kb jd">权重值太大，则导数为正</strong>。因此，<strong class="kb jd">为了最小化误差函数，需要从权重中减去导数，以使该点更接近全局最小值</strong>。如果加上导数，误差反而会增加。</li><li id="6ecf" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated"><strong class="kb jd">随着权重更新使误差值更接近全局最小值，导数值正在减少</strong>。从几何角度来看，这也是正确的，因为切线的<strong class="kb jd">“斜率”也随着每次迭代变得不那么陡</strong>。</li><li id="248b" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated">点与点之间的距离称为<strong class="kb jd">梯度步长</strong>。请注意，导数值远大于 x 轴上的重量值。如果第一个陈述为真，从重量中减去导数值不会以如此小的数字改变重量值。这是因为<strong class="kb jd">当使用梯度下降更新参数时，只需要使用导数的很小一部分来保持数值稳定性，并且不会跳过全局最小值</strong>。应该使用导数的哪一部分或者梯度步长应该有多大由<strong class="kb jd">学习速率</strong>超参数决定。</li></ul><p id="f3d6" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">当学习率太大时，数值开始从曲线反弹，误差无限增加:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi on"><img src="../Images/e01c3bc919aa4c3b1678d74e8d4aca63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*-hteLTxxRhVDAwhNPWvXzA.gif"/></div></div></figure><p id="8443" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果模型依赖于两个参数而不是一个参数，那么为了显示误差值，需要额外的轴(用于附加参数)。这自动将可视化从 2D 平面带到 3D 表面。下面是在网上找到的一个很好的例子，说明这种错误表面可能是什么样子:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi oo"><img src="../Images/e8bf481ab7b89935459cbae9f9b36ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*7VyTVnlVj2Ooqa3MRSRFEQ.gif"/></div></div><figcaption class="iv iw gj gh gi ix iy bd b be z dk">Source: <a class="ae iz" rel="noopener" target="_blank" href="/improving-vanilla-gradient-descent-f9d91031ab1d">https://towardsdatascience.com/improving-vanilla-gradient-descent-f9d91031ab1d</a></figcaption></figure><p id="214f" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于两个以上的参数，可视化是非常困难和棘手的。通常，依靠诸如<a class="ae iz" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank"> PCA </a>等降维方法是很重要的。幸运的是，数学与此非常相似，可以扩展到任意数量的参数。</p><h2 id="87c0" class="nj kz jc bd la np nq dn le nr ns dp li kk nt nu lm ko nv nw lq ks nx ny lu nz bi translated">更新规则</h2><p id="d99e" class="pw-post-body-paragraph jz ka jc kb b kc lw ke kf kg lx ki kj kk ly km kn ko lz kq kr ks ma ku kv kw ij bi translated">为了更新模型参数，从而实现<strong class="kb jd">收敛</strong>，必须迭代应用以下数学公式:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi op"><img src="../Images/e2b45fdd7d1ce966a84c1cafd8546c27.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/0*wOYOnCSQ9EJccJhy.png"/></div></figure><p id="3535" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">其中:</p><ul class=""><li id="feb8" class="mg mh jc kb b kc kd kg kh kk mi ko mj ks mk kw ml mm mn mo bi translated">w’—新重量值，</li><li id="8383" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated">b’—新偏差值，</li><li id="ed41" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated">w —当前重量值，</li><li id="25d2" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated">b —电流偏置值，</li><li id="27a5" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated">α —学习率，</li><li id="ccee" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated">dJ(w_0，b)/dw_0—J 关于 w _ 0 的导数，</li><li id="f611" class="mg mh jc kb b kc mp kg mq kk mr ko ms ks mt kw ml mm mn mo bi translated">dJ(w0，b)/db—J 相对于 b 的导数。</li></ul><p id="2275" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">应该更新参数，直到成本函数值不再降低，并且如果当前模型状态已经令人满意，则可以停止整个过程。</p><h2 id="3c36" class="nj kz jc bd la np nq dn le nr ns dp li kk nt nu lm ko nv nw lq ks nx ny lu nz bi translated"><strong class="ak">实施</strong></h2><p id="7be8" class="pw-post-body-paragraph jz ka jc kb b kc lw ke kf kg lx ki kj kk ly km kn ko lz kq kr ks ma ku kv kw ij bi translated">让我们编写使用<strong class="kb jd">梯度下降</strong>算法的新<code class="fe nb nc nd ne b">train</code>函数:</p><figure class="mc md me mf gt is"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="c206" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">由于仅使用特定点的导数值<code class="fe nb nc nd ne b">0.0005</code>,参数变化很小。它接管<code class="fe nb nc nd ne b">20000</code>迭代以获得好的结果。如果输入数据事先经过<a class="ae iz" href="https://en.wikipedia.org/wiki/Normalization_(statistics)" rel="noopener ugc nofollow" target="_blank">标准化</a>，则整个过程可以加速。</p><p id="919c" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在每次迭代期间，计算每个数据样本的部分梯度，并对整个数据集求和。此后，累积的梯度被平均并用于在<strong class="kb jd">梯度步骤</strong>结束时更新参数(参数被更新的迭代)。</p><p id="9653" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在新的<code class="fe nb nc nd ne b">train</code>功能可以像以前一样使用<strong class="kb jd">:</strong></p><figure class="mc md me mf gt is"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="d70f" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">这产生以下结果:</p><pre class="mc md me mf gt nf ne ng nh aw ni bi"><span id="9d43" class="nj kz jc ne b gy nk nl l nm nn">Initial state:<br/> - error: [75870.4884482]<br/> - parameters: {'w': array([0.]), 'b': 0.0}<br/><br/>Iteration 0:<br/> - error: [13536.3070032]<br/> - parameters: {'w': array([10.17501967]), 'b': array([0.17843399])}<br/><br/>Iteration 4000:<br/> - error: [1737.28457739]<br/> - parameters: {'w': array([7.09101188]), 'b': array([10.96966037])}<br/><br/>Iteration 8000:<br/> - error: [1707.33242182]<br/> - parameters: {'w': array([6.9583785]), 'b': array([18.67110985])}<br/><br/>Iteration 12000:<br/> - error: [1692.21685452]<br/> - parameters: {'w': array([6.86415678]), 'b': array([24.14215949])}<br/><br/>Iteration 16000:<br/> - error: [1684.5886765]<br/> - parameters: {'w': array([6.79722241]), 'b': array([28.02875048])}<br/><br/>Final state:<br/> - error: [1680.73973307]<br/> - parameters: {'w': array([6.74968272]), 'b': array([30.78917543])}</span></pre><p id="3192" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">比较这两种训练方法没有意义。蛮力训练的主要问题是效率低下。<strong class="kb jd">在梯度下降算法的情况下，模型迭代地达到收敛</strong> <strong class="kb jd">，不需要检查所有可能的修改。在处理复杂的问题——高维数据时，这将带来巨大的不同。</strong></p><p id="7d01" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">最后让我们看看模型投影:</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi no"><img src="../Images/0e18aec1186c4a61b29706fefdefc9d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c4xGhvog8yd0fHhDyX0J9w.png"/></div></div><figcaption class="iv iw gj gh gi ix iy bd b be z dk">Model projection after training. Code used to create the chart is available <a class="ae iz" href="https://gist.github.com/FisherKK/ff1489616c25f2295c594040dd9212c3" rel="noopener ugc nofollow" target="_blank">here</a>.</figcaption></figure><h1 id="cdc9" class="ky kz jc bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">摘要</h1><p id="0201" class="pw-post-body-paragraph jz ka jc kb b kc lw ke kf kg lx ki kj kk ly km kn ko lz kq kr ks ma ku kv kw ij bi translated">我们非常接近创建我们的第一个机器学习模型——线性回归，用 Python 从头开始编写，能够预测克拉科夫公寓的价格。利用已经提供的知识，你可以自己创造最终的解决方案！</p><p id="30e7" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在本文中，我已经向您展示了<strong class="kb jd">训练循环如何工作</strong>以及<strong class="kb jd">如何使用成本函数</strong>来找到模型的最佳参数。然后指出了现有方法的诸多缺点。在下一部分，我们解释了<strong class="kb jd">什么是导数，以及它在</strong>中的用途。我已经从理论、数值和几何角度告诉过你什么是<strong class="kb jd">梯度下降算法，</strong>使用<strong class="kb jd"> </strong>均方误差函数示例作为解释的帮助。最后，我已经把解释过的概念变成了代码，并用它来训练我们的模型的单一特征——公寓大小。</p><p id="b122" class="pw-post-body-paragraph jz ka jc kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们现在缺的是… <strong class="kb jd">泛化</strong>。所有的概念和代码片段都是针对一维数据的简单问题而提出的。这种简化当然是有意的。在下一篇文章中，<strong class="kb jd">,我将向你展示如何重构所有这些概念，使它们适用于任何数量的特性，并且永远不需要再次编辑！</strong>通过矩阵乘法可以实现，整个过程称为<strong class="kb jd">向量化</strong>。</p><h1 id="ea08" class="ky kz jc bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">下一篇文章</h1><p id="a08b" class="pw-post-body-paragraph jz ka jc kb b kc lw ke kf kg lx ki kj kk ly km kn ko lz kq kr ks ma ku kv kw ij bi translated">期待下一篇<strong class="kb jd">2018 年 8 月 30 日左右</strong>。</p></div></div>    
</body>
</html>