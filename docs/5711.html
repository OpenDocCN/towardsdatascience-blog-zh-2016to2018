<html>
<head>
<title>SuperConvergence — Quick training with inbuilt regularization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超收敛——内置正则化的快速训练</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/superconvergence-with-inbuilt-regularization-49f914173cd8?source=collection_archive---------13-----------------------#2018-11-04">https://towardsdatascience.com/superconvergence-with-inbuilt-regularization-49f914173cd8?source=collection_archive---------13-----------------------#2018-11-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="e272" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你有像我一样在训练大型神经网络时陷入局部极小值的恐惧症吗？等待长时间用大量的 epocs 训练网络，以获得对你的测试数据的良好准确性。</p><p id="38ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">并且仍然得到一个像哑巴一样坐在局部最小值中的模型，以及如何尝试越来越多的正则化方法并再次等待很长时间的开销。</p><blockquote class="kl"><p id="e4bd" class="km kn iq bd ko kp kq kr ks kt ku kk dk translated">你需要这个家伙一个超级收敛的魔术，节省你的时间，把你从深度学习实践者生活中的问题中拯救出来，如局部最小值，泛化错误，训练数据过度拟合等。</p></blockquote><blockquote class="kv kw kx"><p id="9769" class="jn jo ky jp b jq kz js jt ju la jw jx lb lc ka kb ld le ke kf lf lg ki kj kk ij bi translated"><strong class="jp ir">超收敛- </strong></p></blockquote><p id="9f75" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通常，在超收敛方法中，我们试图以大的学习速率学习到一个比传统的低学习速率学习更少的总迭代次数。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lh"><img src="../Images/c75fa54ce7dfa1863698c7745535c113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3EEgoA-u9BCK8hDE1chKrQ.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Left: getting fast progress in starting, Right: in valley areas very little progress on a large number of epochs</strong></figcaption></figure><p id="2e07" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从较小的学习速率开始，在正确的方向上获取梯度，然后我们增加学习速率以快速通过平坦的山谷区域，在平坦区域之后，我们再次使用较低的学习速率来收敛到最佳损失值。这样在更少的时间内找到一个更好的超收敛解。</p><p id="51e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是在固定学习率的情况下，我们可能会在开始时迅速减少损失，但是当我们停留在山谷区域时，我们会在大量迭代中取得一些进展，如右图所示。</p><blockquote class="kv kw kx"><p id="7476" class="jn jo ky jp b jq jr js jt ju jv jw jx lb jz ka kb ld kd ke kf lf kh ki kj kk ij bi translated"><strong class="jp ir">自动正规化背后有什么魔力- </strong></p></blockquote><p id="2838" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有几种方法，如辍学，下降连接，噪声与大学习率和批量大小等。超参数这种方法给我们提供了梯度噪声。另一方面，我们可以用我们的神经网络的输入来添加一些正则化，如数据增强或添加噪声，这种正则化提供了梯度多样性。</p><h2 id="f985" class="ly lz iq bd ma mb mc dn md me mf dp mg jy mh mi mj kc mk ml mm kg mn mo mp mq bi translated">核心是，我们需要一种既能提供梯度噪声又能提供多样性的方法来训练神经网络模型以获得良好的泛化能力。</h2><p id="5f57" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">下面是实验结果来了解一下——</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/30a2956adf4774a3f645a933120c3bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*_ObAeGLPBqFRQ8ilgsIMZQ.png"/></div></figure><p id="9528" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，当我们从较低的学习率开始，但逐渐增加它们，并在训练期间达到较高的学习率时，我们将梯度噪声赋予我们的优化方法，这些方法充当我们模型的正则化器，如图所示，可能我们的训练损失增加了，但我们的验证损失降低了。最后对学习率进行退火处理，得到更平坦、更好的极小点。</p><p id="bf44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这支持了<em class="ky">【Wu et al .，2017】</em>的论文，该论文指出宽的、<strong class="jp ir">平坦的局部极小值产生比尖锐极小值概括得更好的解。</strong></p><p id="585f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以利用这一点与数据扩增，以获得梯度经验噪声和多样性。</p><blockquote class="kv kw kx"><p id="4e56" class="jn jo ky jp b jq jr js jt ju jv jw jx lb jz ka kb ld kd ke kf lf kh ki kj kk ij bi translated"><strong class="jp ir">为什么我使用这种方法而不是其他的正则化方法- </strong></p></blockquote><p id="6848" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为我已经很快完成了训练部分，这对我一直都很有效。当我第一次在<a class="ae mx" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> FastAI </strong> </a> <strong class="jp ir"> </strong>从<strong class="jp ir"> </strong> <a class="my mz ep" href="https://medium.com/u/34ab754f8c5e?source=post_page-----49f914173cd8--------------------------------" rel="noopener" target="_blank"> <strong class="jp ir">【杰瑞米·霍华德】</strong> </a> <strong class="jp ir"> </strong>中了解到这个方法，第二次看了<a class="ae mx" href="https://sgugger.github.io/" rel="noopener ugc nofollow" target="_blank"> Sylvain Gugger </a>的这个<a class="ae mx" href="https://sgugger.github.io/the-1cycle-policy.html#the-1cycle-policy" rel="noopener ugc nofollow" target="_blank">博客</a>时，我就对这个方法印象深刻。</p><p id="6191" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">为了更好地理解和实际使用，推荐阅读- </strong></p><ul class=""><li id="f228" class="na nb iq jp b jq jr ju jv jy nc kc nd kg ne kk nf ng nh ni bi translated"><a class="ae mx" href="https://sgugger.github.io/the-1cycle-policy.html#the-1cycle-policy" rel="noopener ugc nofollow" target="_blank"><em class="ky">1 周期政策</em> </a></li><li id="ce10" class="na nb iq jp b jq nj ju nk jy nl kc nm kg nn kk nf ng nh ni bi translated"><a class="ae mx" href="https://www.fast.ai/2018/07/02/adam-weight-decay/" rel="noopener ugc nofollow" target="_blank">https://www.fast.ai/2018/07/02/adam-weight-decay/</a></li><li id="09ec" class="na nb iq jp b jq nj ju nk jy nl kc nm kg nn kk nf ng nh ni bi translated"><a class="ae mx" href="https://medium.com/@lipeng2/cyclical-learning-rates-for-training-neural-networks-4de755927d46" rel="noopener">循环学习率</a></li><li id="551c" class="na nb iq jp b jq nj ju nk jy nl kc nm kg nn kk nf ng nh ni bi translated"><a class="ae mx" href="https://arxiv.org/pdf/1708.07120.pdf" rel="noopener ugc nofollow" target="_blank">超收敛:使用大学习率非常快速地训练神经网络</a></li></ul></div></div>    
</body>
</html>