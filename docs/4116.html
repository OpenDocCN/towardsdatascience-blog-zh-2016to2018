<html>
<head>
<title>A Newbie’s Guide to Stochastic Gradient Descent With Restarts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">重启随机梯度下降新手指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163?source=collection_archive---------3-----------------------#2018-07-20">https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163?source=collection_archive---------3-----------------------#2018-07-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="49b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简单地找到一个学习率来经历梯度下降将有助于最小化神经网络的损失。但是，还有其他方法可以使这一过程更顺利、更快速、更准确。</p><p id="f16d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一种技术是重启随机梯度下降(SGDR)，这是学习率退火的一种变体，它通过训练逐渐降低学习率。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/feb9778d06ae2daee4d21f86cb6c350e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*_6nUFOhjPv3ftr8CrrWANQ.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Image 1: Each step decreases in size</figcaption></figure><p id="45c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有不同的退火方法，不同的减小步长的方法。一种流行的方法是逐步降低学习率:在最初的几次迭代中简单地使用一个学习率，然后在接下来的几次迭代中降低到另一个学习率，然后在接下来的几次迭代中进一步降低学习率。另一种变型是随着每次迭代线性降低学习速率。</p><div class="km kn ko kp gt ab cb"><figure class="kx kq ky kz la lb lc paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><img src="../Images/cd19df9bac46ef65faf5141f1f4392a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*lE1Yun-ImDpxjxro1aP9Pw.png"/></div></figure><figure class="kx kq lh kz la lb lc paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><img src="../Images/2130354fcb84bdd85d2fa1ec716833a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*hlKV89B3y2YRk1BFKWRsnQ.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk li di lj lk">Left: Decrease learning rate by steps; Right: Linearly decrease learning rate</figcaption></figure></div><p id="6379" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">SGDR 使用<em class="ll">余弦退火</em>，以半余弦曲线的形式降低学习率，就像这样:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/73c804b614fcda88a6991becd6ff10a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*2NAuh6DbcrrMv4Voq5yG9A.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Image 4: Cosine Annealing</figcaption></figure><p id="d9d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是一个好方法，因为我们可以在开始时以相对较高的学习速率开始几次迭代，以快速接近局部最小值，然后随着我们接近最小值而逐渐降低学习速率，以几次小的学习速率迭代结束。</p><p id="0512" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，我们可能会发现自己处于局部极小值，其中权重的小变化会导致损失的大变化。在下面这个损失函数中，通过训练，我们已经降落在局部最小值。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi gj"><img src="../Images/4354c47bf764c42eef6a0a0a8c47fdfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0k0dUpICE5BJe6VQFmBLJA.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Image 5: Approaching a local minimum</figcaption></figure><p id="724c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是，如果我们在不同的数据集上测试这个网络，损失函数可能会略有不同。在这个局部最小值中损失函数的小移动将导致损失的大变化:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/82bc9db24683ed1c47cf590f0258b6ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*oknMWoyQAHYWEODK4aITqA.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Image 6: Slight shift in loss function resulting in a large change in loss</figcaption></figure><p id="949b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">突然间，这个局部最小值成了一个可怕的解决方案。另一方面，如果我们在这个更平坦的槽中找到了解决方案:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi gj"><img src="../Images/5bd39c50a94eeba51bcef0796fb051a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NzUKt_r6kxY3mkxfTH64Sg.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Image 7: Approaching a more stable local minimum</figcaption></figure><p id="d120" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">即使不同的数据集稍微改变了损失函数，损失也会保持相对稳定:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi lo"><img src="../Images/4a3a4d3301292dd859c6ddfb3465e950.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cSsUuu26FR1AIZ2bp2N9LA.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Image 8: Slight shift in loss function doesn’t impact loss as much</figcaption></figure><p id="aaa8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种更平的槽更好，因为它提供了精确和稳定的解决方案。它更多的是<em class="ll">广义</em>；也就是对新数据有更高的反应能力。为了找到更稳定的局部最小值，我们可以不时地增加学习率，鼓励模型在处于陡峭的低谷时从一个局部最小值“跳到”另一个局部最小值。这是 SGDR 的“重新开始”。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/5a0fe3c8df462e05c50f0a26e7fb2419.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*Hv8kFl7sG9YZcqn0KmxxKg.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Image 9: Increasing the learning rates every few iterations to “restart” the gradient descent // <a class="ae lq" href="http://course.fast.ai" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="125a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上图的第一个“半余弦”周期中，我们下降到局部最小值，如下所示:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi gj"><img src="../Images/4354c47bf764c42eef6a0a0a8c47fdfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0k0dUpICE5BJe6VQFmBLJA.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Image 10: Descending into local minimum</figcaption></figure><p id="75ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">突然，我们提高了学习率，在下一次迭代中迈出了一大步:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi gj"><img src="../Images/58aad2a48e5ba2a06158a739253ad870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b9sXtuhVH_T9FNcpWoEyxQ.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Image 11: “Restarting”</figcaption></figure><p id="fd9e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在第二个“半余弦”周期，我们下降到另一个局部最小值。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi gj"><img src="../Images/9efd98a616035def9288a321c3babade.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5T8mc-cCBabeGYVSG3VPBw.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Image 12: Descending into a more stable local minimum</figcaption></figure><p id="c033" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后我们再次大幅提高学习率。只是这一次，因为我们处于损失函数的一个更稳定的区域，这种“重新开始”并没有把我们带出局部最小值:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi gj"><img src="../Images/d9b2410d7adab6baefc5c8cb59b5a1d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8DGUx04imRUuH1G5_F7gyw.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Image 13: “Restarting” doesn’t take us out of this local minimum</figcaption></figure><p id="05ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们再次逐渐降低学习率，直到我们最小化损失函数，找到稳定的解决方案。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi gj"><img src="../Images/139b89365f8ddae2ff3c0ea0ee4e0637.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F9cCD6ajU-MLS3_67mMnLQ.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Image 14: Settling into this local minimum</figcaption></figure><p id="6384" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些“周期”中的每一个都被称为一个<em class="ll">周期</em>，在训练我们的神经网络时，我们可以自己选择周期的数量和每个周期的长度。还有，既然我们是逐渐降低学习率，那么最好从略大于最优学习率的学习率开始(你是怎么确定最优学习率的？点击<a class="ae lq" href="https://thisgirlreina.wordpress.com/2018/07/10/gradient-descent-and-learning-rate/" rel="noopener ugc nofollow" target="_blank">这里</a>。选择一个足够大的学习率也很重要，以允许函数在“复位”时跳到不同的最小值。</p><p id="81b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了在 3D 中可视化，这里有两个模型:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi lr"><img src="../Images/1dbc1b7c953c910ec2b0170a1bb227a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kbP1fkMFrcjwBO3NVP9OfQ.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Image 15: Stochastic Gradient Descent with Restarts // <a class="ae lq" href="https://arxiv.org/pdf/1704.00109.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="d952" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">左边的模型缓慢下降到一个局部最小值，而右边的模型在几个局部最小值之间跳跃，寻找一个更稳定的。</p><p id="8ca6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您也可以在每次循环后增加循环长度，如下所示:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="gh gi ls"><img src="../Images/8c970ec6c6d5d1abe09d3f0c1850846a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GjdEoZcEcQF19JS1y8nVYw.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Image 16: Increasing cycle length // <a class="ae lq" href="http://course.fast.ai" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="6fbb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这似乎输出了更好、更准确的结果，因为它允许我们更精确地找到稳定区域中的最小值点。</p></div></div>    
</body>
</html>