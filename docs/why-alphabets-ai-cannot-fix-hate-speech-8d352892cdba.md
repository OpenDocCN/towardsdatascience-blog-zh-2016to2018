# 为什么 Alphabet 的 AI 无法识别仇恨言论

> 原文：<https://towardsdatascience.com/why-alphabets-ai-cannot-fix-hate-speech-8d352892cdba?source=collection_archive---------8----------------------->

最近，Alphabet(谷歌的母公司)一直在开发一种基于人工智能的解决方案，以检测仇恨言论。这个解决方案([被称为 Perspective](http://perspectiveapi.com/) )正在作为一种打击社区中的在线钓鱼和恶意攻击的方式进行营销，并且已经开放给公众在他们自己的网站上使用。

然而，尽管这个系统很有趣，但它有一个重大缺陷:

[它真的不起作用。](https://qz.com/918640/alphabets-hate-fighting-ai-doesnt-understand-hate-yet/)事实上，尽管谷歌和它的合作伙伴都在谈论它的品质，但这个系统实际上似乎无法决定一篇帖子是否真的是一次可恶的人身攻击。例如，正如《技术评论》的[大卫·奥尔巴奇发现的](https://www.facebook.com/david.auerbach/posts/10101520012739084)，像这样的短语:

1.  “垃圾车”
2.  “你不是种族主义者”
3.  “很少有穆斯林是恐怖分子的威胁”
4.  和“我他妈的爱你，伙计。生日快乐"

被标记为有毒。此外，后者实际上被标记为比“唐纳德·特朗普是一个俗气的小丑”和“现在是种族战争”等明显的侮辱更具毒性。当你试图阻止仇恨而不仅仅是人们在互联网上骂人时，这不是很好。

那么为什么会这样呢？谷歌怎么会犯这么大的错误？

我认为这可以归结为人类和机器都会犯的一个简单的错误。那个错误？

假设一个论点的措辞方式足以告诉你它是一种侮辱还是一种无害的信息。或者换句话说，假设一个论点的有效性与它的文明程度有关。

然而，事实并非如此。一个巨魔非常有礼貌，但仍然让许多人感到不安，这是完全合理的，就像有意义的批评以极其简洁的咆哮的形式出现一样。

但是机器很难检测到。对于像这样的系统来说，真正理解内容和它被使用的上下文是非常困难的，可能超出了当今许多机器学习系统的能力。

因此，Alphabet 和合作伙伴基本上回到了情绪分析，并认为这对于检测在线恶霸和巨魔来说“足够好了”。显然不是。

尽管如此，如果你需要一些例子呢？这是 YouTube 上游戏视频世界中的两个。

第一种显然是建设性的批评，措辞非常礼貌，非常正式。这是马克·布朗“老板钥匙”系列的一部分，显然会被每个人，无论是人还是机器，视为“建设性的批评”:

据推测，这里提到的人工智能会意识到它不是“有毒”的，并让它去吧。很公平。

另一方面，虽然有一些很好的关于游戏或游戏事件的建设性视频，但它们的设置并不那么礼貌。比如，典型的愤怒的电子游戏书呆子评论:

他不文明，他的视频充满了脏话，但这是他性格的一部分。这是一种行为，视频的实际内容通常是对游戏或相关配件的有意义的批评。

但我怀疑这个人工智能不会明白。它会看到所有的脏话和侮辱，并立即使用它的“仇恨言论”或“trolling”或“非建设性”。同样的事情可能会发生在大礼帽游戏人和他关于 WatchMojo 肮脏行为的视频上:

这是一个关于一家窃取他人作品的公司的刻薄但有建设性的视频。对于一个正常人来说，这对于像 YouTube、脸书或 Twitter 这样的频道来说是再好不过了。

但对于这种人工智能，我怀疑它会被标记为“有毒”。因为人工智能无法知道它的好内容，只知道视频主持人使用的语气“不礼貌”。

信息发送的方式并不是人工智能出错的唯一原因。它也无法确定某人是种族主义者/性别主义者/什么的，以及某人保护某人(或他们自己)免受这样的攻击。

例如，回到脸书上发布的列表。许多被标记的词并不是对某人的攻击，但是试图说像新纳粹攻击这样的话是不好的。“我认为你是种族主义者”怎么会有毒呢？

它不是。但是机器人系统不(也不能)在乎。这就像学校的零容忍政策。在那里，他们不是真正调查欺凌行为或弄清楚谁在攻击谁，而是立即暂停或驱逐任何被发现“打架”的人，不管这是否是自卫。

所以，是的，这个人工智能不起作用，它不起作用是因为它既不能理解消极语气和非建设性论点之间的区别，也不能理解人身攻击和某人防御攻击之间的区别。

先别用，以现在的形式基本上比没用还不如。