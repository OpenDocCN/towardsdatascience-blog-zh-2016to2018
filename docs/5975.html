<html>
<head>
<title>Forecasting Air Pollution with Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用递归神经网络预测空气污染</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/forecasting-air-pollution-with-recurrent-neural-networks-ffb095763a5c?source=collection_archive---------10-----------------------#2018-11-19">https://towardsdatascience.com/forecasting-air-pollution-with-recurrent-neural-networks-ffb095763a5c?source=collection_archive---------10-----------------------#2018-11-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/25276fe1f9ea23da6e3d236736f192ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6XEYi4xnCQQGrEdC"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@carolinachadwick?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Carolina Pimenta</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ba05" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在完成了公民科学项目之后，我想了解更多关于空气污染的知识，看看我是否可以用它来做一个数据科学项目。在<a class="ae kc" href="https://www.eea.europa.eu/data-and-maps/data/aqereporting-8" rel="noopener ugc nofollow" target="_blank">欧洲环境署</a>的网站上，你可以找到大量关于空气污染的数据和信息。</p><p id="8972" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这本笔记本中，我们将关注<strong class="kf ir">比利时</strong>的空气质量，更具体地说是<strong class="kf ir">二氧化硫(SO2) </strong>造成的污染。数据可以通过<a class="ae kc" href="https://www.eea.europa.eu/data-and-maps/data/aqereporting-2/be" rel="noopener ugc nofollow" target="_blank">https://www . EEA . Europa . eu/data-and-maps/data/aqe reporting-2/be</a>下载。zip 文件包含不同空气污染物和聚集水平的单独文件。第一个数字代表污染物 ID，如<a class="ae kc" href="http://dd.eionet.europa.eu/vocabulary/aq/pollutant" rel="noopener ugc nofollow" target="_blank">词汇表</a>中所述。这个笔记本用的文件是<strong class="kf ir">BE _ 1 _ 2013–2015 _ aggregated _ time series . CSV</strong>这是比利时的 SO2 污染情况，但是你也可以找到其他欧洲国家的类似数据。</p><p id="028f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae kc" href="https://www.eea.europa.eu/data-and-maps/data/aqereporting-2/be." rel="noopener ugc nofollow" target="_blank">数据下载页面</a>上可以找到 CSV 文件中字段的描述。更多关于空气污染物的背景信息可以在<a class="ae kc" href="https://nl.wikipedia.org/wiki/Luchtvervuiling" rel="noopener ugc nofollow" target="_blank">维基百科</a>上找到。</p><h1 id="61c9" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">项目设置</h1><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="b13f" class="mi lc iq me b gy mj mk l ml mm"><em class="mn"># Importing packages</em><br/><strong class="me ir">from</strong> <strong class="me ir">pathlib</strong> <strong class="me ir">import</strong> Path<br/><strong class="me ir">import</strong> <strong class="me ir">pandas</strong> <strong class="me ir">as</strong> <strong class="me ir">pd</strong><br/><strong class="me ir">import</strong> <strong class="me ir">numpy</strong> <strong class="me ir">as</strong> <strong class="me ir">np</strong><br/><strong class="me ir">import</strong> <strong class="me ir">pandas_profiling</strong><br/>%<strong class="me ir">matplotlib</strong> inline<br/><strong class="me ir">import</strong> <strong class="me ir">matplotlib.pyplot</strong> <strong class="me ir">as</strong> <strong class="me ir">plt</strong><br/><strong class="me ir">import</strong> <strong class="me ir">warnings</strong><br/>warnings.simplefilter(action = 'ignore', category = <strong class="me ir">FutureWarning</strong>)<br/><strong class="me ir">from</strong> <strong class="me ir">sklearn.preprocessing</strong> <strong class="me ir">import</strong> MinMaxScaler<br/><br/><strong class="me ir">from</strong> <strong class="me ir">keras.preprocessing.sequence</strong> <strong class="me ir">import</strong> TimeseriesGenerator<br/><strong class="me ir">from</strong> <strong class="me ir">keras.models</strong> <strong class="me ir">import</strong> Sequential<br/><strong class="me ir">from</strong> <strong class="me ir">keras.layers</strong> <strong class="me ir">import</strong> Dense, LSTM, SimpleRNN<br/><strong class="me ir">from</strong> <strong class="me ir">keras.optimizers</strong> <strong class="me ir">import</strong> RMSprop<br/><strong class="me ir">from</strong> <strong class="me ir">keras.callbacks</strong> <strong class="me ir">import</strong> ModelCheckpoint, EarlyStopping<br/><strong class="me ir">from</strong> <strong class="me ir">keras.models</strong> <strong class="me ir">import</strong> model_from_json<br/><br/><em class="mn"># Setting the project directory</em><br/>project_dir = Path('/Users/bertcarremans/Data Science/Projecten/air_pollution_forecasting')</span></pre></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h1 id="5f15" class="lb lc iq bd ld le mv lg lh li mw lk ll lm mx lo lp lq my ls lt lu mz lw lx ly bi translated">加载数据</h1><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="67da" class="mi lc iq me b gy mj mk l ml mm">date_vars = ['DatetimeBegin','DatetimeEnd']<br/><br/>agg_ts = pd.read_csv(project_dir / 'data/raw/BE_1_2013-2015_aggregated_timeseries.csv', sep='<strong class="me ir">\t</strong>', parse_dates=date_vars, date_parser=pd.to_datetime)<br/>meta = pd.read_csv(project_dir / 'data/raw/BE_2013-2015_metadata.csv', sep='<strong class="me ir">\t</strong>')<br/><br/>print('aggregated timeseries shape:<strong class="me ir">{}</strong>'.format(agg_ts.shape))<br/>print('metadata shape:<strong class="me ir">{}</strong>'.format(meta.shape))</span></pre><h1 id="7007" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数据探索</h1><p id="187f" class="pw-post-body-paragraph kd ke iq kf b kg na ki kj kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">让我们使用<strong class="kf ir"> pandas_profiling </strong>来检查数据。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="636b" class="mi lc iq me b gy mj mk l ml mm">pandas_profiling.ProfileReport(agg_ts)</span></pre><p id="135b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了不与图表混淆，我不会在本文中显示 pandas_profiling 的输出。但是你可以在我的<a class="ae kc" href="https://github.com/bertcarremans/air_pollution_forecasting" rel="noopener ugc nofollow" target="_blank"> GitHub 回购</a>里找到。</p><p id="6167" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">pandas_profiling 报告告诉我们:</p><ul class=""><li id="3e10" class="nf ng iq kf b kg kh kk kl ko nh ks ni kw nj la nk nl nm nn bi translated">有 6 个常量变量。我们可以把这些从数据集中去掉。</li><li id="d5a4" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">不存在缺失值，因此我们可能不需要应用插补。</li><li id="1e15" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">有一些零，但这可能是完全正常的。另一方面，这些变量有一些极值，可能是空气污染的不正确记录。</li><li id="dbf3" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">共有 53 个<strong class="kf ir">空气质量站</strong>，可能与<strong class="kf ir">采样点</strong>相同。airqualitystationoicode 只是 AirQualityStation 的一个较短的代码，因此变量也可以被删除。</li><li id="fd73" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated"><strong class="kf ir"> AirQualityNetwork </strong>有 3 个值(布鲁塞尔、佛兰德和瓦隆)。大多数测量来自佛兰德斯。</li><li id="60c1" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated"><strong class="kf ir"> DataAggregationProcess </strong>:大多数行包含作为一天测量(P1D)的 24 小时平均值聚集的数据。关于其他值的更多信息可以在<a class="ae kc" href="http://dd.eionet.europa.eu/vocabulary/aq/aggregationprocess" rel="noopener ugc nofollow" target="_blank">这里</a>找到。在这个项目中，我们将只考虑 P1D 值。</li><li id="0d68" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated"><strong class="kf ir"> DataCapture </strong>:平均周期内有效测量时间相对于总测量时间(时间覆盖)的比例，以百分比表示。几乎所有行都有大约 100%的有效测量时间。一些行的数据捕获率略低于 100%。</li><li id="ca60" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated"><strong class="kf ir"> DataCoverage </strong>:平均周期内聚合过程中包含的有效测量的比例，以百分比表示。在这个数据集中，我们至少有 75%。根据该变量<a class="ae kc" href="https://www.eea.europa.eu/data-and-maps/data/aqereporting-2/be" rel="noopener ugc nofollow" target="_blank">的定义，低于 75%的值不应包括在空气质量评估中，这解释了为什么这些行不出现在数据集中。</a></li><li id="2f20" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated"><strong class="kf ir"> TimeCoverage </strong>:与数据覆盖率高度相关，将从数据中删除。</li><li id="dff4" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated"><strong class="kf ir">单位空气污染等级</strong> : 423 行有一个单位<em class="mn">计数</em>。为了有一个一致的目标变量，我们将删除这种类型的单位的记录。</li><li id="d6b3" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated"><strong class="kf ir">日期时间开始</strong>和<strong class="kf ir">日期时间结束</strong>:柱状图在这里没有提供足够的细节。这需要进一步分析。</li></ul><h1 id="a8f7" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">日期时间开始和日期时间结束</h1><p id="5260" class="pw-post-body-paragraph kd ke iq kf b kg na ki kj kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">pandas_profiling 中的直方图结合了每个 bin 的多个天数。让我们看看这些变量在日常水平上的表现。</p><h2 id="b0df" class="mi lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">每个日期多个汇总级别</h2><ul class=""><li id="0262" class="nf ng iq kf b kg na kk nb ko oe ks of kw og la nk nl nm nn bi translated"><strong class="kf ir">datetime begin</strong>:2013、2014、2015 年 1 月 1 日和 2013、2014 年 10 月 1 日的大量记录。</li><li id="9d68" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated"><strong class="kf ir">datetime end</strong>:2014、2015、2016 年 1 月 1 日和 2014、2015 年 4 月 1 日的大量记录。</li></ul><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="a5ab" class="mi lc iq me b gy mj mk l ml mm">plt.figure(figsize=(20,6))<br/>plt.plot(agg_ts.groupby('DatetimeBegin').count(), 'o', color='skyblue')<br/>plt.title('Nb of measurements per DatetimeBegin')<br/>plt.ylabel('number of measurements')<br/>plt.xlabel('DatetimeBegin')<br/>plt.show()</span></pre><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oh"><img src="../Images/28a9c214621cb236113dfb241e7a14c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nP33p0hVPjLpxTFAwDQJyQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Number of rows per date</figcaption></figure><p id="d91e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">记录数量中的异常值与多个聚合级别(DataAggregationProcess)有关。这些日期的 DataAggregationProcess 中的值反映了 DatetimeBegin 和 DatetimeEnd 之间的时间段。例如，2013 年 1 月 1 日是一年测量期的开始日期，直到 2014 年 1 月 1 日。</p><p id="9d65" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于我们只对每日汇总级别感兴趣，<strong class="kf ir">过滤掉其他汇总级别</strong>将解决这个问题。为此，我们也可以删除 DatetimeEnd。</p><h2 id="9634" class="mi lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">每日汇总级别缺少时间步长</h2><p id="31be" class="pw-post-body-paragraph kd ke iq kf b kg na ki kj kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">正如我们在下面看到的，<strong class="kf ir">并非所有采样点都有三年期间</strong>内所有日期时间 Begin 的数据。这是 DataCoverage 变量低于 75%的最有可能的日子。所以在这些日子里，我们没有足够的有效测量。在本笔记本的后面，我们将使用前几天的测量值来预测当天的污染情况。</p><p id="9acd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了获得类似大小的时间步长，我们需要为每个采样点插入缺失的 DatetimeBegin 行。我们将<strong class="kf ir">用有效数据</strong>插入第二天的测量数据。</p><p id="9f39" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其次，我们将<strong class="kf ir">删除太多丢失时间步长的采样点</strong>。这里我们将任意取 1.000 个时间步长作为所需时间步长的最小数目。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="b216" class="mi lc iq me b gy mj mk l ml mm">ser_avail_days = agg_ts.groupby('SamplingPoint').nunique()['DatetimeBegin']<br/>plt.figure(figsize=(8,4))<br/>plt.hist(ser_avail_days.sort_values(ascending=<strong class="me ir">False</strong>))<br/>plt.ylabel('Nb SamplingPoints')<br/>plt.xlabel('Nb of Unique DatetimeBegin')<br/>plt.title('Distribution of Samplingpoints by the Nb of available measurement days')<br/>plt.show()</span></pre><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/bfebb8964a65538d971de73c7941cc15.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*tcdKTqJJjjtBZ4QbNN2uzg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Distribution of SamplingPoints by the number of available measurement days</figcaption></figure><h1 id="d210" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">数据准备</h1><h2 id="d270" class="mi lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">数据清理</h2><p id="a2ab" class="pw-post-body-paragraph kd ke iq kf b kg na ki kj kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">基于数据探索，我们将执行以下操作来清理数据:</p><ul class=""><li id="6b78" class="nf ng iq kf b kg kh kk kl ko nh ks ni kw nj la nk nl nm nn bi translated">仅保留 P1D 的 DataAggregationProcess 记录</li><li id="b985" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">正在删除计数为 UnitOfAirPollutionLevel 的记录</li><li id="43f4" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">删除一元变量和其他冗余变量</li><li id="8b69" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">移除少于 1000 个测量日的采样点</li></ul><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="63e2" class="mi lc iq me b gy mj mk l ml mm">df = agg_ts.loc[agg_ts.DataAggregationProcess=='P1D', :] <br/>df = df.loc[df.UnitOfAirPollutionLevel!='count', :]<br/>df = df.loc[df.SamplingPoint.isin(ser_avail_days[ser_avail_days.values &gt;= 1000].index), :]<br/>vars_to_drop = ['AirPollutant','AirPollutantCode','Countrycode','Namespace','TimeCoverage','Validity','Verification','AirQualityStation',<br/>               'AirQualityStationEoICode','DataAggregationProcess','UnitOfAirPollutionLevel', 'DatetimeEnd', 'AirQualityNetwork',<br/>               'DataCapture', 'DataCoverage']<br/>df.drop(columns=vars_to_drop, axis='columns', inplace=<strong class="me ir">True</strong>)</span></pre><h2 id="4a20" class="mi lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">为缺失的时间步长插入行</h2><p id="f25a" class="pw-post-body-paragraph kd ke iq kf b kg na ki kj kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">对于每个采样点，我们将首先插入(空)没有 DatetimeBegin 的行。这可以通过在最小和最大 DatetimeBegin 之间的范围内创建包含所有采样点的完整多索引来实现。然后，<strong class="kf ir"> reindex </strong>将插入丢失的行，但是对于列使用 NaN。</p><p id="b24a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其次，我们使用<strong class="kf ir"> bfill </strong>并指定用有效数据的下一行的值来估算缺失值。bfill 方法应用于 groupby 对象，以将回填限制在每个采样点的行内。这样，我们就不会使用另一个采样点的值来填充缺失的值。</p><p id="1550" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">测试该操作是否正确工作的样本点是日期为<em class="mn">2013–01–29</em>的<em class="mn"> SPO-BETR223_00001_100 </em>。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="0a30" class="mi lc iq me b gy mj mk l ml mm">dates = list(pd.period_range(min(df.DatetimeBegin), max(df.DatetimeBegin), freq='D').values)<br/>samplingpoints = list(df.SamplingPoint.unique())<br/><br/>new_idx = []<br/><strong class="me ir">for</strong> sp <strong class="me ir">in</strong> samplingpoints:<br/>    <strong class="me ir">for</strong> d <strong class="me ir">in</strong> dates:<br/>        new_idx.append((sp, np.datetime64(d)))<br/><br/>df.set_index(keys=['SamplingPoint', 'DatetimeBegin'], inplace=<strong class="me ir">True</strong>)<br/>df.sort_index(inplace=<strong class="me ir">True</strong>)<br/>df = df.reindex(new_idx)<br/><em class="mn">#print(df.loc['SPO-BETR223_00001_100','2013-01-29'])  # should contain NaN for the columns</em><br/><br/>df['AirPollutionLevel'] = df.groupby(level=0).AirPollutionLevel.bfill().fillna(0)<br/><em class="mn">#print(df.loc['SPO-BETR223_00001_100','2013-01-29'])  # NaN are replaced by values of 2013-01-30</em><br/>print('<strong class="me ir">{}</strong> missing values'.format(df.isnull().sum().sum()))</span></pre><h2 id="9b85" class="mi lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">处理多个时间序列</h2><p id="399c" class="pw-post-body-paragraph kd ke iq kf b kg na ki kj kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">好了，现在我们有了一个干净的数据集，不包含任何丢失的值。使这个数据集特别的一个方面是我们有<strong class="kf ir">多个采样点</strong>的数据。所以我们有多个时间序列。</p><p id="3483" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一种处理方法是为采样点创建<strong class="kf ir">虚拟变量，并使用所有记录来训练模型。另一种方法是为每个采样点</strong>建立一个<strong class="kf ir">单独的模型。在本笔记本中，我们将采用后者。但是，我们将限制笔记本电脑只能在一个采样点上这样做。但是同样的逻辑可以应用于每个采样点。</strong></p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="ecf6" class="mi lc iq me b gy mj mk l ml mm">df = df.loc['SPO-BETR223_00001_100',:]</span></pre><h2 id="c611" class="mi lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">分割训练、测试和验证集</h2><p id="34fe" class="pw-post-body-paragraph kd ke iq kf b kg na ki kj kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">为了评估模型的性能，我们划分了一个测试集。在训练阶段将不使用该测试集。</p><ul class=""><li id="abc0" class="nf ng iq kf b kg kh kk kl ko nh ks ni kw nj la nk nl nm nn bi translated">列车组:截至 2014 年 7 月的数据</li><li id="a344" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">验证集:2014 年 7 月至 2015 年 1 月之间的 6 个月</li><li id="d9ed" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">测试集:2015 年的数据</li></ul><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="0e75" class="mi lc iq me b gy mj mk l ml mm">train = df.query('DatetimeBegin &lt; "2014-07-01"')<br/>valid = df.query('DatetimeBegin &gt;= "2014-07-01" and DatetimeBegin &lt; "2015-01-01"')<br/>test = df.query('DatetimeBegin &gt;= "2015-01-01"')</span></pre><h2 id="8202" class="mi lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">缩放比例</h2><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="f615" class="mi lc iq me b gy mj mk l ml mm"><em class="mn"># Save column names and indices to use when storing as csv</em><br/>cols = train.columns<br/>train_idx = train.index<br/>valid_idx = valid.index<br/>test_idx = test.index<br/><br/><em class="mn"># normalize the dataset</em><br/>scaler = MinMaxScaler(feature_range=(0, 1))<br/>train = scaler.fit_transform(train)<br/>valid = scaler.transform(valid)<br/>test = scaler.transform(test)</span></pre><h2 id="3a48" class="mi lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">保存已处理的数据集</h2><p id="1f57" class="pw-post-body-paragraph kd ke iq kf b kg na ki kj kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">这样，我们就不需要在每次重新运行笔记本时重复预处理。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="a768" class="mi lc iq me b gy mj mk l ml mm">train = pd.DataFrame(train, columns=cols, index=train_idx)<br/>valid = pd.DataFrame(valid, columns=cols, index=valid_idx)<br/>test = pd.DataFrame(test, columns=cols, index=test_idx)<br/><br/>train.to_csv('../data/processed/train.csv')<br/>valid.to_csv('../data/processed/valid.csv')<br/>test.to_csv('../data/processed/test.csv')</span></pre><h1 id="e28d" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">建模</h1><p id="b89d" class="pw-post-body-paragraph kd ke iq kf b kg na ki kj kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">首先，我们读入经过处理的数据集。其次，我们创建一个函数来绘制我们将构建的不同模型的训练和验证损失。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="63f7" class="mi lc iq me b gy mj mk l ml mm">train = pd.read_csv('../data/processed/train.csv', header=0, index_col=0).values.astype('float32')<br/>valid = pd.read_csv('../data/processed/valid.csv', header=0, index_col=0).values.astype('float32')<br/>test = pd.read_csv('../data/processed/test.csv', header=0, index_col=0).values.astype('float32')<br/><br/><strong class="me ir">def</strong> plot_loss(history, title):<br/>    plt.figure(figsize=(10,6))<br/>    plt.plot(history.history['loss'], label='Train')<br/>    plt.plot(history.history['val_loss'], label='Validation')<br/>    plt.title(title)<br/>    plt.xlabel('Nb Epochs')<br/>    plt.ylabel('Loss')<br/>    plt.legend()<br/>    plt.show()<br/>    <br/>    val_loss = history.history['val_loss']<br/>    min_idx = np.argmin(val_loss)<br/>    min_val_loss = val_loss[min_idx]<br/>    print('Minimum validation loss of <strong class="me ir">{}</strong> reached at epoch <strong class="me ir">{}</strong>'.format(min_val_loss, min_idx))</span></pre><h2 id="986f" class="mi lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">使用时间序列生成器准备数据</h2><p id="b24c" class="pw-post-body-paragraph kd ke iq kf b kg na ki kj kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">Keras 的<a class="ae kc" href="https://keras.io/preprocessing/sequence/#timeseriesgenerator" rel="noopener ugc nofollow" target="_blank"> TimeseriesGenerator 帮助我们以正确的格式构建数据用于建模。</a></p><ul class=""><li id="d060" class="nf ng iq kf b kg kh kk kl ko nh ks ni kw nj la nk nl nm nn bi translated"><strong class="kf ir">长度:</strong>生成序列中的时间步长数。这里我们想回顾任意数量的<em class="mn"> n_lag </em>时间步。实际上，n_lag 可能取决于如何使用预测。假设比利时政府可以采取一些措施来减少采样点周围的 SO2 污染(例如在一定时间内禁止柴油车进入某个城市)。假设政府在纠正措施生效前需要 14 天。那么将 n_lag 设置为 14 是有意义的。</li><li id="0c10" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated"><strong class="kf ir"> sampling_rate: </strong>生成序列中连续时间步之间的时间步数。我们希望保留所有的时间步长，因此我们将其保留为默认值 1。</li><li id="100a" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated"><strong class="kf ir">步距:</strong>该参数影响生成的序列重叠的程度。由于我们没有太多的数据，我们将其保留为默认值 1。这意味着相继生成的两个序列与除一个时间步长之外的所有时间步长重叠。</li><li id="a779" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated"><strong class="kf ir"> batch_size: </strong>每批生成的序列数</li></ul><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="ab04" class="mi lc iq me b gy mj mk l ml mm">n_lag = 14<br/><br/>train_data_gen = TimeseriesGenerator(train, train, length=n_lag, sampling_rate=1, stride=1, batch_size = 5)<br/>valid_data_gen = TimeseriesGenerator(train, train, length=n_lag, sampling_rate=1, stride=1, batch_size = 1)<br/>test_data_gen = TimeseriesGenerator(test, test, length=n_lag, sampling_rate=1, stride=1, batch_size = 1)</span></pre><h2 id="bb83" class="mi lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">递归神经网络</h2><p id="4a71" class="pw-post-body-paragraph kd ke iq kf b kg na ki kj kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated"><strong class="kf ir">传统的神经网络没有记忆</strong>。因此，在处理当前输入时，它们不会考虑以前的输入。在时序数据集中，如时间序列，先前时间步骤的信息通常与当前步骤的预测相关。因此需要保持关于先前时间步的状态<strong class="kf ir">和状态</strong>。</p><p id="7f99" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们的例子中，时间 t 的空气污染可能受到先前时间点的空气污染的影响。所以我们需要考虑到这一点。递归神经网络或 rnn 有一个内部循环，通过它它们保持先前时间步长的状态。然后，该状态用于当前时间步长中的预测。当一个新的序列被处理时，该状态被复位。关于 RNNs 的图解指南，你绝对应该阅读 Michael Nguyen 的文章。</p><p id="f32d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们的例子中，我们使用 Keras 包的一个<a class="ae kc" href="https://keras.io/layers/recurrent/#simplernn" rel="noopener ugc nofollow" target="_blank"> SimpleRNN </a>。我们还指定了一个<strong class="kf ir">提前停止</strong>回调，当有 10 个时期没有任何验证损失的改善时停止训练。<strong class="kf ir">模型检查点</strong>允许我们保存最佳模型的权重。模型架构仍然需要单独保存。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="b120" class="mi lc iq me b gy mj mk l ml mm">simple_rnn = Sequential()<br/>simple_rnn.add(SimpleRNN(4, input_shape=(n_lag, 1)))<br/>simple_rnn.add(Dense(1))<br/>simple_rnn.compile(loss='mae', optimizer=RMSprop())<br/><br/>checkpointer = ModelCheckpoint(filepath='../model/simple_rnn_weights.hdf5'<br/>                               , verbose=0<br/>                               , save_best_only=<strong class="me ir">True</strong>)<br/>earlystopper = EarlyStopping(monitor='val_loss'<br/>                             , patience=10<br/>                             , verbose=0)<br/><strong class="me ir">with</strong> open("../model/simple_rnn.json", "w") <strong class="me ir">as</strong> m:<br/>    m.write(simple_rnn.to_json())<br/><br/>simple_rnn_history = simple_rnn.fit_generator(train_data_gen<br/>                                              , epochs=100<br/>                                              , validation_data=valid_data_gen<br/>                                              , verbose=0<br/>                                              , callbacks=[checkpointer, earlystopper])<br/>plot_loss(simple_rnn_history, 'SimpleRNN - Train &amp; Validation Loss')</span></pre><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/1a95cb4b2b345009068f46d9865ecd92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*5iUNaezYr3rlTqqmtVm_IA.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Training and validation loss for a SimpleRNN</figcaption></figure><h2 id="86c9" class="mi lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">长短期记忆网络</h2><p id="df25" class="pw-post-body-paragraph kd ke iq kf b kg na ki kj kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">RNN 人记性很差。很难记住许多时间点以前的信息。当序列非常长时会出现这种情况。其实是因为<strong class="kf ir">消失渐变问题</strong>。梯度是更新神经网络权重的值。当你的 RNN 中有很多时间步长时，第一层的梯度会变得非常小。结果，第一层的权重的更新可以忽略。这意味着 RNN 不能学习早期地层中的东西。</p><p id="ceaa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们需要一种方法将第一层的信息传递给后面的层。LSTMs 更适合考虑长期依赖性。Michael Nguyen 写了一篇关于 LSTMs 的<a class="ae kc" rel="noopener" target="_blank" href="/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">视觉描述的优秀文章。</a></p><h2 id="4224" class="mi lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">简单 LSTM 模型</h2><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="b5dc" class="mi lc iq me b gy mj mk l ml mm">simple_lstm = Sequential()<br/>simple_lstm.add(LSTM(4, input_shape=(n_lag, 1)))<br/>simple_lstm.add(Dense(1))<br/>simple_lstm.compile(loss='mae', optimizer=RMSprop())<br/><br/>checkpointer = ModelCheckpoint(filepath='../model/simple_lstm_weights.hdf5'<br/>                               , verbose=0<br/>                               , save_best_only=<strong class="me ir">True</strong>)<br/>earlystopper = EarlyStopping(monitor='val_loss'<br/>                             , patience=10<br/>                             , verbose=0)<br/><strong class="me ir">with</strong> open("../model/simple_lstm.json", "w") <strong class="me ir">as</strong> m:<br/>    m.write(simple_lstm.to_json())<br/><br/>simple_lstm_history = simple_lstm.fit_generator(train_data_gen<br/>                                                , epochs=100<br/>                                                , validation_data=valid_data_gen<br/>                                                , verbose=0<br/>                                                , callbacks=[checkpointer, earlystopper])<br/>plot_loss(simple_lstm_history, 'Simple LSTM - Train &amp; Validation Loss')</span></pre><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/b338be4cca50d4f556d8d60eaee8e216.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*5jE6sI6yxkGxQovhcaEC2A.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Training and validation loss for a simple LSTM</figcaption></figure><h2 id="df2e" class="mi lc iq bd ld nt nu dn lh nv nw dp ll ko nx ny lp ks nz oa lt kw ob oc lx od bi translated">堆叠 LSTM 模型</h2><p id="2972" class="pw-post-body-paragraph kd ke iq kf b kg na ki kj kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">在这个模型中，我们将堆叠多个 LSTM 层。这样，模型将随着时间的推移学习输入数据的其他抽象。换句话说，<strong class="kf ir">表示不同时间尺度的输入数据</strong>。</p><p id="9400" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 Keras 中要做到这一点，我们需要在另一个 LSTM 层之前的 LSTM 层中指定参数<strong class="kf ir"> return_sequences </strong>。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="2424" class="mi lc iq me b gy mj mk l ml mm">stacked_lstm = Sequential()<br/>stacked_lstm.add(LSTM(16, input_shape=(n_lag, 1), return_sequences=<strong class="me ir">True</strong>))<br/>stacked_lstm.add(LSTM(8, return_sequences=<strong class="me ir">True</strong>))<br/>stacked_lstm.add(LSTM(4))<br/>stacked_lstm.add(Dense(1))<br/>stacked_lstm.compile(loss='mae', optimizer=RMSprop())<br/><br/>checkpointer = ModelCheckpoint(filepath='../model/stacked_lstm_weights.hdf5'<br/>                               , verbose=0<br/>                               , save_best_only=<strong class="me ir">True</strong>)<br/>earlystopper = EarlyStopping(monitor='val_loss'<br/>                             , patience=10<br/>                             , verbose=0)<br/><strong class="me ir">with</strong> open("../model/stacked_lstm.json", "w") <strong class="me ir">as</strong> m:<br/>    m.write(stacked_lstm.to_json())<br/><br/>stacked_lstm_history = stacked_lstm.fit_generator(train_data_gen<br/>                                                  , epochs=100<br/>                                                  , validation_data=valid_data_gen<br/>                                                  , verbose=0<br/>                                                  , callbacks=[checkpointer, earlystopper])<br/>plot_loss(stacked_lstm_history, 'Stacked LSTM - Train &amp; Validation Loss')</span></pre><figure class="lz ma mb mc gt jr gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/6d1b94494bb5673793ed58085560e043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*0CBPciBeReSNz2FOfuZJuw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Training and validation loss for a stacked LSTM</figcaption></figure><h1 id="23f2" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">评估绩效</h1><p id="9431" class="pw-post-body-paragraph kd ke iq kf b kg na ki kj kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">基于最小验证损失，SimpleRNN 似乎优于 LSTM 模型，尽管度量标准彼此接近。使用<strong class="kf ir"> evaluate_generator </strong>方法，我们可以对测试数据(生成器)上的模型进行评估。这会给我们带来测试数据上的损失。我们将首先从 JSON 文件中加载模型架构和最佳模型的权重。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="03d2" class="mi lc iq me b gy mj mk l ml mm"><strong class="me ir">def</strong> eval_best_model(model):<br/>    <em class="mn"># Load model architecture from JSON</em><br/>    model_architecture = open('../model/'+model+'.json', 'r')<br/>    best_model = model_from_json(model_architecture.read())<br/>    model_architecture.close()<br/>    <em class="mn"># Load best model's weights</em><br/>    best_model.load_weights('../model/'+model+'_weights.hdf5')<br/>    <em class="mn"># Compile the best model</em><br/>    best_model.compile(loss='mae', optimizer=RMSprop())<br/>    <em class="mn"># Evaluate on test data</em><br/>    perf_best_model = best_model.evaluate_generator(test_data_gen)<br/>    print('Loss on test data for <strong class="me ir">{}</strong> : <strong class="me ir">{}</strong>'.format(model, perf_best_model))<br/><br/>eval_best_model('simple_rnn')<br/>eval_best_model('simple_lstm')<br/>eval_best_model('stacked_lstm')</span></pre><ul class=""><li id="aedb" class="nf ng iq kf b kg kh kk kl ko nh ks ni kw nj la nk nl nm nn bi translated">simple_rnn 的测试数据的损失:0 . 54686 . 38686868661</li><li id="3f1c" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">simple_lstm 的测试数据损失:0 . 54686 . 68686868661</li><li id="1169" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">stacked_lstm 的测试数据损失:0 . 46866 . 38886888661</li></ul><h1 id="ab93" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">结论</h1><p id="b5b5" class="pw-post-body-paragraph kd ke iq kf b kg na ki kj kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">在这个故事中，我们为 LSTM 使用了一个递归神经网络和两种不同的架构。最佳性能来自于由几个隐藏层组成的堆叠 LSTM。</p><p id="be4d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">肯定有许多事情值得进一步研究，以提高模型性能。</p><ul class=""><li id="b0d9" class="nf ng iq kf b kg kh kk kl ko nh ks ni kw nj la nk nl nm nn bi translated">使用每小时的数据(EEA 网站上的另一个 CSV 文件),尝试除每日数据之外的其他采样策略。</li><li id="dbca" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">使用关于其他污染物的数据作为特征来预测 SO2 污染。也许其他污染物与二氧化硫污染有关。</li><li id="6c3e" class="nf ng iq kf b kg no kk np ko nq ks nr kw ns la nk nl nm nn bi translated">基于日期构建其他<strong class="kf ir">特征。在<a class="ae kc" href="https://www.drivendata.org/" rel="noopener ugc nofollow" target="_blank">驱动数据</a>的幂律预测竞赛的获胜者之一的<a class="ae kc" href="https://github.com/drivendataorg/power-laws-forecasting/blob/master/3rd%20Place/Model_Documentation_and_Write_up.pdf" rel="noopener ugc nofollow" target="_blank"> PDF 中可以找到一篇很好的文章</a></strong></li></ul><p id="c80b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过做这个项目，我学到了很多递归神经网络。我希望你喜欢它。欢迎留下任何评论！</p></div></div>    
</body>
</html>