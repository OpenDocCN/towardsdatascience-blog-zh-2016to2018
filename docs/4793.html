<html>
<head>
<title>This Is How Twitter Sees The World : Sentiment Analysis Part One</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这就是推特如何看待世界:情感分析第一部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-real-world-as-seen-on-twitter-sentiment-analysis-part-one-5ac2d06b63fb?source=collection_archive---------3-----------------------#2018-09-07">https://towardsdatascience.com/the-real-world-as-seen-on-twitter-sentiment-analysis-part-one-5ac2d06b63fb?source=collection_archive---------3-----------------------#2018-09-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/883a4fe9c87e937cb0a992ea1253b95a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Cby8nTrXuoNc4Vl9"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">by<a class="ae jd" href="https://unsplash.com/@hughhan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Hugh Ha</a>n on<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplas</a>h</figcaption></figure><div class=""/><p id="ad08" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Twitter 是一个在线社交网络，截至 2018 年 2 月，月活跃用户超过 3.3 亿。twitter 上的用户创建称为 tweets 的短消息，与其他 twitter 用户分享，这些用户通过转发和回复进行互动。Twitter 采用了 280 个字符或更少的消息大小限制，这迫使用户专注于他们希望传播的消息。这一特性使得 twitter 上的消息非常适合情感分析的机器学习(ML)任务。情感分析属于自然语言处理的范畴，自然语言处理是人工智能的一个分支，处理计算机如何处理和分析人类语言。</p><p id="0f5b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练数据是从<a class="ae jd" href="http://help.sentiment140.com/" rel="noopener ugc nofollow" target="_blank">感知 140 </a>获得的，由大约 160 万条随机推文组成，并带有相应的二进制标签。0 表示负面情绪，1 表示正面情绪。在这篇博客文章中，我们将使用<a class="ae jd" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank">朴素贝叶斯分类器</a>从这个训练集中学习正确的标签，并进行二元分类。在坚果壳中，朴素贝叶斯定理根据某些其他事件的联合概率分布来计算某个事件发生的概率。我使用 twitter 的 API 下载了测试数据集，并将用于测试模型的真实性能。API 的完整文档和条款可在<a class="ae jd" href="https://developer.twitter.com/en/docs" rel="noopener ugc nofollow" target="_blank">developer.twitter.com/en/docs</a>获得。</p><p id="79ec" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇博客文章的第一部分，我们将介绍一个情感分析 ML 任务中对文本数据执行的标准预处理步骤。第二部分<a class="ae jd" href="https://medium.com/p/3ed2670f927d" rel="noopener">将在后续的博客文章中介绍，我们将预处理合并到 ML 管道中的一个步骤中。第一篇文章解释了当我们使用管道来处理预处理时，在幕后发生了什么。</a></p><h1 id="111e" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">第 1 部分:探索带标签的训练数据集</h1><p id="d665" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">在我们的工作流程中，我们将使用带有 Python 的 Jupyter Notebook。</p><ul class=""><li id="8aed" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">导入此项目中使用的 python 库。</li><li id="daff" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">将数据导入到 pandas 数据框架中，并进行一些探索性的数据分析。</li></ul><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="273d" class="nb lc jg mx b gy nc nd l ne nf"><strong class="mx jh"># you can see the full list of imports on GitHub!</strong></span><span id="ba6c" class="nb lc jg mx b gy ng nd l ne nf"><strong class="mx jh"># Machine Learning imports<br/>import</strong> <strong class="mx jh">nltk<br/>from</strong> <strong class="mx jh">sklearn.pipeline</strong> <strong class="mx jh">import</strong> Pipeline<br/><strong class="mx jh">from</strong> <strong class="mx jh">sklearn.model_selection</strong> <strong class="mx jh">import</strong> train_test_split<br/><strong class="mx jh">from</strong> <strong class="mx jh">sklearn.metrics</strong> <strong class="mx jh">import</strong> classification_report, confusion_matrix, accuracy_score<br/><strong class="mx jh">from</strong> <strong class="mx jh">sklearn.naive_bayes</strong> <strong class="mx jh">import</strong> MultinomialNB<br/><strong class="mx jh">from</strong> <strong class="mx jh">sklearn.model_selection</strong> <strong class="mx jh">import</strong> KFold, cross_val_score<br/><strong class="mx jh">from</strong> <strong class="mx jh">sklearn.ensemble</strong> <strong class="mx jh">import</strong> RandomForestClassifier<br/><strong class="mx jh">from</strong> <strong class="mx jh">sklearn.feature_extraction.text</strong> <strong class="mx jh">import</strong> CountVectorizer<br/><strong class="mx jh">from</strong> <strong class="mx jh">sklearn.feature_extraction.text</strong> <strong class="mx jh">import</strong> TfidfTransformer<br/><strong class="mx jh">from</strong> <strong class="mx jh">sklearn.model_selection</strong> <strong class="mx jh">import</strong> GridSearchCV<br/><strong class="mx jh">from</strong> <strong class="mx jh">sklearn.externals</strong> <strong class="mx jh">import</strong> joblib<br/><strong class="mx jh">from</strong> <strong class="mx jh">nltk.corpus</strong> <strong class="mx jh">import</strong> stopwords<br/><strong class="mx jh">from</strong> <strong class="mx jh">nltk.tokenize</strong> <strong class="mx jh">import</strong> TweetTokenizer<br/><strong class="mx jh">from</strong> <strong class="mx jh">nltk.stem.wordnet</strong> <strong class="mx jh">import</strong> WordNetLemmatizerLoad training dataset to Pandas and preview the top rows.</span></pre></div><div class="ab cl nh ni hu nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ij ik il im in"><pre class="mw mx my mz aw na bi"><span id="9a24" class="nb lc jg mx b gy no np nq nr ns nd l ne nf"><em class="nt"># load train data</em><br/>data = pd.read_csv('Sentiment Analysis Dataset.csv',<br/>                   error_bad_lines=<strong class="mx jh">False</strong>)<br/>data.columns = ['id','label','source','text']<br/>data.head(2)</span><span id="b052" class="nb lc jg mx b gy ng nd l ne nf"><em class="nt"># get text and matching label columns</em><br/>data = data.drop(['id','source'],axis=1)<br/>data.head(10)</span></pre><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/2ac4f26d7219f1691f842ff449456ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*BFV4ghaFg6BZ9IQ4yh_7JQ.png"/></div></figure><ul class=""><li id="beed" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">我们可以观察到数据确实来自 twitter 上发布的 tweet 消息。</li><li id="544b" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">标签和文本似乎没有按照列出的顺序排列。如果数据不是随机分布的，这可能是一个问题，因为它会给学习模型带来偏差。在任何情况下，我们都将使用<a class="ae jd" href="http://scikit-learn.org/stable/documentation.html" rel="noopener ugc nofollow" target="_blank"> Scikit Learn </a>库，该库具有拆分我们的训练和测试数据并同时混洗数据的功能。</li><li id="12bf" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">洗牌数据减少了方差，并确保我们的模型可以更好地概括数据，减少过度拟合。我们希望确保训练和测试数据集能够代表数据的总体分布。</li><li id="e264" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">为此，我们还想检查数据中的标签频率分布，如下所示。</li><li id="5a21" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">我们可以观察到的另一个现象是文本包含不同的格式。一些单词包含混合的大小写字母，需要规范化为它们的基本单词。例如:‘哭’变‘哭’了。</li><li id="465c" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">第一个字母大写的单词可以尝试，因为它们可能包含不同的特征空间，如人名或国名等。</li></ul><p id="f07c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">探索标签类型的分布。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="f12f" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># check the number of positive vs. negative tagged sentences</em><br/>positives = data['label'][data.label == 0]<br/>negatives = data['label'][data.label == 1]</span><span id="0c4d" class="nb lc jg mx b gy ng nd l ne nf">print('number of positve tagged sentences is:  <strong class="mx jh">{}</strong>'.format(len(positives)))<br/>print('number of negative tagged sentences is: <strong class="mx jh">{}</strong>'.format(len(negatives)))<br/>print('total length of the data is:            <strong class="mx jh">{}</strong>'.format(data.shape[0]))</span></pre><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/05bbd46c908636a3472e0d470e15d283.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*SeVM35bbuBIsjVwSKZzNKA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">positive VS negative</figcaption></figure><ul class=""><li id="b979" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">考虑到数据集的大小，标签似乎“大约”均匀地分布在 788435 和 790177 处，分别为正和负。</li><li id="8311" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">接下来，我们希望看到每个句子中包含的单词数，所以我创建了一个函数来提取这些信息，并将其附加到<em class="nt">文本列</em>旁边的列中。下面是一个示例输出。</li></ul><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="96ad" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># get a word count per sentence column</em><br/><strong class="mx jh">def</strong> word_count(sentence):<br/>    <strong class="mx jh">return</strong> len(sentence.split())<br/>    <br/>data['word count'] = data['text'].apply(word_count)<br/>data.head(3)</span></pre><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/6fa9ec335d22fdbeff77df3341a99baf.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*M92j9faYXiv7NFRfKOUejw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">word count per message</figcaption></figure><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="6289" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># plot word count distribution for both positive and negative sentiments</em><br/>x = data['word count'][data.label == 1]<br/>y = data['word count'][data.label == 0]<br/>plt.figure(figsize=(12,6))<br/>plt.xlim(0,45)<br/>plt.xlabel('word count')<br/>plt.ylabel('frequency')<br/>g = plt.hist([x, y], color=['r','b'], alpha=0.5, label=['positive','negative'])<br/>plt.legend(loc='upper right')</span></pre><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/0914ac14cfe3f3887990358edfa02943.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*rBPVELAq1GmfZG5W4Q5p9w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><em class="ny">word count distribution for both positive and negative</em></figcaption></figure><ul class=""><li id="d990" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">从上面的图表来看，大多数句子在 5 到 10 个单词之间，但是可以说 twitter 上的大多数文本在 1 到 25 个单词之间。考虑到 twitter 对一条消息中可以使用的字符数有限制，这就不足为奇了。在撰写本文时，280 个字符已经是极限了。</li><li id="446e" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">总之，看起来 1-20 个单词覆盖了所有句子的 80%以上，这使得该数据集成为一个很好的训练候选集。</li><li id="a9b2" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">5 个单词以内的肯定句比否定句多，这种差异目前看来还不足以引起任何关注。</li></ul><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="d865" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># get most common words in training dataset</em><br/>all_words = []<br/><strong class="mx jh">for</strong> line <strong class="mx jh">in</strong> list(data['text']):<br/>    words = line.split()<br/>    <strong class="mx jh">for</strong> word <strong class="mx jh">in</strong> words:<br/>        all_words.append(word.lower())<br/>    <br/>    <br/>Counter(all_words).most_common(10)</span></pre><p id="376e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下输出:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="531c" class="nb lc jg mx b gy nc nd l ne nf">[('i', 741876),<br/> ('to', 556149),<br/> ('the', 516654),<br/> ('a', 374024),<br/> ('my', 309966),<br/> ('and', 294805),<br/> ('you', 236109),<br/> ('is', 229444),<br/> ('for', 212852),<br/> ('in', 209009)]</span></pre><ul class=""><li id="4f80" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">在上面的单元格中，我们提取了数据集中最常见的单词，并列出了前十名。</li><li id="5f7e" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">也许毫不奇怪，我们会遇到像<strong class="kf jh"> i、</strong>和<strong class="kf jh">是</strong>这样的词，因为它们在人类表达中使用频率很高。这种词通常在消极和积极的表达中同样出现，因此它们带来的信息很少，可以纳入模型中，所以我们必须在以后的道路上摆脱它们。</li><li id="5371" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">在后面的文本预处理步骤中，我们将学习如何处理这些对特征空间没有太大贡献的常用词。</li><li id="78ea" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">下面的代码输出一个图表，显示前 25 个单词的频率。</li></ul><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="2760" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># plot word frequency distribution of first few words</em><br/>plt.figure(figsize=(12,5))<br/>plt.title('Top 25 most common words')<br/>plt.xticks(fontsize=13, rotation=90)<br/>fd = nltk.FreqDist(all_words)<br/>fd.plot(25,cumulative=<strong class="mx jh">False</strong>)</span><span id="2f55" class="nb lc jg mx b gy ng nd l ne nf"><em class="nt"># log-log plot</em><br/>word_counts = sorted(Counter(all_words).values(), reverse=<strong class="mx jh">True</strong>)<br/>plt.figure(figsize=(12,5))<br/>plt.loglog(word_counts, linestyle='-', linewidth=1.5)<br/>plt.ylabel("Freq")<br/>plt.xlabel("Word Rank")<br/>plt.title('log-log plot of words frequency')</span></pre><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/dcd8cbedf0a44683897da9569043f8a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*XTWJB1M5VOb2fYCkEiQX0w.png"/></div></figure><ul class=""><li id="6b72" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">我还为单词频率创建了一个<a class="ae jd" href="https://en.wikipedia.org/wiki/Log%E2%80%93log_plot" rel="noopener ugc nofollow" target="_blank"> <strong class="kf jh">双对数</strong> </a>图，它类似于之前的频率图，但包括所有单词，并以 10 为基数的对数标度绘制，这有助于我们可视化单词频率随着排名下降而快速减少的情况。</li><li id="078e" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">本数据字典中出现的单词分布是大样本单词中非常常见的现象，如<a class="ae jd" href="https://simple.wikipedia.org/wiki/Zipf%27s_law" rel="noopener ugc nofollow" target="_blank">齐夫定律</a>所示，其中最频繁出现的单词的出现频率大约是第二频繁出现的单词的两倍，第三频繁出现的单词的三倍，等等。</li><li id="563c" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">如果我们从上面的观察中去掉像<strong class="kf jh"> i、</strong>和<strong class="kf jh"> is </strong>这样的词，看看这是否成立将会很有趣。</li></ul><h1 id="fb68" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">第 2 部分:探索原始 Twitter 数据</h1><p id="3360" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">在训练模型后，我们对原始 twitter 数据进行情感预测，但在此之前，我们需要下载并进行一些基本的数据清理。您可以通过关键字<em class="nt">从 twitter RESTful API 下载推文，并获得历史推文流和元数据。我使用了<a class="ae jd" href="https://tweepy.readthedocs.io/en/v3.6.0/" rel="noopener ugc nofollow" target="_blank"> Tweepy </a>，这是 twitter API 的一个 python 包装器库，它可以让您更好地控制如何查询 API。你可以在<a class="ae jd" href="https://github.com/RonKG/machine-learning-portfolio-projects.......in-progress/tree/master/3.%20NLP_twitter_sentiment_analysis" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上看到代码。</em></p><p id="a983" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用于下载历史推文的关键词是“保罗·瑞安”，他是本文撰写时的美国众议院议长。我对这个关键字的选择是任意的，但我觉得足够好来返回一些两极分化的推文。这很重要，因为推文本身可以作为自己的基准。考虑到语言是相对的并且总是在变化的，NLP 的障碍之一是评估模型的性能。</p><ul class=""><li id="19e0" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">导入和预览原始数据。</li></ul><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="8256" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># create column names</em><br/>col_names=['date','user_loc','followers','friends','message','bbox_coords',\<br/>           'full_name','country','country_code','place_type']</span><span id="bf47" class="nb lc jg mx b gy ng nd l ne nf"><em class="nt"># read csv</em><br/>df_twtr = pd.read_csv('paul_ryan_twitter.csv', names=col_names)</span><span id="f2a9" class="nb lc jg mx b gy ng nd l ne nf"><em class="nt"># check head</em><br/>df_twtr.head()</span></pre><div class="ms mt mu mv gt ab cb"><figure class="oa is ob oc od oe of paragraph-image"><img src="../Images/e2a9fe78982c2e0b73aef2ac14e4981c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*we0gOjLE1h5IQrahRis7yg.png"/></figure><figure class="oa is og oc od oe of paragraph-image"><img src="../Images/6bb2ced3de784d94c24e7f7c092c9a4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*nrOtNlxzHgxsXU_WbEDDVg.png"/></figure></div><p id="ad0c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将保留位置数据，并在以后使用它在地理地图上绘制出我们的结果。</p><h1 id="e12b" class="lb lc jg bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">第 3 部分:文本预处理</h1><ul class=""><li id="3c01" class="me mf jg kf b kg lz kk ma ko oh ks oi kw oj la mj mk ml mm bi translated">我们希望删除数据中任何不必要的质量，这些质量会使训练好的模型成为不良的概括器。</li><li id="41ae" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">文本预处理涉及许多事情，如删除表情符号，正确格式化文本以删除文本中多余的空格或任何其他我们认为不会给我们的模型添加信息的信息。我们将在下面看到一些例子。</li><li id="5618" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">我们还必须确保我们传递给模型的信息是计算机能够理解的格式。我们还将在下面完成其中的一些步骤。</li><li id="581f" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">在这个预处理步骤之后，我们的数据应该准备好用于机器学习分类任务。</li></ul><p id="f7af" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意:无论我们在训练数据上做什么样的文本预处理，也必须在测试和原始数据上完成。</p><h2 id="ba2e" class="nb lc jg bd ld ok ol dn lh om on dp ll ko oo op lp ks oq or lt kw os ot lx ou bi translated">预处理 1:通过删除链接、特殊字符</h2><ul class=""><li id="8adb" class="me mf jg kf b kg lz kk ma ko oh ks oi kw oj la mj mk ml mm bi translated">这一步我们可能会花很多时间，但目标总是找到最佳平衡。</li><li id="950f" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">NLP 中的大部分工作是在特征工程上完成的。现在我们将去掉链接和表情符号。值得一提的是，我们可以将它们用作特性，但目前我们只是在构建一个基本模型。</li></ul><p id="5954" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我创建了一个函数，使用<em class="nt"> regex </em>对数据集中的每条 tweet 进行批量格式化。示例输出包含在代码中。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="248c" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># helper function to clean tweets</em><br/><strong class="mx jh">def</strong> processTweet(tweet):<br/>    <em class="nt"># Remove HTML special entities (e.g. &amp;amp;)</em><br/>    tweet = re.sub(r'\&amp;\w*;', '', tweet)<br/>    <em class="nt">#Convert @username to AT_USER</em><br/>    tweet = re.sub('@[^\s]+','',tweet)<br/>    <em class="nt"># Remove tickers</em><br/>    tweet = re.sub(r'\$\w*', '', tweet)<br/>    <em class="nt"># To lowercase</em><br/>    tweet = tweet.lower()<br/>    <em class="nt"># Remove hyperlinks</em><br/>    tweet = re.sub(r'https?:\/\/.*\/\w*', '', tweet)<br/>    <em class="nt"># Remove hashtags</em><br/>    tweet = re.sub(r'#\w*', '', tweet)<br/>    <em class="nt"># Remove Punctuation and split 's, 't, 've with a space for filter</em><br/>    tweet = re.sub(r'[' + punctuation.replace('@', '') + ']+', ' ', tweet)<br/>    <em class="nt"># Remove words with 2 or fewer letters</em><br/>    tweet = re.sub(r'\b\w{1,2}\b', '', tweet)<br/>    <em class="nt"># Remove whitespace (including new line characters)</em><br/>    tweet = re.sub(r'\s\s+', ' ', tweet)<br/>    <em class="nt"># Remove single space remaining at the front of the tweet.</em><br/>    tweet = tweet.lstrip(' ') <br/>    <em class="nt"># Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:</em><br/>    tweet = ''.join(c <strong class="mx jh">for</strong> c <strong class="mx jh">in</strong> tweet <strong class="mx jh">if</strong> c &lt;= '<strong class="mx jh">\uFFFF</strong>') <br/>    <strong class="mx jh">return</strong> tweet<br/><em class="nt"># ______________________________________________________________</em></span><span id="8927" class="nb lc jg mx b gy ng nd l ne nf"><em class="nt"># clean dataframe's text column</em><br/>df_paulry['message'] = df_paulry['message'].apply(processTweet)<br/><em class="nt"># preview some cleaned tweets</em><br/>df_paulry['message'].head()</span></pre><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/0cd59373a59f61376cfcaf6ca749fb5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*AZD6musAuOVGAR-RJKy3wQ.png"/></div></figure><ul class=""><li id="3446" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">还要注意，你不可能有一套完美的清理步骤，因为一些清理步骤最终会在数据中引入一些缺陷。</li><li id="b043" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">下面是一个前后示例，其中<em class="nt"/>已从<em class="nt">重选</em>中删除，这改变了该词的含义，或许也改变了用户的本意。单词<em class="nt">不会</em>也会变成<em class="nt">赢了</em>显然有完全不同的意思。</li></ul><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/d4462e8cb278425676d2f880935e99d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*3HGtyfEDoJq6oyW6JP-Xsw.png"/></div></figure><p id="c827" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们删除重复的推文，因为它们没有给数据集带来新的信息，而且计算效率也很低。我们还将语料库中最常见的单词可视化。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="f6e0" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># most common words in twitter dataset</em><br/>all_words = []<br/><strong class="mx jh">for</strong> line <strong class="mx jh">in</strong> list(df_paulry['message']):<br/>    words = line.split()<br/>    <strong class="mx jh">for</strong> word <strong class="mx jh">in</strong> words:<br/>        all_words.append(word.lower())</span><span id="a2d2" class="nb lc jg mx b gy ng nd l ne nf"><em class="nt"># plot word frequency distribution of first few words</em><br/>plt.figure(figsize=(12,5))<br/>plt.xticks(fontsize=13, rotation=90)<br/>fd = nltk.FreqDist(all_words)<br/>fd.plot(25,cumulative=<strong class="mx jh">False</strong>)</span><span id="15af" class="nb lc jg mx b gy ng nd l ne nf"><em class="nt"># log-log of all words </em><br/>word_counts = sorted(Counter(all_words).values(), reverse=<strong class="mx jh">True</strong>)plt.figure(figsize=(12,5))</span><span id="582c" class="nb lc jg mx b gy ng nd l ne nf">plt.loglog(word_counts, linestyle='-', linewidth=1.5)<br/>plt.ylabel("Freq")<br/>plt.xlabel("Word Rank")</span></pre><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/4a6dfbc5afb19d79a421204a8314a161.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*JJ3Uhh2LM-9WTgITXevWsw.png"/></div></figure><h2 id="7150" class="nb lc jg bd ld ok ol dn lh om on dp ll ko oo op lp ks oq or lt kw os ot lx ou bi translated">预处理 2:不带停用词的标记化</h2><ul class=""><li id="5cc2" class="me mf jg kf b kg lz kk ma ko oh ks oi kw oj la mj mk ml mm bi translated">我们的训练数据现在已经被转换成更加精简的文本体，对于特征提取来说更加清晰。正如我们之前提到的，我们在数据集中确实有一些词，它们在自然人类语言中很常见，但在大多数句子成分中使用，我们最好不要使用它们，因为它们不会给我们的模型带来有用的特征。</li><li id="aec7" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">这些词在 NLP 中通常被称为<a class="ae jd" href="https://en.wikipedia.org/wiki/Stop_words" rel="noopener ugc nofollow" target="_blank">停用词</a>，并且<a class="ae jd" href="http://text-processing.com/demo/sentiment/" rel="noopener ugc nofollow" target="_blank"> NLTK 库</a>带有一个函数，可以从数据集中过滤出这些词。以下是停用字词表中包含的实际字词。我们还可以制作自己的特殊停用词列表，以适应任何独特的情况。例如，如果您正在对法律文档进行情感分析，考虑到其中包含的法律术语在本质上是独特的，您可能需要一个特殊的集合。</li></ul><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="cc53" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># show stop words examples</em></span><span id="ab28" class="nb lc jg mx b gy ng nd l ne nf">("i , me , my , myself , we , our , ours , ourselves , you , you're , you've , you'll , you'd , your , yours , yourself , yourselves , he , him , his , himself , she , her , hers , herself , it ")</span></pre><ul class=""><li id="a9ce" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">在移除停用词后，我们将数据集中的所有句子进行拆分，以获得单个单词<strong class="kf jh">(令牌)</strong>，这基本上是新处理的 tweet 中包含的每个句子的单词列表。现在我们可以看到，数据帧中有两个新列，包含这些 tweet 的标记化版本。</li><li id="4e9c" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">下面是一个示例输出。</li></ul><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="6234" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># tokenize helper function</em><br/><strong class="mx jh">def</strong> text_process(raw_text):<br/>    <em class="nt">"""</em><br/><em class="nt">    Takes in a string of text, then performs the following:</em><br/><em class="nt">    1. Remove all punctuation</em><br/><em class="nt">    2. Remove all stopwords</em><br/><em class="nt">    3. Returns a list of the cleaned text</em><br/><em class="nt">    """</em><br/>    <em class="nt"># Check characters to see if they are in punctuation</em><br/>    nopunc = [char <strong class="mx jh">for</strong> char <strong class="mx jh">in</strong> list(raw_text) <strong class="mx jh">if</strong> char <strong class="mx jh">not</strong> <strong class="mx jh">in</strong> string.punctuation]</span><span id="8c53" class="nb lc jg mx b gy ng nd l ne nf">    <em class="nt"># Join the characters again to form the string.</em><br/>    nopunc = ''.join(nopunc)<br/>    <br/>    <em class="nt"># Now just remove any stopwords</em><br/>    <strong class="mx jh">return</strong> [word <strong class="mx jh">for</strong> word <strong class="mx jh">in</strong> nopunc.lower().split() <strong class="mx jh">if</strong> word.lower() <strong class="mx jh">not</strong> <strong class="mx jh">in</strong> stopwords.words('english')]<br/></span><span id="21f5" class="nb lc jg mx b gy ng nd l ne nf"><strong class="mx jh">def</strong> remove_words(word_list):<br/>    remove = ['paul','ryan','...','“','”','’','…','ryan’']<br/>    <strong class="mx jh">return</strong> [w <strong class="mx jh">for</strong> w <strong class="mx jh">in</strong> word_list <strong class="mx jh">if</strong> w <strong class="mx jh">not</strong> <strong class="mx jh">in</strong> remove]</span><span id="d566" class="nb lc jg mx b gy ng nd l ne nf"><em class="nt"># -------------------------------------------</em></span><span id="1e32" class="nb lc jg mx b gy ng nd l ne nf"><em class="nt"># tokenize message column and create a column for tokens</em><br/>df_paulry = df_paulry.copy()<br/>df_paulry['tokens'] = df_paulry['message'].apply(text_process) <em class="nt"># tokenize style 1</em><br/>df_paulry['no_pauls'] = df_paulry['tokens'].apply(remove_words) <em class="nt">#tokenize style 2</em><br/>df_paulry.head()</span></pre><figure class="ms mt mu mv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ox"><img src="../Images/8f75d33df75ffdf14792922efcd794ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c37PEaifaYzhKVot6Bu57A.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Compare the message column VS tokens column VS no_pauls.</figcaption></figure><ul class=""><li id="feed" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">还有其他标准化技术，如<a class="ae jd" href="http://www.nltk.org/howto/stem.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kf jh">词干</strong> </a>和<a class="ae jd" href="http://www.nltk.org/api/nltk.stem.html?highlight=lemmatizer#nltk.stem.wordnet.WordNetLemmatizer" rel="noopener ugc nofollow" target="_blank"> <strong class="kf jh">词汇化</strong> </a>，我们可以在我们的数据上尝试，但 twitter 消息被设计得很短，上述方法可能不会很好，因为它们本质上是将单词缩短为基本单词。例如:<em class="nt">运行</em>到<em class="nt">运行</em>。</li><li id="642b" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">现在，我们将坚持我们的消息数据的当前规范化状态，我们现在可以将它转换成一个向量，这个向量可以输入到适当的 ML 算法中。</li><li id="b942" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">我还创建了一个词云，描述标准化后整个 twitter 数据集中最常见的词。</li><li id="1e8c" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">我们可以看到我们的关键词——Paul 和 Ryan——显然非常突出，但一些领域知识将有助于理解为什么其他一些词会在那里。毕竟这是 twitter，所以你总是可以期待各种各样的情绪。数据集中的一些单词确实使用了非常强烈的语言。单词 cloud 还排除了单词<em class="nt"> paul </em>和<em class="nt"> ryan </em>，因为它们在数据集中出现的频率过高。</li></ul><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="89e1" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># split sentences to get individual words</em><br/>all_words = []<br/><strong class="mx jh">for</strong> line <strong class="mx jh">in</strong> df_paulry['no_pauls']: <em class="nt"># try 'tokens'</em><br/>    all_words.extend(line)<br/>    <br/><em class="nt"># create a word frequency dictionary</em><br/>wordfreq = Counter(all_words)</span><span id="60ca" class="nb lc jg mx b gy ng nd l ne nf"><em class="nt"># draw a Word Cloud with word frequencies</em><br/>wordcloud = WordCloud(width=900,<br/>                      height=500,<br/>                      max_words=500,<br/>                      max_font_size=100,<br/>                      relative_scaling=0.5,<br/>                      colormap='Blues',<br/>                      normalize_plurals=<strong class="mx jh">True</strong>).generate_from_frequencies(wordfreq)</span><span id="eab1" class="nb lc jg mx b gy ng nd l ne nf">plt.figure(figsize=(17,14))<br/>plt.imshow(wordcloud, interpolation='bilinear')<br/>plt.axis("off")<br/>plt.show()</span></pre><figure class="ms mt mu mv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oy"><img src="../Images/5619a43725a733451204c1d3ddf5881e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*05LInvAGSD9iB8mpudrXxQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Tokenized words word cloud</figcaption></figure><h2 id="4383" class="nb lc jg bd ld ok ol dn lh om on dp ll ko oo op lp ks oq or lt kw os ot lx ou bi translated">预处理 3:特征提取</h2><h2 id="008a" class="nb lc jg bd ld ok ol dn lh om on dp ll ko oo op lp ks oq or lt kw os ot lx ou bi translated">矢量化—(单词袋)</h2><p id="2f21" class="pw-post-body-paragraph kd ke jg kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们将把每条由一系列符号表示的消息转换成机器学习模型可以理解的向量。</p><p id="947b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为此，我们使用了包含三个步骤的<a class="ae jd" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">单词袋</a>模型。</p><ul class=""><li id="a6fd" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">1.计算一个单词在每条消息中出现的次数(称为词频)</li><li id="33a3" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">2.对计数进行加权，以使频繁出现的令牌获得较低的权重(逆文档频率)</li><li id="97be" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">3.将向量归一化为单位长度，以从原始文本长度中提取(L2 范数)</li></ul><p id="5f96" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个向量将具有与 tweeter 语料库中的唯一单词一样多的维度。</p><p id="d2de" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将首先使用 SciKit Learn 的 CountVectorizer 函数，该函数将一组文本文档转换成一个令牌计数矩阵。</p><p id="40cf" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">想象这是一个二维矩阵，其中一维是消息中包含的全部词汇，另一维是每条推文的一列。</p><p id="d8ab" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于有如此多的消息，我们可以预期数据中每个单词的出现都有很多零计数，但是 SciKit Learn 将输出一个<a class="ae jd" href="https://en.wikipedia.org/wiki/Sparse_matrix" rel="noopener ugc nofollow" target="_blank">稀疏矩阵</a>。下面是一个矢量化句子的例子——代码和输出。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="ff36" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># vectorize</em><br/>bow_transformer = CountVectorizer(analyzer=text_process).fit(df_paulry['message'])<br/><em class="nt"># print total number of vocab words</em><br/>print(len(bow_transformer.vocabulary_))</span><span id="aab7" class="nb lc jg mx b gy ng nd l ne nf"># output<br/>6865</span><span id="cc22" class="nb lc jg mx b gy ng nd l ne nf"><em class="nt"># example of vectorized text</em><br/>sample_tweet = df_paulry['message'][111]<br/>print(sample_tweet)<br/>print('<strong class="mx jh">\n</strong>')</span><span id="5fbb" class="nb lc jg mx b gy ng nd l ne nf"><em class="nt"># vector representation</em><br/>bow_sample = bow_transformer.transform([sample_tweet])<br/>print(bow_sample)<br/>print('<strong class="mx jh">\n</strong>')</span></pre><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/40210ff813ecfd8b8c88bfc7f001e148.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*XS8M302NwFVY9s4cOYJf6w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Sample sentence in vector representation</figcaption></figure><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="a7f3" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># transform the entire DataFrame of messages</em><br/>messages_bow = bow_transformer.transform(df_paulry['message'])</span><span id="aaa9" class="nb lc jg mx b gy ng nd l ne nf"><em class="nt"># check out the bag-of-words counts for the entire corpus as a large sparse matrix</em><br/>print('Shape of Sparse Matrix: ', messages_bow.shape)<br/>print('Amount of Non-Zero occurences: ', messages_bow.nnz)</span></pre><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/c99c00fddf51c90c861988652139017e.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*lDxkYWC5p7CMvazLVHmxnQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Whole corpus in vector representation</figcaption></figure><h2 id="df3d" class="nb lc jg bd ld ok ol dn lh om on dp ll ko oo op lp ks oq or lt kw os ot lx ou bi translated">术语频率，逆文档频率</h2><ul class=""><li id="62d8" class="me mf jg kf b kg lz kk ma ko oh ks oi kw oj la mj mk ml mm bi translated">tf-idf 代表术语频率-逆文档频率，TF-IDF 权重是信息检索和文本挖掘中经常使用的权重。该权重是一种统计度量，用于评估一个单词对集合或语料库中的文档有多重要。</li><li id="464c" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">重要性与单词在文档中出现的次数成比例增加，但是被单词在语料库中的频率抵消。tf-idf 加权方案的变体通常被搜索引擎用作给定用户查询时对文档相关性进行评分和排序的中心工具。</li><li id="ac0f" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">最简单的排名函数之一是通过对每个查询项的 tf-idf 求和来计算的；许多更复杂的排名函数是这个简单模型的变体。</li><li id="e939" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">通常，tf-idf 权重由两项组成:第一项计算归一化项频率(tf)，aka。单词在文档中出现的次数，除以该文档中的总单词数；第二项是逆文档频率(IDF ),计算为语料库中文档数量的对数除以特定术语出现的文档数量。</li><li id="2d16" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">TF:术语频率，衡量一个术语在文档中出现的频率。因为每个文档的长度不同，所以一个术语在长文档中出现的次数可能比短文档多得多。因此，术语频率通常除以文档长度(又名。文档中的术语总数)作为标准化的一种方式:</li></ul><p id="b432" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">TF(t) =(术语 t 在文档中出现的次数)/(文档中的总术语数)。</p><ul class=""><li id="8f91" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">IDF:逆文档频率，衡量一个术语的重要程度。在计算 TF 时，所有项都被认为是同等重要的。然而，众所周知，某些术语，如“是”、“的”和“那个”，可能会出现很多次，但并不重要。因此，我们需要通过计算以下各项来降低常用术语的权重，同时提高罕见术语的权重:</li></ul><p id="95c2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">IDF(t) = log_e(文档总数/包含术语 t 的文档数)。</p><ul class=""><li id="8566" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">下面是一个简单的例子。</li><li id="c70c" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated"><code class="fe pb pc pd mx b">Example: Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.</code></li></ul><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="609c" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># from sklearn.feature_extraction.text import TfidfTransformer</em><br/>tfidf_transformer = TfidfTransformer().fit(messages_bow)<br/>tfidf_sample = tfidf_transformer.transform(bow_sample)<br/>print(tfidf_sample)</span></pre><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/a5c91c64e7e9684c42bdd9318dff5118.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*Xi9PkqanqeKBBC48zEHTeQ.png"/></div></figure><ul class=""><li id="9660" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">我们将继续检查单词“trump”和单词“gop”的 IDF ( <a class="ae jd" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank">逆文档频率</a>)是多少？</li></ul><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="ecf8" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># some IDF (inverse document frequency) example</em><br/>print(tfidf_transformer.idf_[bow_transformer.vocabulary_['trump']])<br/>print(tfidf_transformer.idf_[bow_transformer.vocabulary_['gop']])</span></pre><figure class="ms mt mu mv gt is gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/c650776428f31057b7631f786e7f645a.png" data-original-src="https://miro.medium.com/v2/resize:fit:252/format:webp/1*6Sh3ebR0F80pdP-6fESe1w.png"/></div></figure><ul class=""><li id="9b33" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">为了将我们的整个 twitter 单词库一次转换成 TF-IDF 语料库，我们将使用下面的代码:</li></ul><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="07dc" class="nb lc jg mx b gy nc nd l ne nf"><em class="nt"># to transform the entire bag-of-words corpus</em><br/>messages_tfidf = tfidf_transformer.transform(messages_bow)<br/>print(messages_tfidf.shape)</span><span id="f22c" class="nb lc jg mx b gy ng nd l ne nf"># Output<br/>(4164, 6865)</span></pre><ul class=""><li id="341b" class="me mf jg kf b kg kh kk kl ko mg ks mh kw mi la mj mk ml mm bi translated">在对数据进行预处理之后，我们现在准备让它通过一个 ML 分类算法。</li><li id="e5a2" class="me mf jg kf b kg mn kk mo ko mp ks mq kw mr la mj mk ml mm bi translated">这就结束了我们工作流程的文本预处理阶段，但是我们将在这里继续的<a class="ae jd" href="https://medium.com/p/3ed2670f927d" rel="noopener">下一部分</a>中作为流水线中的<strong class="kf jh">一步</strong>重新审视这些步骤。</li></ul></div></div>    
</body>
</html>