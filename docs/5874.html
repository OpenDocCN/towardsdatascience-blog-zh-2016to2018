<html>
<head>
<title>Predicting Probability Distributions Using Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用神经网络预测概率分布</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-probability-distributions-using-neural-networks-abef7db10eac?source=collection_archive---------10-----------------------#2018-11-13">https://towardsdatascience.com/predicting-probability-distributions-using-neural-networks-abef7db10eac?source=collection_archive---------10-----------------------#2018-11-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="1872" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">本帖原载于</em> </strong> <a class="ae km" href="https://engineering.taboola.com" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> <em class="kl"> Taboola 的工程博客</em> </strong> </a></p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi kn"><img src="../Images/eee6d41d421911b0547e354bf6879382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*g-6ZlyOTqMoFApt_.jpg"/></div></div></figure><p id="67c1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你最近一直在关注我们的<a class="ae km" href="https://engineering.taboola.com/" rel="noopener ugc nofollow" target="_blank">科技博客</a>，你可能已经注意到<a class="ae km" href="https://engineering.taboola.com/uncertainty-ctr-prediction-one-model-clarify" rel="noopener ugc nofollow" target="_blank">我们正在使用</a>一种特殊类型的神经网络，称为混合密度网络(MDN)。MDN 不仅预测目标的期望值，还预测潜在的概率分布。</p><p id="7cf8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇博文将关注如何使用 Tensorflow 从头开始实现这样一个模型，包括解释、图表和一个包含全部源代码的<a class="ae km" href="https://github.com/taboola/mdn-tensorflow-notebook-example" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>。</p><h2 id="1fc2" class="kz la iq bd lb lc ld dn le lf lg dp lh jy li lj lk kc ll lm ln kg lo lp lq lr bi translated">什么是 MDN，为什么它们有用？</h2><p id="5600" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">现实生活中的数据是嘈杂的。虽然相当恼人，但这种噪音是有意义的，因为它提供了数据来源的更广泛的视角。根据输入的不同，目标值可能有不同程度的噪声，这可能会对我们对数据的理解产生重大影响。</p><p id="3263" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个用一个例子更好解释。假设以下二次函数:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/3708c95dabc32cd097106b3b99f2d564.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/0*BQ6fN3YDIBgELPBW.png"/></div></figure><p id="3318" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">给定 x 作为输入，我们有一个确定的输出<em class="kl"> f(x) </em>。现在，让我们把这个函数变成一个更有趣(也更真实)的函数:我们将在<em class="kl"> f(x) </em>中加入一些正态分布的噪声。这种噪音会随着<em class="kl"> x </em>的增加而增加。我们将称新函数为<em class="kl"> g(x) </em>，形式上等于<em class="kl">g(x)= f(x)+</em>𝜺<em class="kl">(x)</em>，其中𝜺 <em class="kl"> (x) </em>为正态随机变量。</p><p id="184a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们针对不同的<em class="kl"> x </em>值对<em class="kl"> g(x) </em>进行采样:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/3ea5310621bb3cac0c7cbc7d13295470.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*MXEQdMbqqmK4_5Y0yWEZ3A.png"/></div></figure><p id="749a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">紫色线代表无噪声函数<em class="kl"> f(x) </em>，我们可以很容易地看到添加噪声的增加。让我们检查一下<em class="kl"> x = 1 </em>和<em class="kl"> x = 5 </em>的情况。对于这两个值，<em class="kl"> f(x) = 4，</em>so 4 是<em class="kl"> g(x) </em>可以取的合理值。4.5 对于<em class="kl"> g(x) </em>也是一个合理的预测吗？答案显然是否定的。虽然 4.5 似乎是<em class="kl"> x = 5 </em>的合理值，但当<em class="kl"> x = 1 </em>时，我们不能接受它作为 g(x)的有效值。如果我们的模型只是简单地学习预测<em class="kl">y’(x)= f(x)</em>，这些有价值的信息就会丢失。这里我们真正需要的是一个能够预测<em class="kl">y’(x)= g(x)</em>的模型。这正是 MDN 所做的。</p><p id="036a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">MDN 的概念是由 Christopher Bishop 在 1994 年发明的。他的<a class="ae km" href="https://publications.aston.ac.uk/373/1/NCRG_94_004.pdf" rel="noopener ugc nofollow" target="_blank">原始论文</a>很好地解释了这个概念，但它可以追溯到史前时代，当时神经网络不像今天这样普遍。因此，它缺少实际实现的部分。这正是我们现在要做的。</p><h2 id="b8c2" class="kz la iq bd lb lc ld dn le lf lg dp lh jy li lj lk kc ll lm ln kg lo lp lq lr bi translated">我们开始吧</h2><p id="fbe7" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">让我们从一个简单的神经网络开始，它只从有噪声的数据集中学习<em class="kl"> f(x) </em>。我们将使用 3 个隐藏的密集层，每个层有 12 个节点，如下所示:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/7256b43fc1b5f07938b12d2b503acc12.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/0*6lT7iYufdmVy2NeH.jpg"/></div></figure><p id="a7bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将使用均方差作为损失函数。让我们在 Tensorflow 中对此进行编码:</p><pre class="ko kp kq kr gt ma mb mc md aw me bi"><span id="e0a7" class="kz la iq mb b gy mf mg l mh mi">x = tf.placeholder(name='x',shape=(None,1),dtype=tf.float32)<br/>layer = x<br/>for _ in range(3):<br/>   layer = tf.layers.dense(inputs=layer, units=12, activation=tf.nn.tanh)<br/>output = tf.layers.dense(inputs=layer, units=1)</span></pre><p id="0b4f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练后，输出如下所示:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/852772bbf74e7ce9ac1948afd1a80037.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*5liF1e8ImHE2bF907pNncw.png"/></div></figure><p id="3e97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们看到网络成功地学习了<em class="kl"> f(x) </em>。现在缺少的只是对噪声的估计。让我们修改网络来获得这条额外的信息。</p><h2 id="1a29" class="kz la iq bd lb lc ld dn le lf lg dp lh jy li lj lk kc ll lm ln kg lo lp lq lr bi translated">走向 MDN</h2><p id="5363" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">我们将继续使用我们刚刚设计的相同网络，但我们将改变两件事:</p><ol class=""><li id="8d1f" class="mj mk iq jp b jq jr ju jv jy ml kc mm kg mn kk mo mp mq mr bi translated">输出层将有两个节点，而不是一个，我们将这些节点命名为<em class="kl"> mu </em>和<em class="kl"> sigma </em></li><li id="a24e" class="mj mk iq jp b jq ms ju mt jy mu kc mv kg mw kk mo mp mq mr bi translated">我们将使用不同的损失函数</li></ol><p id="a8c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们的网络看起来像这样:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/1c8af6741f31bfaa992c67cd2ad1e121.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/0*-4DsT0dJPpvJd2vD.jpg"/></div></figure><p id="210c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们来编码一下:</p><pre class="ko kp kq kr gt ma mb mc md aw me bi"><span id="9cfc" class="kz la iq mb b gy mf mg l mh mi">x = tf.placeholder(name='x',shape=(None,1),dtype=tf.float32)<br/>layer = x<br/>for _ in range(3):<br/>   layer = tf.layers.dense(inputs=layer, units=12, activation=tf.nn.tanh)<br/>mu = tf.layers.dense(inputs=layer, units=1)<br/>sigma = tf.layers.dense(inputs=layer, units=1,activation=lambda x: tf.nn.elu(x) + 1)</span></pre><p id="ffbd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们花一点时间来理解 sigma 的激活函数——记住，根据定义，任何分布的标准差都是非负数。<a class="ae km" href="http://image-net.org/challenges/posters/JKU_EN_RGB_Schwarz_poster.pdf" rel="noopener ugc nofollow" target="_blank">指数线性单位</a> (ELU)，定义为:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/dfc6f94c922ccdc7758e2e1244bd01a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/0*NEXsS-AhAkZHFug-.png"/></div></figure><p id="7d61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">产出<em class="kl"> -1 </em>作为其最低值，因此<em class="kl"> ELU+1 </em>将总是非负的。一定要去 ELU 吗？不，任何总是产生非负输出的函数都可以，例如 sigma 的绝对值。ELU 似乎做得更好。</p><p id="7e8d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们需要调整我们的损失函数。让我们试着去理解我们现在到底在寻找什么。我们的新输出层为我们提供了正态分布的参数。这种分布应该能够描述采样产生的数据<em class="kl"> g(x) </em>。我们如何衡量它？例如，我们可以从输出中创建一个正态分布，并最大化从中抽取目标值的概率。从数学上来说，我们希望最大化整个数据集的正态分布的概率密度函数(PDF)的值。同样，我们可以最小化 PDF 的负对数:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi my"><img src="../Images/87ffa2c7b8ebaaaacdcd2d54a50ee975.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*_XdnGrTSY260307bE9sq2w.png"/></div></figure><p id="8978" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到损失函数对于𝜇和𝝈.都是可微的您会惊讶于编码是多么容易:</p><pre class="ko kp kq kr gt ma mb mc md aw me bi"><span id="c6dd" class="kz la iq mb b gy mf mg l mh mi">dist = tf.distributions.Normal(loc=mu, scale=sigma)<br/>loss = tf.reduce_mean(-dist.log_prob(y))</span></pre><p id="41c1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">仅此而已。在训练模型之后，我们可以看到它确实在<em class="kl"> g(x) </em>上拾取:</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/c54bd8346930444f8239032af27df3be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*F-ZfmT1xwaAbMv8U9mXVVg.png"/></div></figure><p id="7106" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的图中，蓝色的线和点代表用于生成数据的实际标准偏差和平均值，而红色的线和点代表由网络预测的不可见的<em class="kl"> x </em>值的相同值。大获成功！</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/4fc1ca2e326057f23590a94dd7de08b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*OmWcvtJtkD9rsSdv.gif"/></div></figure><h2 id="8b73" class="kz la iq bd lb lc ld dn le lf lg dp lh jy li lj lk kc ll lm ln kg lo lp lq lr bi translated">后续步骤</h2><p id="dd02" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">我们已经了解了如何预测简单的正态分布——但不用说，MDN 可以更通用——我们可以预测完全不同的分布，甚至几种不同分布的组合。您需要做的就是确保分布变量的每个参数都有一个输出节点，并验证分布的 PDF 是可微分的。</p><p id="6ae0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我只能鼓励你尝试和预测更复杂的分布——使用本文附带的笔记本，修改代码，或者在现实生活数据中尝试你的新技能！愿机会永远对你有利。</p></div></div>    
</body>
</html>