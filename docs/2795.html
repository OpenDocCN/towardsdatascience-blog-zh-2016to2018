<html>
<head>
<title>Implementing word2vec in PyTorch (skip-gram model)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 PyTorch 中实现 word2vec(跳格模型)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb?source=collection_archive---------0-----------------------#2018-03-06">https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb?source=collection_archive---------0-----------------------#2018-03-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="44a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可能听说过 word2vec 嵌入。但是你真的了解它是如何工作的吗？我想我知道。但我没有，直到实现它。</p><p id="42ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这就是为什么我创建这个指南。</p><p id="761d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 2021 年更新</strong>:更详细的文章见:<a class="ae kl" href="https://neptune.ai/blog/word-embeddings-guide" rel="noopener ugc nofollow" target="_blank">https://neptune.ai/blog/word-embeddings-guide</a></p><h1 id="3540" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">先决条件</h1><p id="1b9c" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">我假设你知道更多-更少 word2vec 是什么。</p><p id="4a4e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">文集</strong></p><p id="246b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了能够跟踪每一步，我使用了以下 nano 语料库:</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="lu lv l"/></div></figure><h2 id="cecf" class="lw kn iq bd ko lx ly dn ks lz ma dp kw jy mb mc la kc md me le kg mf mg li mh bi translated">创造词汇</h2><p id="417e" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">第一步是用 word2vec 创建词汇表。它必须从头开始构建，因为不支持扩展它。</p><p id="76b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">词汇表基本上是一个带有指定索引的独特单词列表。</p><p id="b4aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">语料库非常简单和简短。在实际实现中，我们必须执行大小写规范化，删除一些标点符号等，但为了简单起见，让我们使用这些漂亮而干净的数据。无论如何，我们必须把它符号化:</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="lu lv l"/></div></figure><p id="b3e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这将为我们提供一个令牌列表:</p><pre class="lp lq lr ls gt mi mj mk ml aw mm bi"><span id="7ff0" class="lw kn iq mj b gy mn mo l mp mq">[['he', 'is', 'a', 'king'],<br/> ['she', 'is', 'a', 'queen'],<br/> ['he', 'is', 'a', 'man'],<br/> ['she', 'is', 'a', 'woman'],<br/> ['warsaw', 'is', 'poland', 'capital'],<br/> ['berlin', 'is', 'germany', 'capital'],<br/> ['paris', 'is', 'france', 'capital']]</span></pre><p id="19a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们迭代语料库中的标记，并生成唯一单词(标记)的列表。接下来，我们创建两个字典，用于单词和索引之间的映射。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="lu lv l"/></div></figure><p id="4e87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这给了我们:</p><pre class="lp lq lr ls gt mi mj mk ml aw mm bi"><span id="0c92" class="lw kn iq mj b gy mn mo l mp mq"> 0: 'he',<br/> 1: 'is',<br/> 2: 'a',<br/> 3: 'king',<br/> 4: 'she',<br/> 5: 'queen',<br/> 6: 'man',<br/> 7: 'woman',<br/> 8: 'warsaw',<br/> 9: 'poland',<br/> 10: 'capital',<br/> 11: 'berlin',<br/> 12: 'germany',<br/> 13: 'paris',<br/> 14: 'france'</span></pre><p id="1d2c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在可以生成对<code class="fe mr ms mt mj b">center word</code>，<code class="fe mr ms mt mj b">context word</code>。让我们假设上下文窗口是对称的并且等于 2。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="lu lv l"/></div></figure><p id="3fd0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它为我们提供了一组<code class="fe mr ms mt mj b">center</code>、<code class="fe mr ms mt mj b">context</code>指数:</p><pre class="lp lq lr ls gt mi mj mk ml aw mm bi"><span id="05e8" class="lw kn iq mj b gy mn mo l mp mq">array([[ 0,  1],<br/>       [ 0,  2],<br/>       ...</span></pre><p id="35f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这很容易翻译成文字:</p><pre class="lp lq lr ls gt mi mj mk ml aw mm bi"><span id="8464" class="lw kn iq mj b gy mn mo l mp mq">he is<br/>he a<br/>is he<br/>is a<br/>is king<br/>a he<br/>a is<br/>a king</span></pre><p id="c976" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这很有道理。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/00a7e3944263d34bbb157d4f5069b98a.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*uYiqfNrUIzkdMrmkBWGMPw.png"/></div></figure><h1 id="4de9" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">定义目标</h1><p id="a032" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">现在，我们正在讨论从第一个等式到工作实现的细节。</p><p id="2129" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于跳格，我们感兴趣的是预测上下文，给定中心词和一些参数化。这是我们单对的概率分布。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/59f9499da480e6f7f251e04203973e57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/0*iAMM9C9rXl-HX4zR."/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">E.g.: P(king | is)</figcaption></figure><p id="5c2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们想通过所有单词/上下文对来最大化它。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nc"><img src="../Images/948eb8fe6c8da42c175b5fcf6bc2806d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HQjPVlwsLmuWu8ZD."/></div></div></figure><p id="0520" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">等等，为什么？</strong></p><p id="02b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于我们对预测给定中心词的上下文感兴趣，所以我们希望最大化每个<code class="fe mr ms mt mj b">context</code>、<code class="fe mr ms mt mj b">center</code>对的<em class="nh"> P(上下文|中心)</em>。由于概率总和为 1——对于所有不存在的<code class="fe mr ms mt mj b">context</code>、<code class="fe mr ms mt mj b">center</code>对，我们隐式地使<em class="nh"> P(上下文|中心)</em>接近于 0。通过将这些概率相乘，如果我们的模型是好的，我们使这个函数接近 1，如果是坏的，这个函数接近 0。当然我们追求的是一个好的——所以一开始就有 max 算子。</p><p id="0e6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个表达式不太适合计算。这就是为什么我们要执行一些非常常见的转换。</p><p id="e166" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">步骤 1——用<em class="nh">负对数似然</em>替换概率。</strong></p><p id="2875" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回想一下，神经网络是关于最小化损失函数的。我们可以简单地将 P 乘以-1，但是应用 log 可以给我们更好的计算特性。这不会改变函数极值的位置(因为 log 是一个严格单调的函数)。所以表达式改为:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ni"><img src="../Images/3cfbd612a037705fed68f7067a157367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LONBNqK0W-94h5gz."/></div></div></figure><p id="a4f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">步骤 2——用总和替换乘积</strong></p><p id="bec0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一步是用总和代替乘积。我们能做到是因为:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nj"><img src="../Images/6ece12aafeafb38ff52ea7cb63e4e1a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JfP8V0NFIS7O8wMA."/></div></div></figure><p id="c15f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">步骤 3——转换成合适的损失函数</strong></p><p id="a696" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除以 par 数(T)后，我们得到最终的损失项:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nk"><img src="../Images/927b1c7b68323c52560c971a063e9e35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lak7Q89Tc39oKc_5."/></div></div></figure><h2 id="b17e" class="lw kn iq bd ko lx ly dn ks lz ma dp kw jy mb mc la kc md me le kg mf mg li mh bi translated">定义 P</h2><p id="1a2d" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">很好，但是我们如何定义 P(context|center) ？现在，让我们假设到达词实际上有两个向量。一个 if 作为中心词出现(<strong class="jp ir"> v </strong>)，第二个 if 上下文(<strong class="jp ir"> u </strong>)。假设 P 的定义如下:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nl"><img src="../Images/497462e9d628d9bc2c2c6dae206a7272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AotdIFVvn-jyC93v."/></div></div></figure><p id="d390" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">太吓人了！</p><p id="c7f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我把它分解成更小的部分。请参见以下结构:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/97645bb4d0339544dab83a03389bb22e.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/0*-OzB9lJ57NBqPh3W."/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Softmax!</figcaption></figure><p id="ca07" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这只是一个<em class="nh"> softmax </em>函数。现在仔细看看提名者</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/2f916894d7c17436e60bcfc275f5022f.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/0*AEUB9Dww-qAjINKr."/></div></figure><p id="d64c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> u </strong>和<strong class="jp ir"> v </strong>都是矢量。这个表达式就是给定的<code class="fe mr ms mt mj b">center</code>，<code class="fe mr ms mt mj b">context</code>对的标量积。更大，因为它们彼此更相似。</p><p id="b18f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，分母:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/fde1df729bc8548043d8a66d92d829d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/0*ESjlK6OSAR-Z756d."/></div></figure><p id="0c53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们正在遍历词汇表中的所有单词。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/1c9aecf4edd62cb2b33bb1c81b0a5850.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/0*dMC4L4XnNdAvjmP1."/></div></figure><p id="7d1a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以及计算给定中心词和词汇中被视为上下文词的每个词的“相似度”。</p><p id="89d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">总结一下:</strong></p><p id="e634" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于语料库中每个现有的中心、上下文对，我们正在计算它们的“相似性得分”。然后除以每一个理论上可能的情境的总和——知道分数是相对高还是相对低。因为 softmax 保证取 0 和 1 之间的值，所以它定义了有效的概率分布。</p><h1 id="12ed" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">太好了，现在我们来编码吧！</h1><p id="d5f3" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">实现这一概念的神经网络包括三层:输入、隐藏和输出。</p><h1 id="b67e" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">输入层</h1><p id="b9df" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">输入层只是以一键编码方式编码的中心字。它的尺寸是<code class="fe mr ms mt mj b">[1, vocabulary_size]</code></p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="lu lv l"/></div></figure><h1 id="683b" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">隐蔽层</h1><p id="aff3" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">隐藏层使我们的 v 向量。因此它必须有<code class="fe mr ms mt mj b">embedding_dims</code>神经元。为了计算它的价值，我们必须定义<code class="fe mr ms mt mj b">W1</code>权重矩阵。当然它必须是<code class="fe mr ms mt mj b">[embedding_dims, vocabulary_size].</code>没有激活函数——只有简单的矩阵乘法。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="lu lv l"/></div></figure><p id="8b85" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">重要的是——<code class="fe mr ms mt mj b">W1</code>的每一列存储单个单词的<strong class="jp ir"> v </strong>向量。为什么？因为 x 是一个热点，如果您将一个热点向量乘以矩阵，结果与从其中选择选择单列相同。用一张纸自己试试；)</p><h2 id="3375" class="lw kn iq bd ko lx ly dn ks lz ma dp kw jy mb mc la kc md me le kg mf mg li mh bi translated">输出层</h2><p id="b02d" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">最后一层必须有<code class="fe mr ms mt mj b">vocabulary_size</code>神经元——因为它为每个单词生成概率。因此，就形状而言，<code class="fe mr ms mt mj b">W2</code>就是<code class="fe mr ms mt mj b">[vocabulary_size, embedding_dims]</code>。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="lu lv l"/></div></figure><p id="a749" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在此之上，我们必须使用 softmax 层。PyTorch 提供了这个的优化版本，结合了<code class="fe mr ms mt mj b">log</code>——因为常规的 softmax 在数值上并不真正稳定:</p><pre class="lp lq lr ls gt mi mj mk ml aw mm bi"><span id="c2b1" class="lw kn iq mj b gy mn mo l mp mq">log_softmax = F.log_softmax(a2, dim=0)</span></pre><p id="d451" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这相当于计算 softmax，然后应用 log。</p><p id="a643" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们可以计算损失。像往常一样，PyTorch 提供了我们需要的一切:</p><pre class="lp lq lr ls gt mi mj mk ml aw mm bi"><span id="f062" class="lw kn iq mj b gy mn mo l mp mq">loss = F.nll_loss(log_softmax.view(1,-1), y_true)</span></pre><p id="d490" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mr ms mt mj b">nll_loss</code>在 logsoftmax 上计算负对数似然。<code class="fe mr ms mt mj b">y_true</code>是上下文单词——我们想让它尽可能高——因为对<code class="fe mr ms mt mj b">x, y_true</code>来自训练数据——所以它们确实是上下文的中心。</p><h2 id="d8e6" class="lw kn iq bd ko lx ly dn ks lz ma dp kw jy mb mc la kc md me le kg mf mg li mh bi translated"><strong class="ak">反向投影</strong></h2><p id="d6b5" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">当我们钓到向前传球时，现在是时候表演向后传球了。简单来说:</p><pre class="lp lq lr ls gt mi mj mk ml aw mm bi"><span id="4484" class="lw kn iq mj b gy mn mo l mp mq">loss.backward()</span></pre><p id="aeac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了优化，使用了 SDG。它如此简单，以至于用手工编写比创建优化器对象更快:</p><pre class="lp lq lr ls gt mi mj mk ml aw mm bi"><span id="2cc4" class="lw kn iq mj b gy mn mo l mp mq">W1.data -= 0.01 * W1.grad.data<br/>W2.data -= 0.01 * W2.grad.data</span></pre><p id="eca4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后一步是将梯度归零，以使下一步清晰:</p><pre class="lp lq lr ls gt mi mj mk ml aw mm bi"><span id="259c" class="lw kn iq mj b gy mn mo l mp mq">W1.grad.data.zero_()<br/>W2.grad.data.zero_()</span></pre><h2 id="c439" class="lw kn iq bd ko lx ly dn ks lz ma dp kw jy mb mc la kc md me le kg mf mg li mh bi translated">训练循环</h2><p id="e07d" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">是时候把它编译成训练循环了。它可能看起来像:</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="lu lv l"/></div></figure><p id="53f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个潜在的棘手问题是<code class="fe mr ms mt mj b">y_true</code>定义。我们并不显式地创建一个 hot，而是由<code class="fe mr ms mt mj b">nll_loss</code>自己创建。</p><pre class="lp lq lr ls gt mi mj mk ml aw mm bi"><span id="8984" class="lw kn iq mj b gy mn mo l mp mq">Loss at epo 0: 4.241989389487675<br/>Loss at epo 10: 3.8398486052240646<br/>Loss at epo 20: 3.5548086541039603<br/>Loss at epo 30: 3.343840673991612<br/>Loss at epo 40: 3.183084646293095<br/>Loss at epo 50: 3.05673006943294<br/>Loss at epo 60: 2.953996729850769<br/>Loss at epo 70: 2.867735825266157<br/>Loss at epo 80: 2.79331214427948<br/>Loss at epo 90: 2.727727291413716<br/>Loss at epo 100: 2.6690095041479385</span></pre><h2 id="f220" class="lw kn iq bd ko lx ly dn ks lz ma dp kw jy mb mc la kc md me le kg mf mg li mh bi translated">提取向量</h2><p id="cc1a" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">好了，我们已经训练好了网络。第一，最后一件事是提取单词的向量。有三种可能:</p><ul class=""><li id="213f" class="nq nr iq jp b jq jr ju jv jy ns kc nt kg nu kk nv nw nx ny bi translated">使用 W1 中的矢量 v</li><li id="9e6f" class="nq nr iq jp b jq nz ju oa jy ob kc oc kg od kk nv nw nx ny bi translated">使用 W2 中的向量 u</li><li id="6896" class="nq nr iq jp b jq nz ju oa jy ob kc oc kg od kk nv nw nx ny bi translated">使用平均 v 和 u</li></ul><p id="8d89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">试着自己思考什么时候用哪个；)</p><h1 id="9c0e" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">待续</h1><p id="a53f" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">我正在做这个的在线互动演示。应该很快就有了。敬请期待；)</p><p id="bf37" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以从<a class="ae kl" href="https://gist.github.com/mbednarski/da08eb297304f7a66a3840e857e060a0" rel="noopener ugc nofollow" target="_blank">这里</a>下载代码。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi oe"><img src="../Images/26acb2042bdf7774e0dc5d5ca1d0234c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-F-Igl53j6bnpsSJpsWPFQ.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Working on this!</figcaption></figure></div></div>    
</body>
</html>