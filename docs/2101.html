<html>
<head>
<title>Using pyspark with Jupyter on a local computer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在本地计算机上使用pyspark和Jupyter</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-pyspark-with-jupyter-on-a-local-computer-edca6ae64bb6?source=collection_archive---------4-----------------------#2017-12-18">https://towardsdatascience.com/using-pyspark-with-jupyter-on-a-local-computer-edca6ae64bb6?source=collection_archive---------4-----------------------#2017-12-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/2371dd657e5ef5494a097b28a5f3ca35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*h7xQUkvB_dyJKpJ3EP_TvA.jpeg"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image orogin: <a class="ae jy" href="https://www.linkedin.com/pulse/how-run-apache-spark-jupyter-notebook-praneeth-bellamkonda/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a></figcaption></figure><p id="217a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在版本2.0发布之后，随着许多新特性和改进的SQL功能的出现，<a class="ae jy" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>成为我所掌握的更强大的工具之一。<br/>因此，我想通过添加从Jupyter notebook / IPython控制台使用它的功能来增加这一强大工具的易用性。由于能够添加<a class="ae jy" href="https://github.com/jupyter/jupyter/wiki/Jupyter-kernels" rel="noopener ugc nofollow" target="_blank">自定义内核</a>，我创建了一组非常简单的指令(在Ubuntu / CentOS上测试)来在本地机器上安装带有Jupyter内核的Spark。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="1c98" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">在Linux上安装Spark</h1><p id="8199" class="pw-post-body-paragraph jz ka iq kb b kc mc ke kf kg md ki kj kk me km kn ko mf kq kr ks mg ku kv kw ij bi translated">本手册在2.2.0版上进行了测试，但应该适用于所有版本。我假设您已经安装了Python和Java。为了在您的机器上安装Spark，请遵循以下步骤:</p><ol class=""><li id="ae48" class="mh mi iq kb b kc kd kg kh kk mj ko mk ks ml kw mm mn mo mp bi translated">从Apache <a class="ae jy" href="http://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank">网站</a>下载tar.gz文件(我假设你是下载到<em class="mq"> /opt </em> ): <br/> <code class="fe mr ms mt mu b">wget <a class="ae jy" href="https://www.apache.org/dyn/closer.lua/spark/spark-2.2.1/spark-2.2.1-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">https://www.apache.org/dyn/closer.lua/spark/spark-2.2.1/spark-2.2.1-bin-hadoop2.7.tgz</a></code></li><li id="7516" class="mh mi iq kb b kc mv kg mw kk mx ko my ks mz kw mm mn mo mp bi translated">提取文件并创建一个软链接到文件夹:<br/> <code class="fe mr ms mt mu b">tar -xvzf spark-2.2.1-bin-hadoop2.7.tgz <br/>ln -s spark-2.2.1-bin-hadoop2.7 spark</code></li><li id="c47b" class="mh mi iq kb b kc mv kg mw kk mx ko my ks mz kw mm mn mo mp bi translated">验证py4j版本(我们将需要它来连接Spark和Jupyter): <br/> <code class="fe mr ms mt mu b">ls -1 /opt/spark/python/lib/py4j* | awk -F "-" '{print $2}'<br/></code>我得到的输出是0.10.4，我们稍后将使用它作为<em class="mq">&lt;&lt;PY4J _ VERSION&gt;&gt;</em></li><li id="967d" class="mh mi iq kb b kc mv kg mw kk mx ko my ks mz kw mm mn mo mp bi translated">验证你使用的Python路径:<br/> <code class="fe mr ms mt mu b">which python<br/></code>我得到的输出是<em class="mq">/HOME/Nimrod/miniconda/envs/compass/bin/Python 2，</em>我们将此作为&lt;&lt;<em class="mq">Python _ HOME</em>&gt;&gt;</li></ol><p id="0453" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">完成以上操作后，创建一个内核json文件:<br/> <code class="fe mr ms mt mu b">mkdir -p ~/.local/share/jupyter/kernels/spark2-local</code></p><p id="5782" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">编辑文件:<br/> <code class="fe mr ms mt mu b">vim ~/.local/share/jupyter/kernels/spark2-local/kernel.json<br/></code>，添加以下内容(不要忘记替换两个占位符):</p><pre class="na nb nc nd gt ne mu nf ng aw nh bi"><span id="2900" class="ni lf iq mu b gy nj nk l nl nm">{<br/> "display_name": "spark2-local-compass",<br/> "language": "python",<br/> "argv": [<br/>  "<em class="mq">&lt;&lt;PYTHON_HOME&gt;&gt;</em>",<br/>  "-m",<br/>  "IPython.kernel",<br/>  "-f",<br/>  "{connection_file}"<br/> ],<br/> "env": {<br/>  "SPARK_HOME": "/opt/spark/",<br/>  "PYTHONPATH": "/home/Nimrod/dev/theGarage/:/opt/spark/python/:/opt/spark/python/lib/py4j-<em class="mq">&lt;&lt;PY4J_VERSION&gt;&gt;</em>-src.zip",<br/>  "PYTHONSTARTUP": "/opt/spark/python/pyspark/shell.py",<br/>  "PYSPARK_SUBMIT_ARGS": "--master local[*] --driver-memory 3g --executor-memory 2g pyspark-shell",<br/>  "PYSPARK_PYTHON": "<em class="mq">&lt;&lt;PYTHON_HOME&gt;&gt;</em>"<br/> }<br/>}</span></pre><p id="dabb" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">现在，您应该能够观察到在<code class="fe mr ms mt mu b">jupyter kernelspec list</code>中列出的新内核，或者在jupyter UI中的<em class="mq">新笔记本</em>类型下。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/afccad5eee67546695066938f9f652bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*PoBOgbWFaFpr79t8N733aw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Example of The new kernel in the Jupyter UI</figcaption></figure><p id="273a" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">上面提到的当前问题是，使用<code class="fe mr ms mt mu b">--master local[*]</code>参数是将Derby作为本地DB使用，这导致无法在同一个目录下打开多个笔记本。</p><p id="6e2d" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">对于大多数用户来说，论文并不是一个真正的大问题，但是自从我们开始使用<a class="ae jy" href="https://drivendata.github.io/cookiecutter-data-science/" rel="noopener ugc nofollow" target="_blank">数据科学Cookiecutter </a>以来，文件系统的逻辑结构将所有笔记本放在同一个目录下。每当我们想要在多台笔记本电脑上同时工作时，这会导致一个问题。</p><p id="4e04" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我花了很长时间寻找解决方案，最终在很长时间后，Amit Wolfenfeld很快找到了解决方案。第一步是安装<a class="ae jy" href="https://www.postgresql.org/" rel="noopener ugc nofollow" target="_blank"> postgresql </a>并确保其运行！</p><p id="cfa8" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">为了让pySpark使用postgresql，我们需要JDBC驱动程序，从这里的<a class="ae jy" href="https://jdbc.postgresql.org/download.html" rel="noopener ugc nofollow" target="_blank">下载它们并保存到<em class="mq"> /opt/spark/jars/中。</em></a></p><p id="d873" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">接下来，将用户更改为postgres ( <code class="fe mr ms mt mu b">sudo su postgres</code>)并运行<code class="fe mr ms mt mu b">psql</code> : <br/> <code class="fe mr ms mt mu b">CREATE USER hive;<br/>ALTER ROLE hive WITH PASSWORD 'mypassword';<br/>CREATE DATABASE hive_metastore;<br/>GRANT ALL PRIVILEGES ON DATABASE hive_metastore TO hive;<br/>\q</code></p><p id="fdaa" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">运行该命令后，确保<strong class="kb ir">重启postgreql服务</strong>。最后一步是在Spark的config目录下创建一个文件(假设您遵循了我上面建议的路径，命令应该是:<code class="fe mr ms mt mu b">vim /opt/spark/conf/hive-site.xml</code></p><p id="c818" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">将以下内容添加到文件中:</p><pre class="na nb nc nd gt ne mu nf ng aw nh bi"><span id="769b" class="ni lf iq mu b gy nj nk l nl nm">&lt;configuration&gt;<br/>&lt;property&gt;<br/>  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;<br/>  &lt;value&gt;jdbc:postgresql://localhost:5432/hive_metastore&lt;/value&gt;<br/>&lt;/property&gt;</span><span id="86fc" class="ni lf iq mu b gy nq nk l nl nm">&lt;property&gt;<br/>  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;<br/>  &lt;value&gt;org.postgresql.Driver&lt;/value&gt;<br/>&lt;/property&gt;</span><span id="255d" class="ni lf iq mu b gy nq nk l nl nm">&lt;property&gt;<br/>&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;<br/>  &lt;value&gt;hive&lt;/value&gt;<br/>&lt;/property&gt;</span><span id="1c32" class="ni lf iq mu b gy nq nk l nl nm">&lt;property&gt;<br/>  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;<br/>  &lt;value&gt;mypassword&lt;/value&gt;<br/>&lt;/property&gt;</span><span id="2506" class="ni lf iq mu b gy nq nk l nl nm">&lt;/configuration&gt;</span></pre><p id="50d2" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">就是这样！现在，您可以开始尽可能多的pyspark笔记本。</p></div></div>    
</body>
</html>