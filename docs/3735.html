<html>
<head>
<title>Building Neural Network from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始构建神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-neural-network-from-scratch-9c88535bf8e9?source=collection_archive---------0-----------------------#2018-06-13">https://towardsdatascience.com/building-neural-network-from-scratch-9c88535bf8e9?source=collection_archive---------0-----------------------#2018-06-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7d7e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用 Python 中的 Numpy 对多层感知器的简单介绍。</h2></div><p id="938d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这本笔记本中，我们将使用 numpy 构建一个神经网络(多层感知器)，并成功训练它识别图像中的数字。深度学习是一个庞大的主题，但我们必须从某个地方开始，所以让我们从多层感知器的神经网络的基础开始。你可以在笔记本版本<a class="ae lb" href="https://github.com/aayushmnit/Deep_learning_explorations/tree/master/1_MLP_from_scratch" rel="noopener ugc nofollow" target="_blank">这里</a>或者我的<a class="ae lb" href="https://aayushmnit.github.io/posts/2018/06/Building_neural_network_from_scratch/" rel="noopener ugc nofollow" target="_blank">网站</a>找到同样的博客。</p><h1 id="e770" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">什么是神经网络？</h1><p id="f56c" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">神经网络是一种机器学习模型，它受到我们大脑中神经元的启发，其中许多神经元与许多其他神经元相连，以将输入转化为输出(简单吧？).大多数情况下，我们可以看到任何机器学习模型，并将其视为一个接受输入并产生所需输出的函数；神经网络也是一样。</p><h1 id="d821" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">什么是多层感知器？</h1><p id="2ed1" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">多层感知器是一种网络类型，其中一组感知器的多个层堆叠在一起形成一个模型。在我们进入一个层和多个感知器的概念之前，让我们从这个网络的构建模块开始，它是一个感知器。将感知器/神经元视为一个线性模型，它接受多个输入并产生一个输出。在我们的例子中，感知器是一个线性模型，它接受一组输入，将它们与权重相乘，并添加一个偏差项以生成一个输出。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi lz"><img src="../Images/adc9a0a3987ea6423ec23613a1e5c02c.png" data-original-src="https://miro.medium.com/v2/resize:fit:202/format:webp/0*k6ziYMhOBB258avh.png"/></div></div></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi ml"><img src="../Images/ceeb732df66b5af256e0e1c97ab12d55.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/0*BcQ26EY75dXdztUF.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 1: Perceptron image</figcaption></figure><p id="3174" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mq">Image credit = https://commons . wikimedia . org/wiki/File:perceptron . png/</em></p><p id="a069" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，如果我们将这些感知机堆叠在一起，它就变成了一个隐藏层，在现代深度学习术语中也称为密集层。<br/> <strong class="kh ir">密集层，</strong></p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/e019141bc8be41312f5e5fe5f136d727.png" data-original-src="https://miro.medium.com/v2/resize:fit:230/format:webp/0*w5CkilAiPnS3vQvQ.png"/></div></figure><p id="e1c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mq">注意，偏差项现在是一个向量，W 是一个权重矩阵</em></p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/79246b1ad4dfeef4fea6692620e9f9a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*nipS8XYMKvUjvLoc.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 2: Single dense layer perceptron network</figcaption></figure><p id="cc46" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mq">Image credit = http://www . t example . net/tikz/examples/neural-network/</em></p><p id="ca5f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们了解了密集层，让我们把它们加起来，这个网络就变成了一个多层感知器网络。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mt"><img src="../Images/122bdf5d36fea5c81cfd34509072a0f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*T5d_brtoGf6JSnW_.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Fig 3: Multi layer perceptron network</figcaption></figure><p id="4936" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mq">Image credit = http://pubs . scie pub . com/ajmm/3/3/1/figure/2s</em></p><p id="09f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你已经注意到我们的稠密层，只有线性函数，并且线性函数的任何组合只导致线性输出。由于我们希望我们的 MLP 具有灵活性并学习非线性决策边界，我们还需要将非线性引入网络。我们通过添加激活函数来实现引入非线性的任务。有各种各样的激活函数可以使用，但我们将实现整流线性单位(ReLu)，这是一个流行的激活函数。ReLU 函数是一个简单的函数，它对于任何小于零的输入值都是零，对于大于零的值也是相同的值。<br/> <strong class="kh ir"> ReLU 功能</strong></p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/14a81dd54f935673baa605d300f7f5f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:270/format:webp/0*CUVFZM3eXIJEFXiO.png"/></div></figure><p id="7fc1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们理解了密集层，也理解了激活函数的目的，剩下的唯一事情就是训练网络。为了训练神经网络，我们需要一个损失函数，每一层都应该有一个<strong class="kh ir">前馈回路</strong>和<strong class="kh ir">反向传播回路</strong>。前馈回路接收输入并产生输出以进行预测，反向传播回路通过调整层中的权重以降低输出损失来帮助训练模型。在反向传播中，权重更新通过使用链规则的反向传播梯度来完成，并使用优化算法来优化。在我们的例子中，我们将使用 SGD(随机梯度下降)。如果你不理解梯度权重更新和 SGD 的概念，我推荐你看 Andrew NG 讲座机器学习<a class="ae lb" href="https://www.coursera.org/ml" rel="noopener ugc nofollow" target="_blank">第一周。</a></p><p id="fd67" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，总结一个神经网络需要几个构件</p><ul class=""><li id="7abf" class="mv mw iq kh b ki kj kl km ko mx ks my kw mz la na nb nc nd bi translated"><strong class="kh ir">致密层</strong> —全连通层，</li></ul><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/c82ea78544a260884803b210270151c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:230/format:webp/0*zuvatxF1Z-gcLzpa.png"/></div></figure><ul class=""><li id="73e6" class="mv mw iq kh b ki kj kl km ko mx ks my kw mz la na nb nc nd bi translated"><strong class="kh ir"> ReLU layer </strong>(或任何其他引入非线性的激活功能)</li><li id="bc0e" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated"><strong class="kh ir">损失函数</strong>——(多类分类问题时的交叉熵)</li><li id="5433" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated"><strong class="kh ir">反向传播算法</strong> —具有反向传播梯度的随机梯度下降</li></ul><p id="6ea3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们一个一个地接近他们。</p><h1 id="a3af" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">编码从这里开始:</h1><p id="8a9e" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">让我们从导入创建神经网络所需的一些库开始。</p><pre class="ma mb mc md gt nj nk nl nm aw nn bi"><span id="f849" class="no ld iq nk b gy np nq l nr ns">from __future__ import print_function<br/>import numpy as np ## For numerical python<br/>np.random.seed(42)</span></pre><p id="9210" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每一层都有一个向前传递和向后传递的实现。让我们创建一个可以向前传递的主类层<em class="mq">。向前()</em>和向后传球<em class="mq">。向后()。</em></p><pre class="ma mb mc md gt nj nk nl nm aw nn bi"><span id="848b" class="no ld iq nk b gy np nq l nr ns">class Layer:<br/>    <br/>    #A building block. Each layer is capable of performing two things:</span><span id="768f" class="no ld iq nk b gy nt nq l nr ns">    #- Process input to get output:           output = layer.forward(input)<br/>    <br/>    #- Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)<br/>    <br/>    #Some layers also have learnable parameters which they update during layer.backward.<br/>    <br/>    def __init__(self):<br/>        # Here we can initialize layer parameters (if any) and auxiliary stuff.<br/>        # A dummy layer does nothing<br/>        pass<br/>    <br/>    def forward(self, input):<br/>        # Takes input data of shape [batch, input_units], returns output data [batch, output_units]<br/>        <br/>        # A dummy layer just returns whatever it gets as input.<br/>        return input</span><span id="6354" class="no ld iq nk b gy nt nq l nr ns">    def backward(self, input, grad_output):<br/>        # Performs a backpropagation step through the layer, with respect to the given input.<br/>        <br/>        # To compute loss gradients w.r.t input, we need to apply chain rule (backprop):<br/>        <br/>        # d loss / d x  = (d loss / d layer) * (d layer / d x)<br/>        <br/>        # Luckily, we already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.<br/>        <br/>        # If our layer has parameters (e.g. dense layer), we also need to update them here using d loss / d layer<br/>        <br/>        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly<br/>        num_units = input.shape[1]<br/>        <br/>        d_layer_d_input = np.eye(num_units)<br/>        <br/>        return np.dot(grad_output, d_layer_d_input) # chain rule</span></pre><h1 id="d72c" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">非线性关系层</h1><p id="96d7" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">这是你能得到的最简单的层:它简单地将非线性应用于你的网络的每个元素。</p><pre class="ma mb mc md gt nj nk nl nm aw nn bi"><span id="2908" class="no ld iq nk b gy np nq l nr ns">class ReLU(Layer):<br/>    def __init__(self):<br/>        # ReLU layer simply applies elementwise rectified linear unit to all inputs<br/>        pass<br/>    <br/>    def forward(self, input):<br/>        # Apply elementwise ReLU to [batch, input_units] matrix<br/>        relu_forward = np.maximum(0,input)<br/>        return relu_forward<br/>    <br/>    def backward(self, input, grad_output):<br/>        # Compute gradient of loss w.r.t. ReLU input<br/>        relu_grad = input &gt; 0<br/>        return grad_output*relu_grad</span></pre><h1 id="7fed" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">致密层</h1><p id="16fb" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">现在让我们构建一些更复杂的东西。与非线性不同，密集层实际上有东西要学。</p><p id="24dd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">密集层应用仿射变换。在矢量化形式中，它可以描述为:</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/1c1453ba4c3acea1e51fa9200c1bf632.png" data-original-src="https://miro.medium.com/v2/resize:fit:230/format:webp/0*YVPsg8d8fn9tnrED.png"/></div></figure><p id="cdca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在哪里</p><ul class=""><li id="64b3" class="mv mw iq kh b ki kj kl km ko mx ks my kw mz la na nb nc nd bi translated">x 是形状[批量大小，数量特征]的对象特征矩阵，</li><li id="ec76" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated">w 是权重矩阵[特征数量，输出数量]</li><li id="0f0c" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated">b 是 num_outputs 偏差的向量。</li></ul><p id="3894" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">W 和 b 都在层创建期间初始化，并在每次调用 backward 时更新。请注意，我们正在使用<strong class="kh ir"> Xavier 初始化</strong>，这是一个训练我们的模型更快收敛的技巧<a class="ae lb" href="http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization" rel="noopener ugc nofollow" target="_blank">阅读更多</a>。我们不是用随机分布的小数字初始化我们的权重，而是用平均值 0 和方差 2/(输入数+输出数)初始化我们的权重</p><pre class="ma mb mc md gt nj nk nl nm aw nn bi"><span id="f369" class="no ld iq nk b gy np nq l nr ns">class Dense(Layer):<br/>    def __init__(self, input_units, output_units, learning_rate=0.1):<br/>        # A dense layer is a layer which performs a learned affine transformation:<br/>        # f(x) = &lt;W*x&gt; + b<br/>        <br/>        self.learning_rate = learning_rate<br/>        self.weights = np.random.normal(loc=0.0, <br/>                                        scale = np.sqrt(2/(input_units+output_units)), <br/>                                        size = (input_units,output_units))<br/>        self.biases = np.zeros(output_units)<br/>        <br/>    def forward(self,input):<br/>        # Perform an affine transformation:<br/>        # f(x) = &lt;W*x&gt; + b<br/>        <br/>        # input shape: [batch, input_units]<br/>        # output shape: [batch, output units]<br/>        <br/>        return np.dot(input,self.weights) + self.biases<br/>    <br/>    def backward(self,input,grad_output):<br/>        # compute d f / d x = d f / d dense * d dense / d x<br/>        # where d dense/ d x = weights transposed<br/>        grad_input = np.dot(grad_output, self.weights.T)<br/>        <br/>        # compute gradient w.r.t. weights and biases<br/>        grad_weights = np.dot(input.T, grad_output)<br/>        grad_biases = grad_output.mean(axis=0)*input.shape[0]<br/>        <br/>        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape<br/>        <br/>        # Here we perform a stochastic gradient descent step. <br/>        self.weights = self.weights - self.learning_rate * grad_weights<br/>        self.biases = self.biases - self.learning_rate * grad_biases<br/>        <br/>        return grad_input</span></pre><h1 id="5dc6" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">损失函数</h1><p id="a03e" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">由于我们希望预测概率，因此在我们的网络上定义 softmax 非线性并计算给定预测概率的损失是合乎逻辑的。但是，有一种更好的方法可以做到这一点。</p><p id="6f4e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们将交叉熵的表达式写为 softmax logits (a)的函数，您会看到:</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/e78aaffca43996a25b5fba975091edb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/0*NyJWhcPauclC4Gqw.png"/></div></figure><p id="63f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们仔细看看，我们会发现它可以重写为:</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/51b742e24f48d837d50a1d11fa1ba2a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/0*sIm8xCuwT2yigCWJ.png"/></div></figure><p id="94a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它被称为 Log-softmax，它在各个方面都优于 naive log(softmax(a)):</p><ul class=""><li id="4c07" class="mv mw iq kh b ki kj kl km ko mx ks my kw mz la na nb nc nd bi translated">更好的数值稳定性</li><li id="18f5" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated">更容易得到正确的导数</li><li id="cbe2" class="mv mw iq kh b ki ne kl nf ko ng ks nh kw ni la na nb nc nd bi translated">计算速度略微加快</li></ul><p id="2bda" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，为什么不在我们的计算中使用 log-softmax，而不去估算概率呢？</p><pre class="ma mb mc md gt nj nk nl nm aw nn bi"><span id="de06" class="no ld iq nk b gy np nq l nr ns">def softmax_crossentropy_with_logits(logits,reference_answers):<br/>    # Compute crossentropy from logits[batch,n_classes] and ids of correct answers<br/>    logits_for_answers = logits[np.arange(len(logits)),reference_answers]<br/>    <br/>    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))<br/>    <br/>    return xentropy</span><span id="b5c5" class="no ld iq nk b gy nt nq l nr ns">def grad_softmax_crossentropy_with_logits(logits,reference_answers):<br/>    # Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers<br/>    ones_for_answers = np.zeros_like(logits)<br/>    ones_for_answers[np.arange(len(logits)),reference_answers] = 1<br/>    <br/>    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)<br/>    <br/>    return (- ones_for_answers + softmax) / logits.shape[0]</span></pre><h1 id="cef7" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">全网络</h1><p id="84c7" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">现在，让我们将刚刚构建的内容结合到一个有效的神经网络中。正如我之前所说的，我们将使用手写数字的 MNIST 数据作为我们的例子。幸运的是，Keras 已经有了 numpy 数组格式的，所以让我们导入它吧！。</p><pre class="ma mb mc md gt nj nk nl nm aw nn bi"><span id="2970" class="no ld iq nk b gy np nq l nr ns">import keras<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="832b" class="no ld iq nk b gy nt nq l nr ns">def load_dataset(flatten=False):<br/>    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()</span><span id="c501" class="no ld iq nk b gy nt nq l nr ns">    # normalize x<br/>    X_train = X_train.astype(float) / 255.<br/>    X_test = X_test.astype(float) / 255.</span><span id="b418" class="no ld iq nk b gy nt nq l nr ns">    # we reserve the last 10000 training examples for validation<br/>    X_train, X_val = X_train[:-10000], X_train[-10000:]<br/>    y_train, y_val = y_train[:-10000], y_train[-10000:]</span><span id="4880" class="no ld iq nk b gy nt nq l nr ns">    if flatten:<br/>        X_train = X_train.reshape([X_train.shape[0], -1])<br/>        X_val = X_val.reshape([X_val.shape[0], -1])<br/>        X_test = X_test.reshape([X_test.shape[0], -1])</span><span id="685d" class="no ld iq nk b gy nt nq l nr ns">    return X_train, y_train, X_val, y_val, X_test, y_test</span><span id="f648" class="no ld iq nk b gy nt nq l nr ns">X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)</span><span id="df09" class="no ld iq nk b gy nt nq l nr ns">## Let's look at some example<br/>plt.figure(figsize=[6,6])<br/>for i in range(4):<br/>    plt.subplot(2,2,i+1)<br/>    plt.title("Label: %i"%y_train[i])<br/>    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');</span></pre><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/c7efa5782f108b27e1359a7709f021da.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/0*WKXveMllOWXkE5Iq.png"/></div></figure><p id="cab4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将网络定义为一系列层，每一层都应用在前一层之上。在这种情况下，计算预测和训练变得微不足道。</p><pre class="ma mb mc md gt nj nk nl nm aw nn bi"><span id="6acf" class="no ld iq nk b gy np nq l nr ns">network = []<br/>network.append(Dense(X_train.shape[1],100))<br/>network.append(ReLU())<br/>network.append(Dense(100,200))<br/>network.append(ReLU())<br/>network.append(Dense(200,10))</span><span id="8284" class="no ld iq nk b gy nt nq l nr ns">def forward(network, X):<br/>    # Compute activations of all network layers by applying them sequentially.<br/>    # Return a list of activations for each layer. <br/>    <br/>    activations = []<br/>    input = X</span><span id="b028" class="no ld iq nk b gy nt nq l nr ns">    # Looping through each layer<br/>    for l in network:<br/>        activations.append(l.forward(input))<br/>        # Updating input to last layer output<br/>        input = activations[-1]<br/>    <br/>    assert len(activations) == len(network)<br/>    return activations</span><span id="f2e9" class="no ld iq nk b gy nt nq l nr ns">def predict(network,X):<br/>    # Compute network predictions. Returning indices of largest Logit probability</span><span id="c340" class="no ld iq nk b gy nt nq l nr ns">    logits = forward(network,X)[-1]<br/>    return logits.argmax(axis=-1)</span><span id="ad22" class="no ld iq nk b gy nt nq l nr ns">def train(network,X,y):<br/>    # Train our network on a given batch of X and y.<br/>    # We first need to run forward to get all layer activations.<br/>    # Then we can run layer.backward going from last to first layer.<br/>    # After we have called backward for all layers, all Dense layers have already made one gradient step.<br/>    <br/>    <br/>    # Get the layer activations<br/>    layer_activations = forward(network,X)<br/>    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]<br/>    logits = layer_activations[-1]<br/>    <br/>    # Compute the loss and the initial gradient<br/>    loss = softmax_crossentropy_with_logits(logits,y)<br/>    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)<br/>    <br/>    # Propagate gradients through the network<br/>    # Reverse propogation as this is backprop<br/>    for layer_index in range(len(network))[::-1]:<br/>        layer = network[layer_index]<br/>        <br/>        loss_grad = layer.backward(layer_inputs[layer_index],loss_grad) #grad w.r.t. input, also weight updates<br/>        <br/>    return np.mean(loss)</span></pre><h1 id="2497" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">训练循环</h1><p id="7aed" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">我们将数据分成小批，将每个这样的小批输入网络并更新权重。这种训练方法被称为小批量随机梯度下降。</p><pre class="ma mb mc md gt nj nk nl nm aw nn bi"><span id="b15e" class="no ld iq nk b gy np nq l nr ns">from tqdm import trange<br/>def iterate_minibatches(inputs, targets, batchsize, shuffle=False):<br/>    assert len(inputs) == len(targets)<br/>    if shuffle:<br/>        indices = np.random.permutation(len(inputs))<br/>    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):<br/>        if shuffle:<br/>            excerpt = indices[start_idx:start_idx + batchsize]<br/>        else:<br/>            excerpt = slice(start_idx, start_idx + batchsize)<br/>        yield inputs[excerpt], targets[excerpt]</span><span id="1b01" class="no ld iq nk b gy nt nq l nr ns">from IPython.display import clear_output<br/>train_log = []<br/>val_log = []</span><span id="be6c" class="no ld iq nk b gy nt nq l nr ns">for epoch in range(25):</span><span id="72b6" class="no ld iq nk b gy nt nq l nr ns">    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):<br/>        train(network,x_batch,y_batch)<br/>    <br/>    train_log.append(np.mean(predict(network,X_train)==y_train))<br/>    val_log.append(np.mean(predict(network,X_val)==y_val))<br/>    <br/>    clear_output()<br/>    print("Epoch",epoch)<br/>    print("Train accuracy:",train_log[-1])<br/>    print("Val accuracy:",val_log[-1])<br/>    plt.plot(train_log,label='train accuracy')<br/>    plt.plot(val_log,label='val accuracy')<br/>    plt.legend(loc='best')<br/>    plt.grid()<br/>    plt.show()</span><span id="2462" class="no ld iq nk b gy nt nq l nr ns">Epoch 24<br/>Train accuracy: 1.0<br/>Val accuracy: 0.9809</span></pre><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/1a834072aedadb40fcc9f8d842dbfd57.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/0*CKHFRJNnk547glVm.png"/></div></figure><p id="fac0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们所看到的，我们已经成功地训练了一个完全用 numpy 编写的 MLP，具有很高的验证准确性！</p></div></div>    
</body>
</html>