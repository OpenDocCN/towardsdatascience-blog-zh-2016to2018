<html>
<head>
<title>Understanding Bidirectional RNN in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解PyTorch中的双向RNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66?source=collection_archive---------2-----------------------#2017-11-13">https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66?source=collection_archive---------2-----------------------#2017-11-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7c9f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">快速回顾</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/db1b3b6db88d3b162ae03e3d106682d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6QnPUSv_t9BY9Fv8_aLb-Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Fig 1: General Structure of Bidirectional Recurrent Neural Networks. <a class="ae kv" href="http://colah.github.io/posts/2015-09-NN-Types-FP/" rel="noopener ugc nofollow" target="_blank">Source: colah’s blog</a></figcaption></figure><p id="0630" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">双向递归神经网络(RNN)实际上只是将两个独立的递归神经网络放在一起。对于一个网络，输入序列以正常的时间顺序馈送，而对于另一个网络，输入序列以相反的时间顺序馈送。两个网络的输出通常在每个时间步被连接，尽管还有其他选择，例如求和。</p><p id="064c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种结构允许网络在每个时间步都具有关于序列的前向和后向信息。这个概念似乎很简单。但是当实际实现一个利用双向结构的神经网络时，就出现了混淆…</p><h2 id="03fd" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">困惑</h2><p id="4884" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">第一个困惑是关于<strong class="ky ir">将双向RNN的输出转发到密集神经网络</strong>的方式。对于普通的RNNs，我们可以只转发最后一个时间步的输出，下面的图片是我通过谷歌找到的，展示了双向RNN的类似技术。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/675c99c8b22a94fd52bcb017a50a48b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*GRQ91HNASB7MAJPTTlVvfw.jpeg"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Fig 2: A confusing formulation. <a class="ae kv" href="http://doc.paddlepaddle.org/develop/doc/_images/bi_lstm.jpg" rel="noopener ugc nofollow" target="_blank">Image Source</a></figcaption></figure><p id="6fc3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是等等…如果我们选择最后一个时间步的输出，反向RNN只会看到最后一个输入(图中的x_3)。它几乎不会提供任何预测能力。</p><p id="0ca0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二个困惑是关于<strong class="ky ir">返回隐藏状态</strong>。在seq2seq模型中，我们需要编码器的隐藏状态来初始化解码器的隐藏状态。直观地说，如果我们只能在一个时间步选择隐藏状态(如PyTorch ),我们会选择RNN刚刚消耗完序列中最后一个输入的状态。但是<strong class="ky ir">如果</strong>返回时间步长<em class="mr"> n </em>(最后一个)的隐藏状态，和以前一样，我们将得到反转RNN的隐藏状态，只看到一步输入。</p><h2 id="fa5e" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">查看Keras实现</h2><p id="a6e7" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">Keras为双向rnn提供了一个包装器。如果您查看wrappers.py中的第<em class="mr"> 292 </em>行:</p><div class="ms mt gp gr mu mv"><a href="https://github.com/fchollet/keras/blob/4edd0379e14c7b502b3c81c95c7319b5df2af65c/keras/layers/wrappers.py#L292" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd ir gy z fp na fr fs nb fu fw ip bi translated">fchollet/keras</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">用于Python的深度学习库。在TensorFlow、Theano或CNTK上运行。</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">github.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj kp mv"/></div></div></a></div><p id="1bf6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你会发现，默认情况下，反向RNN的输出按时间步长向后排序(<em class="mr"> n…1 </em>)。当<code class="fe nk nl nm nn b">return_sequences</code>为真(默认为假)时，Keras将反转它。因此，如果我们采用一个时间步长输出，Keras将在正常RNN的时间步长<em class="mr"> n </em>采用一个，在反向RNN的时间步长<em class="mr"> 1 </em>采用一个。这很好地证实了图2显示了有缺陷的结构。</p><h2 id="3d29" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">在PyTorch中它是如何工作的</h2><p id="9691" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">随着第一个困惑的解决。我们现在感兴趣的是如何在PyTorch中正确使用双向rnn:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="6010" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的笔记本回答了我们遇到的两个困惑(假设<code class="fe nk nl nm nn b">batch_first</code>为假):</p><ol class=""><li id="a7dd" class="nq nr iq ky b kz la lc ld lf ns lj nt ln nu lr nv nw nx ny bi translated">我们要取<code class="fe nk nl nm nn b">output[-1, :, :hidden_size]</code>(正常的RNN)和<code class="fe nk nl nm nn b">output[0, :, hidden_size:]</code>(反向的RNN)，把它们串联起来，把结果反馈给后续的密集神经网络。</li><li id="1659" class="nq nr iq ky b kz nz lc oa lf ob lj oc ln od lr nv nw nx ny bi translated">返回的隐藏状态是消耗完整个序列后的状态。它们可以被安全地传递给解码器。</li></ol><p id="d431" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(边注)<a class="ae kv" href="http://pytorch.org/docs/master/nn.html#torch.nn.GRU" rel="noopener ugc nofollow" target="_blank"><code class="fe nk nl nm nn b">batch_first</code>为假时PyTorch </a>中GRU的输出形状:</p><blockquote class="oe of og"><p id="7f76" class="kw kx mr ky b kz la jr lb lc ld ju le oh lg lh li oi lk ll lm oj lo lp lq lr ij bi translated"><strong class="ky ir">输出</strong>(序列长度，批次，隐藏尺寸*数量方向)</p><p id="3fb5" class="kw kx mr ky b kz la jr lb lc ld ju le oh lg lh li oi lk ll lm oj lo lp lq lr ij bi translated"><strong class="ky ir"> h_n </strong>(层数*方向数，批次，隐藏大小)</p></blockquote><p id="c66a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">LSTM的方法类似，但是返回一个附加的单元格状态变量，形状与<strong class="ky ir"> h_n. </strong>相同</p></div></div>    
</body>
</html>