<html>
<head>
<title>Apache Spark: Hashing or Dictionary?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark:哈希还是字典？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/apache-spark-hashing-or-dictionary-d23c0e046a19?source=collection_archive---------2-----------------------#2017-06-09">https://towardsdatascience.com/apache-spark-hashing-or-dictionary-d23c0e046a19?source=collection_archive---------2-----------------------#2017-06-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="130b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作为构建推荐系统的一部分，我最近不得不计算大量数据中的文本相似度。如果你对 Spark 的一个字符串相似性算法感兴趣，可以看看。</p><p id="8f97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文的结构。</p><ol class=""><li id="2201" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">实验背后的动机。</li><li id="187b" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">TF-IDF 的 Spark 实现及其重要性。</li><li id="3937" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">比较和结论。</li></ol><h1 id="367e" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">动机</h1><p id="522b" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">这不是我的问题陈述，但为了简单起见，让我们假设你有两列这样的单词，我们必须匹配它们。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="bd7d" class="ml la iq mh b gy mm mn l mo mp">List A       | List B<br/>---------------------<br/>GOOGLE INC.  | Google<br/>MEDIUM.COM   | Medium Inc<br/>Amazon labs  | Amazon<br/>Google, inc  | Yahoo</span></pre><p id="c0ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个例子中，我们的目标是将 GOOGLE INC .和 GOOGLE，INC .匹配到 GOOGLE；并使 MEDIUM.COM 与媒体公司相匹配；以及亚马逊实验室到亚马逊等。</p><p id="5747" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最简单的方法之一是计算这个单词列表的 TF-IDF 分数，并创建一个这样的矩阵。</p><figure class="mc md me mf gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mq"><img src="../Images/5b40c99bbf1a944ace26c348e1ddc8dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PGn2InTYsyv-r8fS.png"/></div></div></figure><p id="0268" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该矩阵基本上具有 TF-IDF 得分，行和列元素为“GOOGLE”、“MEDIUM”。com '，'亚马逊实验室'，'谷歌'，'雅虎'，' COM '按顺序。我只是加了‘com’来展示 IDF 有多牛逼。如果一个术语在语料库中出现多次，那么它对于该特定文档就变得不太重要。添加了虚构的公司“com ”,因此“Medium.com”中的“Medium”变得更加重要。</p><p id="eea2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果 sklearn 行就能搞定这个，为什么还要用 spark？嗯，我的输入文件有 660 万行，正如你所猜测的，我的 Mac 真的不能处理这些。</p><p id="7ced" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们找到动机了。让我们点燃激情。</p><h1 id="e920" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">Spark 和 TF-IDF</h1><p id="7ef8" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">Spark 附带的一个示例脚本是针对文档集的 TF-IDF 的<a class="ae my" href="https://spark.apache.org/docs/2.1.0/ml-features.html#tf-idf" rel="noopener ugc nofollow" target="_blank">实现</a>。这当然使事情变得更容易，因为只需要一点点的预处理，我就可以开始了。</p><p id="fcd1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于我不能在这里展示<em class="mz">机密</em>数据的解决方案，我将只处理这个例子<a class="ae my" href="https://spark.apache.org/docs/2.1.0/ml-features.html#tf-idf" rel="noopener ugc nofollow" target="_blank">这里</a>给出的微小数据。</p><figure class="mc md me mf gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi na"><img src="../Images/f41e7afc82f91bec0105944ce4ac4e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XKPWxhjm_ShfaRLA.png"/></div></div></figure><p id="d39f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您查看第一个表，它是应用标准 TF-TDF 脚本的结果，通过简单的分析，您可以将单词列中的单词映射到特征列中的值。比如‘我’大概 29。</p><p id="a29b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个方法在我的情况下完全没有用。我有大约 200 万个不同的单词，却没有办法将这些值映射回单词。:(</p><p id="56e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为什么？Spark 在这里，使用的是一个<strong class="jp ir">哈希函数</strong>。哈希函数利用了哈希技巧。通过应用散列函数将原始特征映射到索引(术语)中。这里使用的哈希函数是 MurmurHash 3。然后基于映射的索引计算术语频率。</p><p id="00d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然这种方法避免了计算全局术语-索引映射的需要，这对于大型语料库来说可能是昂贵的，但是它遭受潜在的哈希冲突，其中不同的原始特征在哈希之后可能变成相同的术语。:/</p><p id="1225" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">必须找到一种方法来检索单词到值的映射。解决方案？<strong class="jp ir">计数矢量器。</strong></p><p id="11f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上图中，第二个表是使用 countvectorizer 对数据进行令牌化并获得 TF 而不是散列函数的结果。“词汇表”存储所有不同的单词，它们的索引是值。是的。:)</p><h1 id="eef1" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">比较和结论</h1><p id="f8a8" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated"><em class="mz">但是等等。</em></p><p id="7617" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是哈希函数和字典之间的一个古老的权衡。在什么情况下哈希函数会更好？为什么 Spark 有 2 个版本计算 TF-IDF？</p><p id="49ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">再来看<strong class="jp ir">存储:</strong></p><p id="a6e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">HashingTF 需要固定大小的存储，与词汇表的大小无关。CountVectorizer 需要与词汇表大小成比例的存储空间。实际上，计数通常需要额外的特征选择。</p><p id="239e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">转型呢？</strong></p><p id="b775" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">HashingTF 只需要对数据进行一次扫描，不需要额外的存储和转换。CountVectorizer 必须扫描数据<a class="ae my" href="https://spark.apache.org/docs/2.1.0/ml-features.html#countvectorizer" rel="noopener ugc nofollow" target="_blank">两次</a>(一次用于构建模型，一次用于转换)，需要与唯一标记数量成比例的额外空间和昂贵的排序。</p><p id="ea62" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">显然，这两种实现各有优缺点。这就是我的结论。如果 TF-IDF 只是另一个需要数字特征的算法的预处理步骤，那么与 CountVectorizer 相比，HashingTF 的工作速度更快，内存效率更高。由于我当前的模块仅仅停留在计算单词的相似性，我需要知道反向映射和 CountVectorizer 似乎更合适。:)</p><p id="b03c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读！</p><p id="5e4a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PS:这篇<a class="ae my" href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>让我对特性散列有了很好的了解，可能值得一读。</p></div></div>    
</body>
</html>