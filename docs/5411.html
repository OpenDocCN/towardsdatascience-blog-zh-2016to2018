<html>
<head>
<title>Learning neural network architectures</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习神经网络架构</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-neural-network-architectures-6109cb133caf?source=collection_archive---------12-----------------------#2018-10-16">https://towardsdatascience.com/learning-neural-network-architectures-6109cb133caf?source=collection_archive---------12-----------------------#2018-10-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6678" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">如何自动创建神经网络</em></h2></div><p id="1b86" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">上次我们谈到了<a class="ae lc" href="https://medium.com/@culurciello/at-the-limits-of-learning-46122b99dfc5" rel="noopener">学习的限制</a>，以及消除对神经网络架构设计的需求将如何导致更好的结果和深度神经网络的使用。</p><p id="b597" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这里，我们将分析所有最近的学习神经网络架构的方法，并揭示利弊。这篇文章会随着时间的推移而发展，所以一定要不时登录以获取更新。</p><h1 id="6f20" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">首先…</h1><p id="f2ad" class="pw-post-body-paragraph kg kh iq ki b kj lv jr kl km lw ju ko kp lx kr ks kt ly kv kw kx lz kz la lb ij bi translated">学习一个神经网络架构意味着什么？这意味着尝试以最佳方式组合所有的<a class="ae lc" href="https://medium.com/@culurciello/neural-networks-building-blocks-a5c47bcd7c8d" rel="noopener">神经网络构建模块</a>，以在某些任务中获得最高性能，比如说:对 ImageNet 图像进行分类。</p><p id="ba72" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><a class="ae lc" rel="noopener" target="_blank" href="/neural-network-architectures-156e5bad51ba">这里你可以看到</a>过去 10 年左右所有最近的神经网络架构。所有这些建筑都是人类凭直觉和原始脑力精心打造的。这是人类最大的智慧，但是正如你所想象的，它并不总是导致最好的解决方案。正如<a class="ae lc" href="https://medium.com/@culurciello/at-the-limits-of-learning-46122b99dfc5" rel="noopener">我们在这里提到的</a>，这不是进步的方式，因为我们将受到人类思维所能想到的限制，并且永远不会完全搜索神经架构的大空间。想想最近<em class="ma">象棋</em>和<em class="ma">围棋</em>的游戏发生了什么！人类被搜索最佳行动和策略的神经网络算法打得落花流水。</p><blockquote class="mb"><p id="17e5" class="mc md iq bd me mf mg mh mi mj mk lb dk translated">神经网络算法将在神经结构搜索领域击败人类，就像他们在围棋和国际象棋中击败人类一样！</p></blockquote><p id="97fe" class="pw-post-body-paragraph kg kh iq ki b kj ml jr kl km mm ju ko kp mn kr ks kt mo kv kw kx mp kz la lb ij bi translated">所以要走的路真的是使用神经网络来寻找越来越好的神经架构。实际上，我们将使用相同的梯度下降技术来引导神经结构的巨大搜索。</p><p id="39df" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">但是正如你所看到的，有许多可能的神经构建模块，搜索空间是巨大的。想象我们已经尝试了所有可能的卷积层:不同数量的输入输出平面、膨胀、深度、池化、非线性等…搜索空间可以从 10 个⁴到 10 个⁸or <a class="ae lc" href="https://arxiv.org/abs/1712.00559" rel="noopener ugc nofollow" target="_blank">更多</a>选项！这是一个巨大的空间。想象一下，我们花 1 个小时来训练和测试一个架构……那么我们将需要 10 个⁴小时，或者永恒！</p><h1 id="ba19" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">梯度方法</h1><p id="c976" class="pw-post-body-paragraph kg kh iq ki b kj lv jr kl km lw ju ko kp lx kr ks kt ly kv kw kx lz kz la lb ij bi translated">基于控制器的方法，如<a class="ae lc" href="https://arxiv.org/abs/1611.01578" rel="noopener ugc nofollow" target="_blank"> Zoph，Le (2017 </a>)使用递归神经网络来创建新的架构，然后用强化学习来测试它们。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/7c34c8202fdb09caab9e15bd4a7a8b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fcS91mjnDJ0SG-Tb3uZckQ.jpeg"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">An overview of Neural Architecture Search</figcaption></figure><p id="0f65" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">他们将神经网络的连接和结构编码成可变长度的字符串，并使用 RNN 控制器来生成新的架构。“子网络”是在数据集上训练的，以产生训练和验证精度。验证准确度被用作训练控制器的奖励信号。随着控制器随着时间的推移在搜索中改进，这又在接下来的迭代中产生更好的神经网络。</p><p id="342f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><a class="ae lc" href="https://arxiv.org/abs/1712.00559" rel="noopener ugc nofollow" target="_blank">刘看着艾尔通。(2017) </a>使用启发式搜索，从简单的神经网络结构开始，逐步增加复杂度。本文基于<a class="ae lc" href="https://arxiv.org/abs/1707.07012" rel="noopener ugc nofollow" target="_blank">Zoph et al(2018)</a>的工作。在后一篇论文中，他们再次使用相同的 RNN 控制器来搜索架构:</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ng"><img src="../Images/bd29df7064e8a9ccb8f1ee9625250fa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cFyP-inpecDK-TPNJ0pd4A.jpeg"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Controller model architecture for recursively constructing one block of a convolutional cell.</figcaption></figure><p id="bba9" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">该控制器产生一系列输出，这些输出对上图顶部的每个元素进行编码，基本上包括:使用前两层，对两个输入中的每一个应用哪种操作，以及用哪种方法将它们的两个输出合并成一个输出。</p><p id="b90e" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">为了减少可能的参数组合数量，他们使用下图所示的固定构造块架构。该区块可以实现<a class="ae lc" rel="noopener" target="_blank" href="/neural-network-architectures-156e5bad51ba">类残差</a>层和<a class="ae lc" rel="noopener" target="_blank" href="/neural-network-architectures-156e5bad51ba">类 Google net</a>层。它们允许 RNN 控制器为每层找到多个块，通常是 5 个，这是有效性和搜索空间之间的一个很好的折衷。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/09c9deacb923921e561624668178852c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*h6es3rakckhCyDjgYx1JVg.jpeg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Example constructed block</figcaption></figure><p id="2297" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这项工作中，他们还将可能的基本构建模块的数量限制在以下列表中:</p><ul class=""><li id="018c" class="ni nj iq ki b kj kk km kn kp nk kt nl kx nm lb nn no np nq bi translated">身份；1x7 然后 7x1 卷积；1×3 然后 3×1 卷积；3x3 平均池化；3x3 或 5x5 或 7x7 最大池化；1x1 或 3x3 卷积；3×3 或 5×5 或 7×7 深度方向可分离卷积；3x3 扩展卷积</li></ul><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nr"><img src="../Images/155bd0cbc9b81c9a0c87036e9cfd3231.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e49Fr-CQ6a5CgPIY3WytiQ.jpeg"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Scalable architectures for image classification consist of two repeated motifs termed Normal Cell and Reduction Cell.</figcaption></figure><p id="3f53" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">他们创造了如左图所示的架构。基于以前的架构历史，他们决定整体架构应由两种类型的单元组成:(1)返回相同维度的特征图的卷积单元或<em class="ma">正常单元</em>；以及(2)返回高度、宽度或<em class="ma">缩减单元</em> l 缩减两次的特征图的卷积单元。注意，在这些架构中，每当空间激活大小减小时，它们还会将输出中的滤波器数量增加一倍，以保持大致恒定的隐藏状态维度。还要注意的是，左图中的这个架构有点类似于 ResNet 中的<a class="ae lc" rel="noopener" target="_blank" href="/neural-network-architectures-156e5bad51ba">模块，包括普通和简化单元的选择。</a></p><p id="c41b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">为了训练控制器 RNN，他们使用强化学习。一旦 RNN 建造了一个建筑，它就被训练了。利用验证数据集中的准确性，产生奖励信号，并用于向 RNN 控制器提供反馈，最终训练它在建筑搜索方面越来越好。由于奖励信号是不可微分的，他们使用策略梯度方法，如<a class="ae lc" href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" rel="noopener ugc nofollow" target="_blank">加强</a>，并通过使用以前验证奖励的移动平均值减去当前验证值。这有效地充当了验证准确性的增量改进度量。</p><p id="2ae3" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">有趣的是，在下图中你可以看到算法找到的最佳细胞。它们似乎是更复杂版本的 Inception-ResNet 层。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ns"><img src="../Images/6996ca3ee1327b4b7097447e3db69e7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDVvTHiRo97fyUpRqiBOTA.jpeg"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Architecture of the best convolutional cells (NASNet-A) with B = 5 blocks identified with CIFAR-10</figcaption></figure><p id="b2c1" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">结果很好！他们发现，与人类手工制作相比，大型和小型神经网络都非常高效和高性能。下面我们报告一个小模型的表格。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nt"><img src="../Images/edd00dc6dd65a931ee1a367e606b0feb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YSSJROlL7ckmGJOser1Srw.jpeg"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Performance on ImageNet classification on a subset of models operating in a constrained computational setting, i.e., &lt; 1.5 B multiply-accumulate operations per image</figcaption></figure><p id="db2a" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">最后，本文对强化学习搜索相对于随机搜索的性能做了一些有益的评论。在过去的努力中，击败随机搜索出人意料地困难。RL 搜索更好，可以找到很多更好的模型，但随机搜索的效果也令人惊讶。毕竟这就是创造我们人类大脑的东西，但在此之前只花了几十万年和 50 亿年…</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nu"><img src="../Images/f2dfe4b00a5fa3e18583c3d63202de41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N7k9rl7OZpZwv38kgCan5A.jpeg"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Comparing the efficiency of random search (RS) to re- inforcement learning (RL) for learning neural architectures</figcaption></figure><p id="4069" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">总的来说，这是一个很有指导意义的模型搜索，但方法和结果可以用于许多不同类型的神经网络。我们希望在接下来的几年里看到这项技术得到更多的应用。一个缺点是所使用的模型架构非常类似于 ResNet 块架构。此外，构造的块仅捕获可能集合的子集。这当然都是为了<a class="ae lc" href="https://arxiv.org/abs/1712.00559" rel="noopener ugc nofollow" target="_blank">减少搜索空间</a>并使问题更易处理，但是它并没有例如推广到编码器-解码器架构或更小的应用。为此，人们将不得不创建新的构造块和基础网络。</p><p id="b31d" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">此外，对体系结构的搜索计算量非常大，并且花费大量时间。你需要 500 个 GPU(贵的像英伟达 P100！)而且你需要在 4 天内测试 20，000 个神经网络才能<a class="ae lc" href="https://arxiv.org/abs/1712.00559" rel="noopener ugc nofollow" target="_blank">找到结果</a>。</p><h1 id="260d" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">遗传搜索</h1><p id="c46d" class="pw-post-body-paragraph kg kh iq ki b kj lv jr kl km lw ju ko kp lx kr ks kt ly kv kw kx lz kz la lb ij bi translated">遗传搜索是穷举搜索方法，创建不同的神经架构，然后逐一尝试。因此，它们经常受到无向导搜索的缓慢过程的限制。例如 Wierstra 等人(2005 年)、Floreano 等人(2008 年)、Stanley 等人(2009 年)。这些方法使用进化算法来搜索新的架构。</p><p id="b2f8" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><a class="ae lc" href="https://eng.uber.com/deep-neuroevolution/" rel="noopener ugc nofollow" target="_blank">神经进化</a>是一种最近在强化学习算法中获得成功的方法。它最近在强化学习任务中取得了成功，而其他算法(DQN、A2C 等)却没有取得成功。)失败了。</p><h1 id="9790" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">作为网络架构搜索进行修剪</h1><p id="39ed" class="pw-post-body-paragraph kg kh iq ki b kj lv jr kl km lw ju ko kp lx kr ks kt ly kv kw kx lz kz la lb ij bi translated"><a class="ae lc" href="https://arxiv.org/abs/1810.05270" rel="noopener ugc nofollow" target="_blank">最近，神经网络的修剪作为一种神经网络结构搜索技术被重新审视</a>。本文展示了两个主要的见解:1)可以从修剪过的神经网络开始，并从头开始训练它；2)修剪因此可以被视为类似于网络结构搜索的优化神经网络结构的技术。</p><p id="0484" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">注意，修剪产生的结果不如上面讨论的基于梯度的搜索技术好。</p><h1 id="d09f" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">关于作者</h1><p id="7e75" class="pw-post-body-paragraph kg kh iq ki b kj lv jr kl km lw ju ko kp lx kr ks kt ly kv kw kx lz kz la lb ij bi translated">我在硬件和软件方面都有将近 20 年的神经网络经验(一个罕见的组合)。在这里看关于我:<a class="ae lc" href="https://medium.com/@culurciello/" rel="noopener">媒介</a>、<a class="ae lc" href="https://e-lab.github.io/html/contact-eugenio-culurciello.html" rel="noopener ugc nofollow" target="_blank">网页</a>、<a class="ae lc" href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ" rel="noopener ugc nofollow" target="_blank">学者</a>、<a class="ae lc" href="https://www.linkedin.com/in/eugenioculurciello/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>等等…</p></div></div>    
</body>
</html>