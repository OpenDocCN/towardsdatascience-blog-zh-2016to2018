<html>
<head>
<title>Perceptual Losses for Real-Time Style Transfer and Super-Resolution</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实时风格转换和超分辨率的感知损失</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/perceptual-losses-for-real-time-style-transfer-and-super-resolution-637b5d93fa6d?source=collection_archive---------5-----------------------#2018-09-08">https://towardsdatascience.com/perceptual-losses-for-real-time-style-transfer-and-super-resolution-637b5d93fa6d?source=collection_archive---------5-----------------------#2018-09-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/4695e2a16548aae5386f63fd591ffb43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G3J8PXE8WcJUA8D7v-Easw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">[Fig. 1] Two toons discussing Neural Style Transfer at the beach.</strong></figcaption></figure><p id="d413" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">这是一篇论文的论文摘要:<br/> </em> <strong class="kf ir"> <em class="lb">实时风格传递和超分辨率的感知损失<br/> </em> </strong> <em class="lb">作者:贾斯廷·约翰逊，亚历山大阿拉希，李菲菲。<br/>论文:</em><a class="ae lc" href="https://arxiv.org/pdf/1603.08155.pdf" rel="noopener ugc nofollow" target="_blank"><em class="lb"/></a></p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><h1 id="42b2" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">概观</h1><p id="f422" class="pw-post-body-paragraph kd ke iq kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">本文提出使用<strong class="kf ir"> <em class="lb">感知损失函数</em> </strong> <em class="lb"> </em>来训练用于图像变换任务的前馈网络<em class="lb">，而不是使用</em> <strong class="kf ir"> <em class="lb">逐像素损失函数</em> </strong>。</p><blockquote class="mn mo mp"><p id="0e6c" class="kd ke lb kf b kg kh ki kj kk kl km kn mq kp kq kr mr kt ku kv ms kx ky kz la ij bi translated"><strong class="kf ir">每像素损失函数？</strong> <br/>根据两幅图像各自的像素值进行比较。<br/>因此，如果两个图像在感知上相同，但是基于甚至一个像素彼此不同，那么基于每像素损失函数，它们将彼此非常不同。</p><p id="3fee" class="kd ke lb kf b kg kh ki kj kk kl km kn mq kp kq kr mr kt ku kv ms kx ky kz la ij bi translated"><strong class="kf ir">感知损失函数？</strong> <br/>基于来自预训练卷积神经网络(在图像分类任务上训练，比如说 ImageNet 数据集)的高级表示，比较两幅图像<em class="iq">。</em></p></blockquote><p id="2256" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">他们在两个图像转换任务上评估他们的方法:<br/> (i)风格转换<br/> (ii)单幅图像超分辨率</p><p id="fb49" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于风格转移，他们训练前馈网络，试图解决<a class="ae lc" href="https://arxiv.org/pdf/1508.06576v2.pdf" rel="noopener ugc nofollow" target="_blank"> Gatys et al. 2015 </a>提出的优化问题。</p><p id="e706" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于超分辨率，他们尝试使用感知损失，并表明它比使用每像素损失函数获得更好的结果。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><h1 id="e72e" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">方法</h1><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/02df1ed36d9056b0c9e944875e35e97d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TdkNFoecrvBZZbLOHGse0Q.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">[Fig. 2] Model Architecture</strong></figcaption></figure><p id="3bbc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所提出的模型架构由两个组件组成:<br/> (i)图像变换网络(<code class="fe my mz na nb b">f_{w}</code> ) <br/> (ii)损失网络(φ)</p><h2 id="0f15" class="nc ll iq bd lm nd ne dn lq nf ng dp lu ko nh ni ly ks nj nk mc kw nl nm mg nn bi translated">图像变换网络</h2><p id="6a7a" class="pw-post-body-paragraph kd ke iq kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">图像变换网络是一个深度残差卷积神经网络，它被训练来解决 Gatys 提出的优化问题。</p><p id="5d10" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给定一个输入图像(x ),该网络将其转换成输出图像(ŷ).</p><p id="8589" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用使用输出图像(ŷ)计算的损失并将其与以下各项进行比较来学习该网络的权重(w):风格转移情况下的风格图像(<code class="fe my mz na nb b">y_{s}</code>)和内容图像(<code class="fe my mz na nb b">y_{c}</code>)的表示<br/> -超分辨率情况下的内容图像<code class="fe my mz na nb b">y_{c}</code>。</p><p id="d7df" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用<a class="ae lc" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">随机梯度下降</a>来训练图像变换网络，以获得最小化所有损失函数的加权和的权重(W)。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/0e411407801b74aa9c7db3784c886fc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7FGWEUePVIPYipkDza4paA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">[Fig. 3] Weight update equation Explained.</strong></figcaption></figure><h2 id="5aaf" class="nc ll iq bd lm nd ne dn lq nf ng dp lu ko nh ni ly ks nj nk mc kw nl nm mg nn bi translated">损耗网络</h2><p id="1165" class="pw-post-body-paragraph kd ke iq kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">损耗网络(φ)是 ImageNet 数据集上预先训练的 VGG16。</p><p id="6128" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">损失网络用于从内容和样式图像中获得内容和样式表示:<br/> (i)内容表示取自层`<em class="lb"> relu3_3 </em>`。【<em class="lb">图 2</em><br/>(二)样式表示取自图层`<em class="lb"> relu1_2 </em>`、`<em class="lb"> relu2_2 </em>`、`<em class="lb"> relu3_3 </em>`、`<em class="lb"> relu4_3 </em>`。【<em class="lb">图 2 </em></p><p id="2da4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些表示用于定义两种类型的损失:</p><p id="f75e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lb">特征重建损失</em> </strong> <br/>用输出图像(ŷ)和来自层`<em class="lb"> relu3_3 </em>'的内容表示并在图像中使用以下损失函数</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi np"><img src="../Images/1285bb8f2eff1e879e861bb771f192c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CVfjP23JUzWKgLIRkf-2Cw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">[Fig. 4] Feature Reconstruction Loss Explained.</strong></figcaption></figure><p id="8af7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="lb">风格重建损失<br/> </em> </strong>利用输出图像(ŷ)和来自层`<em class="lb"> relu1_2 </em>、`<em class="lb"> relu2_2 </em>、`<em class="lb"> relu3_3 </em>和`<em class="lb"> relu4_3 </em>的风格表示，并使用来自图像的以下损失函数</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nq"><img src="../Images/4e871bc121415c235bba34c88405370e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kTJm8fSR3n2uooYY0V-vpg.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">[Fig. 5] Style Reconstruction Loss Explained.</strong></figcaption></figure><p id="b346" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在风格转移的情况下，总损失通常是特征重建损失和风格重建损失的加权和。和超分辨率的特征重建损失的加权乘积。</p><p id="530c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些损失用于学习图像变换网络的权重。</p><h1 id="e5cd" class="lk ll iq bd lm ln nr lp lq lr ns lt lu lv nt lx ly lz nu mb mc md nv mf mg mh bi translated">结果</h1><h2 id="42a7" class="nc ll iq bd lm nd ne dn lq nf ng dp lu ko nh ni ly ks nj nk mc kw nl nm mg nn bi translated">风格转移</h2><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nw"><img src="../Images/03efd84d8ead95e386e8b82281318328.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Koe6-AKe_xDxh4tp8BIBQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">[Fig. 6] Results of Style Transfer (from the paper)</strong></figcaption></figure><ul class=""><li id="30a5" class="nx ny iq kf b kg kh kk kl ko nz ks oa kw ob la oc od oe of bi translated">在 COCO 数据集上训练的网络(用于内容图像)。</li><li id="d01d" class="nx ny iq kf b kg og kk oh ko oi ks oj kw ok la oc od oe of bi translated">80k 训练图像调整为 256x256 补丁。</li><li id="493e" class="nx ny iq kf b kg og kk oh ko oi ks oj kw ok la oc od oe of bi translated">批量:4 个</li><li id="cf18" class="nx ny iq kf b kg og kk oh ko oi ks oj kw ok la oc od oe of bi translated">40k 次迭代(大约 2 个时期)</li><li id="51e4" class="nx ny iq kf b kg og kk oh ko oi ks oj kw ok la oc od oe of bi translated">使用的优化器:Adam</li><li id="c8bf" class="nx ny iq kf b kg og kk oh ko oi ks oj kw ok la oc od oe of bi translated">学习率:1e-3</li><li id="7fe9" class="nx ny iq kf b kg og kk oh ko oi ks oj kw ok la oc od oe of bi translated">在 Titan X GPU 上进行培训大约需要 4 个小时</li><li id="f943" class="nx ny iq kf b kg og kk oh ko oi ks oj kw ok la oc od oe of bi translated">对比 Gatys 等人提出的方法<em class="lb">【1】</em>。</li></ul><h2 id="e05d" class="nc ll iq bd lm nd ne dn lq nf ng dp lu ko nh ni ly ks nj nk mc kw nl nm mg nn bi translated">单幅图像超分辨率</h2><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ol"><img src="../Images/71c845ffb7271753b5a19fad8abe8cdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_IcCoZdW7UcZKlpnSqph1g.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">[Fig. 7] Results of Super-Resolution (x4) (from paper)</strong></figcaption></figure><ul class=""><li id="0fd1" class="nx ny iq kf b kg kh kk kl ko nz ks oa kw ob la oc od oe of bi translated">使用来自 MS-COCO 的 10k 图像的 288x288 补丁进行训练</li><li id="622e" class="nx ny iq kf b kg og kk oh ko oi ks oj kw ok la oc od oe of bi translated">通过使用宽度σ=1.0 的高斯内核进行模糊处理和使用双三次插值进行下采样，准备好低分辨率输入。</li><li id="b5a2" class="nx ny iq kf b kg og kk oh ko oi ks oj kw ok la oc od oe of bi translated">批量:4 个</li><li id="a347" class="nx ny iq kf b kg og kk oh ko oi ks oj kw ok la oc od oe of bi translated">迭代次数:20 万次</li><li id="53a1" class="nx ny iq kf b kg og kk oh ko oi ks oj kw ok la oc od oe of bi translated">优化器:亚当</li><li id="079a" class="nx ny iq kf b kg og kk oh ko oi ks oj kw ok la oc od oe of bi translated">学习率:1e-3</li><li id="f1f4" class="nx ny iq kf b kg og kk oh ko oi ks oj kw ok la oc od oe of bi translated">对比 Sr CNN<em class="lb">【3】</em></li></ul><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi om"><img src="../Images/c5e5eae212ef41053b3bef2192e36ea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iSxvawlkgk9JagHu53xZJw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><strong class="bd kc">[Fig. 8] Table showing the speed in seconds of the approach in this paper against Gatys’ paper (from the paper)</strong></figcaption></figure><p id="da1d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们所看到的，我们使用本文的方法得到了相似的结果，并且在推理过程中比 Gatys 的方法几乎快<strong class="kf ir"><em class="lb"/></strong>3 个数量级。</p><p id="7ccd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种方法的最大缺点是，我们必须针对每种风格或每种分辨率训练<strong class="kf ir"> <em class="lb">一个网络，</em> </strong>即不能仅使用一个网络来执行<strong class="kf ir"> <em class="lb">任意</em> </strong>风格转换。</p><p id="bf4b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用 Gatys，我们能够仅使用一个网络来执行<strong class="kf ir"> <em class="lb">任意</em> </strong>风格的传输，这是我们使用本文提出的方法无法做到的。</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><h2 id="dff1" class="nc ll iq bd lm nd ne dn lq nf ng dp lu ko nh ni ly ks nj nk mc kw nl nm mg nn bi translated">参考</h2><p id="06eb" class="pw-post-body-paragraph kd ke iq kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated"><strong class="kf ir"><em class="lb">【1】</em></strong>贾斯廷·约翰逊、阿拉希、李菲菲。:实时风格转换和超分辨率的感知损失。arXiv:1603.08155(2016 年 3 月)</p><p id="065c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"><em class="lb">【2】</em></strong>Gatys，L.A .，Ecker，A.S .，Bethge，m .:艺术风格的神经算法。arXiv 预印本 arXiv:1508.06576 (2015)</p><p id="0ca8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"><em class="lb">【3】</em></strong>董，c，洛伊，C.C .，何，k，唐，x .:学习一种用于图像超分辨率的深度卷积网络。参加:计算机视觉-ECCV 2014。施普林格(2014 年)</p></div><div class="ab cl ld le hu lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="ij ik il im in"><p id="5ec1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">所以，简而言之，这就是论文的全部内容！<br/> </em> <strong class="kf ir"> <em class="lb">如果我发现一些有趣的见解需要补充，我会更新这个故事！<br/> </em> </strong> <em class="lb">一定要走纸！</em></p><p id="ff27" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">喜欢什么你请 read❓ <br/>👏 👏 👏 👏 👏</em></p><p id="f0ed" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">喜欢我的作品❓ <br/>跟着我❗️ </em></p><div class="on oo gp gr op oq"><a href="https://www.linkedin.com/in/iarunava/" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd ir gy z fp ov fr fs ow fu fw ip bi translated">阿鲁纳瓦查克拉博蒂| LinkedIn</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">查看 Arunava Chakraborty 在 LinkedIn 上的职业简介。LinkedIn 是世界上最大的商业网络，帮助…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">www.linkedin.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe jw oq"/></div></div></a></div><div class="on oo gp gr op oq"><a href="https://twitter.com/amArunava" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd ir gy z fp ov fr fs ow fu fw ip bi translated">阿鲁纳瓦(@amArunava) |推特</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">阿鲁纳瓦(@amArunava)的最新推文:“从@fastdotai 上的#DeepLearning 课程 1 开始，已完成第 1 周…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">twitter.com</p></div></div><div class="oz l"><div class="pf l pb pc pd oz pe jw oq"/></div></div></a></div><div class="on oo gp gr op oq"><a href="https://www.youtube.com/channel/UC2ZFGaNzZt-sUy2qZT6L7Zw" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd ir gy z fp ov fr fs ow fu fw ip bi translated">阿鲁纳瓦</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">订阅深入了解 Python 里的所有东西。</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">www.youtube.com</p></div></div><div class="oz l"><div class="pg l pb pc pd oz pe jw oq"/></div></div></a></div><p id="be8a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给我发消息！<br/>我们连线吧！</p></div></div>    
</body>
</html>