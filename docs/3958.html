<html>
<head>
<title>Smart way to serialize/deserialize classes to/from Tensorflow graph</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将类序列化/反序列化到 Tensorflow 图或从 tensor flow 图序列化/反序列化类的智能方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/smart-way-to-srialize-deserialise-class-to-from-tensorflow-graph-1b131db50c7d?source=collection_archive---------6-----------------------#2018-07-04">https://towardsdatascience.com/smart-way-to-srialize-deserialise-class-to-from-tensorflow-graph-1b131db50c7d?source=collection_archive---------6-----------------------#2018-07-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/b4ec07d753b603a8e96d66fba8f66632.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ovYciex-k-JBDf9WaCz7A.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/photos/RHaF3u3YjTM?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">John Fowler</a> on <a class="ae jd" href="https://unsplash.com/collections/2313048/astres?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="2312" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">自动将您的字段与张量流图绑定</em></p><p id="8448" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将类字段自动绑定到图中的 tensorflow 变量并恢复它们，而不用手动从图中获取每个变量，这是不是很酷？</p><p id="647a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">本文的代码可以在</em> <a class="ae jd" href="https://github.com/FrancescoSaverioZuppichini/TFGraphConvertible" rel="noopener ugc nofollow" target="_blank"> <em class="lb">这里找到，</em> </a> <em class="lb">一个笔记本版本可以在</em> <a class="ae jd" href="https://github.com/FrancescoSaverioZuppichini/TFGraphConvertible/blob/master/example.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="lb">这里找到</em> </a></p><p id="a560" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">想象你有一个<code class="fe lc ld le lf b">Model</code>类</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><p id="bfa3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通常，你首先<strong class="kf jh">建造</strong>你的模型，然后<strong class="kf jh">训练</strong>它。之后，你想从保存的图中得到旧的变量，而不是从头开始重建整个模型。</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><pre class="lg lh li lj gt lm lf ln lo aw lp bi"><span id="587b" class="lq lr jg lf b gy ls lt l lu lv">&lt;tf.Variable 'variable:0' shape=(1,) dtype=int32_ref&gt;</span></pre><p id="c407" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，假设我们刚刚训练了我们的模型，我们想要存储它。通常的模式是</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><p id="11c1" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在你想执行<strong class="kf jh">推理</strong>，也就是通过加载存储的图来取回你的东西。在我们的例子中，我们想要名为<code class="fe lc ld le lf b">variable</code>的变量</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><pre class="lg lh li lj gt lm lf ln lo aw lp bi"><span id="0c24" class="lq lr jg lf b gy ls lt l lu lv">INFO:tensorflow:Restoring parameters from /tmp/model.ckpt</span></pre><p id="5263" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们可以从图中找回我们的<code class="fe lc ld le lf b">variable</code></p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><pre class="lg lh li lj gt lm lf ln lo aw lp bi"><span id="9dbc" class="lq lr jg lf b gy ls lt l lu lv">name: "variable" op: "VariableV2" attr { key: "container" value { s: "" } } attr { key: "dtype" value { type: DT_INT32 } } attr { key: "shape" value { shape { dim { size: 1 } } } } attr { key: "shared_name" value { s: "" } }</span></pre><p id="94c9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，如果我们想再次使用我们的<code class="fe lc ld le lf b">model</code>类呢？如果我们现在试着打电话给<code class="fe lc ld le lf b">model.variable</code>,我们没有收到</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><pre class="lg lh li lj gt lm lf ln lo aw lp bi"><span id="16cf" class="lq lr jg lf b gy ls lt l lu lv">None</span></pre><p id="c7d9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个解决方案是<strong class="kf jh">重新构建</strong>整个模型，然后恢复图形</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><pre class="lg lh li lj gt lm lf ln lo aw lp bi"><span id="8835" class="lq lr jg lf b gy ls lt l lu lv">INFO:tensorflow:Restoring parameters from /tmp/model.ckpt &lt;tf.Variable 'variable:0' shape=(1,) dtype=int32_ref&gt;</span></pre><p id="48f9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你已经可以看到这是浪费时间。我们可以通过以下方式将<code class="fe lc ld le lf b">model.variable</code>直接绑定到正确的图节点</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><pre class="lg lh li lj gt lm lf ln lo aw lp bi"><span id="4bb9" class="lq lr jg lf b gy ls lt l lu lv">name: "variable" op: "VariableV2" attr { key: "container" value { s: "" } } attr { key: "dtype" value { type: DT_INT32 } } attr { key: "shape" value { shape { dim { size: 1 } } } } attr { key: "shared_name" value { s: "" } }</span></pre><p id="21a3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在想象一下，我们有一个非常大的嵌套变量模型。为了正确恢复模型中的每个变量指针，您需要:</p><ul class=""><li id="9448" class="lw lx jg kf b kg kh kk kl ko ly ks lz kw ma la mb mc md me bi translated">命名每个变量</li><li id="e033" class="lw lx jg kf b kg mf kk mg ko mh ks mi kw mj la mb mc md me bi translated">从图表中获取变量</li></ul><p id="fac0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们可以自动检索在模型类中设置为字段的所有变量，会不会很酷？</p><h1 id="1bb2" class="mk lr jg bd ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bi translated">TFGraphConvertible</h1><p id="36f6" class="pw-post-body-paragraph kd ke jg kf b kg nh ki kj kk ni km kn ko nj kq kr ks nk ku kv kw nl ky kz la ij bi translated">我创建了一个类，叫做<code class="fe lc ld le lf b">TFGraphConvertible</code>。你可以使用<code class="fe lc ld le lf b">TFGraphConvertible</code>来自动<strong class="kf jh">序列化</strong>和<strong class="kf jh">反序列化</strong>一个类。</p><p id="1c90" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们重新创建我们的模型</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><p id="c6fd" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它公开了两个方法:<code class="fe lc ld le lf b">to_graph</code>和<code class="fe lc ld le lf b">from_graph</code></p><h1 id="b6a5" class="mk lr jg bd ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bi translated">序列化-到图形</h1><p id="e8af" class="pw-post-body-paragraph kd ke jg kf b kg nh ki kj kk ni km kn ko nj kq kr ks nk ku kv kw nl ky kz la ij bi translated">为了使<strong class="kf jh">序列化一个类</strong>，你可以调用<strong class="kf jh"> to_graph </strong>方法来创建一个字段名字典- &gt; tensorflow 变量名。你需要传递一个<code class="fe lc ld le lf b">fields</code>参数，一个我们想要序列化的字段的字典。在我们的情况下，我们可以通过所有的测试。</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><pre class="lg lh li lj gt lm lf ln lo aw lp bi"><span id="7da8" class="lq lr jg lf b gy ls lt l lu lv">{'variable': 'variable_2:0'}</span></pre><p id="dbad" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它将创建一个字典，将所有字段作为键，将相应的 tensorflow 变量名作为值</p><h1 id="4373" class="mk lr jg bd ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bi translated">反序列化-从图形</h1><p id="fa95" class="pw-post-body-paragraph kd ke jg kf b kg nh ki kj kk ni km kn ko nj kq kr ks nk ku kv kw nl ky kz la ij bi translated">为了<strong class="kf jh">反序列化一个类</strong>，您可以调用<strong class="kf jh"> from_graph </strong>方法，该方法获取之前创建的字典并将每个类字段绑定到正确的 tensorflow 变量</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><pre class="lg lh li lj gt lm lf ln lo aw lp bi"><span id="6c9b" class="lq lr jg lf b gy ls lt l lu lv">None &lt;tf.Tensor 'variable_2:0' shape=(1,) dtype=int32_ref&gt;</span></pre><p id="b79f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在你有你的<code class="fe lc ld le lf b">model</code>回来了！</p><h1 id="bf1f" class="mk lr jg bd ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bi translated">完整示例</h1><p id="774f" class="pw-post-body-paragraph kd ke jg kf b kg nh ki kj kk ni km kn ko nj kq kr ks nk ku kv kw nl ky kz la ij bi translated">我们来看一个更有趣的例子！我们将为 MNIST 数据集训练/恢复一个模型</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><p id="9f95" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们去拿数据集吧！</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><pre class="lg lh li lj gt lm lf ln lo aw lp bi"><span id="b6f1" class="lq lr jg lf b gy ls lt l lu lv">Using TensorFlow backend.</span></pre><p id="d9c2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在是时候训练它了</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><pre class="lg lh li lj gt lm lf ln lo aw lp bi"><span id="6d70" class="lq lr jg lf b gy ls lt l lu lv">0.125<br/>0.46875<br/>0.8125<br/>0.953125<br/>0.828125<br/>0.890625<br/>0.796875<br/>0.9375<br/>0.953125<br/>0.921875</span></pre><p id="f2ac" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完美！让我们将序列化模型存储在内存中</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><pre class="lg lh li lj gt lm lf ln lo aw lp bi"><span id="aa29" class="lq lr jg lf b gy ls lt l lu lv">{'x': 'ExpandDims:0', <br/>'y': 'one_hot:0', <br/>'forward_raw': 'dense_1/BiasAdd:0', <br/>'accuracy': 'Mean:0', <br/>'loss': 'Mean_1:0', <br/>'train_step': 'Adam'}</span></pre><p id="fe65" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们重置图表并重新创建模型</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><pre class="lg lh li lj gt lm lf ln lo aw lp bi"><span id="9fc1" class="lq lr jg lf b gy ls lt l lu lv">INFO:tensorflow:Restoring parameters from /tmp/model.ckpt</span></pre><p id="98a8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，我们的变量在<code class="fe lc ld le lf b">mnist_model</code>不存在</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><pre class="lg lh li lj gt lm lf ln lo aw lp bi"><span id="6350" class="lq lr jg lf b gy ls lt l lu lv">--------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-21-9def5e0d8f6c&gt; in &lt;module&gt;() ----&gt; 1 mnist_model.accuracy AttributeError: 'MNISTModel' object has no attribute 'accuracy'</span></pre><p id="012b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们通过调用<code class="fe lc ld le lf b">from_graph</code>方法来重新创建它们。</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><pre class="lg lh li lj gt lm lf ln lo aw lp bi"><span id="bff9" class="lq lr jg lf b gy ls lt l lu lv">&lt;tf.Tensor 'Mean:0' shape=() dtype=float32&gt;</span></pre><p id="102d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在<code class="fe lc ld le lf b">mnist_model</code>已经准备好了，让我们看看测试集的准确性</p><figure class="lg lh li lj gt is"><div class="bz fp l di"><div class="lk ll l"/></div></figure><pre class="lg lh li lj gt lm lf ln lo aw lp bi"><span id="bb23" class="lq lr jg lf b gy ls lt l lu lv">INFO:tensorflow:Restoring parameters from /tmp/model.ckpt <br/>1.0</span></pre><h1 id="ce7a" class="mk lr jg bd ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bi translated">结论</h1><p id="52d5" class="pw-post-body-paragraph kd ke jg kf b kg nh ki kj kk ni km kn ko nj kq kr ks nk ku kv kw nl ky kz la ij bi translated">在本教程中，我们已经看到了如何序列化一个类并将每个字段绑定回张量流图中正确的张量。请注意，您可以将<code class="fe lc ld le lf b">serialized_model</code>存储为<code class="fe lc ld le lf b">.json</code>格式，并从任何地方直接加载。这样，您可以通过使用面向对象编程直接创建您的模型，并检索其中的所有变量，而不必重新构建它们。</p><p id="75dc" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢您的阅读</p><p id="b457" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">弗朗西斯科·萨维里奥·祖皮奇尼</p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><p id="d491" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">原载于</em><a class="ae jd" href="https://gist.github.com/002b66e4b384fe31bd005a00845f5fa3" rel="noopener ugc nofollow" target="_blank"><em class="lb">gist.github.com</em></a><em class="lb">。</em></p></div></div>    
</body>
</html>