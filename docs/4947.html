<html>
<head>
<title>Hyperparameter tuning in XGBoost using genetic algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于遗传算法的 XGBoost 超参数整定</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-tuning-in-xgboost-using-genetic-algorithm-17bd2e581b17?source=collection_archive---------9-----------------------#2018-09-17">https://towardsdatascience.com/hyperparameter-tuning-in-xgboost-using-genetic-algorithm-17bd2e581b17?source=collection_archive---------9-----------------------#2018-09-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="3705" class="ju jv iq bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">简介</strong></h1><p id="7704" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在<a class="ae lq" href="https://en.wikipedia.org/wiki/Genetic_algorithm" rel="noopener ugc nofollow" target="_blank">维基百科</a>中定义的遗传算法，其灵感来自于<a class="ae lq" href="https://en.wikipedia.org/wiki/Natural_selection" rel="noopener ugc nofollow" target="_blank">查尔斯·达尔文</a>提出的自然选择过程。在更一般的术语中，我们可以理解自然过程以及它如何与遗传算法相关联，使用下面的描述:</p><p id="7355" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated">我们从初始群体开始，它将具有某些特征，如图 1 所示。该初始群体将在特定环境中进行测试，以观察该群体中的个体(父母)基于预定义的适合度标准表现如何。在机器学习的情况下，适合度可以是任何性能度量——准确度、精确度、召回率、F1 分数、auc 等等。基于适应值，我们选择表现最好的父母(“适者生存”)，作为幸存的群体(图 2)。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi lw"><img src="../Images/9ff35a3caeb087b1b80d01649501599c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uqQLuhCeqbfyOyHtPt3Ixg.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">(Image by author)</figcaption></figure><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi lw"><img src="../Images/8387bb004d9740a79d52fdb916156d17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9mrDW_6WWBuggN6pGLinwg.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">(Image by author)</figcaption></figure><p id="4361" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated">现在幸存群体中的父母将通过结合两个步骤来交配产生后代:交叉/重组和突变。在杂交的情况下，来自交配父母的基因(参数)将被重组，以产生后代，每个孩子从每个父母那里继承一些基因(参数)(图 3)。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mm"><img src="../Images/521449e4b16b3eb6130defcffe950f85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h-rR-u4oyCZy39m4v-Pvhg.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">(Image by author)</figcaption></figure><p id="d04d" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated">最后，在突变的情况下，基因(参数)的一些值将被改变以保持遗传多样性(图 4)。这使得自然/遗传算法通常能够得到更好的解决方案。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mn"><img src="../Images/729c311238d81609f70ff09f7a07e8dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zlcm1KQCD3Enzx5F1WhufQ.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">(Image by author)</figcaption></figure><p id="9359" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated">图 5 显示了第二代人口，包括幸存的父母和孩子。我们保留幸存的双亲，以便保留最佳适应度参数，以防后代的适应度值不如双亲。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi lw"><img src="../Images/00661499ad33ebbc9805788718cb036d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pn-Ab86YbsqVH_C5t_OAmA.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">(Image by author)</figcaption></figure><h1 id="10a4" class="ju jv iq bd jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr bi translated"><strong class="ak">遗传算法模块为</strong><a class="ae lq" href="https://xgboost.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"><strong class="ak">XGBoost</strong></a><strong class="ak">:</strong></h1><p id="4b7f" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们将创建一个为 XGBoost 定制的遗传算法模块。以下是 XGboost 的描述:</p><blockquote class="mt mu mv"><p id="6f62" class="ks kt mw ku b kv lr kx ky kz ls lb lc mx lt lf lg my lu lj lk mz lv ln lo lp ij bi translated"><strong class="ku ir"> XGBoost </strong>是一个优化的分布式梯度增强库，旨在高效<strong class="ku ir"/>、<strong class="ku ir">灵活</strong>、<strong class="ku ir">便携</strong>。它在<a class="ae lq" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">梯度提升</a>框架下实现机器学习算法。</p></blockquote><p id="001c" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated">该模块将具有遵循四个步骤的函数:(I)初始化，(ii)选择，(iii)交叉，和(iv)变异，类似于上面讨论的内容(该代码的一小部分灵感来自帖子<a class="ae lq" rel="noopener" target="_blank" href="/genetic-algorithm-implementation-in-python-5ab67bb124a6">这里</a>)。</p><p id="4337" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated"><strong class="ku ir">初始化:</strong></p><p id="1a21" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated">第一步是初始化，参数被随机初始化以创建群体。它类似于图 1 所示的第一代人口。下面的代码显示了初始化过程，其中我们生成了一个包含参数的向量。对于 XGBoost，我们选择了 7 个参数进行优化:learning_rate、n_estimators、max_depth、min_child_weight、subsample、colsample_bytree 和 gamma。这些参数的详细描述可以在<a class="ae lq" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="lx ly lz ma gt na nb nc nd aw ne bi"><span id="3f3d" class="nf jv iq nb b gy ng nh l ni nj">def initilialize_poplulation(numberOfParents):<br/>    learningRate = np.empty([numberOfParents, 1])<br/>    nEstimators = np.empty([numberOfParents, 1], dtype = np.uint8)<br/>    maxDepth = np.empty([numberOfParents, 1], dtype = np.uint8)<br/>    minChildWeight = np.empty([numberOfParents, 1])<br/>    gammaValue = np.empty([numberOfParents, 1])<br/>    subSample = np.empty([numberOfParents, 1])<br/>    colSampleByTree =  np.empty([numberOfParents, 1])</span><span id="8477" class="nf jv iq nb b gy nk nh l ni nj">for i in range(numberOfParents):<br/>        print(i)<br/>        learningRate[i] = round(random.uniform(0.01, 1), 2)<br/>        nEstimators[i] = random.randrange(10, 1500, step = 25)<br/>        maxDepth[i] = int(random.randrange(1, 10, step= 1))<br/>        minChildWeight[i] = round(random.uniform(0.01, 10.0), 2)<br/>        gammaValue[i] = round(random.uniform(0.01, 10.0), 2)<br/>        subSample[i] = round(random.uniform(0.01, 1.0), 2)<br/>        colSampleByTree[i] = round(random.uniform(0.01, 1.0), 2)<br/>    <br/>    population = np.concatenate((learningRate, nEstimators, maxDepth, minChildWeight, gammaValue, subSample, colSampleByTree), axis= 1)<br/>    return population</span></pre><p id="cbc8" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated">参数的限制要么基于 XGBoost 文档中描述的限制，要么基于合理的猜测(如果上限设置为无穷大)。我们首先为每个参数创建一个空数组，然后用随机值填充它。</p><p id="ea4b" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated"><strong class="ku ir">亲代选择(适者生存)</strong></p><p id="50a3" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated">在第二步中，我们使用初始群体训练我们的模型，并计算适应值。在这种情况下，我们将计算 F1 分数。</p><pre class="lx ly lz ma gt na nb nc nd aw ne bi"><span id="414f" class="nf jv iq nb b gy ng nh l ni nj">def fitness_f1score(y_true, y_pred):<br/>    fitness = round((f1_score(y_true, y_pred, average='weighted')), 4)<br/>    return fitness</span><span id="a2f2" class="nf jv iq nb b gy nk nh l ni nj">#train the data annd find fitness score<br/>def train_population(population, dMatrixTrain, dMatrixtest, y_test):<br/>    fScore = []<br/>    for i in range(population.shape[0]):<br/>        param = { 'objective':'binary:logistic',<br/>              'learning_rate': population[i][0],<br/>              'n_estimators': population[i][1], <br/>              'max_depth': int(population[i][2]), <br/>              'min_child_weight': population[i][3],<br/>              'gamma': population[i][4], <br/>              'subsample': population[i][5],<br/>              'colsample_bytree': population[i][6],<br/>              'seed': 24}<br/>        num_round = 100<br/>        xgbT = xgb.train(param, dMatrixTrain, num_round)<br/>        preds = xgbT.predict(dMatrixtest)<br/>        preds = preds&gt;0.5<br/>        fScore.append(fitness_f1score(y_test, preds))<br/>    return fScore</span></pre><p id="def3" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated">我们将定义我们想要选择多少个父节点，并根据它们的适合度值用所选择的父节点创建一个数组。</p><pre class="lx ly lz ma gt na nb nc nd aw ne bi"><span id="6145" class="nf jv iq nb b gy ng nh l ni nj">#select parents for mating<br/>def new_parents_selection(population, fitness, numParents):<br/>    selectedParents = np.empty((numParents, population.shape[1])) #create an array to store fittest parents<br/>    <br/>    #find the top best performing parents<br/>    for parentId in range(numParents):<br/>        bestFitnessId = np.where(fitness == np.max(fitness))<br/>        bestFitnessId  = bestFitnessId[0][0]<br/>        selectedParents[parentId, :] = population[bestFitnessId, :]<br/>        fitness[bestFitnessId] = -1 #set this value to negative, in case of F1-score, so this parent is not selected again<br/>    return selectedParents</span></pre><p id="9c35" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated"><strong class="ku ir">交叉</strong></p><p id="701a" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated">在<a class="ae lq" href="https://en.wikipedia.org/wiki/Crossover_(genetic_algorithm)" rel="noopener ugc nofollow" target="_blank">遗传算法</a> <a class="ae lq" href="https://en.wikipedia.org/wiki/Crossover_(genetic_algorithm))," rel="noopener ugc nofollow" target="_blank">、</a>的情况下，有多种方法定义交叉，如单点、两点和 k 点交叉，均匀交叉和有序列表交叉。我们将使用均匀交叉，其中孩子的每个参数将基于某种分布从父母中独立选择。在我们的例子中，我们将使用来自<a class="ae lq" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randint.html#numpy.random.randint" rel="noopener ugc nofollow" target="_blank"> numpy 随机函数</a>的“离散均匀”分布。</p><pre class="lx ly lz ma gt na nb nc nd aw ne bi"><span id="37ab" class="nf jv iq nb b gy ng nh l ni nj">'''<br/>Mate these parents to create children having parameters from these parents (we are using uniform crossover method)<br/>'''<br/>def crossover_uniform(parents, childrenSize):<br/>    <br/>    crossoverPointIndex = np.arange(0, np.uint8(childrenSize[1]), 1, dtype= np.uint8) #get all the index<br/>    crossoverPointIndex1 = np.random.randint(0, np.uint8(childrenSize[1]), np.uint8(childrenSize[1]/2)) # select half  of the indexes randomly<br/>    crossoverPointIndex2 = np.array(list(set(crossoverPointIndex) - set(crossoverPointIndex1))) #select leftover indexes<br/>    <br/>    children = np.empty(childrenSize)<br/>    <br/>    '''<br/>    Create child by choosing parameters from two parents selected using new_parent_selection function. The parameter values<br/>    will be picked from the indexes, which were randomly selected above. <br/>    '''<br/>    for i in range(childrenSize[0]):<br/>        <br/>        #find parent 1 index <br/>        parent1_index = i%parents.shape[0]<br/>        #find parent 2 index<br/>        parent2_index = (i+1)%parents.shape[0]<br/>        #insert parameters based on random selected indexes in parent 1<br/>        children[i, crossoverPointIndex1] = parents[parent1_index, crossoverPointIndex1]<br/>        #insert parameters based on random selected indexes in parent 1<br/>        children[i, crossoverPointIndex2] = parents[parent2_index, crossoverPointIndex2]<br/>    return children</span></pre><p id="2fc2" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated"><strong class="ku ir">突变</strong></p><p id="51e1" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated">最后一步将是通过随机选择一个参数并以一个<a class="ae lq" href="https://en.wikipedia.org/wiki/Mutation_(genetic_algorithm)" rel="noopener ugc nofollow" target="_blank">随机量</a>改变它的值，在孩子中引入多样性。我们还将引入一些限制，以便将改变的值限制在一定的范围内。跳过这些约束可能会导致错误。</p><pre class="lx ly lz ma gt na nb nc nd aw ne bi"><span id="90c6" class="nf jv iq nb b gy ng nh l ni nj">def mutation(crossover, numberOfParameters):<br/>    #Define minimum and maximum values allowed for each parameter</span><span id="1716" class="nf jv iq nb b gy nk nh l ni nj">minMaxValue = np.zeros((numberOfParameters, 2))<br/>    <br/>    minMaxValue[0:] = [0.01, 1.0] #min/max learning rate<br/>    minMaxValue[1, :] = [10, 2000] #min/max n_estimator<br/>    minMaxValue[2, :] = [1, 15] #min/max depth<br/>    minMaxValue[3, :] = [0, 10.0] #min/max child_weight<br/>    minMaxValue[4, :] = [0.01, 10.0] #min/max gamma<br/>    minMaxValue[5, :] = [0.01, 1.0] #min/maxsubsample<br/>    minMaxValue[6, :] = [0.01, 1.0] #min/maxcolsample_bytree<br/> <br/>    # Mutation changes a single gene in each offspring randomly.<br/>    mutationValue = 0<br/>    parameterSelect = np.random.randint(0, 7, 1)<br/>    print(parameterSelect)<br/>    if parameterSelect == 0: #learning_rate<br/>        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)<br/>    if parameterSelect == 1: #n_estimators<br/>        mutationValue = np.random.randint(-200, 200, 1)<br/>    if parameterSelect == 2: #max_depth<br/>        mutationValue = np.random.randint(-5, 5, 1)<br/>    if parameterSelect == 3: #min_child_weight<br/>        mutationValue = round(np.random.uniform(5, 5), 2)<br/>    if parameterSelect == 4: #gamma<br/>        mutationValue = round(np.random.uniform(-2, 2), 2)<br/>    if parameterSelect == 5: #subsample<br/>        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)<br/>    if parameterSelect == 6: #colsample<br/>        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)<br/>  <br/>    #indtroduce mutation by changing one parameter, and set to max or min if it goes out of range<br/>    for idx in range(crossover.shape[0]):<br/>        crossover[idx, parameterSelect] = crossover[idx, parameterSelect] + mutationValue<br/>        if(crossover[idx, parameterSelect] &gt; minMaxValue[parameterSelect, 1]):<br/>            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 1]<br/>        if(crossover[idx, parameterSelect] &lt; minMaxValue[parameterSelect, 0]):<br/>            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 0]    <br/>    return crossover</span></pre><h1 id="52f6" class="ju jv iq bd jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr bi translated"><strong class="ak">实施</strong></h1><p id="b5c5" class="pw-post-body-paragraph ks kt iq ku b kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们将实现上面讨论的模块，在数据集上进行训练。数据集来自<a class="ae lq" href="https://archive.ics.uci.edu/ml/machine-learning-databases/musk/" rel="noopener ugc nofollow" target="_blank"> UCI 机器学习库</a>。它包含一组 102 个分子，其中 39 个被人类识别为具有可用于香水的气味，69 个没有所需的气味。该数据集包含这些分子的 6，590 个低能构象，包含 166 个特征。我们正在做最少的前置处理，作为本教程理解遗传算法的目标。</p><pre class="lx ly lz ma gt na nb nc nd aw ne bi"><span id="c790" class="nf jv iq nb b gy ng nh l ni nj"># Importing the libraries<br/>import numpy as np<br/>import pandas as pd<br/>import geneticXGboost #this is the module we crated above<br/>import xgboost as xgb</span><span id="9dcf" class="nf jv iq nb b gy nk nh l ni nj">np.random.seed(723)</span><span id="77ae" class="nf jv iq nb b gy nk nh l ni nj"># Importing the dataset<br/>dataset = pd.read_csv('clean2.data', header=None)</span><span id="4fe0" class="nf jv iq nb b gy nk nh l ni nj">X = dataset.iloc[:, 2:168].values #discard first two coloums as these are molecule's name and conformation's name</span><span id="a270" class="nf jv iq nb b gy nk nh l ni nj">y = dataset.iloc[:, 168].values #extrtact last coloum as class (1 =&gt; desired odor, 0 =&gt; undesired odor)</span><span id="2423" class="nf jv iq nb b gy nk nh l ni nj"># Splitting the dataset into the Training set and Test set<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 97)</span><span id="5217" class="nf jv iq nb b gy nk nh l ni nj"># Feature Scaling<br/>from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span><span id="bf91" class="nf jv iq nb b gy nk nh l ni nj">#XGboost Classifier</span><span id="2021" class="nf jv iq nb b gy nk nh l ni nj">#model xgboost<br/>#use xgboost API now<br/>xgDMatrix = xgb.DMatrix(X_train, y_train) #create Dmatrix<br/>xgbDMatrixTest = xgb.DMatrix(X_test, y_test)</span></pre><p id="f8d6" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated">我们有 8 个父母开始，我们选择 4 个最合适的父母交配。我们将创建 4 代并监控适应度(F1 分数)。下一代中有一半的父母将是从上一代中选出的最适合的父母。这将允许我们保持最好的健康分数至少与上一代相同，以防孩子的健康分数更差。</p><pre class="lx ly lz ma gt na nb nc nd aw ne bi"><span id="a83d" class="nf jv iq nb b gy ng nh l ni nj">numberOfParents = 8 #number of parents to start<br/>numberOfParentsMating = 4 #number of parents that will mate<br/>numberOfParameters = 7 #number of parameters that will be optimized<br/>numberOfGenerations = 4 #number of genration that will be created</span><span id="c8bd" class="nf jv iq nb b gy nk nh l ni nj">#define the population size</span><span id="c882" class="nf jv iq nb b gy nk nh l ni nj">populationSize = (numberOfParents, numberOfParameters)</span><span id="7d62" class="nf jv iq nb b gy nk nh l ni nj">#initialize the population with randomly generated parameters<br/>population = geneticXGboost.initilialize_poplulation(numberOfParents)</span><span id="cfc2" class="nf jv iq nb b gy nk nh l ni nj">#define an array to store the fitness  hitory<br/>fitnessHistory = np.empty([numberOfGenerations+1, numberOfParents])</span><span id="9fd9" class="nf jv iq nb b gy nk nh l ni nj">#define an array to store the value of each parameter for each parent and generation<br/>populationHistory = np.empty([(numberOfGenerations+1)*numberOfParents, numberOfParameters])</span><span id="5478" class="nf jv iq nb b gy nk nh l ni nj">#insert the value of initial parameters in history<br/>populationHistory[0:numberOfParents, :] = population</span><span id="95a6" class="nf jv iq nb b gy nk nh l ni nj">for generation in range(numberOfGenerations):<br/>    print("This is number %s generation" % (generation))<br/>    <br/>    #train the dataset and obtain fitness<br/>    fitnessValue = geneticXGboost.train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)<br/>    fitnessHistory[generation, :] = fitnessValue<br/>    <br/>    #best score in the current iteration<br/>    print('Best F1 score in the this iteration = {}'.format(np.max(fitnessHistory[generation, :])))</span><span id="63cf" class="nf jv iq nb b gy nk nh l ni nj">#survival of the fittest - take the top parents, based on the fitness value and number of parents needed to be selected<br/>    parents = geneticXGboost.new_parents_selection(population=population, fitness=fitnessValue, numParents=numberOfParentsMating)<br/>    <br/>    #mate these parents to create children having parameters from these parents (we are using uniform crossover)<br/>    children = geneticXGboost.crossover_uniform(parents=parents, childrenSize=(populationSize[0] - parents.shape[0], numberOfParameters))<br/>    <br/>    #add mutation to create genetic diversity<br/>    children_mutated = geneticXGboost.mutation(children, numberOfParameters)<br/>    <br/>    '''<br/>    We will create new population, which will contain parents that where selected previously based on the<br/>    fitness score and rest of them  will be children<br/>    '''<br/>    population[0:parents.shape[0], :] = parents #fittest parents<br/>    population[parents.shape[0]:, :] = children_mutated #children<br/>    <br/>    populationHistory[(generation+1)*numberOfParents : (generation+1)*numberOfParents+ numberOfParents , :] = population #srore parent information</span></pre><p id="adad" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated">最后，我们得到最佳分数和相关参数:</p><pre class="lx ly lz ma gt na nb nc nd aw ne bi"><span id="7ca8" class="nf jv iq nb b gy ng nh l ni nj">#Best solution from the final iteration</span><span id="122c" class="nf jv iq nb b gy nk nh l ni nj">fitness = geneticXGboost.train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)<br/>fitnessHistory[generation+1, :] = fitness</span><span id="5ba0" class="nf jv iq nb b gy nk nh l ni nj">#index of the best solution<br/>bestFitnessIndex = np.where(fitness == np.max(fitness))[0][0]</span><span id="344a" class="nf jv iq nb b gy nk nh l ni nj">#Best fitness<br/>print("Best fitness is =", fitness[bestFitnessIndex])</span><span id="86a4" class="nf jv iq nb b gy nk nh l ni nj">#Best parameters<br/>print("Best parameters are:")<br/>print('learning_rate', population[bestFitnessIndex][0])<br/>print('n_estimators', population[bestFitnessIndex][1])<br/>print('max_depth', int(population[bestFitnessIndex][2])) <br/>print('min_child_weight', population[bestFitnessIndex][3])<br/>print('gamma', population[bestFitnessIndex][4])<br/>print('subsample', population[bestFitnessIndex][5])<br/>print('colsample_bytree', population[bestFitnessIndex][6])</span></pre><p id="82bf" class="pw-post-body-paragraph ks kt iq ku b kv lr kx ky kz ls lb lc ld lt lf lg lh lu lj lk ll lv ln lo lp ij bi translated">现在让我们想象一下每一代人在健康方面的变化(下图)。虽然我们已经从高 F1 分数(~0.98)开始，但在随机产生的初始群体中的两个亲本中，我们能够在最终一代中进一步改进它。初始群体中父母一方的最低 F1 值为 0.9143，最终世代中父母一方的最高 F1 值为 0.9947。这表明我们可以通过遗传算法的简单实现来改进 XGBoost 中的性能指标。最终代码可以在<a class="ae lq" href="https://github.com/mjain72/Hyperparameter-tuning-in-XGBoost-using-genetic-algorithm" rel="noopener ugc nofollow" target="_blank">我的 github 账号</a>找到。它还包含允许您观察每一代中各种参数的变化的代码。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/a028356a1381b75c186c2a655ae0c753.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*WcmT4t8zxs5S6teMGSoZpg.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">(Image by author)</figcaption></figure></div></div>    
</body>
</html>