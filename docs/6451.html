<html>
<head>
<title>Unpacking (** PCA )</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">拆包(** PCA)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unpacking-pca-b5ea8bec6aa5?source=collection_archive---------11-----------------------#2018-12-14">https://towardsdatascience.com/unpacking-pca-b5ea8bec6aa5?source=collection_archive---------11-----------------------#2018-12-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b4c9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">降维技术的一种实用实现</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="be6c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">尽管互联网上有大量的 PCA(主成分分析)资源，但只有忍者才能在一定的合理时间内抓住整个想法，更不用说在你知道实际发生了什么的同时将想法具体化为代码。话虽如此，我们还是非常喜欢像专业人员一样应用 PCA 和解决真实案例的感觉。</p><p id="04a2" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在这篇文章中，我将尝试并清楚地说明人们可能会感到困惑的想法，我认为像我这样的新手会陷入其中并坚持几个小时。</p><p id="9e67" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">事不宜迟，让我们直接进入正题。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="ac62" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">首先，主成分分析是一种降维方法，更简单地说，是将多个特征(变量)归纳为较少特征的方法。假设，我们需要将新看到的动物分类为'<strong class="kr iu">狗</strong>或'<strong class="kr iu">猫</strong>，我们将测量动物的特征，如'<strong class="kr iu">身高</strong>和'<strong class="kr iu">体重</strong>。我们也可以根据更多的特征对这种看不见的动物进行分类(<strong class="kr iu">颜色</strong>’、<strong class="kr iu">形状</strong>’、<strong class="kr iu">敏捷</strong>’…等等。)</p><p id="88ee" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">但是，问题是，我们不能将它们归类为超过三个特征(维度)，这对我们来说是视觉上无法解释的。另一个原因是，有些特征用起来不太重要，或者更糟的是，如果它们是随机数据或错误，有些可能是噪音，这两种情况都会干扰我们的预测。</p><p id="72dd" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">简而言之，一个特征'<strong class="kr iu">体重</strong>'可能无法有效区分'<strong class="kr iu">狗</strong>'和'<strong class="kr iu">猫</strong>'，因此我们应该从数据中去掉'<strong class="kr iu">体重</strong>'。通过去掉大部分不重要的特征，我们可以节省计算成本。想象一下，我们的数据有 100 x 100 个特征，比如图像或遗传信息，尽可能地降低维度会好得多。</p><p id="d85f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们可以通过使用 PCA 来实现这些愿望。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/7641485742c5a11a61c617614d4c4b47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eO00r-Ee36zfKd4SUnskEg.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">PCA describe multiple variable with fewer Principal Components</figcaption></figure></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="a254" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">最好是实现一下，以获得一个整体的想法。让我们从制作 5 *10 矩阵开始，并采取步骤的过程。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi md"><img src="../Images/05348b3549e0f52a76e1987715c93cf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*y0SNRMaLJ9YJZNpIWqusWg.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Matrix X</figcaption></figure><p id="d578" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">列是变量(特征)，行是样本(比如“猫”或“狗”)。</p><p id="2bf5" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们想对这个矩阵做的是得到特征值和特征向量，它们变成了描述样本的新变量(主成分)。设矩阵<strong class="kr iu"><em class="me">【x】</em></strong>为<strong class="kr iu"> <em class="me"> n *p </em> </strong>大小，则<strong class="kr iu"> <em class="me"> p *p </em> </strong>协方差矩阵<strong class="kr iu"><em class="me"/></strong>其中<strong class="kr iu"><em class="me">【c=xᵀx/n.】</em></strong><strong class="kr iu"><em class="me"/></strong>协方差矩阵是埃尔米特矩阵和半正定矩阵，利用<strong class="kr iu"> <em class="me">谱定理</em> </strong> 让我们得到一个协方差矩阵和特征值。但在此之前，不要忘记从同一列中减去每列的平均值。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="98ac" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们设置'<strong class="kr iu"><em class="me">row var = False '</em></strong>，这样关系就转置了:每一列代表一个变量，而行包含观测值。然后，我们得到<strong class="kr iu"> <em class="me"> w </em> </strong>中的特征值和<strong class="kr iu"> <em class="me"> v </em> </strong>中的特征向量。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/2ea357f3e356622bcb9aac86c1ce896b.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*qRT3RDwiGXU0jj3JZMj1BQ.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Means of each columns</figcaption></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/ee2354590748d41372f2166bd48e5397.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*U-Te6zbCmHoQFkXNFJYSVw.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">10 *10 Covariance Matrix</figcaption></figure><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="1df3" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">此时，特征向量是 10 *10 的矩阵。为了执行从 10 个变量到 2 个变量的降维，我们需要截断矩阵。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="6a36" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">然后，我们要将(主轴上的投影 X)原始的 10 *10 样本变换到新的空间(二维空间)，我们使用等式<strong class="kr iu"><em class="me"/></strong><em class="me">=</em><strong class="kr iu"><em class="me">XV。</em> </strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/c2d20c829e79f115ffd435352308a363.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*8dFDYa8-euVFFbgISuvzBA.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">10 *2 Eigenvectors (Principal components)</figcaption></figure><p id="8b03" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们得到了矩阵 5 *2 矩阵<strong class="kr iu"> <em class="me"> T </em> </strong>，投影到 2 个主成分上。我们最后用两个变量来描述每个样本。</p><p id="2be5" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">有趣的部分应该是可视化这些转换后的样本数据。让我们使用更大的数据。从定义 200 *2 矩阵<strong class="kr iu"> <em class="me"> X </em> </strong>开始。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="30d7" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">(在这种情况下，特征值是 2，因此我们实际上不需要将其截断为 2)</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="0929" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">最后剧情。让我们不要忘记设置<strong class="kr iu"> <em class="me">轴‘相等’</em></strong>，这样我们可以看到主分量的正交性。否则，我们将得到一个不垂直的特征向量。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mi"><img src="../Images/24b7e7c401d262c5ad6f80f58c6d1fd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wGagL2P8xQstDySgr6nk5w.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Original sample plot(Fig. left) and plot eigenvectors(Fig. right)</figcaption></figure><p id="069f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">让我们也绘制转换后的样本。因为我们的样本已经是二维的，我没有必要截断它们，但是，让我们采取降维过程的步骤。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="d868" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">最后剧情。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="4336" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们所看到的，两个主分量是正交的，因为它们被 PCA 去相关，所以变量是相互独立的。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/3664f7fa021d56a411ca63399f545592.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*_8Ckwt0yKvMhnfzUpSITjw.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Fig. 1: Plot transformed sample data</figcaption></figure><p id="aead" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">样本数据也沿着两个主轴进行变换(不相关/旋转)。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/2929c0b2ec504af1bc7621fb195d5393.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*0DKYKqe3BpGS6o5cin3X6A.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Fig.2: Uncorrelated sample data</figcaption></figure><p id="e2ed" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为了更好地理解术语<strong class="kr iu"> <em class="me">【不相关/旋转】</em> </strong>，让我们看一下图 1，并与图 2 进行比较。分散的数据实际上是完全相同的数据，但是，我们对它们进行了去相关(旋转)，从而使两个变量(轴)相互独立(图 2)。</p><p id="1d3f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为什么我们要用特征分解(或者为了效率用奇异值分解)，用一些最大的特征值和对应的特征向量？这是因为 PCA 的目的是降低维数，但同时又尽可能用较少的变量描述样本。换句话说，我们想要在每个样本中变化的变量，而不想要在样本中相同的变量。我们可以通过使用更大的特征值或更大的方差来实现这个想法。根据信息论的观点，最大方差对应于具有最大熵的维度，因此，最大方差编码了最多的信息。然而，特征分解是昂贵的。假设，我们有图像样本，比如说<strong class="kr iu"> <em class="me"> 100 *100 </em> </strong>像素，这意味着它有 10000 个变量。它的协方差矩阵<strong class="kr iu"> <em class="me"> C </em> </strong>将是 10000 * 10000 维。因此，我们通常更喜欢 SVD(奇异值分解)来将维度大小降低到样本大小，而不是可变大小。让我们试一试，用奇异值分解绘制样本。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="aee9" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">SVD 将协方差矩阵<strong class="kr iu"> <em class="me"> C </em> </strong> <em class="me">，</em>where<strong class="kr iu"><em class="me">c=xxᵀ/n</em></strong>，分解成<strong class="kr iu"><em class="me"/></strong>其中<strong class="kr iu"> <em class="me"> S </em> </strong>是一个含有奇异值的(矩形)对角矩阵<strong class="kr iu"><em class="me">【sᵢ</em></strong>，<strong class="kr iu"> <em class="me"> U </em> </strong>是一个酉矩阵，而就奇异值<strong class="kr iu"><em class="me">【sᵢ】</em></strong>而言，我们可以用<strong class="kr iu"><em class="me">【λᵢ=【sᵢ**2)/n】</em></strong>，<strong class="kr iu"> <em class="me"> </em> </strong>其中 n 是样本量，<strong class="kr iu"><em class="me">【λᵢ】</em></strong>是特征值。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="b2e9" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">从这个结果中，我们通过 SVD 和本征分解看到完全相同的主分量。</p><p id="19fc" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">让我们也确定特征向量是相互正交的。一个正交矩阵<strong class="kr iu"> <em class="me"> Q </em> </strong>定义为<strong class="kr iu"> <em class="me"> QᵀQ=QQᵀ=I </em> </strong>，其中<strong class="kr iu"> <em class="me"> I </em> </strong>为一个单位矩阵。我们从奇异值分解得到了特征向量<strong class="kr iu"><em class="me">【vᵀ】</em></strong><strong class="kr iu"><em class="me"/></strong>现在如下图。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mk"><img src="../Images/8be670239ccf1c113353e3c3d20fe2c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*11uUmC9FTs5o-aTKCGYAVQ.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">V<strong class="bd ml"><em class="mm">ᵀ</em></strong> (Eigenvectors)</figcaption></figure><p id="1d2c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><code class="fe mn mo mp mq b">V.dot(V.T)</code>的结果是一个单位矩阵。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/da4c609ce6eeb980bff57736331b5964.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*N8t7qagFpe7DKsA7dTPKvA.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Identity matrix</figcaption></figure><p id="6abb" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在许多数学概念的理论中，例如偏最小二乘法，向量正交的概念经常出现。以这种方式检查它对于确保我们在正确的轨道上是有用的。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/ad742f1dbf529cebb664e00a910da17b.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*3_yUbzNkvvvntAfmMEUV5g.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">PCA with SVD</figcaption></figure><p id="58b7" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">人们可能想观察每个点离中心有多远。其中一个方法是画一个等概率图，或者误差日蚀图。这对于查看两个组中间的样本特别有帮助，例如，我们可以将它用于 K-means 聚类。</p><p id="dc9b" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们从导入模块开始。然后，我们想用奇异值分解原始样本的协方差矩阵。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="3d3e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">酉矩阵<em class="me"> U_ </em>用于旋转样本数据，现在我们分别使用<em class="me">U _【0】</em>和<em class="me">U _【1】</em>，余弦，正弦，这样我们就可以得到正切来计算角度。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/b3532f91cd0dc5c2d4d62381dccba575.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*oZE5nAc5hWHNz_njz7o4cA.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Equiprobability: ecliptic error circle</figcaption></figure><p id="587f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">角度是从<strong class="kr iu"> <em class="me"> x </em> </strong>轴。当坐标轴由标准差定义时，日食由方程定义，<strong class="kr iu"><em class="me">(x/σ₁)【+(y/σ₂)= s</em></strong>，其中<strong class="kr iu"> <em class="me"> σ₁ </em> </strong>为 x 轴的标准差，<strong class="kr iu"> <em class="me"> σ₂ </em> </strong>为 y 轴的标准差，<strong class="kr iu"> <em class="me"> s </em> </strong>为样本的方差。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/9dc6ee01b8ba51cfccaa1d0eb4e0fe35.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*G0Ru9hX9bxIyKxbdHkL-4g.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">The angle between X and PC1</figcaption></figure><p id="90c0" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu"> <em class="me"> x=2σ₁√s </em> </strong>表示一个标准差。我们在上面的代码中绘制了三次。第三个圆圈包含图像上的大部分样本数据，因为它应该是 99.7%的置信区间。</p><p id="5328" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们可以使用 Scikit-learn 进行 PCA。让我们再看一遍同一个样本。像往常一样从样本数据开始。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="8256" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">定义<strong class="kr iu"> <em class="me"> 200 *2 </em> </strong>矩阵<strong class="kr iu"> <em class="me"> X </em> </strong>，为了剧情的缘故，我们使用两个组件。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="1946" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">剩下的过程和以前一样。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/815c3418495afde6229410d93f17e9f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*UwUkqvXHKWWi6WW57gKlnA.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">PCA with Scikit-learn version</figcaption></figure><p id="29d5" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">结果是一样的。从代码中可以看出，我们可以通过<strong class="kr iu"> <em class="me">【分量 _ </em> </strong>获取最大特征向量，通过<strong class="kr iu"> <em class="me">解释 _ 方差 _ </em> </strong>获取特征值。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/515cd49d5594b00a6f2029a94afcd5fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*nfmMxrZROm0Z7qdEWC92zg.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk"><em class="mm">explained_variance_</em></figcaption></figure><p id="8100" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">让我们来看看<strong class="kr iu"> <em class="me">讲解 _ 方差 _ </em> </strong></p><p id="3554" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">主成分解释了样本数据的 97.6%[0.7625315/(0.7625315+0.0184779)]，第二主成分解释了剩余部分。这意味着我们几乎可以不用第二主成分来描述原始数据。Scikit-learn 已经计算了解释方差比率，因此我们可以使用 via<strong class="kr iu"><em class="me">explained _ variance _ ratio _</em></strong>。</p><p id="ebf2" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为了更好地理解，让我们使用 1797 *64 矩阵数字数据，并将这些维度从 64 减少到 10。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="3c2a" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">现在，我们可以看到这 10 个新组件可以在多大程度上描述原始样本数据。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/3b12860b283f400994c7dc3a563f5d80.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*a8YmU7L8OhCXeb57Z8pUaA.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Accuracy vs. number of components</figcaption></figure><p id="cc2f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">事实证明，它描述了 72~73%的样本，而不是使用确保 100%准确性的 64 个维度。请注意，在左图中，第一个成分是指数 0，这就是为什么该图从 14~15%的方差开始。</p><p id="c41c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">假设需要多少元件才能达到 90%的精度呢？我们可以先清空 PCA 函数并绘制图形。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/87cb1463720e7864b04c45f35062764b.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*uPZYEslzWXhpezLoNwKeug.png"/></div></figure><p id="066a" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们认为 20 个左右的成分对于 90%的方差来说就足够了。更简单的方法是将数字直接添加到函数中，例如，<code class="fe mn mo mp mq b">pca = PCA(.9)</code></p><p id="e9a8" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为了进一步理解，让我们将转换后的样本数据转化为原始数据，并对它们应用热图。我们应该观察到，组件的数量越多，它创建的样本数据就越精确。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mw"><img src="../Images/a1a6864b567083d2c4cf5aaf85094625.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7w1LcCB-TWr2flgGrFd0NA.png"/></div></div></figure><p id="8b8a" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">显然，原始数据比具有两个主成分的反演数据具有更多的特征。使用四十个主成分，我们可以更精确地反演转换后的数据。</p><p id="eee8" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们实际上可以显示目前为止我们正在处理的数字数据。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mx"><img src="../Images/25d9174f97ab265df2faad06f14640ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*3MLiZLwp80sbVMk--sauaQ.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Digits from original sample</figcaption></figure><p id="984c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">让我们使这些数字图像更粗糙，或者更准确地说，一旦执行降维并将数字图像反转到原始大小。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mx"><img src="../Images/f292f06fcf832b26c9b9ee1627484339.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*1ZRZdDJHhUpi3Ufhe3y--A.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Reconstructed digits images</figcaption></figure><p id="424c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们也可以将 PCA 应用于特征脸，因为它的特征只有亮度，就像上面的数字图像一样。在重建原始数据方面，我们采取了与数字重建略有不同的步骤。因为我们首先需要一张平均脸。在转换数据后，我们添加平均脸。假设我们想要一个反转的人脸图像<strong class="kr iu"> <em class="me"> X </em> </strong>，<strong class="kr iu"> <em class="me"> X </em> </strong>描述为<strong class="kr iu"><em class="me">x =μ+w</em></strong><em class="me">₁</em><strong class="kr iu"><em class="me">* v</em></strong><em class="me">₁</em><strong class="kr iu"><em class="me">+w</em></strong><em class="me">₂</em><strong class="kr iu"><em class="me">* v</em></strong><em class="me"/></p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi my"><img src="../Images/a0175fdbeed96bd47bcebb54fe6e29c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*KdIRgWYzSKbxzAxFjS1bYA.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Full dimension(Fig. above) and 100 dimensions(Fig. below)</figcaption></figure><p id="0273" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们之前讨论的，当维数很大时，SVD 表现良好。例如，这些图像是 62 *47 = 2914 尺寸。如果我们使用本征分解，我们需要计算相关矩阵的 2914 *2914 维。相反，奇异值分解可以通过样本大小*样本大小来计算。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="3108" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">主成分由<strong class="kr iu"> <em class="me"> XV=US </em> </strong>给出。我们把维度从 500 降低到 100，现在<strong class="kr iu"> <em class="me">我们的</em> </strong>就是一个<strong class="kr iu"> <em class="me"> 500 *100 </em> </strong>的矩阵。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="53d3" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为了与原始人脸图像进行比较，我们要将<strong class="kr iu"> <em class="me">投影为</em> </strong>，并将其逆投影为<strong class="kr iu"> <em class="me"> X </em> </strong>。我们应用方程<strong class="kr iu"><em class="me">【x=usvᵀ】</em></strong><strong class="kr iu"><em class="me"/></strong>，得到 500 * 2914 矩阵<strong class="kr iu"><em class="me">×19</em></strong>。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="0061" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">最后绘制图像。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi my"><img src="../Images/7d76244446a08eb54a4fc5b9c6f80a2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*4wEZebZljsxsrpM3ApUHbA.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Reconstructed face images with SVD(Fig. below)</figcaption></figure><p id="9980" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">得到的图像不像先前的 scikit-learn 方法那样模糊。似乎是更好的方法，我们使用的数据越少，构建的图像越清晰。这是因为样品的噪音更少，机器使用的过滤器更少。这将导致它自己对新图像的预测较低，这是机器以前从未见过的。</p><p id="3934" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">组成这些脸的特征向量称为特征脸，每个特征脸都是唯一的。让我们来看看他们是什么样子的，以及这些人脸图像的平均脸是什么样子的。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mb mc l"/></div></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mz"><img src="../Images/207736241a92e96d8a27957d2198a37d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7oMiZGcL9920ieFH2W3euw.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">The average face(Fig. left), and the first five(most biggest) Eigenfaces(Fig. right group)</figcaption></figure><p id="f3db" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">平均人脸是人脸图像的基础。</p><p id="4fc5" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">最后，这里是奇异值分解处理特征脸的概述。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi na"><img src="../Images/4e158b8db1cb41613a2edbf2417f927c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bb_JwMd67tAH-cyKSUT0-w.png"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">SVD overview</figcaption></figure></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="b44c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">参考</p><div class="nb nc gp gr nd ne"><a href="https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">奇异值分解与主成分分析的关系。如何用 SVD 进行 PCA？</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">主成分分析(PCA)通常通过协方差矩阵的特征分解来解释。然而…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">stats.stackexchange.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns lv ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">理解主成分分析、特征向量和特征值</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">在今天的模式识别课上，我的教授谈到了 PCA，特征向量&amp;特征值。我得到了…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">stats.stackexchange.com</p></div></div><div class="nn l"><div class="nt l np nq nr nn ns lv ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a rel="noopener follow" target="_blank" href="/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">主成分分析的一站式商店</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">在我用于研究生统计理论课的教科书的开始，作者(乔治·卡塞拉和罗杰…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="nu l np nq nr nn ns lv ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a href="http://www.visiondummy.com/2014/04/draw-error-ellipse-representing-covariance-matrix/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">如何画出代表协方差矩阵的误差椭圆？</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">在这篇文章中，我将展示如何为 2D 正态分布数据绘制一个误差椭圆，也称为置信椭圆…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">www.visiondummy.com</p></div></div><div class="nn l"><div class="nv l np nq nr nn ns lv ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a href="http://jotterbach.github.io/content/posts/pca/2016-03-24-Principal_Component_Analysis/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">用于特征选择的主成分分析(PCA)及其一些缺陷</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">数据科学中的一个典型方法是我所说的宇宙特征化。我的意思是我们提取…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">jotterbach.github.io</p></div></div><div class="nn l"><div class="nw l np nq nr nn ns lv ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a href="http://www.scholarpedia.org/article/Eigenfaces#fig:PIE.jpg" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">eigen faces-scholar pedia</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">特征脸可以被认为是表征人脸图像之间的全局变化的一组特征。然后…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">www.scholarpedia.org</p></div></div><div class="nn l"><div class="nx l np nq nr nn ns lv ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a href="http://www.visiondummy.com/2014/05/feature-extraction-using-pca/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">基于主成分分析的特征提取——假人的计算机视觉</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">在这篇文章中，我们讨论如何主成分分析(PCA)的工作，以及它如何可以被用来作为一个维度…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">www.visiondummy.com</p></div></div><div class="nn l"><div class="ny l np nq nr nn ns lv ne"/></div></div></a></div></div></div>    
</body>
</html>