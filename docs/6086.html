<html>
<head>
<title>Self Learning AI-Agents IV: Stochastic Policy Gradient</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自学习人工智能代理 IV:随机策略梯度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/self-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20?source=collection_archive---------13-----------------------#2018-11-25">https://towardsdatascience.com/self-learning-ai-agents-iv-stochastic-policy-gradients-b53f088fce20?source=collection_archive---------13-----------------------#2018-11-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="52c1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在连续的动作空间中控制人工智能:从自动驾驶汽车到机器人。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8b2c53966be91797bef6e0dda313cfa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ff14zY0i4mi3HPa6pCeF4g.png"/></div></div></figure><h2 id="65ea" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">自学习人工智能代理系列—目录</h2><ul class=""><li id="423e" class="lq lr it ls b lt lu lv lw ld lx lh ly ll lz ma mb mc md me bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f">第一部分:马尔可夫决策过程</a></li><li id="35e1" class="lq lr it ls b lt mg lv mh ld mi lh mj ll mk ma mb mc md me bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47">第二部分:深度 Q 学习</a></li><li id="dbdb" class="lq lr it ls b lt mg lv mh ld mi lh mj ll mk ma mb mc md me bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/deep-double-q-learning-7fca410b193a">第三部分:深度(双)Q 学习</a></li><li id="fa8f" class="lq lr it ls b lt mg lv mh ld mi lh mj ll mk ma mb mc md me bi translated">第四部分:随机政策梯度(<strong class="ls iu">本文</strong>)</li><li id="f4c9" class="lq lr it ls b lt mg lv mh ld mi lh mj ll mk ma mb mc md me bi translated">第五部分:确定性政策梯度</li></ul><h2 id="da70" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">0.介绍</h2><p id="e9f9" class="pw-post-body-paragraph ml mm it ls b lt lu ju mn lv lw jx mo ld mp mq mr lh ms mt mu ll mv mw mx ma im bi translated">使用<a class="ae mf" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47"> <em class="my">深度 Q 学习</em> </a>和<a class="ae mf" rel="noopener" target="_blank" href="/deep-double-q-learning-7fca410b193a"> <em class="my">深度(双)Q 学习</em> </a> <em class="my"> </em>我们能够在离散的动作空间中控制 AI，其中可能的动作可能简单到向左或向右、向上或向下。尽管有这些简单的可能性，人工智能代理仍然能够完成令人惊讶的任务，例如以超人的表现玩雅达利游戏，或者在棋盘游戏中击败世界上最好的人类选手 T21。</p><p id="2b01" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">然而，强化学习的许多现实应用，如<strong class="ls iu">训练机器人</strong>或<strong class="ls iu">无人驾驶汽车</strong>需要代理从连续空间中选择最佳行动。让我们用一个例子来讨论连续作用空间这个术语。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/1c4712bae4ecb6702071d41b4080bc6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/1*UVslOEJX2EWwYLvZLrCMCg.gif"/></div></figure><p id="17fc" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">当你开车转动方向盘时，<strong class="ls iu">你可以控制方向盘转动的幅度</strong>。这就产生了一个<strong class="ls iu">连续的动作空间</strong>:例如，对于某个范围内的每一个正实数<strong class="ls iu"> <em class="my"> x </em> </strong>，“向右转动轮子<strong class="ls iu"> <em class="my"> x </em> </strong>度”。或者你踩油门到什么程度？这也是一个持续的输入。</p><blockquote class="nf ng nh"><p id="ddeb" class="ml mm my ls b lt mz ju mn lv na jx mo ni nb mq mr nj nc mt mu nk nd mw mx ma im bi translated"><strong class="ls iu">记住</strong>:连续的动作空间意味着(理论上)有无限量的可能动作。</p></blockquote><p id="d5de" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">事实上，我们在现实生活中会遇到的大多数动作都来自连续动作空间。这就是为什么理解我们如何训练一个在有无限可能性的情况下选择行动的人工智能是如此重要。</p><p id="eda9" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">这就是随机政策梯度算法显示其优势的地方。</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h2 id="2794" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">你如何实践随机政策梯度？</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/0bc02cde915b2c5b062373edf41d92f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*qajIl14c7IAvmnaDSAZ7Rg.gif"/></div></figure><p id="8334" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">OpenAI 健身房“连续登山”问题的这个例子是用随机政策梯度解决的，如下所示。有据可查的源代码可以在我的 <a class="ae mf" href="https://github.com/artem-oppermann/Deep-Reinforcement-Learning" rel="noopener ugc nofollow" target="_blank"> <em class="my"> GitHub 资源库</em> </a> <em class="my">中找到。我选择 MountainCarContinuous 作为一个例子，因为这个问题的训练时间非常短，你可以很快地自己重现它。如果你看完这篇文章想练习一下，克隆库，执行</em><strong class="ls iu">src/policy gradients/random/random _ pg . py</strong><em class="my">启动算法。</em></p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h2 id="6898" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">1.随机政策</h2><p id="8f13" class="pw-post-body-paragraph ml mm it ls b lt lu ju mn lv lw jx mo ld mp mq mr lh ms mt mu ll mv mw mx ma im bi translated">在<a class="ae mf" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f">马尔可夫决策过程</a>中，我引入了人工智能代理作为一个神经网络，它与<strong class="ls iu">环境</strong>(电脑游戏、棋盘、现实生活等)相互作用。)通过观察它的<strong class="ls iu">状态 s </strong>(屏幕像素、板配置等。)并根据当前可观测状态<strong class="ls iu"><em class="my"/></strong>采取<strong class="ls iu">动作<em class="my"> a、</em> </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/b45d17e0b44aeb5ec4b06e01f3bc3041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7GERX5blVYnRRkW-vVkd0Q.jpeg"/></div></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Fig. 1 Schematic depiction of deep reinforcement learning</figcaption></figure><p id="d640" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">对于处于<strong class="ls iu">状态<em class="my">s</em>T33】的每一个<strong class="ls iu">动作<em class="my">a</em>t</strong>，AI 代理接收一个<strong class="ls iu">奖励</strong>。<strong class="ls iu">奖励</strong>的数量告诉<strong class="ls iu">代理</strong>他的<strong class="ls iu">动作</strong>在这个特定的<strong class="ls iu">状态</strong>中关于解决给定目标的质量，例如学习如何走路或赢得一个计算机游戏。任何给定的<strong class="ls iu">状态</strong>下的<strong class="ls iu">动作</strong>由<strong class="ls iu">策略π </strong>决定。</strong></p><blockquote class="nf ng nh"><p id="11a3" class="ml mm my ls b lt mz ju mn lv na jx mo ni nb mq mr nj nc mt mu nk nd mw mx ma im bi translated">在<a class="ae mf" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f">马尔可夫决策过程</a>中，我引入了<strong class="ls iu">策略</strong>作为 AI 的策略，该策略决定了他从一个状态<strong class="ls iu"> <em class="it"> s </em> </strong>到下一个状态<strong class="ls iu"><em class="it">s’</em></strong>的移动，跨越所有可能状态<strong class="ls iu"> s_1 </strong>的整个序列，...，<strong class="ls iu">环境中的 s_n </strong>。</p></blockquote><p id="8087" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">在<a class="ae mf" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-ii-deep-q-learning-b5ac60c3f47">深度 Q 学习</a>中，代理遵循策略<strong class="ls iu"> π </strong>，该策略告知在状态<strong class="ls iu"> <em class="my"> s </em> </strong>中采取<strong class="ls iu">动作，这对应于最高动作值<strong class="ls iu"> <em class="my"> Q(s，a) </em> </strong>。行动价值函数是我们从状态<strong class="ls iu"> s </strong>开始，采取行动<strong class="ls iu"> a </strong>，然后遵循政策<strong class="ls iu"> π </strong>所获得的预期回报(所有状态的回报总和)(比较<a class="ae mf" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f">马尔可夫决策过程</a> ) <strong class="ls iu">。</strong></strong></p><p id="5007" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated"><strong class="ls iu">在随机策略的情况下，基本思想是通过参数概率分布来表示策略:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/1c4435add3a97735c27eefe61c07654f.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*4oMbUVWaUahecrONpkh-ug.png"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Eq. 1 Stochastic policy as a probability distribution.</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/9b6f40a7bf1fb8dff04b7219c2f8f6bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*XasmE2yAEFJrEJZOI4-eGA.png"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Fig. 1 Sample an action <strong class="bd oa">a</strong> from the policy, which is a normal distribution in this case.</figcaption></figure><p id="6382" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">该分布根据一个参数向量<strong class="ls iu"> <em class="my"> θ </em> </strong>随机选择状态<strong class="ls iu"> <em class="my"> s </em> </strong>中的动作<strong class="ls iu"> <em class="my"> a </em> </strong>。作为概率分布的策略的一个例子是高斯分布，其中我们随机选择一个动作<strong class="ls iu"><em class="my"/></strong>，作为这个分布的样本(图 1)。这就产生了动作<strong class="ls iu"><em class="my"/></strong>是一个连续变量。</p><blockquote class="nf ng nh"><p id="ef0d" class="ml mm my ls b lt mz ju mn lv na jx mo ni nb mq mr nj nc mt mu nk nd mw mx ma im bi translated"><strong class="ls iu"> <em class="it">记住</em> </strong>:与深度 Q 学习相反，策略<strong class="ls iu"/>现在是从状态<strong class="ls iu"><em class="it"/></strong>到动作<strong class="ls iu"><em class="it"/></strong>的直接映射/函数。</p></blockquote><h2 id="6e0e" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">一步法</h2><p id="e7b3" class="pw-post-body-paragraph ml mm it ls b lt lu ju mn lv lw jx mo ld mp mq mr lh ms mt mu ll mv mw mx ma im bi translated">但是我们如何确定当前的政策π是一个好政策呢？<strong class="ls iu"> </strong>为此我们必须为π定义一个性能函数，我们称之为<strong class="ls iu"> <em class="my"> J(θ) </em> </strong>。</p><p id="3e34" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">我们来讨论一个简单的案例，这里我们要衡量的是<strong class="ls iu"> <em class="my"> π </em> </strong>的质量/性能只针对代理的一个步骤(从状态<strong class="ls iu"> <em class="my"> s </em> </strong>到下一个状态<strong class="ls iu"><em class="my">s’</em></strong>)。在这种情况下，我们可以将质量函数定义如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/b46c45bef23b2cd238724f1e836a42b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*BbJO9mA-92zNQ85bfabulA.png"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Eq. 2</figcaption></figure><p id="e90a" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">等式中的第二行无非是期望操作符<strong class="ls iu"> <em class="my"> E </em> </strong>对期望动作的执行——值<strong class="ls iu"> <em class="my"> r(s，a) </em> </strong>对于动作<strong class="ls iu"> <em class="my"> a </em> </strong>处于状态<strong class="ls iu"><em class="my"/></strong><strong class="ls iu"><em class="my">s</em></strong>是从环境中选择的，而<strong class="ls iu"> <em class="my"> a </em> </strong>是根据策略选择的 R_a_s  是状态<strong class="ls iu"> <em class="my"> s </em> </strong>中动作<strong class="ls iu"> <em class="my"> a </em> </strong>的奖励。</p><blockquote class="nf ng nh"><p id="c8f0" class="ml mm my ls b lt mz ju mn lv na jx mo ni nb mq mr nj nc mt mu nk nd mw mx ma im bi translated"><strong class="ls iu">请注意</strong> : <strong class="ls iu"> <em class="it"> r(s，a) </em> </strong>与<strong class="ls iu"> Q(s，a) </strong>或<strong class="ls iu"> q(s，a) </strong>含义相同，但仅针对一步流程<strong class="ls iu">。</strong></p></blockquote><p id="5714" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">必须考虑到，在深度强化学习中，环境是随机的，这意味着采取行动并不能保证代理人最终会处于他想要的状态。环境在一定程度上决定了代理的最终位置。因为动作值<strong class="ls iu"><em class="my">【s，a】</em></strong>依赖于<em class="my">也</em>依赖于下一个状态<strong class="ls iu"><em class="my">【s’</em></strong>(<em class="my">参见等式。17 在</em> <a class="ae mf" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f"> <em class="my">马尔可夫决策过程中</em> </a>)我们必须平均报酬<strong class="ls iu"> <em class="my"> R_a_s </em> </strong>整体转移概率<strong class="ls iu"><em class="my">p(s):= p(s→s’)</em></strong>从状态<strong class="ls iu"> <em class="my"> s </em> </strong>到下一个状态<strong class="ls iu"><em class="my">s’。</em> </strong>再者<strong class="ls iu"><em class="my"/></strong>因为<strong class="ls iu"> <em class="my"> R_a_s </em> </strong>也取决于行动，我们必须将所有可能的<strong class="ls iu"> <em class="my"> π(a，s)上的报酬平均化。</em> </strong></p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h2 id="83bb" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">2.随机政策梯度定理</h2><p id="c368" class="pw-post-body-paragraph ml mm it ls b lt lu ju mn lv lw jx mo ld mp mq mr lh ms mt mu ll mv mw mx ma im bi translated">策略梯度算法通常通过对这种随机策略进行采样并朝着更大的累积回报的方向调整策略参数来进行。</p><p id="8f86" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">现在我们已经定义了策略<strong class="ls iu"> <em class="my"> π、</em> </strong>的性能，我们可以更进一步，讨论如何学习最优策略。由于<strong class="ls iu"> <em class="my"> π(θ) </em> </strong>取决于一些参数<strong class="ls iu"> <em class="my"> θ </em> </strong> ( <em class="my">这些参数在大多数情况下是神经网络</em>的权重和偏差)我们必须找到使性能最大化的最优<strong class="ls iu"> <em class="my"> θ </em> </strong>。</p><p id="c4c1" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">政策梯度法背后的基本思想是在<em class="my">绩效梯度的方向上调整政策的这些参数<strong class="ls iu"><em class="my">【θ】</em></strong>。</em></p><p id="4901" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">如果我们计算<strong class="ls iu"> <em class="my"> J(θ) </em> </strong>的梯度，我们得到下面的表达式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/d909314908606d6f48c26ccb608f0583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*hJOOPe5pWowIWTvfo0H_gw.png"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Eq. 3 The gradient of the performance function for the one-step process.</figcaption></figure><p id="fd28" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">因为我们想要找到使性能最大化的<strong class="ls iu"><em class="my"/></strong>，所以我们必须更新<strong class="ls iu"><em class="my"/></strong>进行<em class="my">梯度上升</em>——与梯度下降相反，我们想要找到使预定义损失函数最小化的参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/30f299255ad356cf5ca7a875ab67dac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*qNdGg04wWxkr-uc-u6tFZw.png"/></div></figure><h2 id="def2" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">多步骤过程</h2><p id="6792" class="pw-post-body-paragraph ml mm it ls b lt lu ju mn lv lw jx mo ld mp mq mr lh ms mt mu ll mv mw mx ma im bi translated">既然我们知道了如何改进一步式流程的策略，我们可以继续这个案例，在这里我们考虑 AI 智能体在整个<em class="my">状态序列中的移动过程。</em></p><p id="cf70" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">实际上，这种情况并没有那么困难，如果我们记住，在遵循一个政策<strong class="ls iu"><em class="my"/></strong>从一个状态到另一个状态的运动过程中，我们将获得的(折扣)奖励的总和，就是行动-价值函数<strong class="ls iu"> <em class="my"> Q(s，a) </em> </strong>的准确定义。这产生了多步骤过程的策略梯度的以下定义，其中单个预期回报<strong class="ls iu"><em class="my">【r(s，a)】</em></strong>与预期累积回报(回报总和)<strong class="ls iu"> <em class="my"> Q(s，a) </em> </strong>交换。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/49077d7fec273d838a4311cc1a2ac62c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*npQPiN_Ib594M2WHwYWhmQ.png"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Eq. 4 The gradient of the performance function for the multi-step process.</figcaption></figure><h2 id="88c6" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">3.演员-评论家算法</h2><p id="ced7" class="pw-post-body-paragraph ml mm it ls b lt lu ju mn lv lw jx mo ld mp mq mr lh ms mt mu ll mv mw mx ma im bi translated">根据等式使用更新规则的算法。4 被称为<em class="my">演员评论家算法</em>。策略<strong class="ls iu"> <em class="my"> π(a|s) </em> </strong>被称为<em class="my">参与者</em>，因为<em class="my">参与者</em>决定了在<strong class="ls iu"> <em class="my"> s </em> </strong>状态下必须采取的动作。与此同时，<em class="my">评论家</em> <strong class="ls iu"> <em class="my"> Q(s，a)，</em> </strong>通过赋予<em class="my">演员</em>的行为一个质量值来达到<em class="my">批评</em>的目的。</p><p id="6789" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">可以看到<strong class="ls iu"><em class="my">J【θ】</em></strong>的渐变与这个<strong class="ls iu"> <em class="my"> </em> </strong>的质量值成比例。高质量值表明在状态<strong class="ls iu"><em class="my"/></strong>中采取的动作<strong class="ls iu"> <em class="my"> a </em> </strong>实际上是一个好的选择，并且可以增加性能方向上的<strong class="ls iu"> <em class="my"> θ </em> </strong>的更新。反之适用于小质量值。</p><p id="5eac" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">现实中，我们无法预先知道<strong class="ls iu"><em class="my"/>【Q(s，a)】T3。因此我们必须用一个函数<strong class="ls iu"> <em class="my"> Q_w(s，a) </em> </strong>来近似它，这个函数依赖于参数<strong class="ls iu"> <em class="my"> w </em> </strong>。一般来说<strong class="ls iu"> <em class="my"> Q_w(s，a) </em> </strong>可以通过神经网络来估计。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/e40d227b3971a6fb2ef19932fb0f35ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*PAsMnhvkaGJDcXn0pu0pbQ.png"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Eq. 5 Estimation of the action-value function.</figcaption></figure><p id="629f" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">这产生了性能梯度的新定义:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/e070b8a447f2007c892f446dba053dcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*HUbGN-1TJl3gFwumOeALuQ.png"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">E6. A new definition of the performance gradient.</figcaption></figure><p id="b24d" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">总之，随机策略梯度算法试图完成以下两件事:</p><ul class=""><li id="d424" class="lq lr it ls b lt mz lv na ld oh lh oi ll oj ma mb mc md me bi translated">更新<em class="my">演员</em> <strong class="ls iu"> <em class="my"> π </em> </strong>的参数<strong class="ls iu"> <em class="my"> θ </em> </strong>朝向表演的渐变<strong class="ls iu"> <em class="my"> J(θ) </em> </strong></li><li id="33e2" class="lq lr it ls b lt mg lv mh ld mi lh mj ll mk ma mb mc md me bi translated">用常规的<em class="my">时间差异学习</em>算法更新<em class="my">评论家</em>的参数<strong class="ls iu"> w </strong>，这是我在深度 Q 学习中介绍的。</li></ul><p id="7306" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">整个 Actor-Critic 算法可以用下面的伪代码来表示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/148f7fc7e0c3384037786e7e643ebff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*dMF9FMs4s7GSZE5M6XjBwQ.png"/></div></figure><p id="8f93" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">算法的关键部分发生在代表人工智能代理生命周期的循环中。让我们更深入地讨论每一步:</p><ul class=""><li id="b7bb" class="lq lr it ls b lt mz lv na ld oh lh oi ll oj ma mb mc md me bi translated">通过在状态<strong class="ls iu"><em class="my"/></strong>中采取动作<strong class="ls iu"><em class="my"/></strong>从环境中获得奖励<strong class="ls iu"> <em class="my"> R </em> </strong>，并观察新状态<strong class="ls iu"><em class="my">【s’</em></strong></li><li id="ea76" class="lq lr it ls b lt mg lv mh ld mi lh mj ll mk ma mb mc md me bi translated">给定这个新状态<strong class="ls iu"><em class="my">s’</em></strong>样本一个动作<strong class="ls iu"> <em class="my">一个’</em></strong>随机地从分布<strong class="ls iu"> <em class="my"> π </em> </strong></li><li id="fd7b" class="lq lr it ls b lt mg lv mh ld mi lh mj ll mk ma mb mc md me bi translated">使用旧的<strong class="ls iu"><em class="my"/></strong>，<strong class="ls iu"> <em class="my"> a </em> </strong>和新的<strong class="ls iu"><em class="my">s’</em></strong>，<strong class="ls iu"><em class="my">a’</em></strong>计算时间差异目标δ</li><li id="63cf" class="lq lr it ls b lt mg lv mh ld mi lh mj ll mk ma mb mc md me bi translated">根据时间差异学习更新规则更新参数<strong class="ls iu"> <em class="my"> w </em> </strong>。(<strong class="ls iu">更好的替代</strong>:用常规的<em class="my">梯度下降</em>算法最小化<strong class="ls iu"><em class="my">r+γQ(s’，a’)</em></strong>和<em class="my"><em class="my">【s，a】</em>之间的距离来更新<strong class="ls iu"/>w</em>。)</li><li id="3a24" class="lq lr it ls b lt mg lv mh ld mi lh mj ll mk ma mb mc md me bi translated">通过向性能梯度<strong class="ls iu"> <em class="my"> J(θ) </em> </strong>执行<em class="my">梯度上升</em>步骤，更新策略的参数<strong class="ls iu"> <em class="my"> θ </em> </strong></li><li id="49fa" class="lq lr it ls b lt mg lv mh ld mi lh mj ll mk ma mb mc md me bi translated">将新状态<strong class="ls iu"> <em class="my"> s' </em> </strong>设为旧状态<strong class="ls iu"> <em class="my"> s </em> </strong>，并将新动作<strong class="ls iu"> <em class="my"> a' </em> </strong>设为旧动作<strong class="ls iu"> <em class="my"> a </em> </strong></li><li id="23d7" class="lq lr it ls b lt mg lv mh ld mi lh mj ll mk ma mb mc md me bi translated">如果进程没有终止，从头开始循环</li></ul><h2 id="099c" class="ku kv it bd kw kx ky dn kz la lb dp lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">4.减少方差</h2><p id="0e43" class="pw-post-body-paragraph ml mm it ls b lt lu ju mn lv lw jx mo ld mp mq mr lh ms mt mu ll mv mw mx ma im bi translated">众所周知，Actor-Critic-Algorithm 的普通实现具有很高的方差。减小这种方差的一种可能性是从动作值<strong class="ls iu"><em class="my">【Q(s，a)】</em></strong>中减去状态值<strong class="ls iu"> <em class="my"> V(s) </em> </strong>(等式 1)。7) <strong class="ls iu"> <em class="my">。</em> </strong>状态值在<a class="ae mf" rel="noopener" target="_blank" href="/self-learning-ai-agents-part-i-markov-decision-processes-baf6b8fc4c5f">马尔可夫决策过程</a>中被定义为<em class="my">预期的</em>总奖励，如果它在状态<strong class="ls iu"> <em class="my">、</em> </strong>中开始它的进程而不考虑动作，AI 代理将会收到该奖励。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/65b65f960d283f0e35414c400a6b33eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*HldH1zDOtTJ5PhGEF3M6XA.png"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Eq. 7 Definition of the advantage.</figcaption></figure><p id="98cb" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">这个新术语被定义为<strong class="ls iu"> <em class="my">优势</em> </strong>并且可以被插入到性能的梯度中。利用这一优势在减少算法的方差方面显示了有希望的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/89b567673ac20437c0d3a245d3939b04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*fRHOci3juUmlAjpP0y_24g.png"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Eq. 8 Advantages is inserted into the policy gradient.</figcaption></figure><p id="b35d" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">在具有优势的另一方面，我们引入了要求具有第三函数逼近器(如神经网络)来估计<strong class="ls iu"><em class="my"/></strong>的问题。然而，可以看出<strong class="ls iu">预期的</strong>时间差误差为<strong class="ls iu"><em class="my">【V(s)】</em></strong>(等式。9) <strong class="ls iu"> <em class="my"> </em> </strong>无非是优点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/fae8027db747f556c030148dde3b932d.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*zFJkRWGK36Nhg1zouS8_YQ.png"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Eq. 9 Temporal difference error of V(s).</figcaption></figure><p id="c32e" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">这可以用<strong class="ls iu"> <em class="my"> Q(s，a) </em> </strong>的一个定义来表示，就是<strong class="ls iu"><em class="my">r</em></strong>+<strong class="ls iu"><em class="my">γV(s’)</em></strong>的<em class="my">期望</em>值。通过减去剩余的项<strong class="ls iu"> <em class="my"> V(s) </em> </strong>我们获得优势<strong class="ls iu"> <em class="my"> A </em> </strong>(等式 1)的先前定义。10) <strong class="ls iu"> <em class="my">。</em>T79】</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/155a46693794effb5a3ff42643396302.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*Cq023J4JFArqpUCltnBrfA.png"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Eq. 10 Advantage equals the expected TD-Error of V(s).</figcaption></figure><p id="63ce" class="pw-post-body-paragraph ml mm it ls b lt mz ju mn lv na jx mo ld nb mq mr lh nc mt mu ll nd mw mx ma im bi translated">最后，我们可以将<strong class="ls iu"><em class="my">【V(s)</em></strong>的时差误差插入到<strong class="ls iu"><em class="my">【J(s)</em></strong>的梯度中。这样做我们一举两得:</p><ul class=""><li id="7a34" class="lq lr it ls b lt mz lv na ld oh lh oi ll oj ma mb mc md me bi translated">我们减少了整个算法的方差</li><li id="0ac1" class="lq lr it ls b lt mg lv mh ld mi lh mj ll mk ma mb mc md me bi translated">同时，我们去掉了三次函数近似</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/a76ab3098eaffce6f4af733223a13838.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*8srWSlXag6woz_oxLt6j6A.png"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Eq. 11</figcaption></figure></div></div>    
</body>
</html>