<html>
<head>
<title>Tuning Hyperparameters (part I): SuccessiveHalving</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">调整超参数(第一部分):成功减半</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tuning-hyperparameters-part-i-successivehalving-c6c602865619?source=collection_archive---------7-----------------------#2018-05-08">https://towardsdatascience.com/tuning-hyperparameters-part-i-successivehalving-c6c602865619?source=collection_archive---------7-----------------------#2018-05-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/24b8b8b6ca08da16b532e4f293bc37cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C8sCpGE0iOhFqFilnrt-Lw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/photos/RqzW7zFB6y0?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">rawpixel</a> on <a class="ae kc" href="https://unsplash.com/search/photos/half?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="6d77" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个系列中，我想讨论一些超参数优化技术，它们的优点/缺点等。我目前不打算遵循任何特定的时间顺序。每一部分都应该单独阅读。</p><h1 id="db87" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">介绍</h1><p id="8ed0" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">为什么我们应该关心优化超参数？</p><p id="4af4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们在机器学习中建立各种模型管道时，我们经常会发现自己有十个、二十个甚至更多的超参数需要我们做出决策。这个决策空间随着每个额外的超参数呈指数增长，使得仔细的分析不切实际，如果不是不可能的话。</p><p id="2795" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了加速项目的开发过程，我们可能希望将这部分工作自动化。</p><p id="9482" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是怎么做呢？</p><p id="744b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简单的网格搜索可能是我们的第一选择，但是正如我们所讨论的，由于维数灾难，这是效率最低(时间)的选择。</p><p id="2716" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下一个最佳解决方案是随机采样超参数空间。这个解决方案是由<a class="ae kc" href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank"> Bengio 等人</a>提出的，并且已经被证明优于网格搜索。在这种情况下，通过超越，我的意思是<strong class="kf ir">随机搜索</strong>能够在更短的时间内使用更少的资源找到模型的更可靠和得分最高的超参数配置。为了理解这一点，让我们看一下图 1。摘自<a class="ae kc" href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank">原文</a>。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi me"><img src="../Images/ba817d82f92d8ccee27dfd60ac8b58a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/0*A30AvsqTZvVKVS-H.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1.: Grid Search vs Random Search</figcaption></figure><p id="a5d3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们所见，这是经常发生的情况，一些超参数比其他更具决定性。在<strong class="kf ir">网格搜索的情况下，</strong>尽管采样了 9 次试验，但实际上我们只试验了一个重要参数的 3 个不同值。在<strong class="kf ir">随机搜索</strong>的情况下，9 次试验将仅测试 9 个不同的决定性参数。</p><p id="ea90" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们能做的比<strong class="kf ir">随机搜索</strong>更好/更多吗？当然可以！但是<em class="mj">“如何”</em>可能取决于我们所说的<em class="mj">“更好</em>”的确切含义。</p><p id="643b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">存在两种类型的算法，它们是对<strong class="kf ir">随机搜索</strong>的自然扩展。第一个是贝叶斯优化算法，如<strong class="kf ir"> Tree-Parzen Estimators </strong>，它有助于找到统计上更稳健(可靠)和得分更高的配置。</p><p id="c9b5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二种类型，例如<strong class="kf ir">成功减半</strong>和<strong class="kf ir">超带</strong>，在资源分配上进行优化。在下一节中，我将讨论<a class="ae kc" href="https://arxiv.org/abs/1502.07943" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir">成功减半</strong> </a> <strong class="kf ir"> </strong>如何通过比<strong class="kf ir">随机搜索</strong>更有效地划分和选择随机生成的超参数配置来改进<strong class="kf ir">随机搜索</strong>，而无需对配置空间的性质做更多假设。更有效的资源分配，意味着给定相同的配置集，<strong class="kf ir">成功减半</strong>将比<strong class="kf ir">随机搜索</strong>更快地找到最优配置。</p><h1 id="d624" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">成功减半</h1><p id="6fc7" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">那么，我们如何更有效地分配我们的资源呢？让我们来看看摘自本文的图 2。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mk"><img src="../Images/c299edf168bc48ca896042b392178137.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AxsE-gz85ksQa5SZ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2: The validation loss as a function of total resources allocated for two conﬁgurations.</figcaption></figure><p id="ddd4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通常在实践中，相对于最差的配置，有希望的配置往往得分更高，甚至在过程的早期。</p><p id="4600" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们走吧！这就是成功减半的意义所在！下面是该算法如何利用这一假设:</p><ol class=""><li id="15d4" class="ml mm iq kf b kg kh kk kl ko mn ks mo kw mp la mq mr ms mt bi translated">随机抽样一组超参数配置</li><li id="2d9e" class="ml mm iq kf b kg mu kk mv ko mw ks mx kw my la mq mr ms mt bi translated">评估所有当前剩余配置的性能</li><li id="097f" class="ml mm iq kf b kg mu kk mv ko mw ks mx kw my la mq mr ms mt bi translated">抛出，最差得分配置的下半部分</li><li id="cfcf" class="ml mm iq kf b kg mu kk mv ko mw ks mx kw my la mq mr ms mt bi translated">回到 2。并重复直到剩下一个配置。</li></ol><p id="a502" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与其浪费大量的训练时间在毫无结果的配置上，不如尽快成功地将投掷减半。因此，可以将更多的训练时间(即资源)分配给更有潜在价值的模型。</p><h1 id="d8c1" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">实际上…</h1><p id="5396" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">为了成功应用减半，我们需要找到机器学习库，它允许我们<strong class="kf ir">暂时停止模型的训练</strong>，最终<strong class="kf ir">保存模型</strong>，然后稍后<strong class="kf ir">重新加载模型</strong>并且<strong class="kf ir">继续训练</strong>。</p><p id="07b0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是，后一点是最有问题的一点。作为一名从业者，不幸的是，您没有时间/预算从头开发自己的库，并且经常依赖于开源云中的可用资源。不幸的是，实践中使用的一些算法缺少这个选项，但幸运的是，我们有很多强大和/或流行的算法！所有 Scikit-Learn 的“热启动”超参数实施都是如此，例如(Extreme)RandomForest、GradientBoosting 等</p><p id="9cd8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于极端的梯度增强库，我只能找到 xgboost 的方法，而 LGBM 在提取增强器时产生了一个 bug。不幸的是，Catboost 没有这个选项，很可能是由于算法本身的性质，它试图通过移动数据来取消每次 boost 迭代的偏向。</p><p id="e870" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于深度学习，Pytorch，Keras 和 TensorFlow 仍然允许最大的灵活性，所以没有问题！</p><p id="85a3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们一步一步来，看看如何在 SuccessiveHalving 的实现中包含所有这些库。</p><p id="38de" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们需要这些模型的包装器，这迫使我们实现保存/加载模型的定义，更重要的是<strong class="kf ir">在重载</strong>后进一步训练模型。</p><p id="7c1b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面的基类对此进行了描述:</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="bd8b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是一个没有提前停止的 XGBOOST 示例:</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="e927" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，我们必须提取助推器并使用功能 API 来进一步训练。出于某种原因，这个类似的技巧会为 Lightgbm 产生一个错误。</p><p id="517e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">借助热启动超参数，Scikit-learn 变得更加简单。</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="833f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果这个超参数不存在，恐怕您将不得不完全改装模型，从而损失重新分配资源所获得的时间。</p><p id="d0a8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们有了 wapper，我们可以继续实施成功减半了！</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="90b7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">变量“eta”是我们增加资源的速率，直到我们达到我们希望使用的资源的最大值。</p><p id="3ab6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，假设我们最终的 xgboost 模型在 200 个配置的样本中最多应该有 1000 棵树。我们将资源单位设置为 10，这样最初的一批配置将只训练 10 次提升。在每一步，我们用“eta ”= 1.93 倍的估计量来训练新的一批。</p><p id="faeb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更多的例子和实现是可用的<a class="ae kc" href="https://github.com/benoitdescamps/Hyperparameters-tuning" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="a170" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">没有灵丹妙药…</h1><p id="c9e9" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">应用成功减半时的一个大问题是在总资源和配置总数之间找到合适的平衡。</p><p id="66c0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">较小的学习率通常与更多的资源、更多的树(在(极端)梯度提升的情况下)或更多的神经网络时期密切相关。</p><p id="25f4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们如何能调整这样一个选择？<strong class="kf ir"> HyperBand </strong>提出了一个解决方案，但这是下次的事了！</p><h1 id="43e0" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">我写了更多精彩的东西！</h1><p id="534f" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">@ <a class="ae kc" href="https://medium.com/bigdatarepublic/building-a-decision-tree-in-tensorflow-742438cb483e" rel="noopener">在 TensorFlow 中构建决策树</a></p><p id="4ca6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">@<a class="ae kc" href="https://www.kdnuggets.com/2018/01/custom-optimizer-tensorflow.html" rel="noopener ugc nofollow" target="_blank">tensor flow 中的自定义优化器</a></p><p id="e9b5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">@<a class="ae kc" href="https://medium.com/bigdatarepublic/regression-prediction-intervals-with-xgboost-428e0a018b" rel="noopener">XGBOOST 回归预测区间</a></p><h1 id="1d02" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">参考资料:</h1><ol class=""><li id="c427" class="ml mm iq kf b kg lz kk ma ko nb ks nc kw nd la mq mr ms mt bi translated">J.Bergstra 和 Y. Bengio，超参数优化的随机搜索，2011 年</li><li id="c3a3" class="ml mm iq kf b kg mu kk mv ko mw ks mx kw my la mq mr ms mt bi translated">K.Jamieson，A. Talwalka，非随机最佳臂识别和超参数优化，2015 年</li><li id="5176" class="ml mm iq kf b kg mu kk mv ko mw ks mx kw my la mq mr ms mt bi translated">长度李、贾米森、德萨沃、罗斯塔米扎德、塔瓦尔卡尔。超带:基于 Bandit 的超参数优化新方法。arXiv 预印本 arXiv:1711.09784，2017 年</li><li id="ac73" class="ml mm iq kf b kg mu kk mv ko mw ks mx kw my la mq mr ms mt bi translated">Scikit-Learn，url，<a class="ae kc" href="http://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank">http://scikit-learn.org/stable/</a></li><li id="b148" class="ml mm iq kf b kg mu kk mv ko mw ks mx kw my la mq mr ms mt bi translated">J.Bergstra，R. Bardenet，Y. Bengio，B. Kégl，超参数优化算法</li><li id="bd14" class="ml mm iq kf b kg mu kk mv ko mw ks mx kw my la mq mr ms mt bi translated">github:【https://github.com/benoitdescamps/Hyperparameters-tuning T2】</li></ol></div></div>    
</body>
</html>