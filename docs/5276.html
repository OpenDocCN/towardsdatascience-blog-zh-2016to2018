<html>
<head>
<title>Review: DeconvNet — Unpooling Layer (Semantic Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:de convnet-un pooling 层(语义分段)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e?source=collection_archive---------7-----------------------#2018-10-08">https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e?source=collection_archive---------7-----------------------#2018-10-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="7063" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">在</span>这个故事中，<strong class="jp ir">反褶积网络</strong>被简单回顾，反褶积网络(DeconvNet)是由反褶积层和反褶积层组成的<strong class="jp ir">。</strong></p><p id="c72c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于传统的 FCN，输出是通过高比率(32 倍、16 倍和 8 倍)上采样获得的，这可能导致粗糙的分割输出(标签图)。在这个去卷积网络中，通过逐步去卷积和去卷积来获得输出标签图。而且是我写这个故事的时候在<strong class="jp ir"> 2015 ICCV </strong>发表的一篇论文【1000 多篇引用。(<a class="ku kv ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----55cf8a6e380e--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="96fa" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">涵盖哪些内容</h1><ol class=""><li id="c9cd" class="mb mc iq jp b jq md ju me jy mf kc mg kg mh kk mi mj mk ml bi translated"><strong class="jp ir">解卷积和去卷积</strong></li><li id="0ad2" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><strong class="jp ir">实例式分割</strong></li><li id="6516" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><strong class="jp ir">两阶段训练</strong></li><li id="4a83" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated"><strong class="jp ir">结果</strong></li></ol></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="e8c7" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">1.去极化和去卷积</h1><p id="1776" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">以下是 DeconvNet 的整体架构:</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mu"><img src="../Images/886ac4b439751a1bd76295385a785c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LW8Anre45o9nfamxIVTY8Q.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk"><strong class="bd nk">DeconvNet Architecture</strong></figcaption></figure><p id="922e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们所见，它使用 VGGNet 作为主干。第一部分是一个卷积网络，通常像 FCN，conv 和池层。第二部分是反卷积网络，这是本文的一个新部分。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nl"><img src="../Images/075b49e5792635288b167ff3caa3dc72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*8RHLqbd2UJshkoDfZMdOmg.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk"><strong class="bd nk">Remember positions when Pooling (Left), Reuse the position information during Unpooling (right)</strong></figcaption></figure><p id="6fc1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要执行取消池化，我们需要在进行最大池化时记住每个最大激活值的位置，如上所示。然后，如上所示，记忆的位置被用于取消绕线。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/8b4603f6961f19a4a6211cab0371425a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*AbCrAqPBfkqGRdhKtiZQqA.png"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk"><strong class="bd nk">Convolution is to conv the input to smaller size (Left) Deconvolution is to conv the input back to larger size (Right)</strong></figcaption></figure><p id="53d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">反卷积只是将 conv 输入恢复到更大的尺寸。(如有兴趣，请阅读<a class="ae nn" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">我的 FCN 评论</a>了解详情。)</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi no"><img src="../Images/acaee6b00c02dab6e43ae394fca9fa0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3fQQW6dWEJp1bPTKXE_-PA.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk"><strong class="bd nk">An example of Deconvolution and Unpooling</strong></figcaption></figure><p id="4327" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上图就是一个例子。(b)是 14×14 解卷积层的输出。(c)是取消轮询后的输出，依此类推。我们可以在(j)中看到，自行车可以在最后的 224×224 解卷积层中重建，这表明<strong class="jp ir">学习过的滤波器可以捕捉特定于类的形状信息。</strong></p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi np"><img src="../Images/bcf2106f0ad03d4efce6408f9b22602b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*55EU9jHtEpp9JQ6wUYEUHA.png"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk"><strong class="bd nk">Input Image (Left), FCN-8s (Middle), DeconvNet (Right)</strong></figcaption></figure><p id="6395" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上所示的其他示例表明，DeconvNet 可以重建比 FCN-8s 更好的形状。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="fc7a" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated"><strong class="ak"> 2。实例式分割</strong></h1><figure class="mv mw mx my gt mz gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/62525717169f49e9fdf30c941829a8be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*ib2xQ7WVzBeOYLCe2Ql1FA.png"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk"><strong class="bd nk">Bad Examples of Semantic Segmentation Without Using Region Proposals</strong></figcaption></figure><p id="825c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上所示，比感受野大得多或小得多的对象可能被分割或错误标记。小物体经常被忽略，被归类为背景</p><p id="0fe3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">语义分割被提出为基于实例的分割问题。首先，2000 个区域提议(边界框)中的前 50 个由对象检测方法 EdgeBox 检测。然后，<strong class="jp ir"> DeconvNet 应用于每个提议</strong>，并将所有提议的输出聚合回原始图像。通过使用建议，<strong class="jp ir">可以有效处理各种规模。</strong></p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="2d5b" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated"><strong class="ak"> 3。两阶段训练</strong></h1><h2 id="3fea" class="nr le iq bd lf ns nt dn lj nu nv dp ln jy nw nx lr kc ny nz lv kg oa ob lz oc bi translated">第一阶段培训</h2><p id="3db1" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">使用地面实况注释裁剪对象实例，以便对象在裁剪的边界框处居中，然后执行训练。这有助于减少对象位置和大小的变化。</p><h2 id="aabb" class="nr le iq bd lf ns nt dn lj nu nv dp ln jy nw nx lr kc ny nz lv kg oa ob lz oc bi translated">第二阶段培训</h2><p id="a488" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">使用了更具挑战性的例子。这些示例是由与基本事实分段重叠的建议生成/裁剪的。</p><h2 id="0963" class="nr le iq bd lf ns nt dn lj nu nv dp ln jy nw nx lr kc ny nz lv kg oa ob lz oc bi translated">一些其他细节</h2><ul class=""><li id="7aa4" class="mb mc iq jp b jq md ju me jy mf kc mg kg mh kk od mj mk ml bi translated">使用批量标准化。</li><li id="adfb" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk od mj mk ml bi translated">conv 部分使用 VGGNet 中的权重进行初始化。</li><li id="2c1a" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk od mj mk ml bi translated">deconv 部分用零均值和高斯函数初始化。</li><li id="dccb" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk od mj mk ml bi translated">每批 64 个样品。</li></ul></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="9fc1" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">4.结果</h1><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oe"><img src="../Images/c71d9fe6d6ca88bf27f6b64c43180790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rY6gTj2QIv4l0taYMt86Hg.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk"><strong class="bd nk">mean IoU results</strong></figcaption></figure><ul class=""><li id="1008" class="mb mc iq jp b jq jr ju jv jy of kc og kg oh kk od mj mk ml bi translated">FCN-8s 的平均欠条只有 64.4%。</li><li id="1f6b" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk od mj mk ml bi translated"><strong class="jp ir">下降率</strong> : 69.6%</li><li id="f2e5" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk od mj mk ml bi translated"><strong class="jp ir"> DeconvNet+CRF </strong> : 70.5%(其中 CRF 只是一个后处理步骤)</li><li id="4550" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk od mj mk ml bi translated">EDeconvNet:71.5%(EDeconvNet 表示用 FCN-8 组合的结果)</li><li id="d577" class="mb mc iq jp b jq mm ju mn jy mo kc mp kg mq kk od mj mk ml bi translated">EDeconvNet+CRF  : 72.5%，平均 IoU 最高。</li></ul><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oi"><img src="../Images/f02babe858d41016de8dabeceac6b7d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k6k_pyAWWgl7gt87mnOUbw.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk"><strong class="bd nk">Benefits of Instance-wise segmentation</strong></figcaption></figure><p id="8c98" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从上图可以看出，基于实例的分段有助于逐个实例地进行分段，而不是一次对所有实例进行分段。</p><p id="28f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">应当注意，DeconvNet 的增益不仅仅来自于逐渐的 deconv 和 unpooling，还可能来自于实例式分割和两阶段训练。</p><figure class="mv mw mx my gt mz gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oj"><img src="../Images/ad003d041d351df74cf6eb436172d6ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zBek2ecpDEaZXDFJjYgclA.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk"><strong class="bd nk">Some Visualization Results</strong></figcaption></figure><p id="0f5f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">EConvNet+CRF 通常有好的结果，即使比 FCN 差。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="de2f" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">参考</h1><ol class=""><li id="838f" class="mb mc iq jp b jq md ju me jy mf kc mg kg mh kk mi mj mk ml bi translated">【2015 ICCV】【de convnet】<br/><a class="ae nn" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.pdf" rel="noopener ugc nofollow" target="_blank">用于语义切分的学习反卷积网络</a></li></ol><h1 id="e9fd" class="ld le iq bd lf lg ok li lj lk ol lm ln lo om lq lr ls on lu lv lw oo ly lz ma bi translated">我的评论</h1><p id="43fe" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">[<a class="ae nn" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a>][<a class="ae nn" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener">VGGNet</a>]</p></div></div>    
</body>
</html>