<html>
<head>
<title>Learn Spark for Big Data Analytics in 15 mins！</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 15 分钟内了解大数据分析的火花！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learn-spark-essentials-in-15-mins-cf1495882ae0?source=collection_archive---------8-----------------------#2017-08-04">https://towardsdatascience.com/learn-spark-essentials-in-15-mins-cf1495882ae0?source=collection_archive---------8-----------------------#2017-08-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/dfe1d639038e2ab6eb1836b120d59961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ybNESo-vFwrY2QuGK8targ.png"/></div></div></figure><div class=""/><p id="fd6d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我向你保证，这个简短的教程将为你节省大量阅读长篇文档的时间。准备好搭乘大数据列车了吗？我们开始吧！</p><h2 id="a58b" class="kx ky jb bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">基本概念</h2><p id="0f20" class="pw-post-body-paragraph jy jz jb ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated"><strong class="ka jc"> RDD </strong> : <em class="lv">火花的基本抽象。它将数据对象分布到多台机器上，并在内存中处理，这比在像 R 或 Python 这样的单台机器上处理要快。但是它没有组织在命名的列中。</em></p><p id="628c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc"> Dataframe </strong> <em class="lv">:构建在 rdd 之上，可以拥有一个包含列名和数据类型的模式。它比 rdd 更快，因为它为 Spark 提供了更多的数据信息。</em></p><p id="2832" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">惰性评估</strong>:当你使用转换时，Spark 实际上并不执行数据，直到你调用一个动作。</p><p id="e81f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">转换</strong>:选择、过滤/where、排序/排序依据、连接、分组依据、聚集</p><p id="d549" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">动作</strong>:显示、获取、计数、收集、描述、缓存、打印模式</p><h2 id="c16b" class="kx ky jb bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">基本语法</h2><p id="9e80" class="pw-post-body-paragraph jy jz jb ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">创建一个 Rdd</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="6698" class="kx ky jb mb b gy mf mg l mh mi"># 1. Read external data in Spark as Rdd<br/>rdd = sc.textFile("path")</span><span id="c96b" class="kx ky jb mb b gy mj mg l mh mi"># 2. Create rdd from a list<br/>rdd = sc.parallelize(["id","name","3","5"])</span><span id="b956" class="kx ky jb mb b gy mj mg l mh mi"># 3. Dataframe to rdd<br/>rdd = df.rdd</span></pre><p id="f8d2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">创建数据框架</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="fef8" class="kx ky jb mb b gy mf mg l mh mi"># 1. Read in Spark as Dataframe directly <br/>#  header and schema are optional</span><span id="026a" class="kx ky jb mb b gy mj mg l mh mi">df = sqlContext.read.csv("path", header = True/False, schema=df_schema)</span><span id="ee89" class="kx ky jb mb b gy mj mg l mh mi"># 2.1 rdd to dataframe with column names<br/>df = spark.createDataFrame(rdd,["name","age"])</span><span id="5306" class="kx ky jb mb b gy mj mg l mh mi"># 2.2 rdd to dataframe with schema</span><span id="e716" class="kx ky jb mb b gy mj mg l mh mi">from pyspark.sql.types import *</span><span id="af26" class="kx ky jb mb b gy mj mg l mh mi">df_schema = StructType([<br/><strong class="mb jc">... </strong>   StructField("name", StringType(), True),<br/><strong class="mb jc">... </strong>   StructField("age", IntegerType(), True)])</span><span id="382f" class="kx ky jb mb b gy mj mg l mh mi">df = spark.createDataFrame(rdd,df_schema)</span></pre><p id="e0cf" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">转换:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="fbb6" class="kx ky jb mb b gy mf mg l mh mi"># Basic transformations:</span><span id="293c" class="kx ky jb mb b gy mj mg l mh mi"># 1. select. Index column by df.column_name or use "column_name"<br/>df.select(df.name)<br/>df.select("name")</span><span id="51f0" class="kx ky jb mb b gy mj mg l mh mi"># 2. filter/where they are the same<br/> df.filter(df.age&gt;20)<br/> df.filter("age&gt;20")<br/> df.where("age&gt;20")<br/> df.where(df.age&gt;20)</span><span id="5342" class="kx ky jb mb b gy mj mg l mh mi"># 3. sort/orderBy <br/> df.sort("age",ascending=False)<br/> df.sort(df.age.desct())</span><span id="f7d4" class="kx ky jb mb b gy mj mg l mh mi"># 4. groupBy and agg<br/>df.groupBy("gender").agg(count("name"),avg("age"))</span><span id="e63c" class="kx ky jb mb b gy mj mg l mh mi"># 5. join<br/>df1.join(df.2, <!-- -->(df1.x1 == df2.x1) &amp; (df1.x2 == df2.x2)<!-- -->,'left')</span><span id="b32b" class="kx ky jb mb b gy mj mg l mh mi"># 6. create a new column from existing column<br/>df.withColumn("first_name",split(name,'_')[0])</span></pre><p id="3d06" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在 Spark 中编写 SQL</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="b4c7" class="kx ky jb mb b gy mf mg l mh mi">1. Run sqlContext=sqlContext(sc) before create a dataframe<br/>2. Create a dataframe called df<br/>3. Run df.createOrReplaceTempView("table1") to create a temp table<br/>4. talbe2=sqlContext("select id, name from table1")</span><span id="c6d5" class="kx ky jb mb b gy mj mg l mh mi">If you are writing multiple lines, use """ like this:<br/>talbe2=sqlContext.sql("""<br/>select id, name <br/>from table1<br/>where age&gt;20<br/>""")</span></pre><p id="99da" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">动作:</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="8c10" class="kx ky jb mb b gy mf mg l mh mi">1. Show: show first n rows of a dataframe (but not rdd) without cells being truncated<br/>df.show(5, truncate=False)</span><span id="2cc9" class="kx ky jb mb b gy mj mg l mh mi">2. Take: display a list of first few rows of dataframe or rdd<br/>df.take(5)</span><span id="16d1" class="kx ky jb mb b gy mj mg l mh mi">3. collect: collect all the data of a dataframe or rdd<br/>df.collect()</span><span id="851b" class="kx ky jb mb b gy mj mg l mh mi">4. count: count number of rows<br/>df.count()</span><span id="58dd" class="kx ky jb mb b gy mj mg l mh mi">6. printSchema: show column names, data types and whether they are nullable.<br/>df.printSchema()</span><span id="db17" class="kx ky jb mb b gy mj mg l mh mi">7. cache : cache the data in memory if it's going to be reused a lot. Use unpersist() to uncache data and free memory.<br/>df.cache()<br/>df.unpersist()</span><span id="6819" class="kx ky jb mb b gy mj mg l mh mi"># Transformations and actions can be connected and executed in sequence from left to right. <br/>df1.filter("age&gt;10").join(df2,df1.x==df2.y).sort("age").show()</span></pre><h2 id="42d9" class="kx ky jb bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">外卖:</h2><ol class=""><li id="71e8" class="mk ml jb ka b kb lq kf lr kj mm kn mn kr mo kv mp mq mr ms bi translated">如果可能的话，使用并寻找在数据帧上使用的函数，而不是 rdd，因为在数据帧上速度更快</li><li id="9f8f" class="mk ml jb ka b kb mt kf mu kj mv kn mw kr mx kv mp mq mr ms bi translated">使用 collect()时要小心，除非你需要下载整个数据；使用显示/拍摄</li><li id="41b6" class="mk ml jb ka b kb mt kf mu kj mv kn mw kr mx kv mp mq mr ms bi translated">如果您以后会经常用到这些数据，请将其缓存起来。</li></ol><figure class="lw lx ly lz gt is"><div class="bz fp l di"><div class="my mz l"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">No spam, I’ll be mindful of your time and attention</figcaption></figure><p id="4ad3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">给我几个掌声，如果你觉得有帮助，就跟我来吧！</p><p id="fae4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您可能对如何通过使用数据科学节省租金感兴趣:</p><div class="ip iq gp gr ir ne"><a href="https://zhenliu.org/2017/11/29/how-to-analyze-seasonality-and-trends-to-save-money-on-your-apartment-lease/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd jc gy z fp nj fr fs nk fu fw ja bi translated">如何分析季节性和趋势，以节省你的公寓租金。</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">当我在寻找一个新的公寓出租时，我开始想知道:是否有一个数据驱动的决策策略…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">zhenliu.org</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ix ne"/></div></div></a></div></div></div>    
</body>
</html>