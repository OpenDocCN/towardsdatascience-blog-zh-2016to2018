<html>
<head>
<title>Tutorial: Linear Regression with Stochastic Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">教程:带随机梯度下降的线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/step-by-step-tutorial-on-linear-regression-with-stochastic-gradient-descent-1d35b088a843?source=collection_archive---------0-----------------------#2018-12-11">https://towardsdatascience.com/step-by-step-tutorial-on-linear-regression-with-stochastic-gradient-descent-1d35b088a843?source=collection_archive---------0-----------------------#2018-12-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/bb7a372ca9798735a8a2efb1d1f360d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yoTrzD_oxO5rs4gQB7A9BQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/photos/7_kRuX1hSXM?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Lindsay Henwood</a> on <a class="ae jd" href="https://unsplash.com/search/photos/stairs?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="06a0" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">在 JavaScript 中实现反向传播</h2></div><p id="198b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">你可以在这里</em>  <em class="lr">找到反向传播演示</em> <a class="ae jd" href="https://remykarem.github.io/backpropagation-demo/" rel="noopener ugc nofollow" target="_blank"> <em class="lr">。</em></a></p><p id="0be9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇文章应该为我们深入研究深度学习提供了一个良好的开端。让我带你一步一步地计算使用随机梯度下降的线性回归任务。</p><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="lw lx l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">A short YouTube clip for the backpropagation demo found <a class="ae jd" href="http://raiboso.me/backpropagation-demo/" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure><h2 id="60d5" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">内容</h2><ol class=""><li id="740f" class="mr ms jg kx b ky mt lb mu le mv li mw lm mx lq my mz na nb bi translated"><a class="ae jd" href="#db94" rel="noopener ugc nofollow"> <strong class="kx jh">准备</strong></a><strong class="kx jh"><br/></strong>1.1<a class="ae jd" href="#1c28" rel="noopener ugc nofollow">数据</a> <br/> 1.2 <a class="ae jd" href="#15e3" rel="noopener ugc nofollow">模型</a> <br/> 1.3 <a class="ae jd" href="#e126" rel="noopener ugc nofollow">定义损失功能</a> <br/> 1.4 <a class="ae jd" href="#7aae" rel="noopener ugc nofollow">最小化损失功能</a></li></ol><p id="8b27" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh"> 2。</strong> <a class="ae jd" href="#0384" rel="noopener ugc nofollow"> <strong class="kx jh">实现</strong></a><strong class="kx jh"><br/>2.1</strong><a class="ae jd" href="#3b4c" rel="noopener ugc nofollow"><strong class="kx jh">正向传播</strong> </a> <br/> 2.1.1 <a class="ae jd" href="#3b4c" rel="noopener ugc nofollow">初始化权重(一次性)</a> <br/> 2.1.2 <a class="ae jd" href="#c551" rel="noopener ugc nofollow">进给数据</a> <br/> 2.1.3 <a class="ae jd" href="#5da9" rel="noopener ugc nofollow">计算<em class="lr">ŷ</em></a><em class="lr"><br/></em>2 . 2 . 1</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="db94" class="nj lz jg bd ma nk nl nm md nn no np mg km nq kn mj kp nr kq mm ks ns kt mp nt bi translated">1 准备</h1><h2 id="1c28" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">1.1 数据</h2><p id="a123" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">我们有一些数据:当我们观察自变量<em class="lr"> x </em> ₁和<em class="lr"> x </em> ₂时，我们也观察因变量(或响应变量)<em class="lr"> y </em>。</p><p id="553b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们的数据集中，我们有 6 个例子(或观察)。</p><pre class="ls lt lu lv gt nx ny nz oa aw ob bi"><span id="8113" class="ly lz jg ny b gy oc od l oe of"><strong class="ny jh">    x1 x2   y</strong><br/><strong class="ny jh">1)</strong>   4  1   2<br/><strong class="ny jh">2)</strong>   2  8 -14<br/><strong class="ny jh">3)</strong>   1  0   1<br/><strong class="ny jh">4)</strong>   3  2  -1<br/><strong class="ny jh">5)</strong>   1  4  -7<br/><strong class="ny jh">6)</strong>   6  7  -8</span></pre><h2 id="15e3" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">1.2 模型</h2><p id="b7a0" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">下一个要问的问题是:“₁和₂是如何联系在一起的？”</p><p id="63c4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们认为它们通过以下等式相互联系:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi og"><img src="../Images/430ad88a766fbfb2e07ef2da90a5005c.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*Nm-4DpNYhO4Tk0CnVZsA2g@2x.png"/></div></figure><p id="c567" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们今天的工作是找到“最佳”w 和 b<em class="lr">b</em>值。</p><blockquote class="oh oi oj"><p id="4128" class="kv kw lr kx b ky kz kh la lb lc kk ld ok lf lg lh ol lj lk ll om ln lo lp lq ij bi translated"><em class="jg">我用过深度学习约定</em> <strong class="kx jh"> w </strong> <em class="jg">和</em> <strong class="kx jh"> b </strong> <em class="jg">，分别代表</em> <strong class="kx jh"> <em class="jg">权重</em> </strong> <em class="jg">和</em> <strong class="kx jh"> <em class="jg">偏差</em> </strong> <em class="jg">。但是注意线性回归不是深度学习。</em></p></blockquote><h2 id="e126" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">1.3 定义损失函数</h2><p id="8ca1" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">假设在本练习结束时，我们已经算出我们的模型是</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi on"><img src="../Images/d14771fbcddf61364b6cf6b3e53b1e7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*ypqIFRkkDViPeHpuxl1H-Q@2x.png"/></div></figure><p id="8376" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们如何知道我们的模型做得好不好？</p><p id="099e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们简单地通过一个<em class="lr">损失函数比较预测的<em class="lr"> ŷ </em>和观测的<em class="lr"> y </em>。</em> <strong class="kx jh"> </strong>有很多方法来定义损失函数，但在本文中，我们将其定义为<em class="lr"> ŷ </em>和<em class="lr"> y </em>的平方差。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/fae4bccb2ca9c735d74a6a4fbd87d1ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*_vZEkrhaNN33h1GsphwlGA@2x.png"/></div></figure><p id="8f9f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一般来说<em class="lr"> L </em>越小越好。</p><h2 id="7aae" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">1.4 最小化损失功能</h2><p id="7c11" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">因为我们希望<em class="lr"> ŷ </em>和<em class="lr"> y </em>之间的差异很小，所以我们希望努力将其最小化。这是通过<strong class="kx jh">随机梯度下降</strong>优化完成的。它基本上是使用梯度值迭代更新<em class="lr"> w </em> ₁和<em class="lr"> w </em> ₂的值，如下式所示:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi op"><img src="../Images/3a257c5ef5eac56c09c7117d71aaa682.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*PcacbKzIjzLnN1Bmr601Yw@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Fig. 2.0: Computation graph for linear regression model with stochastic gradient descent.</figcaption></figure><p id="a6f0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该算法试图通过不断更新来找到正确的权重，记住我们正在寻找最小化损失函数的值。</p><blockquote class="oh oi oj"><p id="03de" class="kv kw lr kx b ky kz kh la lb lc kk ld ok lf lg lh ol lj lk ll om ln lo lp lq ij bi translated"><strong class="kx jh"> <em class="jg">直觉:随机梯度下降</em> </strong></p><p id="5802" class="kv kw lr kx b ky kz kh la lb lc kk ld ok lf lg lh ol lj lk ll om ln lo lp lq ij bi translated"><em class="jg">你是</em> <strong class="kx jh"> w </strong> <em class="jg">你在一个图上(损失函数)。你现在的值是</em> <strong class="kx jh"> w </strong> <em class="jg"> =5。你想移到图中的最低点(最小化损失函数)。</em></p><p id="2377" class="kv kw lr kx b ky kz kh la lb lc kk ld ok lf lg lh ol lj lk ll om ln lo lp lq ij bi translated">你也知道，用你当前的值，你的梯度是 2。你必须以某种方式利用这个价值继续生活。</p><p id="bbbe" class="kv kw lr kx b ky kz kh la lb lc kk ld ok lf lg lh ol lj lk ll om ln lo lp lq ij bi translated">根据高中数学，2 意味着你在一个倾斜的斜坡上，你能下来的唯一方法就是向左移动，在这一点上。</p><p id="fab0" class="kv kw lr kx b ky kz kh la lb lc kk ld ok lf lg lh ol lj lk ll om ln lo lp lq ij bi translated">如果走 5+2 意味着你要去右边爬上斜坡，那么唯一的方法就是走 5–2，它会把你带到左边，向下。所以梯度下降就是从当前值中减去梯度值。</p></blockquote></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="0384" class="nj lz jg bd ma nk nl nm md nn no np mg km nq kn mj kp nr kq mm ks ns kt mp nt bi translated">2.履行</h1><p id="d26f" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">我们的模型的工作流程很简单:前向传播(或前馈或前向传递)和反向传播。</p><blockquote class="oh oi oj"><p id="97cf" class="kv kw lr kx b ky kz kh la lb lc kk ld ok lf lg lh ol lj lk ll om ln lo lp lq ij bi translated"><strong class="kx jh"> <em class="jg">定义:训练<br/> </em> </strong> <em class="jg">训练简单来说就是定期更新你的权值。</em></p></blockquote><p id="5aa0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面是工作流程。单击以跳转到该部分。</p><p id="aca1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">— — — — <br/> <strong class="kx jh"> 2.1 正向传播</strong> <br/> 2.1.1 <a class="ae jd" href="#3b4c" rel="noopener ugc nofollow">初始化权重(一次性)</a> <br/> 2.1.2 <a class="ae jd" href="#c551" rel="noopener ugc nofollow">进给数据</a> <br/> 2.1.3 <a class="ae jd" href="#5da9" rel="noopener ugc nofollow">计算<em class="lr">ŷ</em></a><em class="lr"><br/></em>2 . 1 . 4<a class="ae jd" href="#b9e3" rel="noopener ugc nofollow">计算损耗</a></p><p id="a92c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh"> 2.2 反向传播</strong> <br/> 2.2.1 <a class="ae jd" href="#e0d5" rel="noopener ugc nofollow">计算偏导数</a> <br/> 2.2.2 <a class="ae jd" href="#5dd3" rel="noopener ugc nofollow">更新权重</a> <br/> — — — — — — — — —</p><p id="1013" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们开始吧。</p><p id="6ef8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了跟踪所有的值，我们首先构建一个“计算图”,其中包含了用颜色编码的节点</p><ol class=""><li id="f64f" class="mr ms jg kx b ky kz lb lc le oq li or lm os lq my mz na nb bi translated"><strong class="kx jh">橙色</strong> <em class="lr"> — </em>占位符(<em class="lr"> x </em> ₁、<em class="lr"> x </em> ₂和<em class="lr"> y </em>)，</li><li id="b542" class="mr ms jg kx b ky ot lb ou le ov li ow lm ox lq my mz na nb bi translated"><strong class="kx jh">深绿色</strong> <em class="lr"> — </em>重量和偏差(<em class="lr"> w </em> ₁，<em class="lr"> w </em> ₂和<em class="lr"> b </em>)，</li><li id="ad18" class="mr ms jg kx b ky ot lb ou le ov li ow lm ox lq my mz na nb bi translated"><strong class="kx jh">浅绿色</strong> <em class="lr"> — </em>型号 ( <em class="lr"> ŷ </em>)连接<em class="lr"> w </em> ₁、<em class="lr"> w </em> ₂、<em class="lr"> b </em>、<em class="lr"> x </em> ₁、<em class="lr"> x </em> ₂，以及</li><li id="e5c5" class="mr ms jg kx b ky ot lb ou le ov li ow lm ox lq my mz na nb bi translated"><strong class="kx jh">黄色</strong><em class="lr">—</em><a class="ae jd" href="#d864" rel="noopener ugc nofollow">损失函数</a> ( <em class="lr"> L </em>)连接<em class="lr"> ŷ </em>和<em class="lr"> y. </em></li></ol><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oy"><img src="../Images/b59a8c22e3215325ec091832d20e034f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j2BO6GaJP104BNFV_g4FCg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Fig. 2.0: Computation graph for linear regression model with stochastic gradient descent.</figcaption></figure><p id="6f19" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于正向传播，您应该从上到下阅读此图，对于反向传播，您应该从下到上阅读。</p><blockquote class="oh oi oj"><p id="787f" class="kv kw lr kx b ky kz kh la lb lc kk ld ok lf lg lh ol lj lk ll om ln lo lp lq ij bi translated"><strong class="kx jh"> <em class="jg">注</em> </strong> <em class="jg"> <br/>我采用了“占位符”这个术语，这个术语在</em><a class="ae jd" href="https://www.tensorflow.org/api_docs/python/tf/placeholder" rel="noopener ugc nofollow" target="_blank"><em class="jg">tensor flow</em></a><em class="jg">中用来指代这些“数据变量”。<br/>我也将使用术语“重量”来统称</em> w <em class="jg">和</em> b <em class="jg">。</em></p></blockquote><h2 id="24f3" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">2.1 正向传播</h2><h2 id="3b4c" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">2.1.1 初始化砝码(一次性)</h2><p id="07f8" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">因为梯度下降是关于更新权重的，我们需要它们从一些值开始，称为<em class="lr">初始化</em>权重。</p><p id="348f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里我们初始化权重和偏差如下:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/ca80b1d7b83e661799a4913f1bf4ae55.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*NN3e7f2hjDD8oGRkbDpSXw@2x.png"/></div></figure><p id="a11d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些反映在下面图 2.1.1 中的<strong class="kx jh">深绿色节点</strong>中:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pa"><img src="../Images/32ca9ada0c389fe17a203466531bb51c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*03nH5y_8czFZkwKLMiHl6w.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Fig. 2.1.1: Weights initialised (dark green nodes)</figcaption></figure><p id="a149" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有许多初始化权重的方法(0、1、均匀分布、正态分布、截断正态分布等)。)但我们不会在本帖中涉及它们。在本教程中，我们通过使用截尾正态分布和偏差为 0 来初始化权重。</p><h2 id="c551" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">进料数据</h2><p id="cf9c" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">接下来，我们将批量大小设置为 1，并输入第一批数据。</p><blockquote class="oh oi oj"><p id="6f8c" class="kv kw lr kx b ky kz kh la lb lc kk ld ok lf lg lh ol lj lk ll om ln lo lp lq ij bi translated"><strong class="kx jh"> <em class="jg">批次和批次大小</em> </strong></p><p id="17fb" class="kv kw lr kx b ky kz kh la lb lc kk ld ok lf lg lh ol lj lk ll om ln lo lp lq ij bi translated"><em class="jg">我们可以将数据集分成大小相等的小组。每组称为一个</em><strong class="kx jh"><em class="jg"/></strong><em class="jg">，由指定数量的样本组成，称为</em> <strong class="kx jh"> <em class="jg">批量</em> </strong> <em class="jg">。如果我们将这两个数字相乘，我们应该可以得到数据中的观察次数。</em></p><p id="24e9" class="kv kw lr kx b ky kz kh la lb lc kk ld ok lf lg lh ol lj lk ll om ln lo lp lq ij bi translated"><em class="jg">在这里，我们的数据集由 6 个示例组成，由于我们在本次培训中将批次大小定义为 1，因此我们总共有 6 个批次。</em></p></blockquote><p id="dd51" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">用于输入模型的当前数据批次在下面以粗体显示:</p><pre class="ls lt lu lv gt nx ny nz oa aw ob bi"><span id="9d73" class="ly lz jg ny b gy oc od l oe of"><strong class="ny jh">    x1 x2   y</strong><br/><strong class="ny jh">1)   4  1   2</strong><br/><strong class="ny jh">2)</strong>   2  8 -14<br/><strong class="ny jh">3)</strong>   1  0   1<br/><strong class="ny jh">4)</strong>   3  2  -1<br/><strong class="ny jh">5)</strong>   1  4  -7<br/><strong class="ny jh">6)</strong>   6  7  -8</span></pre><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/5fc7f6f0d0e04fa97dcf568ba5f6b38c.png" data-original-src="https://miro.medium.com/v2/resize:fit:272/format:webp/1*9uynIsf80ZQkOKLFmmxuZA@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Eqn. 2.1.2: First batch of data fed into model</figcaption></figure><p id="985c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在图 2.1.2 中，<strong class="kx jh">橙色节点</strong>是我们输入当前一批数据的地方。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pc"><img src="../Images/6daf67d60cac44cc24621a84b343992c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qECII5vu308PveYSSIlsxA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Fig. 2.1.2: Feeding data to model with first batch (orange nodes)</figcaption></figure><h2 id="5da9" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">2.1.3 计算<strong class="ak"> <em class="pd"> ŷ </em> </strong></h2><p id="3912" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">现在我们已经有了<em class="lr"> x </em> ₁、<em class="lr"> x </em> ₂、<em class="lr"> w </em> ₁、<em class="lr"> w </em> ₂和<em class="lr"> b </em>的值，让我们计算<em class="lr"> ŷ.</em></p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pe"><img src="../Images/61a0eb42ed68ec1eca23d5da174e8a26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZCPCzM3czHMyGqOvjM3jZA@2x.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Eqn. 2.1.3: Compute <em class="pd">ŷ</em></figcaption></figure><p id="0abf" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr"> ŷ </em> (=-0.1)的值反映在下面的<strong class="kx jh">浅绿色节点</strong>中:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pc"><img src="../Images/12fbac704c56b933cf912045910ca4eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I8uQ-vmPq2YL6meYi7m5dQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Fig. 2.1.3: <em class="pd">ŷ computed (light green node)</em></figcaption></figure><h2 id="b9e3" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">计算损失</h2><p id="aa68" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">我们预测的<em class="lr"> ŷ </em>与给定的<em class="lr"> y </em>数据有多远？我们通过计算前面定义的损失函数来比较它们。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/cd756c8daa82182c59f5b5b786d2c88c.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*iQeCltcGcaIYqKj8qkXUNg@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Eqn. 2.1.4: Compute the loss</figcaption></figure><p id="9569" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">您可以在计算图的<strong class="kx jh">黄色节点</strong>中看到该值。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pc"><img src="../Images/5fc7bb701c92e53e5b6e9b688faac9d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WihKa-ucBgb89ELIGnjF5g.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Fig. 2.1.4A: L computed (yellow node)</figcaption></figure><p id="fdf6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通常的做法是记录训练期间的损失，以及其他信息，如纪元、批次和花费的时间。在我的演示中，您可以在<strong class="kx jh">训练进度</strong>面板下看到这一点。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pg"><img src="../Images/f24bd974253255192559081d5293eeac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4iKuzL26v1jr70lHmZXikA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Fig. 2.1.4B: Logging loss and other information</figcaption></figure><h2 id="7230" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">2.2 反向传播</h2><h2 id="e0d5" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">计算偏微分</h2><p id="dbb8" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">在我们开始调整权重和偏差<em class="lr"> w </em> ₁、<em class="lr"> w </em> ₂和<em class="lr"> b </em>的值之前，让我们首先计算所有的偏差值。这些是我们稍后更新权重时需要的。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pc"><img src="../Images/a34cc3a2870f33b9a21da2e14862df99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tSajkdxxeLapdnWbUuJPWA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Fig. 2.2.1: Indicated partial differentials to the relevant edges on the graph</figcaption></figure><p id="8603" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">也就是说，我们只计算<strong class="kx jh">通向每个<em class="lr"> w </em>和<em class="lr"> b </em>的所有可能路径</strong>，因为这些是我们唯一感兴趣更新的变量。从上面的图 2.2.1 中，我们看到有 4 条边被标上了偏导数。</p><p id="0661" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">回想一下模型和损失函数的等式:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi og"><img src="../Images/fe9c4a088ec33cd4cd4bbb9316511a7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*1qHvv7r1OGCYS7XLqKcL0A@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Model</figcaption></figure><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/c5562105bb47bd94e62119835c05d11a.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*Ro-PpjlXRQsADIVMD55zeA@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Loss function</figcaption></figure><p id="f26c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">部分差异如下:</p><p id="4fc5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr"> L </em> ( <strong class="kx jh">黄色</strong>)——<em class="lr">ŷ</em>(<strong class="kx jh">浅绿色</strong> ) <strong class="kx jh"> : </strong></p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/a4d34a15443a41fd473c7912d3dc73b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*322ezTizAdATsKnh56rC9w@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Eqn. 2.2.1A: Partial differential of L w.r.t. <em class="pd">ŷ</em></figcaption></figure><p id="2a3d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr"> ŷ </em> ( <strong class="kx jh">浅绿色</strong>)——<em class="lr">b</em>(<strong class="kx jh">深绿色</strong>):</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/b3ee7bfe2cb001efebda3e18f698ccd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/1*Y47v_az5dMa_-3r8rd6Cqw@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Eqn. 2.2.1B: Partial differential of <em class="pd">ŷ </em>w.r.t. b</figcaption></figure><p id="0d73" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr"> ŷ </em> ( <strong class="kx jh">浅绿色</strong>)——<em class="lr">w</em>₁(<strong class="kx jh">深绿色</strong>):</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/f6d654cdb1433c335dcde2d7910a5647.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*IhtruCft1v1ycmreQtvlVw@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Eqn. 2.2.1C: Partial differential of <em class="pd">ŷ </em>w.r.t. w1</figcaption></figure><p id="f2b0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr"> ŷ </em> ( <strong class="kx jh">浅绿色</strong>)——<em class="lr">w</em>₂(<strong class="kx jh">深绿色</strong>):</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/792b4b76348cbb12262b47a64288d3f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*gbt2Pc7wDPkugQuoKWc0og@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Eqn. 2.2.1D: Partial differential of <em class="pd">ŷ </em>w.r.t. w2</figcaption></figure><p id="ce12" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意，偏微分的值遵循当前批次中的<strong class="kx jh">值。例如，在等式 2 中。2.2.1C，<em class="lr"> x </em> ₁ = 4。</strong></p><h2 id="5dd3" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">更新权重</h2><p id="d584" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">观察下图 2.2.2 中的<strong class="kx jh">深绿色节点</strong>。我们看到三样东西:<br/> i) <em class="lr"> b </em>从 0.000 变化到 0.212 <br/> ii) <em class="lr"> w </em> ₁从-0.017 变化到 0.829 <br/> iii) <em class="lr"> w </em> ₂从-0.048 变化到 0.164</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pc"><img src="../Images/af672c8f5a07819a8f45a13a59605aca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*N1BWnfOC5CKbWEXGyTgrgA.gif"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Fig. 2.2.2: Updating the weights and bias (dark green nodes)</figcaption></figure><p id="373e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">还要注意从<strong class="kx jh">黄色节点</strong>到<strong class="kx jh">绿色节点</strong>的路径的“方向”。他们从下往上走。</p><p id="e7bb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是随机梯度下降-使用反向传播更新权重，利用各自的梯度值。</p><p id="1106" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">先重点更新一下<em class="lr"> b </em>。<em class="lr"> </em>更新<em class="lr"> b </em>的公式为</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/d0b6ed0e1589a2729a712353fa932488.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*fz-JTsnjYxpdZdsFYQy6Tw@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Eqn. 2.2.2A: Stochastic gradient descent update for b</figcaption></figure><p id="b1b4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在哪里</p><ul class=""><li id="143c" class="mr ms jg kx b ky kz lb lc le oq li or lm os lq pm mz na nb bi translated"><em class="lr">b</em><strong class="kx jh"><em class="lr"/></strong><em class="lr">—</em>当前值</li><li id="f2e6" class="mr ms jg kx b ky ot lb ou le ov li ow lm ox lq pm mz na nb bi translated"><em class="lr">b’</em><strong class="kx jh"><em class="lr"/></strong><em class="lr">—</em>更新后的值</li><li id="03b0" class="mr ms jg kx b ky ot lb ou le ov li ow lm ox lq pm mz na nb bi translated"><em class="lr"> η — </em>学习率，设置为 0.05</li><li id="7ed1" class="mr ms jg kx b ky ot lb ou le ov li ow lm ox lq pm mz na nb bi translated"><em class="lr"> ∂L/∂b — </em>梯度，即<em class="lr"> L </em> w.r.t. <em class="lr"> b </em>的偏微分</li></ul><p id="ca65" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了得到梯度，我们需要使用链式法则将从<em class="lr"> L </em>到<em class="lr"> b </em>的<strong class="kx jh">路径</strong>相乘:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/216abbd833321f0c09b4c098542e7921.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*xja8ajy2224WdRNaMsELyA@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Eqn. 2.2.2B: Chain rule for partial differential of L w.r.t. b</figcaption></figure><p id="4413" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将需要当前批次值<em class="lr"> x </em>、<em class="lr"> y、ŷ </em>和偏差值，因此我们将它们放在下面以便于参考:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi po"><img src="../Images/c39f891bd2860dddbdf3a3e250f853dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*j2KSacnJZf03aDeKs8QsyQ@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Eqn. 2.2.2C: Partial differentials</figcaption></figure><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/2b6268d7d379cf57fcafb0e9f105c356.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*FYjzJmqGh-fhW48W0FtaLw@2x.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Eqn. 2.2.2D: Values from current batch and the predicted <em class="pd">ŷ</em></figcaption></figure><p id="1d96" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用方程中的随机梯度下降方程。2.2.2A 并代入方程中的所有值。2.2.2B-D 为我们提供了:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pq"><img src="../Images/43d06b73627e9d5ce0eee774ae0b6077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yRYr4bC-EssF4CWwMJVK7w@2x.png"/></div></div></figure><p id="376c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">更新<em class="lr"> b </em>到此为止！唷！我们只剩下更新<em class="lr"> w </em> ₁和<em class="lr"> w </em> ₂，我们以类似的方式更新它们。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pr"><img src="../Images/45f6888d8892fb0525510f9f2ce76028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ddZgbC2HCkfi2fXNQ6YZg@2x.png"/></div></div></figure><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pr"><img src="../Images/06aa75540644906e399fec1431e3b2e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dV4wn-3je-JWvirX7o6Hqw@2x.png"/></div></div></figure><h2 id="5244" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">批处理迭代结束</h2><p id="dda8" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">恭喜你。处理第一批就这样！</p><pre class="ls lt lu lv gt nx ny nz oa aw ob bi"><span id="af23" class="ly lz jg ny b gy oc od l oe of"><strong class="ny jh">    x1 x2   y</strong><br/><strong class="ny jh">1)</strong>   4  1   2  ✔<br/><strong class="ny jh">2)</strong>  <strong class="ny jh"> 2  8 -14<br/>3)   1  0   1<br/>4)   3  2  -1<br/>5)   1  4  -7<br/>6)   6  7  -8</strong></span></pre><p id="dee5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们需要将上述步骤重复到其他 5 个批次，即实施例 2 至 6。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ps"><img src="../Images/bafcee836958042aa69c93bcf9c48b63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*2gepOobVvcYRW3KEVmFCKg.gif"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Iterating through batch 1 to 6 (apologies for the poor GIF quality! )</figcaption></figure><h2 id="453c" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">时代结束</h2><p id="1e89" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">当模型已经遍历所有批次一次时，我们完成 1 个时期。在实践中，我们将纪元扩展到大于 1。</p><blockquote class="oh oi oj"><p id="8fd8" class="kv kw lr kx b ky kz kh la lb lc kk ld ok lf lg lh ol lj lk ll om ln lo lp lq ij bi translated"><em class="jg">一个</em> <strong class="kx jh"> <em class="jg">历元</em> </strong> <em class="jg">是当我们的设置已经看到了</em> <strong class="kx jh"> <em class="jg">所有</em> </strong> <em class="jg">在我们的数据集中的观察值一次。但是一个纪元几乎永远不足以让损失收敛。实际上，这个数字是手动调整的。</em></p></blockquote><p id="cd4a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这一切结束时，你应该得到一个最终的模型，准备好进行推理，比如说:</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi on"><img src="../Images/d773b8b576982089fbc2e768441ab362.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*Ud_fTDsa93Q5CQqJ5xkqeQ@2x.png"/></div></figure><p id="54ab" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们用伪代码来回顾一下整个工作流程:</p><pre class="ls lt lu lv gt nx ny nz oa aw ob bi"><span id="56bc" class="ly lz jg ny b gy oc od l oe of">initialise_weights()</span><span id="306c" class="ly lz jg ny b gy pt od l oe of">for i in epochs:</span><span id="a4fc" class="ly lz jg ny b gy pt od l oe of">    for j in batches:</span><span id="7ca1" class="ly lz jg ny b gy pt od l oe of">        #forward propagation<br/>        feed_batch_data()<br/>        compute_ŷ()<br/>        compute_loss()</span><span id="323c" class="ly lz jg ny b gy pt od l oe of">        #backpropagation<br/>        compute_partial_differentials()<br/>        update_weights()</span></pre><h2 id="9900" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">改善培训</h2><p id="5c92" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">对于随机梯度下降优化问题，一个历元是远远不够的。记住在<a class="ae jd" href="#5701" rel="noopener ugc nofollow">图 4.1 </a>中，我们的亏损在 4.48。如果我们增加历元的数量，这意味着增加我们更新权重和偏差的次数，我们可以将其收敛到令人满意的低水平。</p><p id="1cf5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下是您可以在培训中改进的地方:</p><ul class=""><li id="aa67" class="mr ms jg kx b ky kz lb lc le oq li or lm os lq pm mz na nb bi translated">将培训扩展到一个以上的时代</li><li id="341a" class="mr ms jg kx b ky ot lb ou le ov li ow lm ox lq pm mz na nb bi translated">增加批量</li><li id="a9f1" class="mr ms jg kx b ky ot lb ou le ov li ow lm ox lq pm mz na nb bi translated">改变优化器(见我关于梯度下降优化算法的帖子<a class="ae jd" rel="noopener" target="_blank" href="/10-gradient-descent-optimisation-algorithms-86989510b5e9">这里</a>)</li><li id="7e95" class="mr ms jg kx b ky ot lb ou le ov li ow lm ox lq pm mz na nb bi translated">调整学习率(更改学习率值或使用学习率计划程序)</li><li id="127e" class="mr ms jg kx b ky ot lb ou le ov li ow lm ox lq pm mz na nb bi translated">拿出一套火车价值测试装置</li></ul></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h2 id="0f5d" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">关于</h2><p id="571d" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">我用 JavaScript 构建了一个交互式的可探索的线性回归演示。以下是我使用的库:</p><ul class=""><li id="9a8c" class="mr ms jg kx b ky kz lb lc le oq li or lm os lq pm mz na nb bi translated">Dagre-D3 (GraphViz + d3.js ),用于呈现图形</li><li id="30a8" class="mr ms jg kx b ky ot lb ou le ov li ow lm ox lq pm mz na nb bi translated">用于呈现数学符号的 MathJax</li><li id="5ff8" class="mr ms jg kx b ky ot lb ou le ov li ow lm ox lq pm mz na nb bi translated">用于绘制折线图的图表</li><li id="6d29" class="mr ms jg kx b ky ot lb ou le ov li ow lm ox lq pm mz na nb bi translated">jQuery</li></ul><p id="2c76" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">点击查看互动演示<a class="ae jd" href="https://remykarem.github.io/backpropagation-demo/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="c478" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">您可能还想看看下面的<em class="lr">使用 TensorFlow 进行线性回归的一行一行的外行指南</em>，该指南着重于使用 TensorFlow 库对线性回归进行编码。</p><div class="ip iq gp gr ir pu"><a href="https://medium.com/datadriveninvestor/a-line-by-line-laymans-guide-to-linear-regression-using-tensorflow-3c0392aa9e1f" rel="noopener follow" target="_blank"><div class="pv ab fo"><div class="pw ab px cl cj py"><h2 class="bd jh gy z fp pz fr fs qa fu fw jf bi translated">使用 TensorFlow 进行线性回归的逐行外行指南</h2><div class="qb l"><h3 class="bd b gy z fp pz fr fs qa fu fw dk translated">线性回归是机器学习之旅的一个良好开端，因为它非常简单明了…</h3></div><div class="qc l"><p class="bd b dl z fp pz fr fs qa fu fw dk translated">medium.com</p></div></div><div class="qd l"><div class="qe l qf qg qh qd qi ix pu"/></div></div></a></div></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h2 id="0720" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">参考</h2><div class="ip iq gp gr ir pu"><a href="https://colah.github.io/posts/2015-08-Backprop/" rel="noopener  ugc nofollow" target="_blank"><div class="pv ab fo"><div class="pw ab px cl cj py"><h2 class="bd jh gy z fp pz fr fs qa fu fw jf bi translated">计算图上的微积分:反向传播——colah 的博客</h2><div class="qb l"><h3 class="bd b gy z fp pz fr fs qa fu fw dk translated">反向传播是使训练深度模型在计算上易于处理的关键算法。对于现代神经…</h3></div><div class="qc l"><p class="bd b dl z fp pz fr fs qa fu fw dk translated">colah.github.io</p></div></div><div class="qd l"><div class="qj l qf qg qh qd qi ix pu"/></div></div></a></div><figure class="ls lt lu lv gt is"><div class="bz fp l di"><div class="qk lx l"/></div></figure><h2 id="5bbf" class="ly lz jg bd ma mb mc dn md me mf dp mg le mh mi mj li mk ml mm lm mn mo mp mq bi translated">深度学习相关文章</h2><p id="7a1f" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/animated-rnn-lstm-and-gru-ef124d06cf45">动画版 RNN、LSTM 和 GRU </a></p><p id="5307" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281">逐行 Word2Vec 实现</a>(关于单词嵌入)</p><p id="7232" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/10-gradient-descent-optimisation-algorithms-86989510b5e9"> 10 种梯度下降优化算法+备忘单</a></p><p id="698e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889">统计深度学习模型中的参数数量</a></p><p id="3d8c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/attn-illustrated-attention-5ec4ad276ee3">经办人:图文并茂</a></p><p id="50b7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/illustrated-self-attention-2d627e33b20a">图文并茂:自我关注</a></p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><p id="d039" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">感谢</em> <a class="ae jd" href="https://medium.com/@renjietan" rel="noopener"> <em class="lr">【任杰】</em></a><em class="lr"/><a class="ae jd" href="https://medium.com/@derekchia" rel="noopener"><em class="lr">德里克</em> </a> <em class="lr">对本文的想法、建议和修正。</em></p><p id="4d00" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">关注我上</em><a class="ae jd" href="https://www.twitter.com/remykarem" rel="noopener ugc nofollow" target="_blank"><em class="lr">Twitter</em></a><em class="lr">@ remykarem 或者</em><a class="ae jd" href="http://www.linkedin.com/in/raimibkarim" rel="noopener ugc nofollow" target="_blank"><em class="lr">LinkedIn</em></a><em class="lr">。你也可以通过 raimi.bkarim@gmail.com 联系我。欢迎访问我的网站</em><a class="ae jd" href="https://remykarem.github.io/" rel="noopener ugc nofollow" target="_blank"><em class="lr">remykarem . github . io</em></a><em class="lr">。</em></p></div></div>    
</body>
</html>