<html>
<head>
<title>Differentiable convolution of probability distributions with Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流概率分布的可微卷积</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/differentiable-convolution-of-probability-distributions-with-tensorflow-79c1dd769b46?source=collection_archive---------7-----------------------#2018-05-06">https://towardsdatascience.com/differentiable-convolution-of-probability-distributions-with-tensorflow-79c1dd769b46?source=collection_archive---------7-----------------------#2018-05-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="008e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Tensorflow 中的卷积运算是为张量设计的，但也可用于卷积可微分函数</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/8b872f791218c8638bc610676f244a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b2Fwl-2hlm5zBb8O513C1A.jpeg"/></div></div></figure><p id="f6a7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在本文中，我描述了一个均匀和高斯概率分布的卷积的例子，将它们与数据进行比较，并适合高斯的宽度参数。如果我们的数据采用基础模型(本例中为均匀分布),但每个数据点都被高斯模型随机“涂抹”了一定量(例如测量不确定性),则在实践中会出现这种情况。我们假设我们不知道分辨率，并想从观测数据中确定它。这里<a class="ae ln" href="https://gist.github.com/andreh7/bd76330507675e6010bddea5005a7a37" rel="noopener ugc nofollow" target="_blank">有一个 Jupyter 笔记本，上面有本文讨论的例子的 Tensorflow 实现</a>。</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><p id="4476" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们首先在函数参数上定义一个网格，我们将在整篇文章中使用它。尽管对于这里给出的这个简单例子，我们可以得到卷积积分的封闭形式的表达式，但在实际应用中往往不是这样。因此，我们在这个网格上离散我们的函数，有效地使它们分段常数。然而，我们可以选择非常大的箱数，使得分段近似对于我们的目的来说足够好:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lv"><img src="../Images/ffb75ea22a5f53e0cb88ba5d22fd2716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vAzt5f0uK_18LiFYkwUK9Q.png"/></div></div></figure><p id="7c38" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然后我们在这个网格上定义一个<a class="ae ln" href="https://www.tensorflow.org/api_docs/python/tf/distributions/Uniform" rel="noopener ugc nofollow" target="_blank">均匀分布</a>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lw"><img src="../Images/6685e277eb11397a4540114b9fd18077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FhuE_04auIRlK2QIe_mv4A.png"/></div></div></figure><p id="ed8f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/7e6e4de5646e9ca1dbf32ad1972ab3a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*xf_h9syMSspKcpfnyPJpkg.png"/></div></figure><p id="c3f1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们现在可以从均匀分布中抽取一个随机数样本:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ly"><img src="../Images/fac4c62574621c7ffaaab577d17a8bab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Z7ATwNFGj4sObsSfqtMQg.png"/></div></div></figure><p id="4837" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">并且用从已知宽度的高斯分布中提取的随机数“涂抹”每个点:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lz"><img src="../Images/989e12c0287cae0235ea7efb3dc0f37a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U9ox1rljM7VWHYoeUlxgHw.png"/></div></div></figure><p id="1280" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这给了我们以下分布:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/776b18ce8e03a6eea2696cf5c1e829e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*16S714YE3wKthBT1iuxt5g.png"/></div></figure><p id="8bd9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在我们定义高斯分布。请注意，我们将宽度参数 sigma 设为<code class="fe mb mc md me b">tf.Variable</code>而不是张量或占位符，因为我们希望稍后使用最大似然拟合来确定它，并因此对其进行求导:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mf"><img src="../Images/1e11c3c41e8e50aa6f528c6dec1c390e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EUBJBgYEftMdB-5TEUexQQ.png"/></div></div></figure><p id="79ce" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">必须考虑如何获得正确的卷积输出范围。在这个例子中，我们使用<code class="fe mb mc md me b">padding='SAME'</code>进行卷积运算，第二张量具有奇数个面元。如果我们选择第二个张量在其中间为 1，在其他地方为零(一个“居中的狄拉克脉冲”)，那么卷积输出将等于第一个张量。因此，我们将高斯核置于第二张量中间面元的中心。卷积乘积的定义域与投影到第一张量的函数的定义域相同。</p><p id="9c18" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">还发现 Tensorflow 的 1D 卷积实际上是一个<a class="ae ln" href="https://en.wikipedia.org/wiki/Cross-correlation" rel="noopener ugc nofollow" target="_blank">互相关</a>而不是一个<a class="ae ln" href="https://en.wikipedia.org/wiki/Convolution#Discrete_convolution" rel="noopener ugc nofollow" target="_blank">卷积</a>。这些本质上不同于第二个函数被“遍历”的方向。通过将第二个输入镜像到围绕垂直轴的卷积运算，或者等效地，翻转函数参数的符号，我们可以得到真正的卷积。</p><p id="70d3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">作为卷积运算的快速检查，我们将均匀分布与其本身进行卷积:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mg"><img src="../Images/62ec52fe4026fb928fb454ff7d9f3ab5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6eGmE8CWpLng2DerGiaZxg.png"/></div></div></figure><p id="57e6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">获得:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/167bc42b1723e6c806ad3acceae4f73e.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*0y4cHc3TH26PxdOb-pIiug.png"/></div></figure><p id="b269" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">其(忽略归一化)对应于从完全解析计算中获得的结果。</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><p id="3ec6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">回到我们最初的问题，我们定义均匀分布和高斯分布之间的卷积:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mh"><img src="../Images/a0c6711f0feee01491ce00f0fdd1b7bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fAPXlhRutF4nBFBhmKaxpg.png"/></div></div></figure><p id="80ee" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">(对于 sigma 参数的原始值)如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/78a601028b548f394bee87d29a1a1485.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*Ykak06juBrHFf5_7G-d4xQ.png"/></div></figure></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><p id="5c5e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们根据卷积乘积定义上面生成的样本数据的负对数似然性。由于我们必须对卷积运算进行离散化，因此我们还需要用相同的宁滨对数据进行分类:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mi"><img src="../Images/145b29be33cd17b19a25c001b26fe172.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lN0yQlDlCKsCp2Jbml0yOQ.png"/></div></div></figure><p id="7676" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">负对数似然函数的定义是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mj"><img src="../Images/0241e6e1ec178400b50c83678f1cf330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1blUKQKSR-q1kFmckDXlGg.png"/></div></div></figure><p id="58f6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在下一步中，我们绘制负对数似然函数，我们期望在 sigma 的“真实”值处看到最小值，该值以前用于抽取样本。Tensorflow 可以计算负对数似然函数相对于 sigma 的导数(如下图中的箭头所示):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/7460bc36764f84a4451cfcdbda3b135a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*j4h5h3UixPmuFcs5idAsSA.png"/></div></figure><p id="997f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在我们有了负对数似然函数及其导数，我们可以使用它的最小值。因为我们只有一个参数要优化，所以我们使用<a class="ae ln" href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm" rel="noopener ugc nofollow" target="_blank"> BFGS 方法</a>而不是随机梯度下降法。Tensorflow 有一个到<a class="ae ln" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize" rel="noopener ugc nofollow" target="_blank"> scipy 的最小化工具箱</a>的接口:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ml"><img src="../Images/54cef5ae9b44d44e33cad9450e891b18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DiohTdUVP-jXqBi0JkD3CQ.png"/></div></div></figure><p id="2d44" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了监控进度，我们定义了一个回调函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mm"><img src="../Images/e9e834a8227641bcebe2e489f853c687.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5HRC3rjDR37-oRxpGgFi7g.png"/></div></div></figure><p id="3ce7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">开始最小化:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mn"><img src="../Images/a0d1a422ca170628dd4dcfac12b50c8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FuKSul0gRI8dwEKcNwz5XQ.png"/></div></div></figure><p id="65d9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这通常在大约 23 次迭代之后收敛，并且应该给出接近原始值 0.1 的 sigma 值，该原始值 0.1 用于涂抹从均匀分布中提取的点。</p></div><div class="ab cl lo lp hu lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ij ik il im in"><p id="ab11" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们在本文中已经表明，在 Tensorflow 这样的框架中，我们可以对概率分布的参数求导，即使我们在卷积运算的(离散版本)中使用这样的分布。</p><p id="0a43" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">顺便提一下，当使用大量面元时，使用<a class="ae ln" href="https://en.wikipedia.org/wiki/Convolution#Fast_convolution_algorithms" rel="noopener ugc nofollow" target="_blank">快速卷积算法</a>在计算上可能更有效。事实上，Tensorflow 依赖于 cuDNN，它支持用于执行<a class="ae ln" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnConvolutionForward" rel="noopener ugc nofollow" target="_blank">卷积</a>的<a class="ae ln" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnConvolutionFwdAlgo_t" rel="noopener ugc nofollow" target="_blank">几种不同算法</a>，包括基于离散傅立叶变换的方法。</p></div></div>    
</body>
</html>