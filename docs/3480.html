<html>
<head>
<title>Interactive Visualization of Decision Trees with Jupyter Widgets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Jupyter 窗口小部件实现决策树的交互式可视化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interactive-visualization-of-decision-trees-with-jupyter-widgets-ca15dd312084?source=collection_archive---------2-----------------------#2018-05-16">https://towardsdatascience.com/interactive-visualization-of-decision-trees-with-jupyter-widgets-ca15dd312084?source=collection_archive---------2-----------------------#2018-05-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/c70155910bcb913dc26b1bf0a8a97f1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rkcshuLHpFbtHY3lm36rag.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/photos/Qy-CBKUg_X8?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Aaron Burden</a> on <a class="ae kc" href="https://unsplash.com/search/photos/trees?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="6a12" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">决策树是广泛用于分类和回归任务的监督模型。在本文中，我们将讨论决策树分类器，以及如何动态地可视化它们。这些分类器在训练数据上建立一系列简单的 if/else 规则，通过它们来预测目标值。决策树很容易解释，因为它们的结构和我们可视化建模树的能力。</p><p id="4dad" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用 sk learn<a class="ae kc" href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html" rel="noopener ugc nofollow" target="_blank"><em class="lb">export _ graphviz</em></a>函数，我们可以在 Jupyter 笔记本中显示该树。在这个演示中，我们将使用 sklearn <a class="ae kc" href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html" rel="noopener ugc nofollow" target="_blank">葡萄酒数据集</a>。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="ef81" class="ll lm iq lh b gy ln lo l lp lq">from sklearn.tree import DecisionTreeClassifier, export_graphviz<br/>from sklearn import tree<br/>from sklearn.datasets import load_wine<br/>from IPython.display import SVG<br/>from graphviz import Source<br/>from IPython.display import display</span><span id="9abf" class="ll lm iq lh b gy lr lo l lp lq"># load dataset<br/>data = load_wine()<br/><br/># feature matrix<br/>X = data.data<br/><br/># target vector<br/>y = data.target<br/><br/># class labels<br/>labels = data.feature_names<br/><br/># print dataset description<br/>print(data.DESCR)</span><span id="59e3" class="ll lm iq lh b gy lr lo l lp lq">estimator = DecisionTreeClassifier()<br/>estimator.fit(X, y)<br/><br/>graph = Source(tree.export_graphviz(estimator, out_file=None<br/>   , feature_names=labels, class_names=['0', '1', '2'] <br/>   , filled = True))</span><span id="5f28" class="ll lm iq lh b gy lr lo l lp lq">display(SVG(graph.pipe(format='svg')))</span></pre><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ls"><img src="../Images/fd147b6c5d80a215c093a49350bbffb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L8YzDxPDbst-wVhCiwdk8g.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Decision Tree with default parameters</figcaption></figure><p id="967a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在树形图中，每个节点都包含分割数据的条件(if/else 规则)，以及该节点的一系列其他指标。Gini 指的是 Gini 杂质，节点杂质的度量，即节点内样本的同质程度。当一个节点的所有样本都属于同一个类时，我们说这个节点是纯的。在这种情况下，没有必要进一步分裂，这个节点被称为叶。Samples 是节点中实例的数量，而 value 数组显示每个类中这些实例的分布。在底部，我们可以看到节点的多数类。当<em class="lb"> export_graphviz 的<em class="lb"> filled </em>选项</em>设置为<em class="lb"> True </em>时，每个节点根据多数类进行着色。</p><p id="2124" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然很容易理解，但决策树往往会通过构建复杂的模型来过度拟合数据。过度拟合的模型很可能不能很好地概括“看不见的”数据。防止过拟合的两种主要方法是预修剪和后修剪。预修剪是指在创建树之前限制树的深度，而后修剪是指在构建树之后移除无信息的节点。</p><p id="6929" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Sklearn 学习决策树分类器只实现预剪枝。预修剪可以通过几个参数来控制，例如树的最大深度、节点保持分裂所需的最小样本数以及叶子所需的最小实例数。下面，我们在相同的数据上绘制一个决策树，这次设置 max_depth = 3。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lt"><img src="../Images/b6d5252e046f9623b40307823cb2b6c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X1GEOj4pLNNdNLFAbLn7Tg.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Decision Tree with max_depth = 3</figcaption></figure><p id="4eca" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个模型没有我们最初训练和绘制的模型那么深入，因此也没有那么复杂。</p><p id="5994" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了预修剪参数，决策树还有一系列其他参数，我们在构建分类模型时会尝试优化这些参数。我们通常通过查看准确性度量来评估这些参数的效果。为了掌握参数的变化如何影响树的结构，我们可以再次在每个阶段可视化树。我们可以利用<a class="ae kc" href="http://jupyter.org/widgets" rel="noopener ugc nofollow" target="_blank">Jupyter Widgets</a>(ipywidgets)来构建我们的树的交互式绘图，而不是每次做出改变时都绘制一个树。</p><p id="c27c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Jupyter 小部件是交互式元素，允许我们在笔记本中呈现控件。安装 ipywidgets 有两个选项，通过 pip 和 conda。</p><p id="eaa4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">和皮普一起</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="1be7" class="ll lm iq lh b gy ln lo l lp lq">pip install ipywidgets<br/>jupyter nbextension enable --py widgetsnbextension</span></pre><p id="9298" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与康达</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="5cba" class="ll lm iq lh b gy ln lo l lp lq">conda install -c conda-forge ipywidgets<br/></span></pre><p id="89d9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于这个应用程序，我们将使用<a class="ae kc" href="http://ipywidgets.readthedocs.io/en/latest/examples/Using%20Interact.html#interactive" rel="noopener ugc nofollow" target="_blank">交互</a>功能。首先，我们定义一个训练和绘制决策树的函数。然后，我们将这个函数以及每个感兴趣的参数的一组值传递给交互函数。后者返回一个我们用 display 显示的小部件实例。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="e9d8" class="ll lm iq lh b gy ln lo l lp lq">from sklearn.tree import DecisionTreeClassifier, export_graphviz<br/>from sklearn import tree<br/>from sklearn.datasets import load_wine<br/>from IPython.display import SVG<br/>from graphviz import Source<br/>from IPython.display import display                               <br/>from ipywidgets import interactive</span><span id="b0d1" class="ll lm iq lh b gy lr lo l lp lq"># load dataset<br/>data = load_wine()</span><span id="8d72" class="ll lm iq lh b gy lr lo l lp lq"># feature matrix<br/>X = data.data</span><span id="afc1" class="ll lm iq lh b gy lr lo l lp lq"># target vector<br/>y = data.target</span><span id="66d0" class="ll lm iq lh b gy lr lo l lp lq"># class labels<br/>labels = data.feature_names<br/>def plot_tree(crit, split, depth, min_split, min_leaf=0.2):</span><span id="bb16" class="ll lm iq lh b gy lr lo l lp lq">estimator = DecisionTreeClassifier(random_state = 0 <br/>      , criterion = crit<br/>      , splitter = split<br/>      , max_depth = depth<br/>      , min_samples_split=min_split<br/>      , min_samples_leaf=min_leaf)<br/>    estimator.fit(X, y)</span><span id="6de9" class="ll lm iq lh b gy lr lo l lp lq">graph = Source(tree.export_graphviz(estimator<br/>      , out_file=None<br/>      , feature_names=labels<br/>      , class_names=['0', '1', '2']<br/>      , filled = True))<br/>   <br/>    display(SVG(graph.pipe(format='svg')))</span><span id="124f" class="ll lm iq lh b gy lr lo l lp lq">return estimator</span><span id="c864" class="ll lm iq lh b gy lr lo l lp lq">inter=interactive(plot_tree <br/>   , crit = ["gini", "entropy"]<br/>   , split = ["best", "random"]<br/>   , depth=[1,2,3,4]<br/>   , min_split=(0.1,1)<br/>   , min_leaf=(0.1,0.5))</span><span id="d97f" class="ll lm iq lh b gy lr lo l lp lq">display(inter)</span></pre><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/ea24e259c41fec42d5e5151afa927e9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*apGvcwBkNm4nwNfKb6O42w.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Initial view of widget</figcaption></figure><p id="c4a1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本例中，我们公开了以下参数:</p><ul class=""><li id="8ee2" class="lv lw iq kf b kg kh kk kl ko lx ks ly kw lz la ma mb mc md bi translated">标准:节点处分割质量的度量</li><li id="20a9" class="lv lw iq kf b kg me kk mf ko mg ks mh kw mi la ma mb mc md bi translated">拆分器:每个节点的拆分策略</li><li id="ee8c" class="lv lw iq kf b kg me kk mf ko mg ks mh kw mi la ma mb mc md bi translated">max_depth:树的最大深度</li><li id="dfd9" class="lv lw iq kf b kg me kk mf ko mg ks mh kw mi la ma mb mc md bi translated">min_samples_split:节点中所需的最小实例数</li><li id="a861" class="lv lw iq kf b kg me kk mf ko mg ks mh kw mi la ma mb mc md bi translated">min_samples_leaf:一个叶节点所需的最小实例数</li></ul><p id="caef" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后两个参数可以设置为整数或浮点数。浮动被解释为实例总数的百分比。关于参数的更多细节，你可以阅读 sklearn 类<a class="ae kc" href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><figure class="lc ld le lf gt jr"><div class="bz fp l di"><div class="mj mk l"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Widget demonstration</figcaption></figure><p id="e167" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个交互式小部件允许我们修改树参数，并动态地查看图形变化。通过这种相互作用，我们能够通过揭示每一步产生的变化来掌握每个参数的影响。</p><p id="d165" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然这不是模型性能评估或参数调整的工具，但它有几个好处。通过检查深度、节点数量和叶子的纯度，它可以作为评估模型复杂性的一种手段。另一方面，它可以给我们关于数据的有用见解，因为我们可以看到树使用了多少和哪些特性。此外，我们也许能够发现清楚地将我们的样品区分到不同类别的条件。</p><p id="be5d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总之，我发现这种交互式可视化是一种有趣的工具，可以更深入地理解构建决策树的抽象过程，脱离特定的数据集，这将为我们下次为我们的一个项目构建决策树提供一个良好的开端！</p></div></div>    
</body>
</html>