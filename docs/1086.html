<html>
<head>
<title>Generalization in AI Systems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能系统中的泛化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generalization-in-ai-systems-79c5b6347f2c?source=collection_archive---------9-----------------------#2017-07-27">https://towardsdatascience.com/generalization-in-ai-systems-79c5b6347f2c?source=collection_archive---------9-----------------------#2017-07-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="87c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">泛化的主要目标是使 AI 系统在测试数据上表现得更好。同样，迁移学习是指在某些任务上训练系统，以提高它在其他任务上的表现。虽然这两种方法在实践中可能看起来非常不同，但它们有一个共同的目标:迫使神经网络或其他 ML 算法在一个场景中学习有用的概念，以便在新的场景中更好地执行。</p><p id="7c28" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，它们与欠拟合和过拟合问题密切相关:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/b8e961c1880ae1b94636512a272438fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4Iaj3m_qKLJPsnse.png"/></div></div></figure><p id="82d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当模型没有足够的能力或者训练的时间不够长以至于不能记住重要的特征时，就会发生欠拟合。当模型尺寸太大和/或训练时间太长时，会发生过度拟合，结果它会根据训练数据调整太多。这篇<a class="ae kx" href="https://arxiv.org/pdf/1611.03530.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>表明，深度神经网络可以很容易地记住整个训练数据集。考虑到这一点，许多关于正则化技术的研究正在进行，其目标是处理这些问题。</p><p id="6e25" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从数学的观点来看，正如这个<a class="ae kx" href="https://arxiv.org/pdf/1706.10239.pdf" rel="noopener ugc nofollow" target="_blank">的工作</a>所示，具有良好推广质量的目标函数的局部极小值具有低的 Hessian 范数。这意味着在这一点附近，网络输出对参数的微小变化不敏感。</p><p id="4a8a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有趣的是，通常的随机梯度下降往往以非常高的概率收敛到一个好的最小值。但是使用一些特殊的技术，你可以达到更好的效果。</p><h2 id="d967" class="ky kz iq bd la lb lc dn ld le lf dp lg jy lh li lj kc lk ll lm kg ln lo lp lq bi translated">常见的正则化方法</h2><ul class=""><li id="6930" class="lr ls iq jp b jq lt ju lu jy lv kc lw kg lx kk ly lz ma mb bi translated">降低模型的复杂性-参数越少，模型从训练数据中记忆的就越少。</li><li id="4767" class="lr ls iq jp b jq mc ju md jy me kc mf kg mg kk ly lz ma mb bi translated">早期停止—通过跟踪验证集的性能，当验证错误开始增加时，您可以立即停止训练。</li><li id="f536" class="lr ls iq jp b jq mc ju md jy me kc mf kg mg kk ly lz ma mb bi translated">权重衰减-保持较小的权重并增加稀疏度。</li><li id="37fc" class="lr ls iq jp b jq mc ju md jy me kc mf kg mg kk ly lz ma mb bi translated">DropOut —在训练过程中停用随机神经元，并强制它们的每个子集给出有意义的结果。</li><li id="1c49" class="lr ls iq jp b jq mc ju md jy me kc mf kg mg kk ly lz ma mb bi translated">批量标准化-重新缩放数据并将数据转换到一个公共值范围。</li></ul></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="bfa2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mo">最初发表于</em> <a class="ae kx" href="http://cognitivechaos.com/generalization-ai-systems/" rel="noopener ugc nofollow" target="_blank"> <em class="mo">认知混乱</em> </a> <em class="mo">。</em></p></div></div>    
</body>
</html>