# 关于数据、科学和认识论的思考

> 原文：<https://towardsdatascience.com/thoughts-on-data-science-and-epistemology-d816b3019743?source=collection_archive---------5----------------------->

这是我很久以来一直想以某种形式写的一篇文章，但是乔安娜·马尔瓦尼昨天提出的一个问题让我的想法变得清晰了一些，所以我在这里尝试阐述一下。

让我首先指出，在我的内心深处，我是一个“数据怀疑论者”，这并不意味着我不相信数据告诉我们的东西，而只是我怀疑我们是否能从数据中学到我们愿意相信的东西，不管我们可能会提出什么样的算法。“科学”是一个两步走的过程，其中数据分析是关键部分，但不是全部。

在我看来，构成“科学”的两个步骤是确定“正常”的范围，然后是观察到的现象是否代表了对正常状态(而不是高斯变化)的足够有趣的偏离。)如何建立“正态性”的范围可以是演绎的(即“公理化的”，遵循“逻辑”原则)或归纳的(即以数据为中心，没有任何先验偏见。)从本质上说，这是“理论”阶段，在这个阶段，假说形成了。从这个意义上来说，这只不过是人们从传统的“研究设计”课程中学到的东西，至少在理论上是如此。我的观点可能与教科书描述略有不同的地方是，可接受的常态范围并不普遍。人们来自不同的背景、需求和兴趣。他们每个人所认为的“正常”(推而广之，对正常的足够“有趣”的偏离)不一定是相同的。对于已经熟悉萤火虫的捕食习惯的昆虫学家来说，如果他们的先验知识被转化为假设，他们应该彼此等距分布，这正是人们通常应该期待的——即使他们自己不一定采取这一步骤。对于不知道萤火虫会互相吞食的外行人来说，它们的分布不是随机的才是有趣的部分——前提是他们能理解遍布于给定空间实际上不是随机的。

一个潜在的严重问题来自于数据消费者之间关于“通常”应该从数据中得到什么的错误沟通——一种“黑天鹅”问题。卡尼曼和特沃斯基对行为经济学的一个重要贡献是，人们对罕见事件的期望过高或过低，这取决于环境。一些人，甚至很多人，以有意义的概率期望赢得彩票，战胜股市，成为职业运动员。与此同时，人们忽略了为数不多但数量可观的人可能同时投票给奥巴马(2012 年也是如此)和特朗普的可能性，也忽略了这一“罕见”事件可能带来的后果。数据分析可以帮助实现的是精确确定正常预期的范围-整个分布，而不仅仅是平均值-以便所有受众都能就基线达成一致。请注意，这不需要用数学术语。数据可能总是为 0，除了 1-2%的情况下为 1(据我所知，这是大多数现实生活数据的典型情况——问题、投诉、评论等)。是罕见的事件。)所以正常的范围可以简单地描述为 98–99%的时间为 0，1–2%的时间为 1。如果我们不能认同手头的数据，再加上我们可能有的任何演绎洞见，那 1——2%是由什么构成的，这就是我们所能说的。请注意，这与“预测性”分析略有不同:我们期望 0，但如果我们得到 1，我们不应该太惊讶，而不是“如果 X，答案是 0，如果 y，答案是 1。”归根结底，我们处理统计数据是因为我们不知道真实世界的全貌，因此，我们不仅需要精确了解我们“知道”的事情，还需要精确了解我们不知道的事情。在这种情况下，即使我们确实知道更多关于 X 和 Y 的信息，如果 X，那么“预测”应该是答案可能是 0，但有 1 的小概率(有希望量化“小概率”，有置信区间)；如果 Y，答案可能是 1，但也有可能是 0。即使“预测”没有明确说明，这些说明也应该是数据消费者所期望的，并且应该被要求提供。(这对于理解 2016 年选举中发生的事情很重要，回想起来，2012 年也是如此:大多数人投票是可预见的，但有些人没有，在一些不寻常的情况下，试图将“通常”应用于不寻常的时期是科学的弊端。)

不过，一般来说，数据分析人员似乎在确定什么是“正常”方面做得很好。似乎很少被关注的重要部分是第二阶段:识别偏离常态的情况，并评估它们对手头目标的意义。为了实现这一点，需要重新思考数据科学中的一个常见实践:将样本划分为训练集和测试集，并使用后者来验证前者中发现的模式。作为第一步的一部分，这是一个很好的实践，因为它有助于将健壮的模式从脆弱的模式中分离出来——但前提是要小心使用，我将在下面详述。它对识别显著偏离常态没有任何作用，事实上，过于盲目地投入交叉验证实际上会抑制有趣模式的发现。

正如我所建议的，发现有趣偏差的方法颠倒了交叉验证逻辑。在交叉验证的逻辑中，敌人是过度拟合:我们希望确保模型在训练集和测试集之间产生一致的预测。如果模型为训练集预测了测试集中没有出现的东西，那么模型可能是坏的。或者，也许模型实际上是好的，甚至比我们想象的更好——我们无法说出这一点，因为我们对数据以及训练集和测试集在某种程度上的差异了解不够，这可能对我们来说并不明显。我们应该询问数据中训练集和测试集之间的偏差出现在哪里，并检查偏差来自哪里。我们应该知道足够多的概率理论来评估偏差，以样本中出现偏差的子集为条件，是否可以用随机性来解释，或者是否有更有意义的东西。也许基于小的子样本邻域排列的重采样方法可以用来评估偏差的统计显著性(这几乎不是一个革命性的建议:匹配是一种足够流行的统计技术，它只是一个相对简单的编程工作——遗憾的是，我并不特别擅长匹配大数据的小片段)。

一个常见的问题是，随着数据变得越来越大，越来越多的数据来自显而易见的、容易的、简单的和平庸的，这往往会强化刻板印象和传统智慧。有趣的潜在发现潜伏在不寻常的、奇怪的、最重要的、罕见的数据子集里，这些子集变得越来越容易被忽略，特别是当在一个由通常和可预测性主导的样本中接受稳健性检查时。与产生可靠预测的绝大多数“容易”数据点相比，样本中不寻常但有趣的子集也不那么容易预测，这并没有什么帮助，因为它们太少了，无法成为统计上有用的样本。套用托尔斯泰关于幸福家庭与不幸福家庭的名言，预测什么使家庭幸福是非常容易的，而不幸福家庭的最佳预测者是他们不幸福。如果目标是理解不幸的家庭，这是有问题的。

这值得吗？如果 99%的家庭是幸福的，或者 99%需要翻译的文件遵循可预测的模式，或者 99%的选民在 99%的时间里表现可预测，那么当模式打破时，在 1%的人身上花费大量时间是否值得呢？我觉得要看那 1%在哪里，什么时候。事实上，大多数日常琐事可能是简单的、常规的、可预测的，但那 1%可能会导致足够重大的事件——比如金融崩溃或选举失败。也许并不是所有奇怪的 1%的数据都值得严格审查，但一个好的分析师应该睁大眼睛，看看从这些不正常的小片段中可以学到什么，并尽他/她所能从中学习。