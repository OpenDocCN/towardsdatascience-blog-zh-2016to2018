<html>
<head>
<title>Experiments with a new kind of convolution</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一种新卷积的实验</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/experiments-with-a-new-kind-of-convolution-dfe603262e4c?source=collection_archive---------1-----------------------#2017-09-03">https://towardsdatascience.com/experiments-with-a-new-kind-of-convolution-dfe603262e4c?source=collection_archive---------1-----------------------#2017-09-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="71b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">警告:这篇文章假设读者对CNN有一定的了解</p><p id="873f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">卷积有很多我不喜欢的地方。它们中最大的，特别是在后面的层中的大多数权重非常接近于零。这说明这些权重中的大多数没有学到任何东西，也没有帮助网络处理任何新信息。</p><p id="de5c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以我想修改卷积运算来解决这个问题。这篇博文强调了我在这个方向上做的实验和结果。</p><h2 id="f156" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">实验#1</h2><p id="c5df" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">本质上，每个2D卷积运算都是一个矩阵乘法运算。其中矩阵的维数为(内核大小x内核大小x输入通道，输出通道)，假设卷积维数为(内核大小，内核大小，输入通道，输出通道)。为了简单起见，我在文章中称这个矩阵为维度为(m，n)的卷积矩阵。</p><p id="d0e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们能够保持卷积矩阵的列是正交的(以可微分的方式)，我们可以确保输出特征图中的每个通道捕获任何其他特征图中不存在的信息。更重要的是，这可以帮助我们创建权重更容易解释的神经网络。</p><p id="ad9b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">是的，使用线性代数中的一些技巧(一个是householder变换，另一个是givens旋转)，有一些可微的方法来确定这一点。我使用户主转换，因为它在GPU上要快得多。</p><p id="6083" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">代码在<a class="ae lj" href="https://github.com/singlasahil14/orthogonal-convolution/blob/master/vecgen_tf.py" rel="noopener ugc nofollow" target="_blank">这个文件</a>里。</p><p id="bad9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个想法是这样的:</p><p id="d6f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不是在卷积滤波器中保持所有mxn个可训练变量，在偏差中保持所有n个可训练变量，而是以可微分的方式从另一组可训练变量生成滤波器和偏差。</p><p id="f40f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更具体地，对于维数为(m，n)的卷积矩阵，创建n个维数为m的向量。第一个向量(比如v1)将有m-n+1个可训练变量(开头用n-1个零填充)，第二个(比如v2) m-n+2个(用n-2个零填充)，第三个(比如v3)，m-n+3个(用n-3个零填充)，等等。接下来归一化所有这些向量。使用这些向量创建n个householder矩阵，按照v1*v2*v3…*vn的顺序乘以向量。合成矩阵的维数为m×m，并且是正交的。取这个矩阵的前n列。结果矩阵的大小为(m，n)。并将其用作卷积矩阵。</p><p id="011a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看起来这个操作非常耗时，但实际上对于一个3×3×64×128的卷积，意味着576×576大小的矩阵的128次矩阵乘法。考虑到这种卷积是在256x256大小的图像上执行的(这意味着(256x256)x(3x3x64x128) flops)，这并不算什么。</p><p id="8b8e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您必须创建一个偏置和过滤器，用m + 1替换m进行上述过程，从大小为(m+1，n)的结果矩阵中提取最上面的行并将其用作偏置，使用剩余的(m，n)矩阵作为过滤器。</p><p id="a10a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您使用批量规范化，这种想法将不起作用，因为列的正交性假设不成立。</p><p id="9087" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于这些实验，我使用了具有类似vgg架构的cifar-10。代码是在<a class="ae lj" href="https://github.com/singlasahil14/orthogonal-convolution/blob/master/cifar_deep.py" rel="noopener ugc nofollow" target="_blank">相同的回购</a>。</p><p id="e77c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">结果是<strong class="jp ir">非常非常</strong>T6】令人失望。</p><div class="lk ll lm ln gt ab cb"><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/cc3bbbdc8eb7a26192b89b68e70ecb3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*sUD3WKAL7qcFAvp3y5eFJQ.png"/></div></figure><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/9f094f0e8728c7ee701a33eba458759d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Qo-lxnUpQsY5C_4ccITo-w.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk mf di mg mh">plots of cross_entropy and accuracy on training data</figcaption></figure></div><div class="ab cb"><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/0a3506d34149faf224a5ecfde7eaae91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*94pDE7sRaPIW39dCpQjBfw.png"/></div></figure><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/61ca5445673b78785c303cf05879c11f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*XexTPzLoNEOhfXbocpxJ3g.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk mf di mg mh">plots of cross_entropy and accuracy on validation data</figcaption></figure></div><p id="ed0d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以看出，基线和正交卷积之间的所有曲线的结果都很差。更重要的是，正交卷积的训练时间明显更长。这是时间单位的曲线图:</p><figure class="lk ll lm ln gt lp gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/0eff6c4ead493ffae881dc1688f8f2fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*tuE_aoMKVkX3gHIYaetUYQ.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">plot of training time per iter for baseline/orthoconv</figcaption></figure><p id="90be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从图中可以看出，时间平均高出7倍。有很多方法可以加速这段代码，但是由于结果不佳，我没有朝那个方向前进。</p><p id="cd1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我的直觉是，它不起作用的原因是，由于所有列向量必须正交的约束，模型的优化前景受到严重限制。为了测试我的直觉，我做了下一个实验。</p><h2 id="e8ca" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">实验#2</h2><p id="2dea" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">在下一个实验中，我没有使用householder乘法产生的卷积权重和偏差，而是添加了另一个损失项，称为“正交性损失”。正交性损失计算如下:</p><p id="a7c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设卷积矩阵的维数为(m，n)，偏差向量的维数为(n)，连接这两个向量形成一个维数为(m+1，n)的矩阵。称这个矩阵为m。计算这个矩阵的列范数，称它为N(它是一个维数为(1，N)的矩阵)。</p><p id="4845" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">计算转置(M)*M .计算转置(N)*N .并将两个矩阵相除。在结果矩阵中，a(i，j)包含矩阵m中索引I和j处的列向量之间角度的余弦值。通过平方此矩阵中的所有元素来创建新矩阵。求矩阵中所有元素的和(除了轨迹)。将所有卷积层的这个值相加，我们将结果值称为正交损失或正交损失。我将其乘以一个称为正交权重的超参数，并将其添加到总损耗中。</p><p id="9dc6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我使用不同的正交权重值做了实验:我尝试了0.1、1、10、100，inf对应于前面实验中描述的卷积。</p><figure class="lk ll lm ln gt lp gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/d6506d3942cdbc8666f7c62bc971b22d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*WzxFtOpjb7Hprt35XfXYMQ.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">orthogonality loss for all different experiments</figcaption></figure><p id="d648" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">初始正交性损失(没有乘以权重)约为40。这远远大于交叉熵本身。然而，网络学会了在几次迭代内将它推到零。</p><p id="beb9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从上图可以看出，网络很快学会了保持卷积矩阵中的所有列向量正交。当正交权重设置为零时，正交性损失不断增加，而对于其他情况(0.1、1、10、100)，它稳定在接近零的值。<strong class="jp ir">这意味着如果我们增加加权正交性损失</strong>，卷积矩阵学习的权重确实是正交的。</p><div class="lk ll lm ln gt ab cb"><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/c070267e375147eb08631172b860177d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*NZJhNvJrSJmG_DSctelTzA.png"/></div></figure><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/e608a0de72bf784a82b9ba613bd98599.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*UNabuLddIScWAFQ3Ca8kAQ.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk mf di mg mh">cross entropy and accuracy on training data</figcaption></figure></div><p id="166e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从上图可以看出，随着正交权重的增加，网络变得难以训练。</p><div class="lk ll lm ln gt ab cb"><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/38561ba13505714866106ea650432c08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*mm8kIKHXUttP_Sw_fP4aMQ.png"/></div></figure><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/12b02eed0015415d8903c8434175f09f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*1xA2J-tNxGW7Ti4wmhau1w.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk mf di mg mh">cross entropy and accuracy on validation data</figcaption></figure></div><p id="2fcc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是该网络实现的验证准确度/交叉熵非常接近正交权重为0的验证准确度/交叉熵。又一次失望，因为我希望它会给出更好的结果。但至少比以前的结果要好。</p><p id="f7c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，为了更仔细地检查，我决定只绘制正交权重0和正交权重0.1。</p><div class="lk ll lm ln gt ab cb"><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/c98e9ffec3c5a1442d881fd3dfd15754.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*WrvHQbKOAFBJfee3f8_n5w.png"/></div></figure><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/dac1c22c42ce5c1ace4ede8f444ba7c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*IJaSOaVg2sRMEMfge6UDUA.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk mf di mg mh">cross entropy and accuracy on training data</figcaption></figure></div><div class="ab cb"><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/57f9a80344da6746e9e7d28043804e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*s_mrR2MLcyAv8o0cuRMT0Q.png"/></div></figure><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/8f91180e05907cd97f91314a73627016.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*uEY2R3CZboLtKQXgowIFkg.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk mf di mg mh">cross entropy and accuracy on validation data</figcaption></figure></div><p id="f0fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以看出，两个网络收敛到相同的验证交叉熵和准确度。正交权重为0的网络训练交叉熵较高。这种方式表明，增加正交权重会使网络更好地泛化(更高的训练损失，但相同的验证损失)。</p><p id="6421" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，为了测试这是否适用于其他数据集，我决定用CIFAR-100进行同样的实验。</p><div class="lk ll lm ln gt ab cb"><figure class="lo lp mj lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/653d24e2458ca3d997bf2007f5b20ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*6RGbMwqafZVNWBdu5LMsbw.png"/></div></figure><figure class="lo lp mj lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/a2eb45a340ec61c0bcbb0b3264d2f774.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*3k3mw4tkXMI299TUE_EXtA.png"/></div></figure><figure class="lo lp mj lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/27e233671076c875fe021e6ca6e2d386.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*vsVtjPbdGQhtoQOw1HIO0A.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk mk di ml mh">cross entropy, accuracy and ortho_loss on training data</figcaption></figure></div><div class="ab cb"><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/56d1b62bb0db9a508a1e4d5fe82aa8d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*_RkSrZzkfK_lAkROMF7B4w.png"/></div></figure><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/99b5926d51a64d2d24261e1c5e2a31d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*pVKDJR4s2bvRmPTwtdXrGw.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk mf di mg mh">cross entropy and accuracy on validation data</figcaption></figure></div><p id="8973" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些结果确实推广到CIFAR-100。更重要的是，<strong class="jp ir">我们可以看到，正交性权重为0.1的网络表现出与正交性权重为0的网络相同的验证精度。如ortho_loss比较图所示，正交权重为0.1的网络学习卷积矩阵中的正交列。</strong></p><h2 id="ca5b" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">实验#3</h2><p id="fa0e" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">接下来，被这些结果激起了兴趣，我决定尝试将<strong class="jp ir">的相同想法用于递归神经网络</strong>。在这个repo 中使用<a class="ae lj" href="https://github.com/singlasahil14/char-rnn" rel="noopener ugc nofollow" target="_blank">代码(除非特别说明，否则是默认架构)，我决定通过给LSTM增加正交性损失来进行实验。注意，正交性损失= sum_square(I-transpose(M)*M)其中M是隐藏到隐藏状态矩阵。它不同于正交性损失，在正交性损失中，我们不关心所有列是否有范数1。</a></p><p id="0e50" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是结果(我对正交性损失尝试了这些不同的权重:0.001、0.0001、0.00001、0.000001、0.000001、0):</p><div class="lk ll lm ln gt ab cb"><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/ad07882ce0e5402d5abc96cc32ce54d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*jNy9ehobK5laJPs9kC1EFQ.png"/></div></figure><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/ee474d6a9367faf0a16144a75ea39cc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*yon7y73x6OX4t6dUBU-Blg.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk mf di mg mh">cross entropy and orthonormality loss as network gets trained for 20 epochs</figcaption></figure></div><p id="8b2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同样，当我们减小正交权重时，交叉熵减小得更快。接下来，为了更好地比较两个最佳模型的正交权重，我决定绘制权重为0和0.0000001的交叉熵和正交损失值。</p><div class="lk ll lm ln gt ab cb"><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/e33500341284f4e6d76d926d024fcd28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*64tUFgoXWCpbg4eU_scsOw.png"/></div></figure><figure class="lo lp lq lr ls lt lu paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/647e33a75a72bc08a69935fa84531193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ik3TXs2sRJXBCo3qHyQqtw.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk mf di mg mh">cross entropy and orthonormality loss as network gets trained for 20 epochs for only two weights (0, 0.0000001)</figcaption></figure></div><p id="34e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同样没有太大的差别，但它设法匹配基线结果。</p><h2 id="d0ef" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">结论</h2><p id="b315" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">我从这个小项目中学到的最重要的事情是神经网络非常复杂。即使看起来不错的想法也可能失败，因为大多数时候，当人们想到一个想法时，他们只是根据网络的最终训练状态来考虑它(就像我的情况一样，我希望矩阵的列是正交的)。但是，如果您考虑到网络必须遍历一条路径以进行优化的事实，大多数好的想法似乎马上就变得索然无味了(就像我的情况一样，很明显，我在实验#1中严格限制了模型可以采用的路径)。</p><p id="ca64" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，我尝试了许多非标准(我自己的发明)的调整，使网络击败基线。没有一个管用。实际上，我很高兴我成功地重现了基线结果，尽管我添加了一个比交叉熵本身更大的损失项。</p><p id="8b7e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我也对图像分割做了同样的实验。得出了相似的结果。为了简洁起见，这里没有提到它们。</p><p id="fef7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我也相信这项研究有助于更好地可视化卷积层的权重。我计划下一步探索这个问题。</p><p id="784d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本博客中给出的所有实验都可以使用这些回复进行复制:</p><div class="mm mn gp gr mo mp"><a href="https://github.com/singlasahil14/orthogonal-convolution" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab fo"><div class="mr ab ms cl cj mt"><h2 class="bd ir gy z fp mu fr fs mv fu fw ip bi translated">信号/正交卷积</h2><div class="mw l"><h3 class="bd b gy z fp mu fr fs mv fu fw dk translated">正交卷积— Github repo用于我的正交卷积思想的实验</h3></div><div class="mx l"><p class="bd b dl z fp mu fr fs mv fu fw dk translated">github.com</p></div></div><div class="my l"><div class="mz l na nb nc my nd lz mp"/></div></div></a></div><div class="mm mn gp gr mo mp"><a href="https://github.com/singlasahil14/char-rnn" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab fo"><div class="mr ab ms cl cj mt"><h2 class="bd ir gy z fp mu fr fs mv fu fw ip bi translated">新加坡14/char-rnn</h2><div class="mw l"><h3 class="bd b gy z fp mu fr fs mv fu fw dk translated">通过在GitHub上创建一个帐户来为char-rnn开发做贡献。</h3></div><div class="mx l"><p class="bd b dl z fp mu fr fs mv fu fw dk translated">github.com</p></div></div><div class="my l"><div class="ne l na nb nc my nd lz mp"/></div></div></a></div><blockquote class="nf ng nh"><p id="2f88" class="jn jo ni jp b jq jr js jt ju jv jw jx nj jz ka kb nk kd ke kf nl kh ki kj kk ij bi translated">如果您喜欢这篇文章，请点击下面的小拍手图标帮助他人找到它。非常感谢！</p></blockquote></div></div>    
</body>
</html>