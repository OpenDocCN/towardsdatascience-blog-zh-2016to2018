<html>
<head>
<title>Can Machine Learn the Concept of Sine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器能学习正弦的概念吗</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/can-machine-learn-the-concept-of-sine-4047dced3f11?source=collection_archive---------2-----------------------#2017-06-14">https://towardsdatascience.com/can-machine-learn-the-concept-of-sine-4047dced3f11?source=collection_archive---------2-----------------------#2017-06-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="0d47" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">众所周知，人工神经网络擅长模拟任何函数。我想知道他们是否能更进一步，学习一个函数的广义模型。为简单起见，让我们尝试学习一个只有一个参数 A 的正弦函数，它控制频率:</p><p id="b340" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">y = sin(A*x)</p><p id="4773" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于我们人类来说，一旦我们理解了正弦函数，我们就知道它在任何参数 A 下的行为。如果我们看到一个部分正弦波，我们就可以知道 A 应该是什么，我们可以将该波外推至无穷大。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/e9a907d32496c1dad01cb10aa4fd062b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-__G7bbAMC3tFTA095iBg.png"/></div></div></figure><p id="8de4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ML 能预测一个它没见过的参数 A 的正弦波吗？</p><h1 id="9ad9" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">实验设置</h1><p id="11e8" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">让我们做一个实验来找出答案。我们把这个问题描述为一个时间序列预测问题。给定一些与函数 sin(A*x)匹配的数据点，尝试预测未来值。当然，挑战在于我们想学习正弦的一般概念。我们希望能够预测未来值，甚至是我们的模型在训练期间从未见过的参数(A)。</p><p id="06d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将使用 Keras，并尝试几种不同的模型——常用于函数建模的全连接网络，常用于模式识别的 CNN，以及常用于序列建模(如 NLP)的 LSTM。</p><p id="6457" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于每个模型，我们将在(0.06，0.12)范围内的参数 A 下进行训练。对于测试，我们将尝试在 0.033、0.06、0.083 和 0.163 的值下进行预测。这样，我们可以看到在训练范围内有 2 个参数，在训练范围外有 2 个参数的性能。</p><p id="effa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在测试过程中，我们将从正确 A 下的真实 sin(A*x)值历史开始，这相当于给人一个部分正弦波。当我们进行预测时，值 y 的未来预测将使用 y 的早期预测值。举个例子，假设我们从 40 个真实数据样本开始，y[0] … y[39]，y[i] = sin(A*i)。我们使用我们的模型来预测 y[40]。然后我们会用 y[1] … y[40]，其中 y[40]是预测值，来预测 y[41]。</p><p id="e9d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们这样做，而不是使用 sin(A*i)来预测 y[i+1]的原因是，通过累积误差，使我们的模型中的误差更容易看到。</p><h1 id="eb50" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">全连接网络</h1><p id="ea5f" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">在 Keras 中，完全连接的层称为密集层。我们在 FC 网络中使用 3 个密集层。</p><pre class="km kn ko kp gt ma mb mc md aw me bi"><span id="ffda" class="mf ky iq mb b gy mg mh l mi mj">model = models.Sequential()<br/>model.add(Dense(100, input_shape=(INPUT_COUNT,)))<br/>model.add(LeakyReLU(alpha=0.03))<br/>model.add(Dense(100))<br/>model.add(LeakyReLU(alpha=0.03))<br/>model.add(Dense(1))</span></pre><p id="845f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输入形状是 INPUT_COUNT(定义为 40)之前的数据点。最后一个密集层有一个单元，因为我们在给定前 40 个值的情况下预测下一个值。</p><p id="3e73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是结果。绿色虚线是预测。同样，在训练期间，参数 A 在 0.06 至 0.12 的范围内。</p><div class="km kn ko kp gt ab cb"><figure class="mk kq ml mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/db1f0271cf8f9b979d53d2c3602033ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*h1kYGRrdK1topH1fYAeaTA.png"/></div></figure><figure class="mk kq ml mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/a267f4ae5610478abeca0b478734355a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ZzgYuY9HgWLs9CM_VxhNtQ.png"/></div></figure></div><div class="ab cb"><figure class="mk kq ml mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/d487b8a6a9c1b88b121fe8f1e6f81289.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*kqg4Q12MuMSMFtmMrlYuqg.png"/></div></figure><figure class="mk kq ml mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/1277b68200b338a6aef9be6c205905fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*A_2-V3hn3KBgZx7orELzJw.png"/></div></figure></div><p id="c3f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们所看到的，我们的模型很好地管理了 0.06 和 0.083 的值，但是对于 0.033 和 0.163 却表现不佳。基本上一旦参数 A 超出训练范围，我们的模型就处理不了。</p><p id="2267" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，在图表上，我们的函数不是从 0 开始，因为我们使用了 40 个数据点作为历史数据来输入模型。所有图表都偏移了这 40 个数据点。</p><h1 id="bbe2" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">美国有线新闻网；卷积神经网络</h1><p id="adda" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">我们使用 Conv1D 层，因为我们的数据是一维的。</p><pre class="km kn ko kp gt ma mb mc md aw me bi"><span id="3aca" class="mf ky iq mb b gy mg mh l mi mj">model = models.Sequential()<br/>model.add(Conv1D(100, 3, strides=1, input_shape=(INPUT_COUNT, 1)))<br/>model.add(LeakyReLU(alpha=0.03))<br/>model.add(Conv1D(100, 3, strides=1))<br/>model.add(LeakyReLU(alpha=0.03))<br/>model.add(Flatten())<br/>model.add(Dense(100))<br/>model.add(LeakyReLU(alpha=0.03))<br/>model.add(Dense(1))</span></pre><p id="a8a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于卷积，我们使用大小为 3 的滤波器，步长为 1。我们没有做最大池，因为位置在回归问题中很重要。</p><p id="75a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与 FC 网络类似，输入是 40 个先前的数据点，输出是曲线上的下一个点。</p><div class="km kn ko kp gt ab cb"><figure class="mk kq ml mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/dd04a58f29fb5f5f4660a3738490b886.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*mLS2sLZMmloAf1ZqlLORvw.png"/></div></figure><figure class="mk kq ml mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/da5b0c4692cc102fa939e2e73628b2aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*NsaOPhKGAZzKO5lRZznIrA.png"/></div></figure></div><div class="ab cb"><figure class="mk kq ml mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/d9f53ccc8675763bb1e79f72e7962c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*GKYNuqo_jdiq363Hy66F4A.png"/></div></figure><figure class="mk kq ml mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/04cac243eb953506964786927a8bcb77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Ccs6Chv9i7qX5q5wI4pYbQ.png"/></div></figure></div><p id="c5ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">结果类似于完全连接的网络。在给定任何参数 a 的情况下，它不能学习正弦的一般公式。</p><h1 id="c77c" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">LSTM</h1><p id="e56e" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">LSTM 网络保留了它在过去看到的数据的记忆。因此，我们输入的数据是不同的形式。我们只需要一次输入一个数据点，而不是 FC 和 CNN 模型过去 40 个数据点的历史记录。如下图所示，input_batch_size 为(1，1，1)。</p><pre class="km kn ko kp gt ma mb mc md aw me bi"><span id="7f40" class="mf ky iq mb b gy mg mh l mi mj">model = models.Sequential()<br/>model.add(LSTM(100, batch_input_shape=(1, 1, 1), return_sequences=True, stateful=True))<br/>model.add(LSTM(100, return_sequences=False, stateful=True))<br/>model.add(Dense(1))</span></pre><p id="edc1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为我们一次只能输入一个数据点，因为后面的数据点依赖于前面的数据点建立的 LSTM 内部状态，所以我们不能利用硬件中的并行性。结果训练真的很慢。因为这个原因，我没有对 LSTM 参数做太多的实验。这是结果。</p><div class="km kn ko kp gt ab cb"><figure class="mk kq ml mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/ff6678560072bfab7067f3acaceffd1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*wmaTTl01Z8JDT2G6jZD9BQ.png"/></div></figure><figure class="mk kq ml mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/c60f4d74ba2da8b3c5b0e76b54740cd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Ef50GnkZwnWmRgkD7fw5Pg.png"/></div></figure></div><div class="ab cb"><figure class="mk kq ml mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/7a6698a22593e0559700b6f5c4579876.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*3LcvZ2dq4Zn5BT6BnoXfGg.png"/></div></figure><figure class="mk kq ml mm mn mo mp paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/9d3ef98512eae0f96f79831cf1e42a7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*j1TSXJdKrlZn3-bsQ7LB9A.png"/></div></figure></div><p id="9af4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">结果比 FC 和 CNN 还惨。同样，这可能是因为我没有做足够的工作。另一方面，我也不指望它表现得更好，因为其他模型有足够的历史数据，而且数据是重复的。</p><h1 id="ac9a" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">结论</h1><p id="c1fb" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">我发现这个问题很有趣，因为在生活中我们经常需要使用历史数据来预测时间序列中的未来。如果神经网络模型能够推广重复模式的概念，甚至在频率变化时也能预测模式，那么它们在我们的应用中将更加强大。</p><p id="643f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们的实验中，我们看到模型都学习了正弦函数的一般形状，但未能在训练范围之外的频率下生成未来的数据点。</p><p id="911c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里的结论是 NN 模型很难概括正弦的概念，还是简单地说我很烂，没能建立一个能解决这个问题的模型？我的代码在 github 上:</p><div class="mq mr gp gr ms mt"><a href="https://github.com/looselyconnected/ml-examples" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd ir gy z fp my fr fs mz fu fw ip bi translated">松散连接/ml-示例</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">机器学习的例子。</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">github.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh kv mt"/></div></div></a></div><p id="47ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请使用它，将您的意见发送给我，并让我知道是否有更好的模型可以解决这个问题。谢谢你。</p></div></div>    
</body>
</html>