<html>
<head>
<title>7 points to ponder, before you use GPUs to speed up Deep Learning apps</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在使用 GPU 加速深度学习应用程序之前，需要思考 7 点</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/7-points-to-ponder-before-you-use-gpus-to-speed-up-deep-learning-apps-cfc53dc29b54?source=collection_archive---------19-----------------------#2018-08-22">https://towardsdatascience.com/7-points-to-ponder-before-you-use-gpus-to-speed-up-deep-learning-apps-cfc53dc29b54?source=collection_archive---------19-----------------------#2018-08-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ff72fbec65d68ee0ed115620eddb880f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tILV2OOWbX-OMKlPnzVD6g.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Deep Learning and GPUs</figcaption></figure><h1 id="27b3" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">介绍</h1><p id="9b87" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">鉴于围绕其加速的宣传，使用<a class="ae ly" href="https://en.wikipedia.org/wiki/Graphics_processing_unit" rel="noopener ugc nofollow" target="_blank">GPU</a>进行深度学习训练和推理是很有诱惑力的。然而，为了有效地使用这些资源，更深入地了解加速的来源是很重要的。在本文中，我们将研究一个典型的 DL 应用程序的性能依赖性。我们将使用英伟达的大肆宣传的机器 DGX-1 作为例子，但以下几点是通用的，足以适用于通用 GPU 或 TPU。</p><h1 id="2ad9" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">1.五金器具</h1><p id="3ddd" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">底层硬件规格在决定 DL 加速方面起着最重要的作用。对于典型的 DL 应用程序来说，拥有一个快速的硬件并不能自动转化为最好的加速，除非你非常小心。</p><ol class=""><li id="aedc" class="lz ma iq lc b ld mb lh mc ll md lp me lt mf lx mg mh mi mj bi translated"><a class="ae ly" href="https://en.wikipedia.org/wiki/Graphics_processing_unit" rel="noopener ugc nofollow" target="_blank">GPU 数量</a> (8 特斯拉 P100s)</li><li id="fc4e" class="lz ma iq lc b ld mk lh ml ll mm lp mn lt mo lx mg mh mi mj bi translated"><a class="ae ly" href="https://en.wikipedia.org/wiki/CUDA" rel="noopener ugc nofollow" target="_blank"> CUDA </a>内核(28672 个 CUDA 内核)</li><li id="4c89" class="lz ma iq lc b ld mk lh ml ll mm lp mn lt mo lx mg mh mi mj bi translated"><a class="ae ly" href="https://en.wikipedia.org/wiki/Tensor_processing_unit" rel="noopener ugc nofollow" target="_blank">张量</a>核心(无)</li><li id="b940" class="lz ma iq lc b ld mk lh ml ll mm lp mn lt mo lx mg mh mi mj bi translated">GPU 内存(128GB)</li><li id="90ed" class="lz ma iq lc b ld mk lh ml ll mm lp mn lt mo lx mg mh mi mj bi translated"><a class="ae ly" href="https://en.wikipedia.org/wiki/Shared_graphics_memory" rel="noopener ugc nofollow" target="_blank">系统内存</a> (512GB，2,133 MHz DDR4)</li></ol><h1 id="df25" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">2.数据传送</h1><p id="a6b5" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">由于 DL 应用程序执行数据密集型操作，因此在训练和推理过程中理解硬件组件之间的数据流非常重要。今天的高档机器中的数据传输范围从网络上的几千兆字节/秒到 GPU 集团内的超过 100 千兆字节/秒。这可能会对 DL 应用程序的性能产生高达 100 倍的影响。例如，采用<a class="ae ly" href="https://en.wikipedia.org/wiki/NVLink" rel="noopener ugc nofollow" target="_blank"> nVLink </a>的特斯拉 P-100 架构允许 GPU 之间的数据传输速度高达 160GB/s，同时，它可以通过 HDD/SSD、NIC 或 RAM 接口来降低速度。</p><p id="5a09" class="pw-post-body-paragraph la lb iq lc b ld mb lf lg lh mc lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ij bi translated">记住这一点很有帮助，数据传输链的速度与其最慢的链路一样快。下面是数据传输所依赖的速度的组成部分。</p><ol class=""><li id="3094" class="lz ma iq lc b ld mb lh mc ll md lp me lt mf lx mg mh mi mj bi translated">硬盘驱动器或 HDD(典型速度 100MB/s)</li><li id="8260" class="lz ma iq lc b ld mk lh ml ll mm lp mn lt mo lx mg mh mi mj bi translated">固态硬盘或 SSD(典型速度 500MB/s)</li><li id="3700" class="lz ma iq lc b ld mk lh ml ll mm lp mn lt mo lx mg mh mi mj bi translated"><a class="ae ly" href="https://en.wikipedia.org/wiki/PCI_Express" rel="noopener ugc nofollow" target="_blank"> PCI express </a> 5(典型速度 4GB/s-60GB/s)</li><li id="17c4" class="lz ma iq lc b ld mk lh ml ll mm lp mn lt mo lx mg mh mi mj bi translated"><a class="ae ly" href="https://en.wikipedia.org/wiki/Network_interface_controller" rel="noopener ugc nofollow" target="_blank">网络接口卡</a>(典型速度 16Gb/s)</li></ol><h1 id="a2fd" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">3.基础设施软件</h1><p id="4a63" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">基础设施软件包括以下选项</p><ol class=""><li id="c413" class="lz ma iq lc b ld mb lh mc ll md lp me lt mf lx mg mh mi mj bi translated"><strong class="lc ir">操作系统:</strong> Ubuntu 14.04 LTS 依然是热门选择。在将机器用于 DL 应用程序之前，请确保删除未使用的守护程序和服务(例如 ftp、防火墙等)。</li><li id="95f1" class="lz ma iq lc b ld mk lh ml ll mm lp mn lt mo lx mg mh mi mj bi translated"><strong class="lc ir">编程语言和 DL 框架:</strong> <a class="ae ly" href="https://www.python.org/" rel="noopener ugc nofollow" target="_blank"> Python </a>和支持多线程的<a class="ae ly" href="http://tensorflow.org" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>是一个流行的组合。深度学习库中对优化算法(如 Allreduce、Parameter-Server &amp; Worker 等)的本机支持可以平衡数据管道，这对于利用快速硬件有很大帮助。其他值得一提的流行 DL 库有<a class="ae ly" href="http://caffe.berkeleyvision.org/" rel="noopener ugc nofollow" target="_blank"> Caffe </a>、<a class="ae ly" href="https://www.microsoft.com/en-us/cognitive-toolkit/" rel="noopener ugc nofollow" target="_blank"> CNTK </a>和<a class="ae ly" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> pyTorch </a>。</li></ol><h1 id="cc6d" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">4.深度学习模型</h1><p id="3a33" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">如果您使用现成的深度学习模型(如 VGG16、resNet50 等)进行推理，请考虑使用针对您选择的深度学习库优化的版本，以确保神经网络结构针对效率进行了优化。Tensorflow 社区继续支持新的 DL 模型，以实现更大的利益。你可以在这里<a class="ae ly" href="https://github.com/tensorflow/models" rel="noopener ugc nofollow" target="_blank">访问它们</a>。</p><h1 id="d128" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">5.数据集</h1><p id="13b9" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">数据集或样本的大小是一个重要的指标。一般来说，现实生活中的数据要比数据挖掘应用程序所需的数据大几个数量级。例如，VGG-16 仅拍摄 224x224 像素的图像，而典型的手机相机则拍摄 4000x3000 像素的图像。与在训练循环或推断中避免不必要的带宽消耗相反，在此之前减少一次大小是值得的。</p><h1 id="05c8" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">6.培养</h1><p id="2e25" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">对 DL 训练原语和分发策略的原生 GPU 支持使得 DL 开发人员工作效率更高。</p><p id="5dda" class="pw-post-body-paragraph la lb iq lc b ld mb lf lg lh mc lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ij bi translated"><strong class="lc ir">样本 DL 训练原语</strong>包括池化、批量标准化、丢弃、ReLU、Sigmoid、Softmax 等</p><p id="75df" class="pw-post-body-paragraph la lb iq lc b ld mb lf lg lh mc lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ij bi translated"><strong class="lc ir">分发策略</strong>涵盖镜像策略、PS-worker、All reduce 等技术。</p><p id="9b20" class="pw-post-body-paragraph la lb iq lc b ld mb lf lg lh mc lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ij bi translated"><strong class="lc ir">分布式</strong>风味的<strong class="lc ir">反向传播</strong>算法如自动微分对计算效率起着至关重要的作用。</p><p id="30c4" class="pw-post-body-paragraph la lb iq lc b ld mb lf lg lh mc lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ij bi translated">确保像<a class="ae ly" href="http://nvidia.com" rel="noopener ugc nofollow" target="_blank"> nVidia </a>和<a class="ae ly" href="http://amd.com" rel="noopener ugc nofollow" target="_blank"> AMD </a>这样的 GPU 提供商为你选择的深度学习库提供支持是很重要的。nVidia 为 DL 训练原语提供了成熟的 cuDNN，而 AMD 的 ROCM 还有很多需要赶上。</p><h1 id="82a1" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">7.推理</h1><p id="6590" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">DL 推断可能会拖累大型 DL 模型，如<a class="ae ly" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> resNet152 </a>，并且可以在生产模式中使用 GPU。推理优化包括以下两种方法。</p><p id="8199" class="pw-post-body-paragraph la lb iq lc b ld mb lf lg lh mc lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ij bi translated"><strong class="lc ir">层和张量融合</strong>改变图的结构，在不影响输出精度的情况下提高推理性能。</p><p id="efa8" class="pw-post-body-paragraph la lb iq lc b ld mb lf lg lh mc lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ij bi translated"><strong class="lc ir">精度校准</strong>降低神经元精度(从 FP32 到 FP24、FP16 或 INT8)会对 DL 模型精度产生不利影响，因此必须谨慎使用。</p><h1 id="336e" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">摘要</h1><p id="9768" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">下图总结了我们上面讨论的 7 点。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/810a432226bcaf239fc26205b57addb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6tQTNZQOQFnDiJkEvSEheA.jpeg"/></div></div></figure><h2 id="4f90" class="mw kd iq bd ke mx my dn ki mz na dp km ll nb nc kq lp nd ne ku lt nf ng ky nh bi translated">信用</h2><p id="2a34" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这些数据大部分是为《设计自动化中的机器智能》这本书收集的。</p></div></div>    
</body>
</html>