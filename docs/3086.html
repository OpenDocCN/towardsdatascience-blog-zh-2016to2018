<html>
<head>
<title>Under The Hood of Neural Networks. Part 1: Fully Connected.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在神经网络的保护下。第 1 部分:完全连接。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/under-the-hood-of-neural-networks-part-1-fully-connected-5223b7f78528?source=collection_archive---------0-----------------------#2018-04-08">https://towardsdatascience.com/under-the-hood-of-neural-networks-part-1-fully-connected-5223b7f78528?source=collection_archive---------0-----------------------#2018-04-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/0a357f9e71477fd73fae48beb3733778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BWjHc_KWpgcF6ryZmd4sdw.png"/></div></div></figure><div class=""/><p id="a27c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">深度学习进展很快，快得令人难以置信。拥有如此大的 AI 开发人员社区的原因之一是我们有许多非常方便的库，如 TensorFlow、PyTorch、Caffe 等。正因为如此，通常实现一个神经网络不需要任何该领域的高深知识，这是非常酷的！然而，随着任务复杂性的增加，了解内部实际发生的事情会非常有用。这些知识可以帮助您选择激活函数、权重初始化、理解高级概念等等。所以在这组文章中，我将解释不同类型的神经网络的推理和训练过程背后的数学原理。</p><p id="2797" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这篇文章我将致力于最基本类型的神经网络:全连接网络。尽管纯全连接网络是最简单的网络类型，但理解它们的工作原理是有用的，原因有二。首先，与其他类型的网络相比，它更容易理解后面的数学。第二，全连接层仍然存在于大多数模型中。</p><p id="72c0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这里，我将解释任何监督神经网络中的两个主要过程:全连接网络中的前向和后向传递。这篇文章的重点将放在称为反向传播的概念上，它成为现代人工智能的主力。</p><h1 id="f98d" class="kw kx jb bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">前进传球</h1><p id="09e9" class="pw-post-body-paragraph jy jz jb ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated"><em class="lz">前向传递</em>基本上是一组将网络输入转换到输出空间的操作。在推理阶段，神经网络仅依赖于正向传递。让我们考虑一个具有两个隐藏层的简单神经网络，它试图将二进制数(这里是十进制 3)分类为偶数或奇数:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/74c4964aa3d59ce77b6a98784a609176.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*ssflDzDB3VIzmwjl1OHXOg.png"/></div></figure><p id="d66c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里我们假设除了最后几层的神经元，每个神经元都使用<a class="ae mf" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU </a>激活函数(最后一层使用<a class="ae mf" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> softmax </a>)。激活函数用于将非线性引入系统，这允许学习复杂的函数。让我们记下在第一个隐藏层中进行的计算:</p><p id="ab7d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">神经元 1(顶部):</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mg"><img src="../Images/60854a028e6f07b79c907a17ff1f5b71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5JJKM82ackOvZ7DPQJpU4g.png"/></div></div></figure><p id="9727" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">神经元 2(底部):</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mh"><img src="../Images/48f0b3ad054a2c9ac2a179d7f88003e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TepPqZ0yGY1TXvU7n19DyQ.png"/></div></div></figure><p id="afcd" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">将它改写成矩阵形式，我们将得到:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mi"><img src="../Images/c4f4c54296e6f2637e7194336d17073c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G0pb7NuZrnZHcVyrNL4i5w.png"/></div></div></figure><p id="f932" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，如果我们将输入表示为矩阵 I(在我们的情况下，它是一个向量，但是如果我们使用批量输入，我们将使它的大小为样本数乘以输入数)，神经元权重为 W，偏差为 B，我们将得到:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mh"><img src="../Images/01053a2696906afb945df3a51a3a013c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6aoPH2_N3ZWjf1YpBkk5qw.png"/></div></div></figure><p id="05dd" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其可以被概括为全连接神经网络的任何层:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mj"><img src="../Images/b8b9f763450040486b953caa1e3e282e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Np-nCh7o6JSs7Vj06BF9ag.png"/></div></div></figure><p id="5013" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<strong class="ka jc"> i </strong> —是层数，<strong class="ka jc"> F </strong> —是给定层的激活函数。将此公式应用于网络的每一层，我们将实现正向传递，并最终获得网络输出。您的结果应该如下所示:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/9d4160ad20b39a801a51873360c6e620.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*tcCho60yzA_mCFCLOxS8tA.png"/></div></figure><h1 id="8f0c" class="kw kx jb bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">偶数道次</h1><p id="25cf" class="pw-post-body-paragraph jy jz jb ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">如果我们做所有的计算，我们最终会得到一个输出，这个输出实际上是不正确的(因为 0.56 &gt; 0.44 我们输出的是偶数作为结果)。知道了这一点，我们想更新神经元的权重和偏差，这样我们就能得到正确的结果。这正是反向传播发挥作用的地方。反向传播是一种计算关于每个网络变量(神经元权重和偏差)的误差梯度的算法。这些梯度随后用于优化算法，如梯度下降，相应地更新它们。权重和偏差更新的过程称为反向传递。</p><p id="d911" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了开始计算误差梯度，首先，我们必须计算误差(即损耗)本身。我们将使用标准分类损失—交叉熵。然而，损失函数可以是任何可微分的数学表达式。回归问题的标准选择是均方根误差(RMSE)。交叉熵损失如下所示:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mk"><img src="../Images/a0ae8110cc0256cc4ea1cccb181d8116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d4hzB0zySKErCdHHJcNRLw.png"/></div></div></figure><p id="46b3" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<strong class="ka jc"> M </strong>是类的数量，<strong class="ka jc"> p </strong>是网络输出的向量，<strong class="ka jc"> y </strong>是真实标签的向量。对于我们的情况，我们得到:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ml"><img src="../Images/754e569efa662c1f396a280c1f728b0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WzzhJPNVLLf8TGXMvHhZwg.png"/></div></div></figure><p id="ec38" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，为了找到每个变量的误差梯度，我们将集中使用链式法则:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mm"><img src="../Images/4efe0cf03bdae616d2f8d6b487922f94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CT__C2IqWQ_nzudPEFemvg.png"/></div></div></figure><p id="c5cf" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，从最后一层开始，对损失相对于神经元权重进行偏导数，我们得到:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mn"><img src="../Images/060562f444080e68b81d23e18d5e47a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G2Gu_KFmyx0z22qkEUDJQw.png"/></div></div></figure><p id="e97a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">了解在 softmax 激活和交叉吸附丢失的情况下我们拥有的事实(你可以自己推导出来作为一个很好的练习):</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mo"><img src="../Images/520bde58efe9db25d3481821ba81f2f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1_9TkdnP4TVx1IWtjqWVMg.png"/></div></div></figure><p id="e38a" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们可以找到最后一层的梯度为:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mp"><img src="../Images/e455876186d9ef8f63520d36a7baa861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2BKfo_iIyCgBwDtIxP1tcA.png"/></div></div></figure><p id="eff8" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">继续第 2 层:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mq"><img src="../Images/acb75b8c0dc90286995d19a54168a82e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jKRuP3cV5jum90Mi6tvcxw.png"/></div></div></figure><p id="a6b2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第一层:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mp"><img src="../Images/dd56aa4cd45ad6bde684a1965d6dcbc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*awAbPTAklOhu_csPLNiKjQ.png"/></div></div></figure><p id="ec5f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对偏差遵循相同的程序:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mr"><img src="../Images/846cd8b042df72f7c557b2164ac0a3de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1J6PO2tv_AWg0vfRlxYJqw.png"/></div></div></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ms"><img src="../Images/5fb28938b20c21dc29f8d2354db9bd66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FAufFjbu-hilgkb91c24xw.png"/></div></div></figure><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mr"><img src="../Images/a4601298288efc9c1aa8a081def224bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IVTJiLqtMggQfJ8KoiofCA.png"/></div></div></figure><p id="84a1" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们可以跟踪一个常见的模式，它可以概括为:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mt"><img src="../Images/b3955984e85267f025bbf8b01f9060ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1qbbBLUjAmoR3BkUqt_F9Q.png"/></div></div></figure><p id="0de7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是反向传播算法的矩阵方程。有了这些等式，我们就可以计算每个权重/偏差的误差梯度。为了减少误差，我们需要在与梯度相反的方向上更新我们的权重/偏差。这种思想用于梯度下降算法，定义如下:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mu"><img src="../Images/12190b837e4e8904d6d393cc378602b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n1g1_tUMHRe0ZLoTEqIwpg.png"/></div></div></figure><p id="c23e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<strong class="ka jc"> x </strong>是任何可训练变量(W 或 B)，<strong class="ka jc"> t </strong>是当前时间步长(算法迭代)，而<strong class="ka jc"> α </strong>是学习率。现在，设置<strong class="ka jc"> α </strong> = 0.1(您可以选择不同的，但请记住，小值假设较长的训练过程，而高值导致不稳定的训练过程)并使用上面的梯度计算公式，我们可以计算梯度下降算法的一次迭代。您应该获得以下重量更新:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/f7a9c66be167ba17911161aeca369990.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*TxEpnGxaHonMqIn_cYopMw.png"/></div></figure><p id="cd10" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">应用此更改并执行向前传递:</p><figure class="mb mc md me gt is gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/df34a82e0d65303f4caf00aa8658c7f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*pb8rJJPMzxG8QBM3IE4d4w.png"/></div></figure><p id="ffbb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以看到，我们的网络性能有所提高，与上例相比，现在奇数输出的值有所提高。对不同的例子(或一批样本)多次运行梯度下降算法，最终将得到一个经过适当训练的神经网络。</p><h1 id="efbd" class="kw kx jb bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">摘要</h1><p id="7f34" class="pw-post-body-paragraph jy jz jb ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">在这篇文章中，我解释了全连接神经网络训练过程的主要部分:向前和向后传球。尽管给出的概念很简单，但对反向传播的理解是建立鲁棒神经模型的一个基本步骤。我希望你从这篇文章中获得的知识能帮助你在训练过程中避免陷阱！</p><p id="6012" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你觉得这篇文章有用，别忘了鼓掌，敬请关注！在下一篇文章中，我将解释递归网络的数学原理。</p><p id="65f6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请在下面的评论中留下你的反馈/想法/建议/修正！</p><p id="f164" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">感谢阅读！</strong></p></div></div>    
</body>
</html>