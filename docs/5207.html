<html>
<head>
<title>Machine Learning — Word Embedding &amp; Sentiment Classification using Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习—使用 Keras 进行单词嵌入和情感分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456?source=collection_archive---------0-----------------------#2018-10-04">https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456?source=collection_archive---------0-----------------------#2018-10-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="e2b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae kl" href="https://medium.com/@javaid.nabi/machine-learning-text-processing-1d5a2d638958" rel="noopener">的上一篇文章</a>中，我们讨论了自然语言处理(NLP)中文本处理的各个步骤，并且使用一些经典的 ML 技术实现了一个基本的情感分析器。</p><p id="25b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">深度学习已经在包括 NLP、计算机视觉和游戏在内的各种任务上表现出优异的性能。为了进一步探索，我们将讨论和使用一些基于深度学习的高级 NLP 技术，来创建一个改进的情感分类器。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/ed739399c03540824662092fc2217f45.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*jvJUYSgLHSHw_cdkNjF7Yg.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Courtesy (<a class="ae kl" href="https://www.kdnuggets.com/2015/12/natural-language-processing-101.html" rel="noopener ugc nofollow" target="_blank">KDnuggets</a>)</figcaption></figure><h2 id="f628" class="ky kz iq bd la lb lc dn ld le lf dp lg jy lh li lj kc lk ll lm kg ln lo lp lq bi translated">情感分类问题</h2><blockquote class="lr ls lt"><p id="c78f" class="jn jo lu jp b jq jr js jt ju jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj kk ij bi translated">情感分类的任务是查看一段文本，并判断某人是否喜欢或不喜欢他们正在谈论的事情。</p></blockquote><p id="f357" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输入 X 是一段文本，输出 Y 是我们想要预测的情感，比如电影评论的星级。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ly"><img src="../Images/e7399bff3f619d9fbe542962885188ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vkuwvn_CglqdbUc_lXJpmw.png"/></div></div></figure><p id="1f3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们可以训练一个系统根据上面的标签数据集从 X 映射到 Y，那么这样的系统可以用来预测一个评论者在看完电影后的情绪。</p><p id="df8c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本帖中，我们将重点关注以下任务:</p><ul class=""><li id="9785" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">构建深度神经网络进行情感分类。</li><li id="e309" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">学习单词嵌入:一边训练网络，一边使用 Word2Vec。</li></ul><h2 id="e008" class="ky kz iq bd la lb lc dn ld le lf dp lg jy lh li lj kc lk ll lm kg ln lo lp lq bi translated">体系结构</h2><p id="7697" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">深度学习文本分类模型架构通常由以下顺序连接的组件组成:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mw"><img src="../Images/8dccc6fcbed1e6c7e1223b5ca4174811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mOmAmJhrMnHW-CBvwvgang.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Deep Learning Architecture</figcaption></figure><ul class=""><li id="430a" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">嵌入层</li></ul><pre class="kn ko kp kq gt mx my mz na aw nb bi"><span id="da49" class="ky kz iq my b gy nc nd l ne nf"><strong class="my ir"><em class="lu">Word Embedding </em></strong><em class="lu">is a representation of text where words that have the same meaning have a similar representation. In other words it represents words in a coordinate system where related words, based on a corpus of relationships, are placed closer together. </em>In the deep learning frameworks such as TensorFlow, Keras, this part is usually handled by an <strong class="my ir">embedding layer</strong> which stores a lookup table to map the words represented by numeric indexes to their dense vector representations.</span></pre><ul class=""><li id="a38c" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">深层网络</li></ul><pre class="kn ko kp kq gt mx my mz na aw nb bi"><span id="adab" class="ky kz iq my b gy nc nd l ne nf">Deep network takes the sequence of embedding vectors as input and converts them to a compressed representation. The compressed representation effectively captures all the information in the sequence of words in the text. The deep neywrok part is usually an RNN or some forms of it like LSTM/GRU. The dropout is added to overcome the tendency to overfit, a very common problem with RNN based networks. <!-- -->Please refer <a class="ae kl" rel="noopener" target="_blank" href="/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">here</a> for detailed discussion on LSTM,GRU.</span></pre><ul class=""><li id="714e" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">全连接层</li></ul><pre class="kn ko kp kq gt mx my mz na aw nb bi"><span id="2a92" class="ky kz iq my b gy nc nd l ne nf">The <strong class="my ir">fully connected layer </strong>takes the deep representation from the RNN/LSTM/GRU and transforms it into the final output classes or class scores. This component is comprised of fully connected layers along with batch normalization and optionally dropout layers for regularization.</span></pre><ul class=""><li id="8366" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">输出层</li></ul><pre class="kn ko kp kq gt mx my mz na aw nb bi"><span id="0a39" class="ky kz iq my b gy nc nd l ne nf">Based on the problem at hand, this layer can have either <strong class="my ir">Sigmoid </strong>for binary classification or <strong class="my ir">Softmax </strong>for both binary and multi classification output.</span></pre><h2 id="9461" class="ky kz iq bd la lb lc dn ld le lf dp lg jy lh li lj kc lk ll lm kg ln lo lp lq bi translated"><strong class="ak">数据集</strong></h2><p id="34d0" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">IMDB 电影评论集可以从<a class="ae kl" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank">这里</a>下载。这个用于二元情感分类的数据集包含用于训练的 25，000 条高度极性的电影评论，以及用于测试的 25，000 条评论。初始预处理后的数据集保存到<code class="fe ng nh ni my b">movie_data.csv</code>文件。首先，我们加载 IMDb 数据集，文本评论分别被标记为正面和负面情绪的 1 或 0。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/24256682ca6d7c562eb0786be065706c.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*PiRaPFd_JQ0nvo0x4fWFtg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">IMDb movie review dataset</figcaption></figure><h2 id="3c59" class="ky kz iq bd la lb lc dn ld le lf dp lg jy lh li lj kc lk ll lm kg ln lo lp lq bi translated">学习单词嵌入</h2><p id="9d24" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">我们数据集的单词嵌入可以在训练关于分类问题的神经网络时学习。在将文本数据呈现给网络之前，首先对其进行编码，以便每个单词由一个唯一的整数表示。这个数据准备步骤可以使用 Keras 提供的<a class="ae kl" href="https://keras.io/preprocessing/text/#tokenizer" rel="noopener ugc nofollow" target="_blank"> Tokenizer API </a>来执行。我们添加填充，使所有的向量长度相同<code class="fe ng nh ni my b">(<em class="lu">max_length</em>)</code>。下面的代码将文本转换为整数索引，现在可以在 Keras 嵌入层中使用。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/2fe3c7e5eecfcb0e333ca75789e95eee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*eK9HnQyq74yo5W53gzV8GQ.png"/></div></figure><p id="dda4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">嵌入层需要词汇表大小<code class="fe ng nh ni my b">(vocab_size)</code>、实值向量空间大小<code class="fe ng nh ni my b">EMBEDDING_DIM = 100</code>和输入文档最大长度<code class="fe ng nh ni my b">max_length</code>的规范。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/18a8b5310532be0e0ef86a9974611e14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*5vS6S5lbxvyReFzNeq_JVg.png"/></div></figure><h2 id="1f99" class="ky kz iq bd la lb lc dn ld le lf dp lg jy lh li lj kc lk ll lm kg ln lo lp lq bi translated">建立模型</h2><p id="e8b0" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">我们现在准备定义我们的神经网络模型。该模型将使用一个嵌入层作为第一个隐藏层。嵌入层用随机权重初始化，并将在模型训练期间学习训练数据集中所有单词的嵌入。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nm"><img src="../Images/a671cc7a7b7795c5c521fb55310ba9c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kIPyZh1JWbxorG1bfgIpKw.png"/></div></div></figure><p id="6f00" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该模式的总结是:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/bd788401d5537c764e4560e4c97495f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*xynHjNB9YIeNL_ns9s6SCQ.png"/></div></figure><p id="048b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">出于演示目的，我们使用了一个简单的深层网络配置。您可以尝试不同的网络配置并比较性能。嵌入参数计数<code class="fe ng nh ni my b">12560200 = (vocab_size * EMBEDDING_DIM)</code>。最大输入长度<code class="fe ng nh ni my b">max_length = 2678</code>。训练期间的模型将从输入文本中学习单词嵌入。可训练参数总数为 12，573，001。</p><h2 id="ee3e" class="ky kz iq bd la lb lc dn ld le lf dp lg jy lh li lj kc lk ll lm kg ln lo lp lq bi translated">火车模型</h2><p id="e7fe" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">现在让我们在训练集上训练模型，并在测试集上交叉验证。我们可以从下面的训练时期看到，每个时期之后的模型都在提高精度。几个时期之后，我们达到了大约 84%的验证准确度。还不错:)</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi no"><img src="../Images/4b25d29bf61ea39abf4956439ac173bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1u_AF5gMYVAHDoTJBUsC9Q.png"/></div></div></figure><h2 id="7440" class="ky kz iq bd la lb lc dn ld le lf dp lg jy lh li lj kc lk ll lm kg ln lo lp lq bi translated">试验模型</h2><p id="a0c1" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">我们可以用一些样本评论来测试我们的模型，以检查它是如何预测每个评论的情绪的。首先，我们必须将文本审查转换为令牌，并使用模型预测如下。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi np"><img src="../Images/54e1a82f177c15030959f105e8b884f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_4l5ARKUJd1FnbWyax4UWg.png"/></div></div></figure><p id="854b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输出给出单词的预测是 1(正面情绪)或 0(负面情绪)。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/c04329a5d3fa0cc4b9deae241692eb54.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*Lq2hoTC7_NHDxZBf8Ffn-A.png"/></div></figure><p id="a5cb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接近 1 的值是强烈的积极情绪，接近 0 的值是强烈的消极情绪。我可以清楚地看到，模型预测对于<em class="lu"> test_sample_7 </em>是错误的，而对于其余的样本却表现得相当好。</p><p id="45db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上述方法中，我们学习单词嵌入作为拟合神经网络模型的一部分。</p><h2 id="f2a8" class="ky kz iq bd la lb lc dn ld le lf dp lg jy lh li lj kc lk ll lm kg ln lo lp lq bi translated">训练 word2vec 嵌入</h2><p id="d1c8" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">还有另一种方法来构建情感澄清模型。我们可以先单独学习单词嵌入，然后传递到嵌入层，而不是训练嵌入层。这种方法还允许使用任何预先训练的单词嵌入，并且还节省了训练分类模型的时间。</p><p id="4c4a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将使用 Word2Vec 的 Gensim 实现。第一步是通过创建单词标记、去除标点符号、去除停用词等来准备用于学习嵌入的文本语料库。word2vec 算法逐句处理文档。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/d5a01d24fbbdba0131ba2fcf701c1dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*it19in7yGJ0vY_du7zLlJg.png"/></div></figure><p id="7967" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的文本语料库中有 50000 条评论。Gensim 的<a class="ae kl" href="https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec" rel="noopener ugc nofollow" target="_blank"> Word2Vec API </a>需要一些参数进行初始化。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ns"><img src="../Images/169de4049d3721fb30851a8562cb56f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ygrGHp3mmx5hw-W3V-Bozw.png"/></div></div></figure><p id="0a24" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一.<code class="fe ng nh ni my b">sentences</code>–句子列表；这里我们通过复习句子列表。</p><p id="b3b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">二。我们希望用多少维度来表达我们的话语。这是单词向量的大小。</p><p id="a3d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">三。<code class="fe ng nh ni my b">min_count</code>–只有频率大于<code class="fe ng nh ni my b">min_count</code>的词才会被纳入模型。通常，你的文本越大越广泛，这个数字就越高。</p><p id="faf4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">四。<code class="fe ng nh ni my b">window</code>–只有出现在<em class="lu">窗口</em>-一个句子中某个术语的邻近区域内的术语，才会在训练过程中与之相关联。通常的值是 4 或 5。</p><p id="d4fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">动词 （verb 的缩写）<code class="fe ng nh ni my b">workers</code>–训练并行化中使用的线程数量，以加快训练速度</p><h2 id="4b0f" class="ky kz iq bd la lb lc dn ld le lf dp lg jy lh li lj kc lk ll lm kg ln lo lp lq bi translated">测试 Word2Vec 模型</h2><p id="3da7" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">在我们在 IMDb 数据集上训练模型之后，它建立了一个词汇表<code class="fe ng nh ni my b">size = 134156</code>。让我们尝试一些从电影评论数据集学习的单词嵌入模型。</p><p id="d27c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与单词<code class="fe ng nh ni my b">horrible </code>最相似的单词是:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/a71b495c53c2d9111a306dc1b2e0a6ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*BXIu4xOKx52QDU4vF_6K7A.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">most similar words</figcaption></figure><p id="9d76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">试着对单词 vectors 做些数学计算— <code class="fe ng nh ni my b">woman+king-man=?</code></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nu"><img src="../Images/57b48faf2ff6c1d2df7f847c450c8bd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GChyzvZStUqBlMGzJ9jbkg.png"/></div></div></figure><p id="4f62" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们找到那个奇怪的单词<code class="fe ng nh ni my b">woman, king, queen, movie = ?</code></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/4a43a03618e9b988823769c820a1e21b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*0zSveSXR7Z1w-FlK4iCViA.png"/></div></figure><p id="3405" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看到我们的 word2vec 模型从文本语料库中学习到的单词嵌入是非常有趣的。下一步是在我们的情感分类模型的嵌入层中直接使用单词嵌入。我们可以保存模型以备后用。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/857bb46a5fc0dd70c2a44083804b3827.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*n9QFNwIOxGc_syToQQYdJA.png"/></div></figure><h2 id="81c7" class="ky kz iq bd la lb lc dn ld le lf dp lg jy lh li lj kc lk ll lm kg ln lo lp lq bi translated">使用预先训练的嵌入</h2><p id="780e" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">因为我们已经用 IMDb 数据集训练了 word2vec 模型，所以我们已经准备好使用单词嵌入了。下一步是将单词 embedding 作为单词目录加载到 vectors 中。单词 embedding 保存在文件<code class="fe ng nh ni my b">imdb_embedding_word2vec.txt</code>中。让我们从存储的文件中提取单词 embeddings。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nx"><img src="../Images/f1f602546d424e3e0860a579481d00f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*encV1S5shO378E_8DanB6g.png"/></div></div></figure><p id="e000" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一步是将单词 embedding 转换成标记化的向量。回想一下，评审文档在传递到嵌入层之前是整数编码的。整数映射到嵌入层中特定向量的索引。因此，重要的是我们在嵌入层中布置矢量，使得编码的字映射到正确的矢量。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c0eeba73404322c2e2686f8adb761967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*EnrhWFT2zZnapGruJpWtoQ.png"/></div></figure><p id="3e03" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们将把每个单词的嵌入从加载的 word2vec 模型映射到<code class="fe ng nh ni my b"> tokenizer_obj.word_index</code>词汇表，并创建一个单词向量矩阵。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/fc2e5902e5c2535357606383d2714bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*gzSI3NDa8dTwMia4J-BFoA.png"/></div></figure><p id="c719" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在已经准备好将训练好的嵌入向量直接用于嵌入层。在下面的代码中，与先前模型的唯一变化是使用<code class="fe ng nh ni my b">embedding_matrix</code>作为嵌入层的输入并设置<code class="fe ng nh ni my b">trainable = False</code>，因为嵌入已经被学习。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi oa"><img src="../Images/f32b35abfdc2820d787c3c6fa485c808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_SD0YhvfeiXWfWVvPjAw8Q.png"/></div></div></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/00f506d38378f8e51d74c67f713c22f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*mQlLz07u1GB71LS73Xxr6A.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Model summary with pre- trained Embedding</figcaption></figure><p id="4e51" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">仔细看，你可以看到型号<code class="fe ng nh ni my b">total params = 13,428,501</code>而不是<code class="fe ng nh ni my b">trainable params = 12801</code>。由于该模型使用预训练的单词嵌入，它具有非常少的可训练参数，因此应该训练得更快。</p><p id="dea1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了训练情感分类模型，我们使用<code class="fe ng nh ni my b">VALIDATION_SPLIT= 0.2</code>，你可以改变它来观察对模型准确性的影响。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/c2094760e4f849df8de699ec4c070095.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*ZiEzwnf17-Gie4T274t_Og.png"/></div></figure><p id="3189" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，在训练和验证测试集上训练分类模型，我们随着每个历元的运行而获得准确度的提高。我们仅用大约 5 个时期就达到了 88%的准确率。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi od"><img src="../Images/3c55056980239aff8de7608b2d16e3f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pg5FKhH4FjUXA6Znjauz_w.png"/></div></div></figure><p id="546e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可以尝试通过改变超参数、运行更多的历元等来提高模型的准确性。此外，您可以使用其他一些预先训练好的嵌入，这些嵌入是在非常大的文本数据语料库上准备的，您可以直接<a class="ae kl" href="http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/" rel="noopener ugc nofollow" target="_blank">下载</a>。</p><h2 id="86c9" class="ky kz iq bd la lb lc dn ld le lf dp lg jy lh li lj kc lk ll lm kg ln lo lp lq bi translated">结论</h2><p id="db20" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated">在这篇文章中，我们详细讨论了用于情感分类的深度学习模型的架构。我们还训练了一个 word2vec 模型，并将其用作情感分类的预训练嵌入。</p><p id="aab2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读，如果你喜欢，请为它鼓掌。</p><h2 id="c65a" class="ky kz iq bd la lb lc dn ld le lf dp lg jy lh li lj kc lk ll lm kg ln lo lp lq bi translated">进一步阅读</h2><p id="2f7a" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mt ka kb kc mu ke kf kg mv ki kj kk ij bi translated"><a class="ae kl" href="http://ruder.io/deep-learning-nlp-best-practices/" rel="noopener ugc nofollow" target="_blank">http://ruder.io/deep-learning-nlp-best-practices</a></p><div class="oe of gp gr og oh"><a href="https://keras.io/getting-started/functional-api-guide/" rel="noopener  ugc nofollow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd ir gy z fp om fr fs on fu fw ip bi translated">功能 API - Keras 文档指南</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">让我们考虑下面的模型。我们试图预测一个新闻标题在……上会有多少转发和点赞</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">keras.io</p></div></div><div class="oq l"><div class="or l os ot ou oq ov ks oh"/></div></div></a></div><div class="oe of gp gr og oh"><a href="https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/" rel="noopener  ugc nofollow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd ir gy z fp om fr fs on fu fw ip bi translated">如何开发预测电影评论情感的单词嵌入模型</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">单词嵌入是一种表示文本的技术，其中具有相似含义的不同单词具有相似的…</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">machinelearningmastery.com</p></div></div><div class="oq l"><div class="ow l os ot ou oq ov ks oh"/></div></div></a></div><p id="74ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://www.oreilly.com/library/view/hands-on-natural-language/9781789139495/" rel="noopener ugc nofollow" target="_blank">Rajesh Arumugam 的 Python 自然语言处理实践，Rajalingappaa Shanmugamani，2018 年 7 月</a></p></div></div>    
</body>
</html>