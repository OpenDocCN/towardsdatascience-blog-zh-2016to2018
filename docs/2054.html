<html>
<head>
<title>Getting Started with Reinforcement Q Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化Q学习入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-with-reinforcement-q-learning-77499b1766b6?source=collection_archive---------2-----------------------#2017-12-10">https://towardsdatascience.com/getting-started-with-reinforcement-q-learning-77499b1766b6?source=collection_archive---------2-----------------------#2017-12-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="879d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">简介</strong></p><p id="a3d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当DeepMind的AlphaGo击败围棋世界冠军李·塞多尔(Lee Sedol)时，人工智能(AI)真正崛起。人工智能代理的基本构建模块是Q学习，所以让我们直接进入它。正如著名作家安德鲁·特拉斯克所说的那样，“我用可以玩的玩具代码学得最好”。所以下面你会发现我们的玩具代码只使用了Numpy，我们将在本文的其余部分更详细地讨论它。</p><p id="dfcb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">什么是强化学习？</strong></p><p id="ee1a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">强化学习有一个“环境”和一个“代理”，前者是我们试图解决的问题集，后者是我们的人工智能算法。环境和代理之间的关系非常简单。代理将执行某些动作(像我们的玩具代码中的出租车上、下、右、左等移动)，作为该动作的结果，他的状态将改变(出租车的新位置)，这将导致代理获得奖励(到达其目的地的正奖励，或者如果您错误地搭载或放下乘客，则为负奖励)。通过行动和奖励过程的迭代，代理学习环境。它开始明白在哪个状态下，哪个动作给他最大的奖励，哪个动作给他负的奖励。这个执行一个动作并从奖励中学习的过程叫做强化学习。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/a784bd146ec40b0920140bab2e5b91db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NyWUkwz1QhrVJj9ygCQ5nA.png"/></div></div></figure><p id="8443" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">贝尔曼方程</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kx"><img src="../Images/bac2892dc7e6768c438624f4d52cb49e.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/1*w6UxGDWpUWr5B1fMgC6g3g.gif"/></div></figure><p id="361e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们假设我们的代理在相邻gif的“开始”图像所示的环境中开始。最初，它只是做一些随机的运动，比如向右、向左、向上、向下等，最终在某个时候，它会到达目的地。现在只要它到达目的地，它就会得到奖励，比如说+1分。这触发算法意识到这个地方(绿色方块)是它需要的地方。然后代理开始问问题，我是怎么到这个方块的，我之前是什么状态，我做了什么让我得到奖励。因此，它回顾和回溯其先前的状态，如下图所示</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kx"><img src="../Images/1163f139b4ffa33b5384096cbda6ef3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/1*WbrSTqvRuwdujG6dhTV9KQ.gif"/></div></figure><p id="f620" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它首先用虚拟奖励“1”标记绿色方块左边的方块，告诉自己他只需要到达前面的方块，然后它需要做的只是向右移动，他就会到达目的地。这个过程继续，直到他回溯他的完整路径。这种路由回溯，代理执行多次迭代。</p><p id="8ef2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，如果我们永远不重置代理，一切都可以工作了，它会一直保持在ON状态，并在内存中记忆他刚才绘制的地图。但是，如果代理重置，并且下次被调用时从不同的状态启动，会发生什么呢？现在他很困惑，不知道该做什么，这就是为什么这种方法没有真正发挥作用。</p><p id="7628" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这就是贝尔曼方程发挥作用的地方。</p><p id="723d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">贝尔曼方程-&gt; <strong class="jp ir"> V(s) = max(R (s，a)+γV(s′))</strong>其中V(s)是任意给定状态下的一个值</p><p id="a6d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">s —状态，a —行动，R —奖励，γ —折扣，s' —采取行动‘a’后的下一个状态</p><p id="9d3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">贝尔曼方程说的是，一个国家的价值。V(s ),等于通过执行任何允许的动作R(s，a)你可以从该状态获得的不同奖励的最大值，以及通过采取特定动作“a”你将到达的新状态的“贴现”值。参考下面的gif图可以更好的理解上面的公式。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kx"><img src="../Images/eab29254141160e7977eaefd00cda20d.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/1*1Ku-uTCv5r3bFA1_4l5aNg.gif"/></div></figure><p id="8b7b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">目的地绿色方块左边的方块的值是1，因为如果我们向右移动，我们可以得到1的奖励。回溯，现在不是像前面的情况一样将下一个左边的方块也指定为1，而是使用伽玛值为0.9的贝尔曼方程，并将下一个方块的值指定为0.9，对于路线的其余部分以此类推。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ky"><img src="../Images/54e41d587a8f297ea77521cab023dcb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*15T-b29s5aPJ6iYX8avOLA.gif"/></div></figure><p id="2505" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同样，我们可以原路返回，填满我们的地图。现在我们已经创建了我们的地图，对于我们的代理来说，无论它从哪个位置开始，它应该走哪条路都变得非常明显。</p><p id="078e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有了对强化学习和贝尔曼方程的清晰理解，让我们来看看我们的玩具代码，看看它是如何实现上述概念的。为了更好的理解，我将逐行解释完整的代码。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kz"><img src="../Images/34424e403593f91f9264ffd0ef32ff93.png" data-original-src="https://miro.medium.com/v2/resize:fit:180/format:webp/1*ESCkndpq0f95gJo5dhc9Sg.jpeg"/></div></figure><p id="ac85" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">问题陈述</strong>:有4个地点(用不同的字母标注)，你的工作是在一个地点接乘客，在另一个地点让他下车。你成功脱手可获得+20分，每完成一个时间步长将失去1分。非法上下车行为还会被扣10分。在此环境中，实心正方形代表出租车，(" | ")代表墙壁，蓝色字母代表上车地点，紫色字母代表下车地点。出租车上有乘客时会变绿</p><p id="bbcd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">代码走查</strong></p><p id="a542" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第1–3行:导入所需的库</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="4ce8" class="lf lg iq lb b gy lh li l lj lk">#Importing Libararies<br/>import gym<br/>import numpy as np</span></pre><p id="b3ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第5–8行:设置开放的健身房环境。这只是建立任何健身房环境的标准方式</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="84b8" class="lf lg iq lb b gy lh li l lj lk">#Environment Setup<br/>env = gym.make("Taxi-v2")<br/>env.reset()<br/>env.render()</span></pre><p id="b706" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第10–17行:首先，让我们看看我们的代理在随机选择要执行的动作时表现如何。</p><p id="c0ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第11行:获取当前状态，即阻塞代理所处的状态。对于此特定环境，该值可以在0–499之间。</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="e835" class="lf lg iq lb b gy lh li l lj lk"># Random Moments<br/>state = env.reset()</span></pre><p id="b0b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第12行和第13行:初始化计数器和奖励。在这一点上，我会建议你手动玩环境，只是为了感受一下。在你的代码中使用下面两个例子来将代理置于特定的状态，或者使它向你喜欢的方向移动。</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="0071" class="lf lg iq lb b gy lh li l lj lk">counter = 0<br/>reward = None</span><span id="c1a2" class="lf lg iq lb b gy ll li l lj lk"># Change state<br/>env.env.s = 114<br/>env.render()</span><span id="54d8" class="lf lg iq lb b gy ll li l lj lk"># Take an action<br/>env.step(3)<br/>env.render()</span></pre><p id="6469" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第14行:直到我们得到20英镑的奖励，这是我们正确放下乘客的奖励。</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="ddd5" class="lf lg iq lb b gy lh li l lj lk">while reward != 20:</span></pre><p id="2079" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第15行:env.action_space.sample()是一个gym函数，它返回从允许的动作中选择的一个随机动作。我们调用env.step()来执行这个随机选择的动作。env.step()总是返回新的状态、对前一个动作的奖励、挑战是否完成，以及一些对调试有用的附加信息。我们将所有这些值存储在各自的变量中，仅供参考。</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="ddea" class="lf lg iq lb b gy lh li l lj lk">state, reward, done, info = env.step(env.action_space.sample())</span></pre><p id="7ac1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第16行:由于我们已经走了一步，我们将计数器加1</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="3ced" class="lf lg iq lb b gy lh li l lj lk">counter += 1</span></pre><p id="77a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第17行:每当我们最终将乘客送到正确的位置时，我们将退出while循环(因为收到的奖励将是20)。打印计数器值，指示该特定迭代完成任务所用的步骤数。玩几次while循环，了解在随机采取行动的情况下完成挑战需要多少步。我们将把这个值与我们通过实现强化学习算法得到的值进行比较。</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="543c" class="lf lg iq lb b gy lh li l lj lk">print(counter)</span></pre><p id="bc19" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第20行:我们通过创建一个“可能状态的数量”到“可能动作的数量”维度的矩阵来开始Q(强化)学习。记住，从我们的学习中，我们需要为每个状态和该状态下允许的每个动作保持一个Q值(贝尔曼方程中的V(s))。正如在解释贝尔曼方程时所说的，最初代理不知道任何Q值，所以我们用全零初始化它。</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="3b6b" class="lf lg iq lb b gy lh li l lj lk">Q = np.zeros([env.observation_space.n, env.action_space.n])</span></pre><p id="feba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第21行:初始化奖励变量G(因为变量‘reward’用于存储env.step()给出的返回值，这里使用其他变量名G)</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="22cd" class="lf lg iq lb b gy lh li l lj lk">G = 0</span></pre><p id="28f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第22行:使用伽玛值0.618。这纯粹是实验出来的。你可以试试你的价值观，在这里分享你的结果，只是为了比较。</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="0852" class="lf lg iq lb b gy lh li l lj lk">gamma = 0.618</span></pre><p id="d779" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第23行:对于上千次迭代</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="d67f" class="lf lg iq lb b gy lh li l lj lk">for episode in range(1,1001):</span></pre><p id="573e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第24行:初始化“done”值，我们将使用它在完成任务后退出while循环。</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="553d" class="lf lg iq lb b gy lh li l lj lk">done = False</span></pre><p id="0ac6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第25行:每次迭代各自的值初始化</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="9108" class="lf lg iq lb b gy lh li l lj lk">G, reward, counter = 0,0,0</span></pre><p id="8a2d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第26行:重置环境并获取其状态</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="5bc7" class="lf lg iq lb b gy lh li l lj lk">state = env.reset()</span></pre><p id="d66e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第27行:虽然我们还没有“完成”</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="ba71" class="lf lg iq lb b gy lh li l lj lk">while done != True:</span></pre><p id="4097" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第28行:现在当创建Q矩阵时，记住我们已经创建了维度行= '可能状态的数量'和列= '可能动作的数量'。np.argmax(Q[state])从Q矩阵的行“state”中挑选出最大值。换句话说，它获取当前状态，在Q矩阵中查找，找到该状态的特定行，并返回沿着该行的最大值的索引(这将是“动作”号)</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="ad95" class="lf lg iq lb b gy lh li l lj lk">action = np.argmax(Q[state])</span></pre><p id="6c87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第29行:使用上面获得的“动作”来执行一个步骤并存储其结果。</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="1b91" class="lf lg iq lb b gy lh li l lj lk">state2, reward, done, info = env.step(action)</span></pre><p id="5bc1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第30行:根据贝尔曼方程和上一步得到的结果更新我们的Q表。如果你遵循了本教程，你应该能够清楚地将这条线与上一节描述的贝尔曼方程对应起来。</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="29ab" class="lf lg iq lb b gy lh li l lj lk">Q[state,action] = (reward + gamma * np.max(Q[state2]))</span></pre><p id="e877" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第31–33行:更新了相应的变量</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="7eff" class="lf lg iq lb b gy lh li l lj lk">G += reward<br/>counter += 1<br/>state = state2</span></pre><p id="1f75" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第35行:打印迭代，每50次迭代的奖励和计数器值。检查这些值，并将其与我们通过随机实现获得的“计数器”值进行比较。还要检查代理如何在初始迭代期间以负回报和高计数器值结束，以及它们如何随着它执行更多迭代而显著改善，从它的动作中学习并根据贝尔曼方程更新Q矩阵。</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="b969" class="lf lg iq lb b gy lh li l lj lk">if episode % 50 == 0:<br/>        print('Episode {} Total Reward: {} counter: {}'.format(episode,G,counter))</span></pre><p id="b43b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是完整的代码</p><pre class="km kn ko kp gt la lb lc ld aw le bi"><span id="af6d" class="lf lg iq lb b gy lh li l lj lk">#Importing Libararies<br/>import gym<br/>import numpy as np</span><span id="a373" class="lf lg iq lb b gy ll li l lj lk">#Environment Setup<br/>env = gym.make("Taxi-v2")<br/>env.reset()<br/>env.render()</span><span id="d1b7" class="lf lg iq lb b gy ll li l lj lk"># Random Moments<br/>state = env.reset()<br/>counter = 0<br/>reward = None<br/>while reward != 20:<br/>    state, reward, done, info = env.step(env.action_space.sample())<br/>    counter += 1<br/>print(counter)</span><span id="2db4" class="lf lg iq lb b gy ll li l lj lk"># Q table implementation<br/>Q = np.zeros([env.observation_space.n, env.action_space.n])<br/>G = 0<br/>gamma = 0.618<br/>for episode in range(1,1001):<br/>    done = False<br/>    G, reward, counter = 0,0,0<br/>    state = env.reset()<br/>    while done != True:<br/>            action = np.argmax(Q[state])<br/>            state2, reward, done, info = env.step(action)<br/>            Q[state,action] = (reward + gamma * np.max(Q[state2]))<br/>            G += reward<br/>            counter += 1<br/>            state = state2   <br/>    if episode % 50 == 0:<br/>        print('Episode {} Total Reward: {} counter: {}'.format(episode,G,counter))</span></pre><p id="274f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">唷！！就是这样伙计们。深呼吸，祝贺你自己学会了一个Q学习人工智能代理的基本概念。高端Atari游戏中使用的AI代理的概念与本教程中解释的非常相似。不同的是，在这些游戏中，环境的状态数量变得非常多，所以不可能用Q矩阵表来实现，因为它的维数非常非常大。所以他们用神经网络来实现同样的功能。遵循OpenAI的文档来了解如何安装openai健身房环境。</p><p id="8ac8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你喜欢这篇文章，<strong class="jp ir">在<a class="ae lm" href="https://twitter.com/percyjaiswal" rel="noopener ugc nofollow" target="_blank">推特</a>上关注、转发</strong>将鼓励我开始我的博客世界之旅。</p><p id="8c8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下次见。干杯！！</p></div></div>    
</body>
</html>