<html>
<head>
<title>How Did We Build Book Recommender Systems in An Hour Part 2 — k Nearest Neighbors and Matrix Factorization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我们如何在一小时内构建图书推荐系统第二部分— k近邻和矩阵分解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-did-we-build-book-recommender-systems-in-an-hour-part-2-k-nearest-neighbors-and-matrix-c04b3c2ef55c?source=collection_archive---------0-----------------------#2017-09-20">https://towardsdatascience.com/how-did-we-build-book-recommender-systems-in-an-hour-part-2-k-nearest-neighbors-and-matrix-c04b3c2ef55c?source=collection_archive---------0-----------------------#2017-09-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/8c1d1a08c07b1b1353b22e266a1a19d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FDs5bVzpSEdTOTIX3Ntb1w.png"/></div></div></figure><p id="a1ba" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在<a class="ae kw" href="https://medium.com/@actsusanli/how-did-we-build-book-recommender-systems-in-an-hour-the-fundamentals-dfee054f978e" rel="noopener">的上一篇文章</a>中，我们看到了如何使用简单的相关技术，根据图书用户的评分记录，在他们之间建立一个相似性的衡量标准。在本帖中，我们将展示如何使用这些相似性度量向读者提出建议。</p><h1 id="9dca" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">基于k近邻的协同过滤</h1><p id="f959" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">kNN是一种机器学习算法，可以根据常见的书籍评级来找到相似用户的聚类，并使用前k个最近邻居的平均评级进行预测。例如，我们首先在矩阵中显示评级，矩阵中每个项目(书籍)占一行，每个用户占一列，如下所示:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ma"><img src="../Images/9e5be441de9fc4d46e88422830f39921.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pyl_iVLSASNvCDOYfIjppg.png"/></div></div></figure><p id="6fe4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后，我们找到具有最相似的用户参与度向量的k个项目。在这种情况下，项目id 5的最近邻居= [7，4，8，…]。现在让我们将kNN实现到我们的图书推荐系统中。</p><h2 id="db06" class="mf ky iq bd kz mg mh dn ld mi mj dp lh kj mk ml ll kn mm mn lp kr mo mp lt mq bi translated">数据</h2><p id="80b3" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我们使用上次使用的相同的图书数据:它由三个表组成:ratings、books info和users info。我从<a class="ae kw" href="http://www2.informatik.uni-freiburg.de/~cziegler/BX/" rel="noopener ugc nofollow" target="_blank">这里</a>下载了这三张表。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mr"><img src="../Images/79efe40cd3b234ce1d7841774b103648.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gWnCpYLt3qtnFS9CkywdDQ.png"/></div></div></figure><p id="7e66" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">评分信息</strong></p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/4a1b269fe30f1c4f12514646e1438e13.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*Z4q2w0H96GqpQHazrYWRtQ.png"/></div></figure><p id="8158" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">用户信息</strong></p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/90f0a01e0649f49baf7842a78491303d.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*xOx0HRlal1ZrRP0HVQx0oA.png"/></div></figure><p id="3fe9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">图书信息</strong></p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mu"><img src="../Images/290dd860c6aabfb5bd3b29904daeedae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vJvUF--yfcNnz_oLkgFmzQ.png"/></div></div></figure><h2 id="daf5" class="mf ky iq bd kz mg mh dn ld mi mj dp lh kj mk ml ll kn mm mn lp kr mo mp lt mq bi translated">为了确保统计上的显著性，我们将只看流行书籍</h2><p id="c49a" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">为了找出哪些书受欢迎，我们需要结合书籍数据和评分数据。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mv"><img src="../Images/7accebd59da19b5d74b5e889de71d42e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q_K23-ha2ex-4uwk4_3_9g.png"/></div></div></figure><p id="3d30" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后，我们按书名分组，并为总评分创建一个新列。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mw"><img src="../Images/6c2e2c174081f31836324207684be4ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fZ3FKH_mW3Eu06byI9VIsg.png"/></div></div></figure><p id="df5d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们现在将评分数据与总评分计数数据相结合，这正好给了我们过滤掉不太出名的书所需的数据。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mx"><img src="../Images/71165e15069daa38fbe14a220d8a20d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z_uuLmTuTdIvuRAWrbJTbg.png"/></div></div></figure><p id="cf05" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们来看看总评分的统计数据:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/a68fb2e65ac4c672f81a85cb80b60aff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*92GXVRTvnahebLQk1bp84g.png"/></div></figure><p id="5a37" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">中值书只被评价过一次。让我们看看分布的顶端:</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mz"><img src="../Images/b35f94ac9c1ae63e266d9c0019dd0b7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PbPpsg4abtEFTVkLIBBniw.png"/></div></div></figure><p id="41e8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">大约1%的书获得了50或更多的评分。因为我们的数据中有如此多的书，我们将把它限制到前1%，这将为我们提供2713本独特的书。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi na"><img src="../Images/bf641b14953d1f722cac210afceeb1ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tt8DUgi8t0FLK6jrpbycaw.png"/></div></div></figure><h2 id="5fea" class="mf ky iq bd kz mg mh dn ld mi mj dp lh kj mk ml ll kn mm mn lp kr mo mp lt mq bi translated">仅过滤美国和加拿大的用户</h2><p id="02b5" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">为了提高计算速度，并避免遇到“内存错误”问题，我将把我们的用户数据限制在美国和加拿大。然后将用户数据与评级数据和总评级计数数据相结合。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/fff041c0c9fab35d0af67a4ccf0b2681.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BK5pQlMGZpIeiFk9kXE1Ag.png"/></div></div></figure><h2 id="9b55" class="mf ky iq bd kz mg mh dn ld mi mj dp lh kj mk ml ll kn mm mn lp kr mo mp lt mq bi translated">实现kNN</h2><p id="d101" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我们将表格转换为2D矩阵，并用零填充缺失的值(因为我们将计算评级向量之间的距离)。然后，我们将矩阵数据帧的值(等级)转换成一个稀疏矩阵，以便进行更有效的计算。</p><h2 id="b329" class="mf ky iq bd kz mg mh dn ld mi mj dp lh kj mk ml ll kn mm mn lp kr mo mp lt mq bi translated">寻找最近的邻居</h2><p id="ff33" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我们使用带有<em class="nc"> sklearn.neighbors. </em>的无监督算法我们用来计算最近邻的算法是“brute”，我们指定“metric=cosine”这样算法就会计算评分向量之间的余弦相似度。最后，我们拟合模型。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nd"><img src="../Images/0a65e43d100151620a92de831d3caf5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Au0iBPTNpCPiwi__vDUILA.png"/></div></div></figure><h2 id="aee5" class="mf ky iq bd kz mg mh dn ld mi mj dp lh kj mk ml ll kn mm mn lp kr mo mp lt mq bi translated">测试我们的模型并提出一些建议:</h2><p id="bf70" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">在这一步中，kNN算法测量距离以确定实例的“接近度”。然后，它通过查找最近的邻居对实例进行分类，并在邻居中挑选最受欢迎的类。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nd"><img src="../Images/109f011fe2607aa4c85deb672dab58d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vE1c64oh6J7lBImKbwD-xw.png"/></div></div></figure><p id="b3ee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">完美！《绿色一英里》系列书籍绝对值得推荐，一本接一本。</p><h1 id="ceaf" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">使用矩阵分解的协同过滤</h1><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/bbe544dc88d09687fa65442b154ed892.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IgyFUxfVIm7YblpmgMWzOA.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">Source: <a class="ae kw" href="http://katbailey.github.io/page/2/" rel="noopener ugc nofollow" target="_blank">ohAI</a></figcaption></figure><p id="7c5c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">矩阵分解只是一个玩弄矩阵的数学工具。矩阵分解技术通常更有效，因为它们允许用户发现用户和项目(书籍)之间交互的潜在(隐藏)特征。</p><p id="3bb6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们使用奇异值分解(SVD)——一种矩阵分解模型来识别潜在因素。</p><p id="644f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">与kNN类似，我们将美加用户评级表转换为2D矩阵(这里称为效用矩阵),并用零填充缺失值。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/dcd90dbc803184cb8b5c0d38e38e9ba7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*23hbi849CDvHQVExZHVMGw.png"/></div></div></figure><p id="951f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后我们转置这个效用矩阵，这样书名变成了行，用户标识变成了列。在使用TruncatedSVD对其进行分解后，我们将其拟合到模型中进行降维。这种压缩发生在数据帧的列上，因为我们必须保留书名。我们选择<em class="nc"> n_components = 12 </em>作为12个潜在变量，您可以看到，我们的数据维度已经从40017 X 2442显著减少到2442 X 12。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/ac3bd2edeb28b89e75adb7c419b03750.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GH85zirVeUCPEtorKhwJYw.png"/></div></div></figure><p id="21d0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们为最终矩阵中的每个图书对计算皮尔逊相关系数。为了与kNN的结果进行比较，我们选择了同一本书“绿色里程:科菲的手(绿色里程系列)”，以找到与它具有高相关系数(在0.9和1.0之间)的书籍。</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nl"><img src="../Images/0adbf285421d6f48bbd56fff98d6b906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pFAEeVF8d3suAiN8bJackQ.png"/></div></div></figure><p id="fa66" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你有它！</p><figure class="mb mc md me gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/f031377ee99c2daa08d7c56e153ebeec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*agToYHI2wF04nJ-teBSE5w.png"/></div></div></figure><p id="c9ee" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">不算太寒酸！我们的系统可以打败亚马逊的，你说呢？看标题截图！</p><p id="e901" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">参考:<a class="ae kw" href="https://beckernick.github.io/music_recommender/" rel="noopener ugc nofollow" target="_blank">音乐推荐</a></p></div></div>    
</body>
</html>