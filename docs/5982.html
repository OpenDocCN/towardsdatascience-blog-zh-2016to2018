<html>
<head>
<title>Filter, Aggregate and Join in Pandas, Tidyverse, Pyspark and SQL</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pandas、Tidyverse、Pyspark 和 SQL 中的过滤、聚合和连接</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/filter-aggregate-and-join-in-pandas-tidyverse-pyspark-and-sql-71d60cbd5330?source=collection_archive---------17-----------------------#2018-11-19">https://towardsdatascience.com/filter-aggregate-and-join-in-pandas-tidyverse-pyspark-and-sql-71d60cbd5330?source=collection_archive---------17-----------------------#2018-11-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/9a2bc80d85ee3e35641f0c12bfad6f0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sjH_glbyXSvSya9KJx2Siw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Alike but different (<a class="ae kc" href="https://www.pexels.com/photo/relaxation-relax-cats-cat-96428/" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><h1 id="2ede" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">祝福与诅咒</h1><p id="ad48" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">鼓舞人心的数据科学家最常问的一个问题是，他们应该为数据科学学习哪种语言。最初的选择通常是在 Python 和 r 之间。已经有很多对话可以帮助做出语言选择(这里的<a class="ae kc" href="https://www.datacamp.com/community/tutorials/r-or-python-for-data-analysis" rel="noopener ugc nofollow" target="_blank"/>和这里的<a class="ae kc" href="https://medium.com/@data_driven/python-vs-r-for-data-science-and-the-winner-is-3ebb1a968197" rel="noopener"/>)。选择合适的语言是第一步，但我怀疑大多数人最终只使用一种语言。在我个人的旅程中，我先学了 R，然后是 SQL，再后来是 Python，再后来是 Spark。现在在我的日常工作中，我使用所有这些工具，因为我发现每个工具都有独特的优势(在速度、易用性、可视化等方面)。有些人确实只停留在一个阵营(有些 Python 人从未想过学习 R)，但是我更喜欢使用现有的东西来创建我的最佳数据解决方案，并且使用多种数据语言有助于我与不同团队的协作。</p><p id="60ca" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">正如<a class="ae kc" href="https://info.algorithmia.com/enterprise-tt-state-of-machine-learning?utm_medium=social&amp;utm_source=twitter&amp;utm_campaign=1810-state-of-machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习的状态</a>所指出的，我们面临着在数据团队中支持多种语言的挑战，跨语言工作的一个不利方面是我会把一种语言与另一种语言混淆，因为它们可能有非常相似的语法。考虑到我们拥有一个多种语言共存的数据科学生态系统，我很乐意并排写下这四种语言中的三种常见数据转换操作，以便阐明它们在语法层面的比较。将语法放在一起还有助于我在数据科学工具箱中更好地协同它们。希望对你也有帮助。</p><h1 id="13d9" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">四种语言的三种数据操作</h1><p id="220c" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">数据科学的前景是广阔的，我决定在这篇文章中关注最大的共同点。没有正式的统计数据支持，我想当然地认为:大多数数据科学工作都是基于表格数据；常见的数据语言有 SQL，Python，R 和 Spark(不是 Julia，C++，SAS 等。).</p><p id="2dbd" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">SQL 有一长串的方言(hive，mysql，postgresql，casandra 等等)，我在这个帖子里选择 ANSI-standard SQL。纯 Python 和 Base R 能够操作数据，但是在这篇文章中我选择 Pandas 作为 Python，Tidyverse 作为 R。Spark 有 RDD 和数据帧，我选择专注于数据帧。Spark 在 Pyspark 和 Sparklyr 中都有 API，我这里选择 Pyspark，因为 Sparklyr API 和 Tidyverse 很像。</p><p id="4045" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">三种常见的数据操作包括筛选、聚合和连接。这三个操作允许您剪切和合并表格，导出平均值和百分比等统计数据，并为绘图和建模做好准备。由于数据争论消耗了大量的数据工作时间，这三种常见的数据操作应该占数据争论时间的很大一部分。</p><h1 id="6429" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak"> 1。使用标准过滤表格</strong></h1><p id="2c41" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">包含和排除在数据处理中是必不可少的。我们保留相关的，丢弃不相关的。我们可以用一个词来称呼包容和排斥:过滤。过滤器在表格数据中有两个部分。一个是过滤列，另一个是过滤行。使用这篇文章中著名的虹膜数据，我在下面列出了四种语言的过滤操作。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="d7c7" class="mn ke iq mj b gy mo mp l mq mr">Question: What is the sepal length, petal length of Setosa with petal width larger than 1 ?</span><span id="c31b" class="mn ke iq mj b gy ms mp l mq mr"># SQL</span><span id="a82a" class="mn ke iq mj b gy ms mp l mq mr">select Sepal_Length, Petal_Length from Iris where Petal_Width &gt; 1 and Species=’setosa’;</span><span id="2247" class="mn ke iq mj b gy ms mp l mq mr"># Pandas</span><span id="20e6" class="mn ke iq mj b gy ms mp l mq mr">Iris[(Iris.Petal_Width &gt; 1) &amp; (Iris.Species==’setosa’)][[‘Sepal_length’,’Petal_Length’]]</span><span id="fa52" class="mn ke iq mj b gy ms mp l mq mr"># Tidyverse</span><span id="a7da" class="mn ke iq mj b gy ms mp l mq mr">Iris %&gt;% <br/> filter(Petal_Width &gt; 1, Species==’setosa’) %&gt;%<br/> select(Sepal_Length, Petal_Length)</span><span id="168b" class="mn ke iq mj b gy ms mp l mq mr"># Pyspark</span><span id="9966" class="mn ke iq mj b gy ms mp l mq mr">Iris.filter((Iris.Petal_Width &gt; 1) &amp; (Iris.Species==’setosa’)).select(Iris.Sepal_Length, Iris.Petal_Length)</span></pre><p id="8033" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">Pandas 使用括号来过滤列和行，而 Tidyverse 使用函数。Pyspark API 是借熊猫和 Tidyverse 两者之所长而定的。正如你在这里看到的，Pyspark 操作与熊猫和 Tidyverse 有相似之处。SQL 和往常一样是声明性的，显示出它的签名“从表中选择列，其中行符合标准”。</p><h1 id="04e9" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak"> 2。按组导出汇总统计数据</strong></h1><p id="4934" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">创建计数、总和及平均值等汇总统计数据对于数据探索和特征工程至关重要。当分类变量可用时，按某些分类变量对汇总统计数据进行分组也很常见。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="1c5c" class="mn ke iq mj b gy mo mp l mq mr">Question: How many sepal length records does each Iris species have in this data and what is their average sepal length ?</span><span id="a327" class="mn ke iq mj b gy ms mp l mq mr"># SQL</span><span id="9710" class="mn ke iq mj b gy ms mp l mq mr">select Species, count(Sepal_Length) as Sepal_Length_Count, avg(Sepal_Length) as Sepal_Length_mean from Iris group by Species;<br/></span><span id="6f19" class="mn ke iq mj b gy ms mp l mq mr"># Pandas<br/>aggregated=Iris.groupby(by=’Species’,as_index=False).agg({‘Sepal_Length’: [‘mean’,’count’]})<br/>aggregated.columns = [‘_’.join(tup).rstrip(‘_’) for tup in temp1.columns.values]</span><span id="9404" class="mn ke iq mj b gy ms mp l mq mr"># Tidyverse<br/>Iris %&gt;%<br/> group_by(Species) %&gt;%<br/> summarize(Sepal_Length_mean=mean(Sepal_Length), Count=n())</span><span id="41cb" class="mn ke iq mj b gy ms mp l mq mr"># Pyspark<br/>from pyspark.sql import functions as F</span><span id="7bbc" class="mn ke iq mj b gy ms mp l mq mr">Iris.groupBy(Iris.species).agg(F.mean(Iris.sepal_length).alias(‘sepal_length_mean’),F.count(Iris.sepal_length).alias(‘sepal_length_count’))</span></pre><p id="ed92" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">这个例子很好地说明了为什么我会对这四种语言感到困惑。编写“group by”有四种略有不同的方法:在 SQL 中使用<code class="fe mt mu mv mj b">group by</code>，在 Pandas 中使用<code class="fe mt mu mv mj b">groupby</code>，在 Tidyverse 中使用<code class="fe mt mu mv mj b">group_by</code>，在 Pyspark 中使用<code class="fe mt mu mv mj b">groupBy</code>(在 Pyspark 中，<code class="fe mt mu mv mj b">groupBy</code>和<code class="fe mt mu mv mj b">groupby</code>都可以，因为<code class="fe mt mu mv mj b">groupby</code>是 Pyspark 中<code class="fe mt mu mv mj b">groupBy</code>的别名。<code class="fe mt mu mv mj b">groupBy</code>看起来更真实，因为它在官方文件中使用得更频繁。</p><p id="4471" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">在聚合方面，Python 在这里非常不同。第一，它使用一个字典来指定聚合操作。第二，它默认使用 group by 变量作为索引，你可能不得不处理 multiindex。</p><h1 id="0cce" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak"> 3。连接表格以将功能放在一起</strong></h1><p id="1db7" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">大数据工作的一个标志是将多个数据源集成到一个源中，用于机器学习和建模，因此 join 操作是必不可少的。有一个可用的连接列表:左连接、内连接、外连接、反左连接和其他连接。以下示例中使用了左连接。</p><pre class="me mf mg mh gt mi mj mk ml aw mm bi"><span id="e324" class="mn ke iq mj b gy mo mp l mq mr">Question: given a table Iris_preference that has my preference on each species, can you join this preference table with the original table for later preference analysis?</span><span id="5776" class="mn ke iq mj b gy ms mp l mq mr"># SQL</span><span id="71a1" class="mn ke iq mj b gy ms mp l mq mr">select a.*, b.* from Iris a left join Iris_preference b on a.Species=b.Species;</span><span id="a66f" class="mn ke iq mj b gy ms mp l mq mr"># Pandas</span><span id="9abb" class="mn ke iq mj b gy ms mp l mq mr">pd.merge(Iris, Iris_preference, how=’left’, on=’Species’)</span><span id="a2a5" class="mn ke iq mj b gy ms mp l mq mr"># Tidyverse</span><span id="cc0f" class="mn ke iq mj b gy ms mp l mq mr">left_join(Iris, Iris_preference, by=”Species”)</span><span id="5a9c" class="mn ke iq mj b gy ms mp l mq mr"># Pyspark</span><span id="1675" class="mn ke iq mj b gy ms mp l mq mr">Iris.join(Iris_preference,[‘Species’],”left_outer”)</span></pre><p id="5acd" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">这真的很神奇，我们有很多方法来表达相同的意图，在编程语言和自然语言中。</p><h1 id="76db" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">数据争论的备忘单</h1><p id="41c1" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">语言丰富是福也是祸。对于社区来说，在未来对跨语言的公共数据操作的 API 进行标准化可能不是一个坏主意，这可以消除摩擦并增加可移植性。到目前为止，我调查了 Pandas、Tidyverse、Pyspark 和 SQL 中的 filter、aggregate 和 join 操作，以突出我们每天最常处理的语法细微差别。上面的并排比较不仅可以作为一个备忘单来提醒我语言的差异，还可以帮助我在这些工具之间进行转换。在我的 git 库<a class="ae kc" href="https://github.com/yuzhoux/pythonUtilityFunctions/tree/master/Cheatsheet" rel="noopener ugc nofollow" target="_blank">这里</a>有这个备忘单的一个更简洁的版本。此外，你可能会发现其他特定语言的备忘单很有帮助，它们来自 Pandas.org、R Studio 和 DataCamp。</p><blockquote class="mw mx my"><p id="b71b" class="lb lc mz ld b le lz lg lh li ma lk ll na mb lo lp nb mc ls lt nc md lw lx ly ij bi translated">与熊猫的数据角力:<a class="ae kc" href="https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="e3c4" class="lb lc mz ld b le lz lg lh li ma lk ll na mb lo lp nb mc ls lt nc md lw lx ly ij bi translated">与 dplyr 和 tidyr 的数据角力:<a class="ae kc" href="https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="fc1d" class="lb lc mz ld b le lz lg lh li ma lk ll na mb lo lp nb mc ls lt nc md lw lx ly ij bi translated">python for Data Science py spark:<a class="ae kc" href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></p></blockquote></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><p id="f213" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">如果你想在我的小抄里增加额外的数据运算对，那就连上吧！</p></div></div>    
</body>
</html>