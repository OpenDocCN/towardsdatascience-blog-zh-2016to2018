# 反向传播的问题是

> 原文：<https://towardsdatascience.com/the-problem-with-back-propagation-13aa84aabd71?source=collection_archive---------1----------------------->

神经网络依靠梯度下降的反向传播来设置神经元连接的权重。它有效，可靠地最小化了成本函数。研究人员喜欢它，因为他们有证据证明反向传播是可行的。然而，它的成功并不排除*其他*方法优化神经元的连接。而且，它有一个主要的限制:一旦网络学习了一组权重，任何新的学习都会导致*灾难性的遗忘*。

[DARPA](https://www.darpa.mil/news-events/2017-03-16) 希望设计出*能够持续学习*的神经网络，不会遗忘，也不需要对它们的整个经历历史进行重新训练。我们知道这样的网络是可能的——我们自己的大脑能够学习新信息，而无需忘记或重新训练。他们寻找的网络不太可能使用反向传播，因为反向传播针对固定目标优化了网络。

**固定目标与动态姿态**

目前，神经网络被训练成擅长于一项*预定的*任务，它们的连接一旦被部署就被*冻结*。没有额外的学习发生。这类似于一辆赛车:该车辆针对在光滑赛道上比赛的任务进行了优化，一旦展开，其设计就*不变。赛车擅长在那些光滑的赛道上行驶，但它不适合越野行驶。是‘一招小马’！*

如果工程师对赛车进行*改装*，希望能让它越野，**这些改装会损害它的赛道性能**。修改后的设计已经'*忘记了'*如何在赛道上表演。

相比之下，人类能够适应各种环境。弹钢琴的手指也能穿针引线。我们的手没有提前为一个单一的任务优化。因此，我们能很好地解决新问题。机器智能不需要这种动力来完成固定的任务，比如语言之间的翻译。然而，这些固定的任务只是我们希望机器将扮演的角色的一小部分。我们需要能做出动态姿势的人工智能。

**在野外学习**

为了学习新的东西，神经网络必须衡量现有知识的*适当性*——“这个新任务是否遵循与我已经知道的东西相同的**原理或动力学**”有没有我已经知道的与这项新任务相关的**特征或品质**？它需要分析现有的技术，并通过这些领域传递信息。目前，[混合专家](https://research.google.com/pubs/pub45929.html)网络最接近这个理想；它们通过较小的神经网络的组合来解析输入，并组合它们的输出。每个专家处理一个子任务或抽象，对于一个给定的输入，只有少数专家被调用。谷歌的杰夫·迪恩是这些稀疏网络的粉丝。

然而，混合专家仍然通过梯度下降的反向传播进行训练。因为每个专家仅用于输入的*少数实例*，反向传播缓慢且不可靠。当新的情况出现时，专家的组合不能快速适应它的分析。如果一种情况需要一种新的专业知识，现有的专家组合不能**增加**这种专业化。我们需要一个替代方案。

相比之下，*决策树*从相反的方向处理问题:反向传播自顶向下修改神经元，而决策树自底向上解析。它们也被证明对于优化各种固定任务是有效的，但是它们的分支仍然不能动态学习。要像人类一样学习，机器智能还需要其他东西。将新信息解析为“我已经知道的事情”和“我不知道的事情”，然后基于该解析形成输入的路由。

例如:假设我们假设的机器智能已经学会预测刚性下落物体的行为，然后给它一个弹力球。弹力球遵循与刚性球相同的抛物线轨迹，因此机器智能应该*识别相似性*并应用抛物线预测。然而，一旦弹力球落在一个表面上，它就会变形，并反弹到比刚性球高得多的位置。这是对它所学内容的修改——机器智能将需要*调整现有的专业知识*，并且它需要学习*何时应用这种调整*。在第一次反弹后，机器智能应该识别出球是有弹性的，并相应地调整其预测轨迹。对现有知识进行类似的解析和改编对于理解下落的粘土球也是必要的——粘土完全变形，根本不会反弹！

**由内向外学习**

我们自己的大脑不会像反向传播那样自上而下地学习，也不会像决策树那样自下而上地学习。相反，我们通过*亲和力*形成新的联系——“什么一起燃烧，什么一起连线”。这个过程发生在每个深度，沿着我们大脑的内部形成连接。

更重要的是，我们的大脑从一套*多样的解释*开始，慢慢地[确定一个折中的方案](https://www.quantamagazine.org/how-nature-solves-problems-through-computation-20170706/)。我们大脑中的每一群“专家”都会得出一个略有不同的结论，他们的评估会反复出现，直到他们找到一个对大多数人都有意义的解释。就像一群鱼一样，它们的个体取向影响着它们的邻居，它们的集体运动是它们个体运动之间的妥协。

更具体地说:假设你看到一个戴着猫耳朵的女孩的图像。你大脑的一部分识别出她的脸型，然后说“人类！”，而其他部分识别猫耳朵，以“猫！”…传统的神经网络会简单地输出一个分布:“我 50%确定这是一张女孩的照片，30%确定这是一张猫的照片。”它无法重新解读自己的类别——**传统的神经网络认为你的图片要么是‘猫’，要么是‘女孩’，*但不能两者都是*！**

与传统的神经网络不同，你的大脑不会停留在“女孩或猫”上。每个区域展示了相声，有效地比较笔记。任何意见上的分歧都会产生不和谐，你的大脑会继续它的串音，直到不和谐消失。你大脑中说“猫”的那部分问那个说“女孩”的部分一个问题:“有没有可能是你搞错了，图片真的是猫？”那个“女孩！”region 回应:“不，这绝对是个女孩… *你*一定搞错了。”“猫！”区域检查，并说“嗯，它可能不是一只猫，但在肯定有*猫耳朵*。”“好吧，”另一个区域说，“那么这一定是一个新的类别——女孩身上的猫耳朵。”“我不能不同意，”猫说。地区。他们已经就一个新的类别达成了一致。

**特征组合**

在我们的“有猫耳朵的女孩”的例子中，你的大脑确信它看到了一个女孩——“女孩”的反应非常强烈。然而，对于“猫耳朵”的特征也有强烈的反应，这鼓励了对猫的解释。你的大脑能够通过问自己来解决这种不协调:“是的，有猫的耳朵，但是猫的其他特征也在那里吗？—不！”

我们的大脑与*紧密聚集的特征探测器*一起工作，它们被连接在一起形成**新的组合**，新的类别。你可能会看到一张照片，你大脑的特征探测器会识别出:“它像一匹马，但很结实，鼻子上有两只角……”**传统的神经网络会试图将图像放在预先存在的类别中**，说“60%确定它是一匹马”，但这将是错误的。相比之下，我们的大脑识别出这不是马——这是一头犀牛！然后**我们在这些特征**之间连线，这样*的组合*中的‘马’、‘矮壮’和‘角’就有了‘犀牛’的路线。

连接新的特征组合是隐喻的本质。**我们的大脑根据存在的*抽象星座*形成类别**。“弹力球像石头一样下落，但它也向上跳跃，并再次开始下落……”“电子像波一样穿过双缝实验，其波纹相互干涉，在探测器表面形成图案。”"爱就像挠痒痒，就像你肚子里的蝴蝶."传统的神经网络不关注当前的特征星座；他们只评估预先存在的类别的可能性。这是我们的智力和他们的智力之间的关键区别。

**功能接线星座**

使用反向传播不可能识别特征群*。当传统神经网络的输出层说“60%是马”时，它不会告诉你**哪些特征是活跃的，从而导致了那个结论**。它已经丢失了那个信息*。所以，反向传播无法区分一头**犀牛**和一头**驴**—*都被简单地称为“六成马”。*

*当我们的大脑学习识别一只猫时，他们正在学习通常存在的一系列特征。如果这些特征中的大部分是活跃的，并且没有竞争的特征是活跃的，我们的大脑会迅速达成一致，认为他们看到了一只猫。然而，如果竞争特征是活跃的，我们的大脑会检查**一致性**——他们会问“*那个特征可能是个错误吗？*“任何*可能出现的错误*都被*压制*，我们的大脑再次检查。如果仍然有竞争的特征，我们的大脑**将它们连接在一起**，形成一个新的星座，一个新的类别。*

*这是一个**抑制不确定性**和**布线亲和力**的过程。并且，它发生在网络的每个深度*，而不需要等待来自输出层的反向传播*。它还为每个要素星座形成新的类别，而不是在输出层保持一组固定的类别。如果我们希望人工智能能够在飞行中学习新事物，并形成抽象和隐喻，我们将需要这种基于亲和力的连接。*

***抑制不确定性和布线亲和性***

*当我们的大脑问“这个特征可能是个错误吗？”他们通过*抑制*该特征的一些输入来实现。在神经网络实现中，这可以通过 ***暂时降低这些神经元*** 的权重，并再次运行输入来表示。如果特征*仍然在抑制权重的情况下*触发，那么可以肯定这不是一个错误。*

*而且，当我们的大脑试图组合一组新的特征时，他们会问“哪些特征是一起出现的？”为了在人工神经网络中实现这一点，一个*独立的*神经元集群被连接**以与触发**的每个特征相连接，这样*新的集群仅在整个星座激活*时才被触发。这些特征可能位于网络的不同深度，有些可能位于低级纹理和边缘检测器图层，而有些则位于高级位置和比例图层。它们都被直接连接成一个新的集群，当所有的特性出现时就会触发。*

*从*每一个*深度连接新的特征星座可以识别不同的物体:一个闪亮、光滑、玫瑰色的球将与一个粗糙、棕色、有纹理图案的球截然不同——网络可能会称一个为“玻璃珠”,另一个为“木制球体”。哑光、棕色和纹理图案的低层次特征可能会出现在窄腿的矩形物体上，即“木制咖啡桌”。网络了解到低级特征的星座与*木制*物体相关联，而高级特征区分它们的形状和功能。*

*如果这样的神经网络看到了一个*新的*物体，并且没有识别出它的高级形状特征，*它可能仍然会意识到这个物体是由木头*制成的，因为它是无光泽的，棕色的，并且有纹理图案。这些洞察力是普通智力的组成部分。如果我们寻找真正学习的机器，我们将需要根据这一原理设计的神经网络。*