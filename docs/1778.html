<html>
<head>
<title>Teaching a Variational Autoencoder (VAE) to draw MNIST characters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">教一个变分自动编码器(VAE)画 MNIST 字符</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/teaching-a-variational-autoencoder-vae-to-draw-mnist-characters-978675c95776?source=collection_archive---------1-----------------------#2017-10-20">https://towardsdatascience.com/teaching-a-variational-autoencoder-vae-to-draw-mnist-characters-978675c95776?source=collection_archive---------1-----------------------#2017-10-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/29ac37a086f2965ba45066550072eebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*woWzbXU2bmshM1czEur72g.gif"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">These characters have not been written by a human — we taught a neural network how to do this!</figcaption></figure><p id="3da5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="ld">要查看完整的 VAE 代码，请参考我的</em> <a class="ae le" href="https://github.com/FelixMohr/Deep-learning-with-Python/blob/master/VAE.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="ld"> github </em> </a> <em class="ld">。</em></p><p id="753b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">自动编码器是一种神经网络，可用于学习输入数据的有效编码。给定一些输入，网络首先应用一系列变换，将输入数据映射到低维空间。网络的这一部分被称为<em class="ld">编码器</em>。</p><p id="e13a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">然后，网络使用编码数据尝试重新创建输入。网络的这部分就是<em class="ld">解码器</em>。使用编码器，我们可以压缩网络能够理解的数据类型。然而，自动编码器很少用于此目的，因为通常存在更有效的手工算法(如<em class="ld"> jpg </em> -compression)。</p><p id="58f2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">相反，自动编码器被反复应用于执行去噪任务。编码器接收被噪声篡改的图片，并学习如何重建原始图像。</p><h1 id="81fd" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">什么是变分自动编码器？</h1><p id="14db" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">然而，自动编码器还有更有趣的应用。</p><p id="0fbf" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">一个这样的应用叫做<strong class="kh iu">变分自动编码器</strong>。使用可变自动编码器，不仅可以压缩数据，还可以生成自动编码器以前见过的新对象类型。</p><p id="8183" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">使用一个通用的自动编码器，我们对网络产生的编码一无所知。我们可以比较不同的编码对象，但是我们不太可能理解发生了什么。这意味着我们将不能使用我们的解码器来创建新的对象。我们只是不知道输入应该是什么样子。</p><p id="1b00" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">使用变分自动编码器，我们采取相反的方法。我们不会去猜测潜在向量的分布。我们只是告诉我们的网络我们希望这个分布看起来像什么。</p><p id="105c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">通常，我们将约束网络产生具有遵循单位正态分布的条目的潜在向量。然后，当试图生成数据时，我们可以简单地从该分布中采样一些值，将它们提供给解码器，解码器将返回给我们全新的对象，这些对象看起来就像我们的网络已经被训练过的对象。</p><p id="936f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们看看如何使用 Python 和 Tensorflow 来实现这一点。我们将教我们的网络如何画出<a class="ae le" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank"> MNIST 人物</a>。</p><h1 id="b895" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">第一步—加载培训数据</h1><p id="6ba5" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">首先，我们执行一些基本的导入。Tensorflow 有一个方便的功能，让我们可以轻松地访问 MNIST 数据集。</p><figure class="mi mj mk ml gt ju"><div class="bz fp l di"><div class="mm mn l"/></div></figure><h1 id="8e08" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">定义我们的输入和输出数据</h1><p id="7348" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">MNIST 图像的尺寸为 28 * 28 像素，具有一个颜色通道。我们的输入<code class="fe mo mp mq mr b">X_in</code>将是一批 MNIST 字符。网络将学习重新构建它们，并在占位符<code class="fe mo mp mq mr b">Y</code>中输出它们，占位符具有相同的尺寸。</p><p id="5ec5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><code class="fe mo mp mq mr b">Y_flat</code>将在以后计算损失时使用。<code class="fe mo mp mq mr b">keep_prob</code>将在申请退学作为正规化手段时使用。在训练期间，它的值将为 0.8。当生成新数据时，我们不会应用 dropout，因此值将为 1。</p><p id="ed4b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">函数<code class="fe mo mp mq mr b">lrelu</code>正在被定义，因为 Tensorflow 不幸没有提出预定义的泄漏 ReLU。</p><figure class="mi mj mk ml gt ju"><div class="bz fp l di"><div class="mm mn l"/></div></figure><h1 id="89ae" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">定义编码器</h1><p id="8448" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">因为我们的输入是图像，所以对它们应用一些卷积变换是最合理的。最值得注意的是，我们在编码器中创建了两个向量，因为编码器应该创建遵循高斯分布的对象:</p><ul class=""><li id="a899" class="ms mt it kh b ki kj km kn kq mu ku mv ky mw lc mx my mz na bi translated">均值向量</li><li id="6765" class="ms mt it kh b ki nb km nc kq nd ku ne ky nf lc mx my mz na bi translated">标准差的向量</li></ul><p id="d0a8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">稍后，您将看到我们如何“强制”编码器确保它真正创建遵循正态分布的值。将被馈送到解码器的返回值是<em class="ld"> z </em>值。在计算损失时，我们将需要分布的平均值和标准差。</p><figure class="mi mj mk ml gt ju"><div class="bz fp l di"><div class="mm mn l"/></div></figure><h1 id="56de" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">定义解码器</h1><p id="f5e3" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">解码器不关心输入值是否是从我们定义的特定分布中采样的。它将简单地尝试重建输入图像。为此，我们使用一系列转置卷积。</p><figure class="mi mj mk ml gt ju"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="5953" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，我们将两部分连接在一起:</p><figure class="mi mj mk ml gt ju"><div class="bz fp l di"><div class="mm mn l"/></div></figure><h1 id="4684" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">计算损失并实施高斯潜在分布</h1><p id="78e4" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">为了计算图像重建损失，我们简单地使用平方差(这可能导致图像有时看起来有点模糊)。这种损失与 Kullback-Leibler 散度相结合，确保我们的潜在值将从正态分布中取样。关于这个话题的更多信息，请看看 Jaan Altosaar 关于 VAEs 的文章。</p><figure class="mi mj mk ml gt ju"><div class="bz fp l di"><div class="mm mn l"/></div></figure><h1 id="82e0" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">训练网络</h1><p id="b83e" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">现在，我们终于可以训练我们的 VAE 了！</p><p id="fe0e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">每走 200 步，我们就会看一看当前的重建是什么样子。在处理了大约 2000 个批次后，大多数重建看起来是合理的。</p><figure class="mi mj mk ml gt ju"><div class="bz fp l di"><div class="mm mn l"/></div></figure><h1 id="8178" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">生成新数据</h1><p id="963b" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">最棒的部分是我们现在能够创造新的角色。为此，我们简单地从一个单位正态分布中抽取值，并将它们馈送给我们的解码器。大多数创造出来的角色看起来就像是人类写的一样。</p><figure class="mi mj mk ml gt ju"><div class="bz fp l di"><div class="mm mn l"/></div></figure><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/9e2859fbad764d0d8c53b789bf73e577.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s8rroD7abLrErIWuIzn_ag.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Some of the automatically created characters.</figcaption></figure><h1 id="bb84" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">结论</h1><p id="bf49" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">这是 VAEs 应用的一个相对简单的例子。但是想想什么是可能的！神经网络可以学习作曲。他们可以自动为书籍、游戏等制作插图。凭借一点创造力，VAEs 将为一些令人敬畏的项目开辟空间。</p><div class="ng nh gp gr ni nj"><a href="https://github.com/FelixMohr/Deep-learning-with-Python/blob/master/VAE.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd iu gy z fp no fr fs np fu fw is bi translated">Felix mohr/使用 Python 进行深度学习</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">在 GitHub 上创建一个帐户，为深度学习 Python 开发做贡献。</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">github.com</p></div></div><div class="ns l"><div class="nt l nu nv nw ns nx jz nj"/></div></div></a></div></div></div>    
</body>
</html>