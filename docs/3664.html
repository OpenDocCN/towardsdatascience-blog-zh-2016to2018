<html>
<head>
<title>Building Neural Networks in F# — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 F#构建神经网络—第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-neural-networks-in-f-part-2-training-evaluation-5e3a68889da6?source=collection_archive---------12-----------------------#2018-06-05">https://towardsdatascience.com/building-neural-networks-in-f-part-2-training-evaluation-5e3a68889da6?source=collection_archive---------12-----------------------#2018-06-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi ju"><img src="../Images/e3a0c8cda1604323b349e0cdbe4afb7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*sn0kIoimpTgJbQvOqxn-Dg.jpeg"/></div><figcaption class="kc kd gj gh gi ke kf bd b be z dk">Well, you’ll need F# for this so better read on!</figcaption></figure><h1 id="70cd" class="kg kh iq bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">快速回顾一下</h1><p id="346e" class="pw-post-body-paragraph le lf iq lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">欢迎来到本系列的第二部分。在<a class="ae mc" rel="noopener" target="_blank" href="/building-neural-networks-in-f-part-1-a2832ae972e6">第一部分</a>中，我经历了执行单一(随机)梯度更新所需的步骤。在这一部分中，我们扩展了这一功能，使网络能够从数据中学习任意关系。在我们开始之前，请确保您已经阅读并实现了我上一篇文章中的代码:</p><div class="md me gp gr mf mg"><a rel="noopener follow" target="_blank" href="/building-neural-networks-in-f-part-1-a2832ae972e6"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd ir gy z fp ml fr fs mm fu fw ip bi translated">用 F#构建神经网络—第 1 部分</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">函数式编码神经网络优雅吗？</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">towardsdatascience.com</p></div></div><div class="mp l"><div class="mq l mr ms mt mp mu ka mg"/></div></div></a></div></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="c3ed" class="kg kh iq bd ki kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld bi translated">通用逼近定理</h1><p id="3054" class="pw-post-body-paragraph le lf iq lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">第一个<a class="ae mc" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.441.7873&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">在 1989 年</a>被证明，神经网络可以被归类为通用函数逼近器。但是，<em class="na">这样的近似器是什么？</em></p><blockquote class="nb nc nd"><p id="b72f" class="le lf na lg b lh ne lj lk ll nf ln lo ng nh lr ls ni nj lv lw nk nl lz ma mb ij bi translated"><em class="iq">给定任意一个连续的 N 维函数</em> f(x) <em class="iq">，一个只有一个隐层和有限个神经元的神经网络有能力在固定的范围</em> x <em class="iq">和有限的误差</em> ε内逼近这样一个函数。</p></blockquote><p id="7b6d" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">我们将利用这个定理，在多个非平凡函数上评估一个简单的架构，以证明我们的网络具有从数据中学习的能力。</p><h1 id="7942" class="kg kh iq bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">一些助手功能</h1><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="68f3" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">让我们定义这些额外的辅助函数，它们将帮助我们进行培训和评估:</p><ul class=""><li id="0cad" class="no np iq lg b lh ne ll nf lp nq lt nr lx ns mb nt nu nv nw bi translated"><code class="fe nx ny nz oa b">genDataSet1D</code> —仅使用开始、结束和步进信息，该函数生成向量元组的数组，每个元组对应于一个<code class="fe nx ny nz oa b">(x, y)</code>对。注意，这可以是任何必要的维度，但是为了简单起见，本教程中的输入和输出都是一维的。</li><li id="ddab" class="no np iq lg b lh ob ll oc lp od lt oe lx of mb nt nu nv nw bi translated"><code class="fe nx ny nz oa b">yRealFn</code> —这是我们定义目标功能的地方。来自该函数的数据点将被采样并绘制在最终图上，以显示我们试图建模的实际底层函数。</li><li id="e8be" class="no np iq lg b lh ob ll oc lp od lt oe lx of mb nt nu nv nw bi translated"><code class="fe nx ny nz oa b">yRealFnAndNoise</code> —该功能只是在<code class="fe nx ny nz oa b">yRealFn</code>的输出中增加一个概率项，模拟数据中存在的噪声。然而，如果该误差没有适当调整，与确定性项(yRealFn)相比，它将过大，导致网络试图学习噪声而不是实际函数。</li><li id="6a98" class="no np iq lg b lh ob ll oc lp od lt oe lx of mb nt nu nv nw bi translated"><code class="fe nx ny nz oa b">meanFeatureSqErr</code>&amp;<code class="fe nx ny nz oa b">evalAngEpochErr</code>—这些功能将在本教程的培训部分解释。</li></ul><h1 id="d384" class="kg kh iq bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">初始化网络</h1><h2 id="463f" class="og kh iq bd ki oh oi dn km oj ok dp kq lp ol om ku lt on oo ky lx op oq lc or bi translated">定义架构</h2><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi os"><img src="../Images/c8d2c6687fd6e0e6d6e8c533d235153b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*72s92WUMi-DpkgwcqNUVSw.png"/></div><figcaption class="kc kd gj gh gi ke kf bd b be z dk">Sometimes, a single hidden layer is all we need…</figcaption></figure><p id="43a2" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">为了测试我们的网络，一个只有一个隐藏层的简单神经网络就足够了。我研究了学习任何合理的平滑函数所需的隐藏节点的数量，发现 8 个节点就足够了。</p><blockquote class="nb nc nd"><p id="a429" class="le lf na lg b lh ne lj lk ll nf ln lo ng nh lr ls ni nj lv lw nk nl lz ma mb ij bi translated">根据经验，选择太少的隐藏节点会导致欠拟合，反之太多会导致过拟合。</p></blockquote><p id="beda" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">当谈到选择激活功能时，手边有许多流行的:<em class="na"> tanh </em>，<em class="na"> sigmoid </em>，<em class="na"> ReLU </em>等。我决定用<strong class="lg ir"> tanh </strong>，因为它非常适合学习<em class="na">平滑函数</em>中存在的非线性，比如我们测试中会用到的那些。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/88da9cc11abbc1988be51f363c1ff9dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*eia9VbjtaA-dn8Cy9FEzBw.png"/></div><figcaption class="kc kd gj gh gi ke kf bd b be z dk">A plot of tanh(x) (red) and its derivative (green) [<a class="ae mc" href="http://www.junlulocky.com/actfuncoverview" rel="noopener ugc nofollow" target="_blank">source</a>].</figcaption></figure><h2 id="842d" class="og kh iq bd ki oh oi dn km oj ok dp kq lp ol om ku lt on oo ky lx op oq lc or bi translated">选择超参数</h2><p id="4cde" class="pw-post-body-paragraph le lf iq lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">如果你一直在跟踪机器学习，甚至从一开始你就学会欣赏超参数搜索或多或少的试错。当然，你也可以查看几个时期的成本函数图，做出你认为的最佳价值的有根据的猜测。</p><blockquote class="nb nc nd"><p id="7a1f" class="le lf na lg b lh ne lj lk ll nf ln lo ng nh lr ls ni nj lv lw nk nl lz ma mb ij bi translated">训练次数为~ <strong class="lg ir"> 3000 </strong>时，学习率为<strong class="lg ir"> 0.03 </strong>通常会给出足够好的结果。由于 F#中惊人的快速训练时间，我只能处理这么多的纪元！</p></blockquote><h2 id="e23a" class="og kh iq bd ki oh oi dn km oj ok dp kq lp ol om ku lt on oo ky lx op oq lc or bi translated">目标函数</h2><p id="d9d1" class="pw-post-body-paragraph le lf iq lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在我们开始训练之前，还有最后一步:选择合理复杂的函数，这些函数可以用来生成精确的数据集，同时在视觉上具有可比性。</p><p id="d4fb" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">您会注意到，对于位于相同范围内的所有输入，输出都受到闭合区间[0，1]的限制。这是一个深思熟虑的决定，以避免使用<a class="ae mc" href="https://en.wikipedia.org/wiki/Standard_score" rel="noopener ugc nofollow" target="_blank">均值归一化</a>，这有助于通过将所有特征保持在相似的范围内来防止过度加权。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><div class="gh gi ou"><img src="../Images/3e443d45fbdbb5f46690c48408895766.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y1jCKYdscq2n6rOUy-1Pww.png"/></div></div><figcaption class="kc kd gj gh gi ke kf bd b be z dk">A definition of our three test functions along with their plots</figcaption></figure><p id="f0c5" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">我们将使用步长为 0.01 的 x ∈ [0，1]来训练我们的网络。因此，在此范围内将产生 101 个等距样本，我们将保留其中的约 6%进行验证(统一选择)。</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="nm nn l"/></div></figure></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="109c" class="kg kh iq bd ki kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld bi translated">培训我们的网络</h1><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="oz nn l"/></div><figcaption class="kc kd gj gh gi ke kf bd b be z dk">If only he was taught about under-fitting…</figcaption></figure><h2 id="73d7" class="og kh iq bd ki oh oi dn km oj ok dp kq lp ol om ku lt on oo ky lx op oq lc or bi translated">概述</h2><p id="da41" class="pw-post-body-paragraph le lf iq lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">让我们首先从顶层收集一下培训需要哪些功能:</p><ul class=""><li id="af1f" class="no np iq lg b lh ne ll nf lp nq lt nr lx ns mb nt nu nv nw bi translated">对于每个时期，我们需要执行梯度下降，在我们的例子中，将随机地进行<em class="na"/>(每个样本一次)。然后，我们将使用更新后的网络在下一个时期进行进一步的训练。</li><li id="67a0" class="no np iq lg b lh ob ll oc lp od lt oe lx of mb nt nu nv nw bi translated">在此过程中，通过在每个时期结束时评估我们的网络并获得误差度量(例如，平方误差)来跟踪我们的训练误差将是有帮助的。</li></ul><h2 id="4c95" class="og kh iq bd ki oh oi dn km oj ok dp kq lp ol om ku lt on oo ky lx op oq lc or bi translated">使用地图折叠</h2><blockquote class="pa"><p id="62d3" class="pb pc iq bd pd pe pf pg ph pi pj mb dk translated">保持接近函数范式，而不是使用传统的 for 循环，我们将使用<code class="fe nx ny nz oa b">List.mapFold for training</code>开发一个巧妙的技巧。</p></blockquote><p id="2cab" class="pw-post-body-paragraph le lf iq lg b lh pk lj lk ll pl ln lo lp pm lr ls lt pn lv lw lx po lz ma mb ij bi translated">那么<em class="na">地图折叠</em>是如何工作的呢？</p><p id="8cf1" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">简单来说就是<code class="fe nx ny nz oa b">List.fold</code>和<code class="fe nx ny nz oa b">List.map</code>的高效结合。它允许在给定当前列表元素和先前状态的情况下计算新的状态，同时使用我们选择的函数转换当前列表元素。该函数的最终输出是一个由<strong class="lg ir">转换列表</strong>和<strong class="lg ir">最终状态</strong>组成的元组。</p><h2 id="4141" class="og kh iq bd ki oh oi dn km oj ok dp kq lp ol om ku lt on oo ky lx op oq lc or bi translated">履行</h2><p id="0023" class="pw-post-body-paragraph le lf iq lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在<code class="fe nx ny nz oa b">trainEpoch</code>函数中，训练数组首先被随机打乱(就地)。这个新洗牌后的数组被送入<code class="fe nx ny nz oa b">perfGradDesc</code>。该函数通过使用<code class="fe nx ny nz oa b">Array.fold</code>传播网络更新，在整个训练数据集上一次执行一个样本的梯度下降(单遍)。</p><p id="f95d" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">一旦我们训练了一遍网络，并从<code class="fe nx ny nz oa b">trainEpoch</code>获得了最终网络，我们需要使用我们选择的成本函数来评估该时期的训练误差。每个样本的<em class="na">误差</em>需要与<code class="fe nx ny nz oa b">lastLayerDeriv</code>功能一致。因此，<em class="na">平方误差</em>是由<code class="fe nx ny nz oa b">meanFeatureSqErr</code>函数实现的误差度量的最合适选择。</p><p id="68fc" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">注意:虽然我们的网络可以支持任何维度的输出，但当涉及到成本函数时，每个样本只有一个标量度量是有用的。因此，每个样本的<em class="na">误差</em>将是每个输出特征的平方误差的<em class="na">平均值。这不会影响一维输出的误差，如果所有输出维度的比例大致相同，这是一个合理的选择。</em></p><p id="0196" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">在对整个训练集的每个样本的<em class="na">误差</em>进行平均后，我们获得了<strong class="lg ir">均方误差</strong>。该值由<code class="fe nx ny nz oa b">evalAvgEpochErr</code>功能计算得出。</p><p id="1728" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">最后，在每个时期结束时，<code class="fe nx ny nz oa b">(xAndyShuffArr,newNet)</code>是传播到下一个时期的新的<em class="na">状态</em>，而<code class="fe nx ny nz oa b">err</code>是替换原始列表中时期号的<em class="na">映射对象</em>。</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="nm nn l"/></div><figcaption class="kc kd gj gh gi ke kf bd b be z dk">The code for training our network</figcaption></figure></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="8d4b" class="kg kh iq bd ki kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld bi translated">结果</h1><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><div class="gh gi pp"><img src="../Images/be34bb817523e5377566d1e0e27706c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*qi8_F6IA0uIEd2qBw0AxOw.gif"/></div></div><figcaption class="kc kd gj gh gi ke kf bd b be z dk">The results for y = <em class="pq">0.4x² + 0.2 + 0.3x*sin8x, </em>spanning multiple epochs.</figcaption></figure><p id="71ab" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">所以这里有一些视觉证明，网络实际上是在训练！为了绘制我们的数据，我们将使用我过去在 F#中用来绘制的<code class="fe nx ny nz oa b">PLplot</code>库。绘制这些图形还需要一些额外的代码，但是可以在 Github 资源库中找到，这个资源库在文章的结尾有链接。</p><h2 id="854e" class="og kh iq bd ki oh oi dn km oj ok dp kq lp ol om ku lt on oo ky lx op oq lc or bi translated">神话；传奇</h2><ul class=""><li id="ce76" class="no np iq lg b lh li ll lm lp pr lt ps lx pt mb nt nu nv nw bi translated"><strong class="lg ir">粉色</strong> <strong class="lg ir">线</strong>代表底层<strong class="lg ir">真函数</strong>无噪声。这仅用于绘图，但从不用于任何计算。</li><li id="0e3d" class="no np iq lg b lh ob ll oc lp od lt oe lx of mb nt nu nv nw bi translated"><strong class="lg ir">青色</strong> <strong class="lg ir">点</strong>是整个<strong class="lg ir">数据集</strong>，包括训练和测试数据点。所有误差都是参照这些点计算的</li><li id="4e4d" class="no np iq lg b lh ob ll oc lp od lt oe lx of mb nt nu nv nw bi translated">绿色<strong class="lg ir"/><strong class="lg ir">圆圈</strong>代表在<strong class="lg ir">训练</strong> <strong class="lg ir">数据</strong>上评估的<strong class="lg ir">最终假设</strong>。随着训练时期数量的增加，您可以观察到这些曲线越来越接近真实的函数曲线</li><li id="3c54" class="no np iq lg b lh ob ll oc lp od lt oe lx of mb nt nu nv nw bi translated">最后，<strong class="lg ir">粉色</strong> <strong class="lg ir">十字</strong>代表基于<strong class="lg ir">测试数据</strong>评估的最终假设。如果你仔细观察，你会注意到他们很好地跟踪了训练数据点，并且这被训练与测试的 MSE 分数所证实</li></ul><h2 id="acd5" class="og kh iq bd ki oh oi dn km oj ok dp kq lp ol om ku lt on oo ky lx op oq lc or bi translated">评估目标函数</h2><div class="jv jw jx jy gt ab cb"><figure class="pu jz pv pw px py pz paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><img src="../Images/b1743bda79760819390071686d52f9da.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*FKT2HEz-_t3L9k7LQoFD0w.png"/></div></figure><figure class="pu jz pv pw px py pz paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><img src="../Images/255abf30db96483242541378d5ff51c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*kGJ2vcsZTUuhEkN1wk1Rtw.png"/></div></figure><figure class="pu jz pv pw px py pz paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><img src="../Images/b092cfc9498a5d3a845c75e4a678784a.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Q7NR4BiR4FHZ-fBEtXYl-g.png"/></div><figcaption class="kc kd gj gh gi ke kf bd b be z dk qa di qb qc">From left to right: f(x) = <em class="pq">0.4x² + 0.2 + 0.3x*sin8x</em>, g(x) = 0.6e^-(0.3sin12x/(0.3+x)), h(x) = 0.4 - 0.1log(x+0.1) + 0.5x*cos12x</figcaption></figure></div><p id="aee2" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">这是我们三个测试函数的结果，你可以点击每一个来查看更多细节。只需快速浏览一下，很明显，网络通常在更平滑的函数上表现良好。随着每个函数的驻点附近的梯度增加，神经网络的训练也变得更加困难(观察损失曲线)。这可以与我们使用 tanh(x)作为激活函数的事实联系起来。</p><p id="9555" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">尽管如此，我们现在有了直观的证据，证明了普适近似定理是成立的！</p><p id="fac1" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">任务已完成💪🏼</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="8a0a" class="kg kh iq bd ki kj mv kl km kn mw kp kq kr mx kt ku kv my kx ky kz mz lb lc ld bi translated">结论</h1><p id="c310" class="pw-post-body-paragraph le lf iq lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">现在，我们已经定义了基本架构，并且能够对任意数据进行训练，我们可以使用网络来训练和测试流行的数据集。这是留给你的任务…为什么不试试著名的<a class="ae mc" href="http://lib.stat.cmu.edu/datasets/boston" rel="noopener ugc nofollow" target="_blank">波士顿房价数据集</a>？</p><p id="18f2" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">这段代码在机器学习的世界里还有很长的路要走。它们是许多扩展，甚至可以对最简单的神经网络进行扩展以改善收敛性:批量标准化、小批量梯度下降、自定义权重初始化等等。</p><p id="5134" class="pw-post-body-paragraph le lf iq lg b lh ne lj lk ll nf ln lo lp nh lr ls lt nj lv lw lx nl lz ma mb ij bi translated">嗯，不要担心，只是在 F#上训练 3000 个历元需要大约<strong class="lg ir"> 3 秒</strong>的事实使它成为快速神经网络修补和评估的良好潜在候选对象。使用非常相似的设计原则和相似的参数选择，这个脚本的 python 版本需要大约 168 秒(在 Google Collab 上)！</p><blockquote class="pa"><p id="c04b" class="pb pc iq bd pd pe pf pg ph pi pj mb dk translated"><em class="pq">正如承诺的，所有的源代码都可以在</em><a class="ae mc" href="https://github.com/hsed/funct-nn" rel="noopener ugc nofollow" target="_blank"><em class="pq">Github</em></a><em class="pq">上获得。</em></p></blockquote><p id="f7e5" class="pw-post-body-paragraph le lf iq lg b lh pk lj lk ll pl ln lo lp pm lr ls lt pn lv lw lx po lz ma mb ij bi translated">我希望你能从过去的两个教程中获得一些新的东西，它们是一个漫长的过程，但我在这个过程中学到了很多。一如既往，请随时分享您的建议，反馈和对我的下一个教程的任何建议。如果你喜欢这篇文章，请留下👏🏼或者两个…下次见！👋🏼</p></div></div>    
</body>
</html>