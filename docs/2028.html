<html>
<head>
<title>Ensemble Learning in Machine Learning | Getting Started</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的集成学习|入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ensemble-learning-in-machine-learning-getting-started-4ed85eb38e00?source=collection_archive---------2-----------------------#2017-12-06">https://towardsdatascience.com/ensemble-learning-in-machine-learning-getting-started-4ed85eb38e00?source=collection_archive---------2-----------------------#2017-12-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/22b3249705cc464a5c6a008fc4c8efe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V2J3_sAk8Tb2M7zm2T_hKA.png"/></div></div></figure><p id="dc0c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">使用各种不同的模型比只使用一种模型要可靠得多。在单个集合上一起工作的几个模型的集合称为集合。这种方法被称为集成学习。</p><h1 id="c83e" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">投票</h1><p id="8f05" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">您可以使用不同的算法训练您的模型，然后集成它们以预测最终输出。比方说，你使用随机森林分类器，SVM 分类器，线性回归等。；模型相互竞争，并通过使用来自<code class="fe mc md me mf b">sklearn.ensemble</code>的<code class="fe mc md me mf b">VotingClassifier</code>类投票选出最佳性能。</p><p id="0f0b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="mg">硬投票</em>是从集合中选择一个模型，通过简单多数投票进行最终预测，以确保准确性。</p><p id="5ae0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="mg">软投票</em>只能在所有分类器都能计算出结果的概率时进行。软投票通过平均各个算法计算出的概率来达到最佳结果。</p><p id="ef6e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">代码:</p><figure class="mh mi mj mk gt ju"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="52ed" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><code class="fe mc md me mf b">VotingClassifier</code>的准确率一般高于个体分类器。确保包含不同的分类器，这样容易出现类似错误的模型不会聚集错误。</p><h1 id="fdb3" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">装袋和粘贴</h1><p id="bcdb" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">您可以在数据集的各种随机子集上使用单个模型，而不是在单个数据集上运行各种模型。替换随机抽样称为<em class="mg">装袋</em>，简称<em class="mg">自举汇总</em>。如果很难在脑海中想象，就想象一下忽略数据集中的几个随机条目，用其余的条目建模。在<em class="mg">粘贴</em>的情况下，同样的过程适用，唯一的区别是粘贴不允许为相同的预测器多次采样训练实例。</p><p id="b04e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">代码:</p><figure class="mh mi mj mk gt ju"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="5bf9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><code class="fe mc md me mf b">bootstrap=True</code>参数指定装袋的用途。对于粘贴，将参数更改为<code class="fe mc md me mf b">bootstrap=False</code>。</p><p id="4597" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果分类器可以计算其预测的概率，则 BaggingClassifier 会自动执行软投票。这可以通过检查你的分类器是否有一个<code class="fe mc md me mf b">predict_proba()</code>方法来验证。</p><p id="e1f5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">装袋通常比粘贴效果好得多。</p><h1 id="ab41" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">袋外评估</h1><p id="58b6" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">当对训练集执行 Bagging 时，只有 63%的实例包括在模型中，这意味着有 37%的实例分类器以前没有见过。这些可以像交叉验证一样用于评估。</p><p id="e1c5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">要使用这个功能，只需在前面示例中的<code class="fe mc md me mf b">BaggingClassifier</code>类中添加一个<code class="fe mc md me mf b">oob_score = True</code>参数。</p><p id="2583" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">密码</p><figure class="mh mi mj mk gt ju"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="17a4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">到目前为止，只对实例进行了采样。对于包含大量要素的数据集，还有其他方法。</p><h1 id="ae33" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">随机补丁和随机子空间</h1><p id="c6a3" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated"><em class="mg">随机补丁</em>对训练实例和特征进行采样。</p><p id="0200" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在<code class="fe mc md me mf b">BaggingClassifier()</code>中设置某些参数可以为我们实现这一点:</p><figure class="mh mi mj mk gt ju"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="f3fb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">随机子空间保留除样本特征之外的所有实例。</p><p id="1e8f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这是通过以下方式实现的:</p><figure class="mh mi mj mk gt ju"><div class="bz fp l di"><div class="ml mm l"/></div></figure><h1 id="035b" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">随机森林</h1><p id="7007" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">决策树的集合是一个随机森林。Random Forests 在内部执行装袋。随机森林创建几棵树，有时是几千棵树，并为给定数据集计算最佳可能模型。随机森林算法不是在分割节点时考虑所有特征，而是从所有特征的子集中选择最佳特征。这用较高的偏差换取了较低的方差，从而产生了更好的模型。</p><p id="a275" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">代码:</p><figure class="mh mi mj mk gt ju"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="eb87" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">参数:<code class="fe mc md me mf b">n_estimators</code>是森林中树木的极限数量。<code class="fe mc md me mf b">max_leaf_nodes</code>用于设置端节点的最大数量，这样算法就不会深入单个特征并过度拟合模型(阅读关于决策树的详细解释)。<code class="fe mc md me mf b">n_jobs</code>指定您的计算机要使用的内核数量；<code class="fe mc md me mf b">-1</code>值意味着所有最大可能的核心。</p><p id="6f7f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">使用网格搜索可以通过改变参数值来改进模型。</p><h1 id="1cbc" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">adaboost 算法</h1><p id="eb51" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">虽然 AdaBoost 技术的函数数学相当令人生畏，但其原理相当简单。首先，您选择一个基础分类器，它对给定的集合进行预测。记下错误分类的实例。错误分类实例的权重增加。用更新的权重在训练集上训练第二分类器。</p><p id="3f94" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">简单来说，运行分类器并进行预测。运行另一个分类器来拟合以前错误分类的实例并进行预测。重复进行，直到所有/大部分训练实例都适合为止。</p><p id="c5fa" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">AdaBoost 使用的不是决策树，而是一个<em class="mg">决策树桩</em>，它是一个带有<code class="fe mc md me mf b">max_depth = 1</code>的决策树，即一个决策节点和两个叶节点的树。AdaBoost 中的<code class="fe mc md me mf b">n_estimators</code>参数设置决策树桩的数量。</p><p id="fee1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">密码</p><figure class="mh mi mj mk gt ju"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="2d53" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">Scikit-learn 使用 Adaboost 的多类版本 SAMME ( <em class="mg">使用多类指数损失函数</em>的阶段式加法建模)。如果预测器可以计算概率(有<code class="fe mc md me mf b">predict_proba()</code>方法)，Scikit Learn 使用 SAMME。R ( <em class="mg"> R 代表实数</em>)依赖于概率，不容易过度拟合。</p><p id="469e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在过度拟合的情况下，尝试调整你的基本估计量。</p><h1 id="11ba" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">梯度推进</h1><p id="c242" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">与 AdaBoost 类似，梯度提升也适用于添加到集成中的连续预测模型。梯度提升不是像 AdaBoost 那样更新训练实例的权重，而是使新模型适合残差。</p><p id="8286" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">简而言之，使模型适合给定的训练集。计算残差，该残差成为新的训练实例。一个新的模型在这些上面被训练等等。选择所有模型的总和来进行预测。</p><p id="93e4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">密码</p><figure class="mh mi mj mk gt ju"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="f73b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">学习率参数缩小了每棵树的贡献。在<code class="fe mc md me mf b">learning_rate</code>和<code class="fe mc md me mf b">n_estimator</code> s 之间有一个折衷。降低 learning_rate 的值会增加集合中的树的数量。这叫做<em class="mg">收缩</em>。将估计数增加到一个较大的值可能会使模型过拟合。见<em class="mg">早停</em>。必须仔细监控这种权衡。</p><h1 id="67d4" class="kz la it bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">XGBoost</h1><p id="867a" class="pw-post-body-paragraph kb kc it kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">XGBoost 是一种最新、最受欢迎且功能强大的梯度增强方法。XGBoost 没有在叶节点上做出艰难的是或否的决定，而是为做出的每个决定分配正值和负值。所有的树都是弱学习者，并且提供比随机猜测稍好的决策。但是总的来说，XGBoost 的表现非常好。</p><p id="a13f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">密码</p><figure class="mh mi mj mk gt ju"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="2c82" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">XGBoost 可以处理树和线性模型。我建议阅读 XGBoost 文档，了解更多的参数调优选项。</p><p id="183b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">XGBoost 最近获得了很大的人气，并被用在了最获胜的 Kaggle 竞赛模型中。这是您数据科学工具箱中的强大工具。</p><p id="0a55" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下次见。建议编辑和改进。保持(机器)学习。</p><blockquote class="mn mo mp"><p id="0790" class="kb kc mg kd b ke kf kg kh ki kj kk kl mq kn ko kp mr kr ks kt ms kv kw kx ky im bi translated">本文的灵感来自 Aurélien Géron 的书《使用 Scikit-Learn 和 TensorFlow 进行机器学习:构建智能系统的概念、工具和技术》。我已经尽力把事情简单化了。希望有帮助。在这里得到这本书:<a class="ae mt" href="http://shop.oreilly.com/product/0636920052289.do" rel="noopener ugc nofollow" target="_blank">http://shop.oreilly.com/product/0636920052289.do</a></p></blockquote></div></div>    
</body>
</html>