<html>
<head>
<title>Deep dive into multi-label classification..! (With detailed Case Study)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入探讨多标签分类..！(附有详细的案例研究)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff?source=collection_archive---------0-----------------------#2018-06-08">https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff?source=collection_archive---------0-----------------------#2018-06-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1e48" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">有毒-评论分类。</h2></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/e3f8575a907f89e8e330d3b4db288da1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r0gYXMSQf5VhdMyl2bRDyg.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Fig-1: Multi-Label Classification to finde genre based on plot summary.</figcaption></figure></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><blockquote class="lc ld le"><p id="161e" class="lf lg lh li b lj lk jr ll lm ln ju lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">随着可用数据的持续增加，迫切需要对其进行组织，并且现代分类问题通常涉及同时与单个实例相关联的多个标签的预测。</p><p id="eb95" class="lf lg lh li b lj lk jr ll lm ln ju lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">被称为多标签分类，它是一个这样的任务，在许多现实世界的问题中无所不在。</p><p id="5748" class="lf lg lh li b lj lk jr ll lm ln ju lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在这个项目中，以 Kaggle 问题为例，我们探索了多标签分类的不同方面。</p><p id="cf33" class="lf lg lh li b lj lk jr ll lm ln ju lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ir"> <em class="iq">数据来源免责声明:</em> </strong> <em class="iq">数据集包含可能被认为亵渎、低俗或冒犯的文字。</em></p></blockquote></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="cf11" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">项目鸟瞰图:</h1><ul class=""><li id="0cbc" class="mu mv iq li b lj mw lm mx my mz na nb nc nd mb ne nf ng nh bi translated"><strong class="li ir">第 1 部分:</strong>多标签分类概述。</li><li id="2137" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated"><strong class="li ir">第二部分:</strong>问题定义&amp;评估指标。</li><li id="9989" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated"><strong class="li ir"> Part-3: </strong>探索性数据分析<em class="lh"> (EDA) </em>。</li><li id="fbc8" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated"><strong class="li ir">第四部分:</strong>数据预处理。</li><li id="e6ae" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated"><strong class="li ir">第五部分:</strong>多标签分类技术。</li></ul></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="3c51" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">第 1 部分:多标签分类概述:</h1><ul class=""><li id="b384" class="mu mv iq li b lj mw lm mx my mz na nb nc nd mb ne nf ng nh bi translated">多标签分类起源于对文本分类问题的研究，其中每个文档可能同时属于几个预定义的主题。</li><li id="76a1" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">文本数据的多标签分类是一个重要的问题。例子从新闻文章到电子邮件。例如，这可以用来根据电影情节的概要找到电影所属的类型。</li></ul><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/89fbfee958850bc5074b061819ab55a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*-be_KRYy6hMZTZk7UONw_A.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Fig-2: Multi-label classification to find genres based on movie posters.</figcaption></figure><ul class=""><li id="04ed" class="mu mv iq li b lj lk lm ln my no na np nc nq mb ne nf ng nh bi translated">或者基于电影海报的流派多标签分类。<em class="lh">(这就进入了计算机视觉的领域。)</em></li><li id="644f" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">在多标签分类中，训练集由与一组标签相关联的实例组成，任务是通过分析具有已知标签集的训练实例来预测未知实例的标签集。</li><li id="342e" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated"><strong class="li ir">多类分类的区别&amp;多标签分类</strong>在于，在多类问题中，类是互斥的，而对于多标签问题，每个标签代表不同的分类任务，但这些任务在某种程度上是相关的。</li><li id="93ca" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">例如，<em class="lh">多类分类</em>假设每个样本被分配给一个且仅一个标签:水果可以是苹果或梨，但不能同时是两者。然而，<em class="lh">多标签分类</em>的一个例子可以是，一个文本可能同时涉及宗教、政治、金融或教育中的任何一个，或者这些都不涉及。</li></ul></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="8096" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">第 2 部分:问题定义和评估标准:</h1><h2 id="041f" class="nr md iq bd me ns nt dn mi nu nv dp mm my nw nx mo na ny nz mq nc oa ob ms oc bi translated">问题定义:</h2><ul class=""><li id="5ec6" class="mu mv iq li b lj mw lm mx my mz na nb nc nd mb ne nf ng nh bi translated">有毒评论分类是一个高度不平衡数据集的多标签文本分类问题。</li><li id="e0ce" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">我们面临的挑战是建立一个多标签模型，能够检测不同类型的毒性，如威胁，淫秽，侮辱和基于身份的仇恨。我们需要创建一个模型来预测每条评论的每种毒性的概率。</li><li id="5455" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">这个问题的 Kaggle 链接可以在<a class="ae od" href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge" rel="noopener ugc nofollow" target="_blank"> <em class="lh">这里</em> </a>找到。</li></ul><h2 id="6aaa" class="nr md iq bd me ns nt dn mi nu nv dp mm my nw nx mo na ny nz mq nc oa ob ms oc bi translated">评估指标:</h2><blockquote class="lc ld le"><p id="9505" class="lf lg lh li b lj lk jr ll lm ln ju lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ir">注:</strong>ka ggle 挑战赛最初的评估指标是<em class="iq"> Log-Loss </em>，后来改为<em class="iq"> AUC </em>。但是在这篇文章中，我们也揭示了其他评估指标。</p></blockquote><ul class=""><li id="153c" class="mu mv iq li b lj lk lm ln my no na np nc nq mb ne nf ng nh bi translated">单标签的评估方法通常不同于多标签的评估方法。这里，在单标签分类中，我们使用简单的度量标准，如精确度、召回率、准确度等。比方说，在单标签分类中，准确度只是:</li></ul><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/e35fbf8b9ec2001f54a60c920edd04cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*A6N4fAv_VXVgdcU-KCabfw.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Fig-3: Accuracy in single-label classification</figcaption></figure><ul class=""><li id="c320" class="mu mv iq li b lj lk lm ln my no na np nc nq mb ne nf ng nh bi translated">在多标签分类中，错误分类不再是绝对的对错。包含实际类别的子集的预测应该被认为比不包含任何类别的预测更好，即，正确预测三个标签中的两个这比根本不预测标签更好。</li></ul><h2 id="fc85" class="nr md iq bd me ns nt dn mi nu nv dp mm my nw nx mo na ny nz mq nc oa ob ms oc bi translated"><em class="of">微平均</em> &amp; M <em class="of">宏平均(基于标签的测量):</em></h2><ul class=""><li id="fd42" class="mu mv iq li b lj mw lm mx my mz na nb nc nd mb ne nf ng nh bi translated">为了测量一个多类分类器，我们必须以某种方式对这些类进行平均。有两种不同的方法可以做到这一点，分别称为<em class="lh">微平均</em>和<em class="lh">宏平均</em>。</li><li id="203e" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">在<em class="lh">微平均</em>中，对每个类别的所有 TPs、TNs、FPs 和 FNs 求和，然后取平均值。</li></ul><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi og"><img src="../Images/2a0fd1e8e59937a397f63ed8735a3ca3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*nWbsBPAFl3WmU_bgtahqKQ.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Fig-4: Micro-Averaging</figcaption></figure><ul class=""><li id="2ab1" class="mu mv iq li b lj lk lm ln my no na np nc nq mb ne nf ng nh bi translated">在<em class="lh">微平均</em>方法中，您将系统对于不同集合的单个真阳性、假阳性和假阴性相加并加以应用。并且微观平均 F1 分数将简单地是上述两个方程的调和平均值。</li><li id="f521" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated"><em class="lh">宏平均</em>非常简单。我们只是取不同集合上系统的精度和召回率的平均值。</li></ul><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/97fbb6df433412d65debd94648b652da.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*AwYON8c48oMm5AcqVxLiWQ.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Fig-5: Macro-Averaging</figcaption></figure><ul class=""><li id="4b8b" class="mu mv iq li b lj lk lm ln my no na np nc nq mb ne nf ng nh bi translated"><em class="lh">当您想要了解系统在数据集上的整体表现时，可以使用宏平均</em>方法。你不应该用这个平均数做出任何具体的决定。另一方面，当数据集大小不同时，<em class="lh">微平均</em>是一个有用的方法。</li></ul><h2 id="ff69" class="nr md iq bd me ns nt dn mi nu nv dp mm my nw nx mo na ny nz mq nc oa ob ms oc bi translated">汉明损失(基于示例的测量):</h2><ul class=""><li id="26ec" class="mu mv iq li b lj mw lm mx my mz na nb nc nd mb ne nf ng nh bi translated">用最简单的术语来说，<em class="lh">汉明损失</em>是被错误预测的标签的分数，即错误标签占标签总数的分数。</li></ul><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi oi"><img src="../Images/09bded3c7de18a236744c55320377994.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_qeJQPY9CKki2xouAQr6fg.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Fig-6: Hamming-Loss</figcaption></figure><h2 id="fd6d" class="nr md iq bd me ns nt dn mi nu nv dp mm my nw nx mo na ny nz mq nc oa ob ms oc bi translated">精确匹配率(子集准确度):</h2><ul class=""><li id="0c85" class="mu mv iq li b lj mw lm mx my mz na nb nc nd mb ne nf ng nh bi translated">这是最严格的指标，表示所有标签都被正确分类的样本的百分比。</li></ul><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/37a28ae8ae4d23532faebfdaefec2c5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*iAqf__O54Tp0HzpLhaONkA.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Fig-7: Exact Match Ratio</figcaption></figure><ul class=""><li id="ee3b" class="mu mv iq li b lj lk lm ln my no na np nc nq mb ne nf ng nh bi translated">这种方法的缺点是多类分类问题有部分正确的机会，但这里我们忽略了那些部分正确的匹配。</li><li id="035d" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">在<em class="lh"> scikit-learn </em>中有一个实现子集精度的函数，叫做<strong class="li ir"> accuracy_score。</strong></li></ul><blockquote class="lc ld le"><p id="16ae" class="lf lg lh li b lj lk jr ll lm ln ju lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ir">注意:</strong>我们将使用<strong class="li ir"> accuracy_score </strong>函数来评估我们在这个项目中的所有模型。</p></blockquote></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="76fe" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">第 3 部分:探索性数据分析(EDA):</h1><blockquote class="lc ld le"><p id="bcb5" class="lf lg lh li b lj lk jr ll lm ln ju lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">探索性数据分析是数据分析过程中的重要步骤之一。在这里，重点是理解手头的数据——比如制定向数据集提问的正确问题，如何操作数据源以获得所需的答案，等等。</p></blockquote><ul class=""><li id="8307" class="mu mv iq li b lj lk lm ln my no na np nc nq mb ne nf ng nh bi translated">首先让我们导入必要的库。</li></ul><pre class="kn ko kp kq gt ok ol om on aw oo bi"><span id="c371" class="nr md iq ol b gy op oq l or os">import os<br/>import csv<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span></pre><ul class=""><li id="e7f0" class="mu mv iq li b lj lk lm ln my no na np nc nq mb ne nf ng nh bi translated">接下来，我们将 csv 文件中的数据加载到 pandas 数据帧中，并检查其属性。</li></ul><pre class="kn ko kp kq gt ok ol om on aw oo bi"><span id="f016" class="nr md iq ol b gy op oq l or os">data_path = "/Users/kartik/Desktop/AAIC/Projects/jigsaw-toxic-comment-classification-challenge/data/train.csv"</span><span id="a513" class="nr md iq ol b gy ot oq l or os">data_raw = pd.read_csv(data_path)</span><span id="de4f" class="nr md iq ol b gy ot oq l or os">print("Number of rows in data =",data_raw.shape[0])<br/>print("Number of columns in data =",data_raw.shape[1])<br/>print("\n")<br/>print("**Sample data:**")<br/>data_raw.head()</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ou"><img src="../Images/b4d14385adfe1e6c5f0f039d48475367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hu1wY2QiI4LkSridSeegfQ.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Fig-8: Data Attributes</figcaption></figure><ul class=""><li id="bfc5" class="mu mv iq li b lj lk lm ln my no na np nc nq mb ne nf ng nh bi translated">现在我们统计每个标签下的评论数量。(详细代码请参考本项目的 GitHub 链接。)</li></ul><pre class="kn ko kp kq gt ok ol om on aw oo bi"><span id="0fcb" class="nr md iq ol b gy op oq l or os">categories = list(data_raw.columns.values)<br/>sns.set(font_scale = 2)<br/>plt.figure(figsize=(15,8))</span><span id="9142" class="nr md iq ol b gy ot oq l or os">ax= sns.barplot(categories, data_raw.iloc[:,2:].sum().values)</span><span id="c20d" class="nr md iq ol b gy ot oq l or os">plt.title("Comments in each category", fontsize=24)<br/>plt.ylabel('Number of comments', fontsize=18)<br/>plt.xlabel('Comment Type ', fontsize=18)</span><span id="93a9" class="nr md iq ol b gy ot oq l or os">#adding the text labels<br/>rects = ax.patches<br/>labels = data_raw.iloc[:,2:].sum().values<br/>for rect, label in zip(rects, labels):<br/>    height = rect.get_height()<br/>    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=18)</span><span id="eee8" class="nr md iq ol b gy ot oq l or os">plt.show()</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ov"><img src="../Images/0cb9e477bc09ae396938c8a1509f2d6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-GSenYy_x2VHxnE1bKyRjQ.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Fig-9: Count of comments under each label</figcaption></figure><ul class=""><li id="e9da" class="mu mv iq li b lj lk lm ln my no na np nc nq mb ne nf ng nh bi translated">计算具有多个标签的评论的数量。</li></ul><pre class="kn ko kp kq gt ok ol om on aw oo bi"><span id="401d" class="nr md iq ol b gy op oq l or os">rowSums = data_raw.iloc[:,2:].sum(axis=1)<br/>multiLabel_counts = rowSums.value_counts()<br/>multiLabel_counts = multiLabel_counts.iloc[1:]</span><span id="d188" class="nr md iq ol b gy ot oq l or os">sns.set(font_scale = 2)<br/>plt.figure(figsize=(15,8))</span><span id="a363" class="nr md iq ol b gy ot oq l or os">ax = sns.barplot(multiLabel_counts.index, multiLabel_counts.values)</span><span id="5a8a" class="nr md iq ol b gy ot oq l or os">plt.title("Comments having multiple labels ")<br/>plt.ylabel('Number of comments', fontsize=18)<br/>plt.xlabel('Number of labels', fontsize=18)</span><span id="76c8" class="nr md iq ol b gy ot oq l or os">#adding the text labels<br/>rects = ax.patches<br/>labels = multiLabel_counts.values<br/>for rect, label in zip(rects, labels):<br/>    height = rect.get_height()<br/>    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')</span><span id="4e07" class="nr md iq ol b gy ot oq l or os">plt.show()</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ow"><img src="../Images/58f1b7626c0a8138719ace007be3004d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YxDzgMZR7eh1lesl3P1uWw.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Fig-10: Count of comments with multiple labels.</figcaption></figure><ul class=""><li id="d90b" class="mu mv iq li b lj lk lm ln my no na np nc nq mb ne nf ng nh bi translated">每一类评论中最常用词的 WordCloud 表示。</li></ul><pre class="kn ko kp kq gt ok ol om on aw oo bi"><span id="3b6a" class="nr md iq ol b gy op oq l or os">from wordcloud import WordCloud,STOPWORDS</span><span id="12fa" class="nr md iq ol b gy ot oq l or os">plt.figure(figsize=(40,25))</span><span id="b5a4" class="nr md iq ol b gy ot oq l or os"># clean<br/>subset = data_raw[data_raw.clean==True]<br/>text = subset.comment_text.values<br/>cloud_toxic = WordCloud(<br/>                          stopwords=STOPWORDS,<br/>                          background_color='black',<br/>                          collocations=False,<br/>                          width=2500,<br/>                          height=1800<br/>                         ).generate(" ".join(text))<br/>plt.axis('off')<br/>plt.title("Clean",fontsize=40)<br/>plt.imshow(cloud_clean)</span><span id="8339" class="nr md iq ol b gy ot oq l or os"># Same code can be used to generate wordclouds of other categories.</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ox"><img src="../Images/ceecfc3b77f2862d9f4507f5dda4067f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TxLNWHhnOidtzNrlExehvw.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Fig-1: Word-cloud Representation of Clean Comments</figcaption></figure></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="df72" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">第 4 部分:数据预处理:</h1><ul class=""><li id="3d92" class="mu mv iq li b lj mw lm mx my mz na nb nc nd mb ne nf ng nh bi translated">我们首先将注释转换成小写，然后使用定制的函数从注释中删除<em class="lh"> html 标签、标点符号和非字母字符</em>。</li></ul><pre class="kn ko kp kq gt ok ol om on aw oo bi"><span id="644e" class="nr md iq ol b gy op oq l or os">import nltk<br/>from nltk.corpus import stopwords<br/>from nltk.stem.snowball import SnowballStemmer<br/>import re<br/>import sys<br/>import warnings</span><span id="9386" class="nr md iq ol b gy ot oq l or os">data = data_raw</span><span id="a6ba" class="nr md iq ol b gy ot oq l or os">if not sys.warnoptions:<br/>    warnings.simplefilter("ignore")</span><span id="328b" class="nr md iq ol b gy ot oq l or os">def cleanHtml(sentence):<br/>    cleanr = re.compile('&lt;.*?&gt;')<br/>    cleantext = re.sub(cleanr, ' ', str(sentence))<br/>    return cleantext</span><span id="a883" class="nr md iq ol b gy ot oq l or os">def cleanPunc(sentence): #function to clean the word of any punctuation or special characters<br/>    cleaned = re.sub(r'[?|!|\'|"|#]',r'',sentence)<br/>    cleaned = re.sub(r'[.|,|)|(|\|/]',r' ',cleaned)<br/>    cleaned = cleaned.strip()<br/>    cleaned = cleaned.replace("\n"," ")<br/>    return cleaned</span><span id="afc6" class="nr md iq ol b gy ot oq l or os">def keepAlpha(sentence):<br/>    alpha_sent = ""<br/>    for word in sentence.split():<br/>        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)<br/>        alpha_sent += alpha_word<br/>        alpha_sent += " "<br/>    alpha_sent = alpha_sent.strip()<br/>    return alpha_sent</span><span id="3282" class="nr md iq ol b gy ot oq l or os">data['comment_text'] = data['comment_text'].str.lower()<br/>data['comment_text'] = data['comment_text'].apply(cleanHtml)<br/>data['comment_text'] = data['comment_text'].apply(cleanPunc)<br/>data['comment_text'] = data['comment_text'].apply(keepAlpha)</span></pre><ul class=""><li id="c4f8" class="mu mv iq li b lj lk lm ln my no na np nc nq mb ne nf ng nh bi translated">接下来，我们使用可以从<em class="lh"> NLTK </em>库中下载的默认停用词集合，删除注释中出现的所有<strong class="li ir"> <em class="lh">停用词</em> </strong>。我们还在标准列表中添加了一些停用词。</li><li id="2758" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">停用词基本上是任何语言中的一组常用词，不仅仅是英语。停用词对许多应用程序至关重要的原因是，如果我们删除给定语言中非常常用的词，我们就可以专注于重要的词。</li></ul><pre class="kn ko kp kq gt ok ol om on aw oo bi"><span id="5f37" class="nr md iq ol b gy op oq l or os">stop_words = set(stopwords.words('english'))<br/>stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])<br/>re_stop_words = re.compile(r"\b(" + "|".join(stop_words) + ")\\W", re.I)<br/>def removeStopWords(sentence):<br/>    global re_stop_words<br/>    return re_stop_words.sub(" ", sentence)</span><span id="770f" class="nr md iq ol b gy ot oq l or os">data['comment_text'] = data['comment_text'].apply(removeStopWords)</span></pre><ul class=""><li id="79dd" class="mu mv iq li b lj lk lm ln my no na np nc nq mb ne nf ng nh bi translated">接下来我们做<strong class="li ir"> <em class="lh">词干</em> </strong> <em class="lh">。</em>存在不同种类的词干，它们基本上将语义大致相同的单词转换成一种标准形式。例如，对于逗乐、娱乐和逗乐，词干应该是 amus。</li></ul><pre class="kn ko kp kq gt ok ol om on aw oo bi"><span id="ea81" class="nr md iq ol b gy op oq l or os">stemmer = SnowballStemmer("english")<br/>def stemming(sentence):<br/>    stemSentence = ""<br/>    for word in sentence.split():<br/>        stem = stemmer.stem(word)<br/>        stemSentence += stem<br/>        stemSentence += " "<br/>    stemSentence = stemSentence.strip()<br/>    return stemSentence</span><span id="2020" class="nr md iq ol b gy ot oq l or os">data['comment_text'] = data['comment_text'].apply(stemming)</span></pre><ul class=""><li id="740d" class="mu mv iq li b lj lk lm ln my no na np nc nq mb ne nf ng nh bi translated">在将数据集分成训练集和测试集之后，我们想要总结我们的注释，并将它们转换成数字向量。</li><li id="dfec" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">一个技巧是挑选最频繁出现的术语(具有高<strong class="li ir"> <em class="lh">术语频率</em> </strong>或<em class="lh"> tf </em>的词)。然而，最频繁出现的单词是一个不太有用的度量，因为像'<em class="lh"> this </em>'、'<em class="lh"> a </em>'这样的单词在所有文档中出现得非常频繁。</li><li id="a5d0" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">因此，我们还希望衡量一个单词的独特性，即该单词在所有文档中出现的频率(I<strong class="li ir"><em class="lh">n 反转文档频率</em> </strong>或<em class="lh"> idf </em>)。</li><li id="4b5a" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">因此，一个单词的 TF &amp; IDF(<strong class="li ir"><em class="lh">TF-IDF</em></strong>)的乘积给出了该单词在文档中的出现频率乘以该单词在整个文档语料库中的独特性的乘积。</li><li id="62cf" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">文档中具有高 tfidf 分数的单词在文档中频繁出现，并且提供关于该特定文档的最多信息。</li></ul><pre class="kn ko kp kq gt ok ol om on aw oo bi"><span id="911c" class="nr md iq ol b gy op oq l or os">from sklearn.model_selection import train_test_split</span><span id="b489" class="nr md iq ol b gy ot oq l or os">train, test = train_test_split(data, random_state=42, test_size=0.30, shuffle=True)</span><span id="cf37" class="nr md iq ol b gy ot oq l or os">from sklearn.feature_extraction.text import TfidfVectorizer<br/>vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')<br/>vectorizer.fit(train_text)<br/>vectorizer.fit(test_text)</span><span id="f10f" class="nr md iq ol b gy ot oq l or os">x_train = vectorizer.transform(train_text)<br/>y_train = train.drop(labels = ['id','comment_text'], axis=1)</span><span id="82b1" class="nr md iq ol b gy ot oq l or os">x_test = vectorizer.transform(test_text)<br/>y_test = test.drop(labels = ['id','comment_text'], axis=1)</span></pre><ul class=""><li id="636e" class="mu mv iq li b lj lk lm ln my no na np nc nq mb ne nf ng nh bi translated"><strong class="li ir"> <em class="lh"> TF-IDF </em> </strong>易于计算，但其缺点是不能捕捉文本中的位置、语义、在不同文档中的共现等。</li></ul></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="cf94" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">第 5 部分:多标签分类技术；</h1><blockquote class="lc ld le"><p id="78ce" class="lf lg lh li b lj lk jr ll lm ln ju lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">大多数传统的学习算法是针对单标签分类问题开发的。因此，文献中的许多方法将多标签问题转化为多个单标签问题，从而可以使用现有的单标签算法。</p></blockquote></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="708a" class="nr md iq bd me ns nt dn mi nu nv dp mm my nw nx mo na ny nz mq nc oa ob ms oc bi translated">1.OneVsRest</h2><ul class=""><li id="797d" class="mu mv iq li b lj mw lm mx my mz na nb nc nd mb ne nf ng nh bi translated">通过限制每个实例只有一个标签，传统的两类和多类问题都可以转化为多标签问题。另一方面，多标签问题的通用性不可避免地增加了学习的难度。解决多标签问题的直观方法是将其分解为多个独立的二分类问题(每个类别一个)。</li><li id="a525" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">在“一对多”策略中，可以建立多个独立的分类器，并且对于一个看不见的实例，选择置信度最大化的类别。</li><li id="ca81" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">这里的主要假设是标签是<em class="lh">互斥的</em>。在这个方法中，您不考虑类之间的任何潜在相关性。</li><li id="7f03" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">例如，它更像是问一些简单的问题，比如，“<em class="lh">评论是否有毒”、“<em class="lh">评论是否具有威胁性？</em>等。此外，这里可能有一个广泛的过度拟合的情况，因为大多数注释是未标记的，也就是说，大多数注释是干净的注释。</em></li></ul><pre class="kn ko kp kq gt ok ol om on aw oo bi"><span id="cf28" class="nr md iq ol b gy op oq l or os">from sklearn.linear_model import LogisticRegression<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.multiclass import OneVsRestClassifier</span><span id="fa50" class="nr md iq ol b gy ot oq l or os"># Using pipeline for applying logistic regression and one vs rest classifier<br/>LogReg_pipeline = Pipeline([<br/>                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),<br/>            ])</span><span id="9b1e" class="nr md iq ol b gy ot oq l or os">for category in categories:<br/>    print('**Processing {} comments...**'.format(category))<br/>    <br/>    # Training logistic regression model on train data<br/>    LogReg_pipeline.fit(x_train, train[category])<br/>    <br/>    # calculating test accuracy<br/>    prediction = LogReg_pipeline.predict(x_test)<br/>    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))<br/>    print("\n")</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/cdeb2083c2c73b55ec7c8e0af75ae4ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*zOywcLTbwngI22Evk9EnJg.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Fig-12: OneVsRest</figcaption></figure></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="94d7" class="nr md iq bd me ns nt dn mi nu nv dp mm my nw nx mo na ny nz mq nc oa ob ms oc bi translated">2.二元相关性</h2><ul class=""><li id="523d" class="mu mv iq li b lj mw lm mx my mz na nb nc nd mb ne nf ng nh bi translated">在这种情况下，训练单标签二元分类器的集合，每个类别一个。每个分类器预测一个类的成员或非成员。所有被预测的类的并集被作为多标签输出。这种方法很受欢迎，因为它易于实现，但是它也忽略了类标签之间可能的相关性。</li><li id="7101" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">换句话说，如果有<em class="lh"> q </em>个标签，二元相关性方法从图像中创建<em class="lh"> q </em>个新数据集，每个标签一个，并在每个新数据集上训练单标签分类器。一个分类器可能对“它包含树吗？”这个问题回答是/否，从而“二元关联”中的“二元”。这是一种简单的方法，但是当标签之间存在依赖关系时，这种方法就不好用了。</li><li id="6596" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated"><em class="lh"> OneVsRest &amp;二元关联性</em>看起来非常相像。如果 OneVsRest 中的多个分类器回答<em class="lh">“是”</em>，那么您将回到二元相关性场景。</li></ul><pre class="kn ko kp kq gt ok ol om on aw oo bi"><span id="ac73" class="nr md iq ol b gy op oq l or os"># using binary relevance<br/>from skmultilearn.problem_transform import BinaryRelevance<br/>from sklearn.naive_bayes import GaussianNB</span><span id="416d" class="nr md iq ol b gy ot oq l or os"># initialize binary relevance multi-label classifier<br/># with a gaussian naive bayes base classifier<br/>classifier = BinaryRelevance(GaussianNB())</span><span id="0ab2" class="nr md iq ol b gy ot oq l or os"># train<br/>classifier.fit(x_train, y_train)</span><span id="499e" class="nr md iq ol b gy ot oq l or os"># predict<br/>predictions = classifier.predict(x_test)</span><span id="931a" class="nr md iq ol b gy ot oq l or os"># accuracy<br/>print("Accuracy = ",accuracy_score(y_test,predictions))</span><span id="4864" class="nr md iq ol b gy ot oq l or os"><br/><strong class="ol ir"><em class="lh">Output:</em></strong></span><span id="9387" class="nr md iq ol b gy ot oq l or os"><em class="lh">Accuracy = 0.856666666667</em></span></pre></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="aea2" class="nr md iq bd me ns nt dn mi nu nv dp mm my nw nx mo na ny nz mq nc oa ob ms oc bi translated">3.分类器链</h2><ul class=""><li id="9b49" class="mu mv iq li b lj mw lm mx my mz na nb nc nd mb ne nf ng nh bi translated">一串二元分类器 C0，C1。。。，Cn，其中分类器 Ci 使用所有分类器 Cj 的预测，其中 j &lt; i. This way the method, also called classifier chains (CC), can take into account label correlations.</li><li id="3e16" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">The total number of classifiers needed for this approach is equal to the number of classes, but the training of the classifiers is more involved.</li><li id="cf61" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">Following is an illustrated example with a classification problem of three categories {C1, C2, C3} chained in that order.</li></ul><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi oz"><img src="../Images/bc6f491eb470ee64e502065a6ea7903f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ycwr_uE8_5lnOMNCnFOuXQ.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Fig-13: Classifier Chains</figcaption></figure><pre class="kn ko kp kq gt ok ol om on aw oo bi"><span id="b243" class="nr md iq ol b gy op oq l or os"># using classifier chains<br/>from skmultilearn.problem_transform import ClassifierChain<br/>from sklearn.linear_model import LogisticRegression</span><span id="2ee1" class="nr md iq ol b gy ot oq l or os"># initialize classifier chains multi-label classifier<br/>classifier = ClassifierChain(LogisticRegression())</span><span id="5bfd" class="nr md iq ol b gy ot oq l or os"># Training logistic regression model on train data<br/>classifier.fit(x_train, y_train)</span><span id="86ae" class="nr md iq ol b gy ot oq l or os"># predict<br/>predictions = classifier.predict(x_test)</span><span id="332f" class="nr md iq ol b gy ot oq l or os"># accuracy<br/>print("Accuracy = ",accuracy_score(y_test,predictions))<br/>print("\n")</span><span id="ccd3" class="nr md iq ol b gy ot oq l or os"><strong class="ol ir"><em class="lh">Output:</em></strong></span><span id="4cd0" class="nr md iq ol b gy ot oq l or os"><em class="lh">Accuracy = 0.893333333333</em></span></pre></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="81b6" class="nr md iq bd me ns nt dn mi nu nv dp mm my nw nx mo na ny nz mq nc oa ob ms oc bi translated">4. Label Powerset</h2><ul class=""><li id="ca66" class="mu mv iq li b lj mw lm mx my mz na nb nc nd mb ne nf ng nh bi translated">This approach does take possible correlations between class labels into account. More commonly this approach is called the label-powerset method, because it considers each member of the power set of labels in the training set as a single label.</li><li id="cc40" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">This method needs worst case (2^|C|) classifiers, and has a high computational complexity.</li><li id="04be" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">However when the number of classes increases the number of distinct label combinations can grow exponentially. This easily leads to combinatorial explosion and thus computational infeasibility. Furthermore, some label combinations will have very few positive examples.</li></ul><pre class="kn ko kp kq gt ok ol om on aw oo bi"><span id="d285" class="nr md iq ol b gy op oq l or os"># using Label Powerset<br/>from skmultilearn.problem_transform import LabelPowerset</span><span id="e2e2" class="nr md iq ol b gy ot oq l or os"># initialize label powerset multi-label classifier<br/>classifier = LabelPowerset(LogisticRegression())</span><span id="06e6" class="nr md iq ol b gy ot oq l or os"># train<br/>classifier.fit(x_train, y_train)</span><span id="2275" class="nr md iq ol b gy ot oq l or os"># predict<br/>predictions = classifier.predict(x_test)</span><span id="3fa9" class="nr md iq ol b gy ot oq l or os"># accuracy<br/>print("Accuracy = ",accuracy_score(y_test,predictions))<br/>print("\n")</span><span id="12a8" class="nr md iq ol b gy ot oq l or os"><strong class="ol ir"><em class="lh">Output:</em></strong></span><span id="1d5a" class="nr md iq ol b gy ot oq l or os"><em class="lh">Accuracy = 0.893333333333</em></span></pre></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="3421" class="nr md iq bd me ns nt dn mi nu nv dp mm my nw nx mo na ny nz mq nc oa ob ms oc bi translated">5. Adapted Algorithm</h2><ul class=""><li id="ef7d" class="mu mv iq li b lj mw lm mx my mz na nb nc nd mb ne nf ng nh bi translated">Algorithm adaptation methods for multi-label classification concentrate on adapting single-label classification algorithms to the multi-label case usually by changes in cost/decision functions.</li><li id="39bc" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">Here we use a multi-label lazy learning approach named <strong class="li ir"> <em class="lh"> ML-KNN </em> </strong>是从传统的 K-最近邻(KNN)算法得到的。</li><li id="50b8" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated"><code class="fe pa pb pc ol b"><a class="ae od" href="http://scikit.ml/api/api/skmultilearn.adapt.html#module-skmultilearn.adapt" rel="noopener ugc nofollow" target="_blank"><strong class="li ir">skmultilearn.adapt</strong></a></code>模块实现多标签分类的算法自适应方法，包括但不限于<strong class="li ir"> <em class="lh"> ML-KNN。</em> </strong></li></ul><pre class="kn ko kp kq gt ok ol om on aw oo bi"><span id="126d" class="nr md iq ol b gy op oq l or os">from skmultilearn.adapt import MLkNN<br/>from scipy.sparse import csr_matrix, lil_matrix</span><span id="7e1d" class="nr md iq ol b gy ot oq l or os">classifier_new = MLkNN(k=10)</span><span id="fe20" class="nr md iq ol b gy ot oq l or os"># Note that this classifier can throw up errors when handling sparse matrices.</span><span id="6604" class="nr md iq ol b gy ot oq l or os">x_train = lil_matrix(x_train).toarray()<br/>y_train = lil_matrix(y_train).toarray()<br/>x_test = lil_matrix(x_test).toarray()</span><span id="2e93" class="nr md iq ol b gy ot oq l or os"># train<br/>classifier_new.fit(x_train, y_train)</span><span id="3cef" class="nr md iq ol b gy ot oq l or os"># predict<br/>predictions_new = classifier_new.predict(x_test)</span><span id="f2f9" class="nr md iq ol b gy ot oq l or os"># accuracy<br/>print("Accuracy = ",accuracy_score(y_test,predictions_new))<br/>print("\n")</span><span id="5fe9" class="nr md iq ol b gy ot oq l or os"><strong class="ol ir"><em class="lh">Output:</em></strong></span><span id="ab01" class="nr md iq ol b gy ot oq l or os"><em class="lh">Accuracy = 0.88166666667</em></span></pre></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="eecb" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">结论:</h1><h2 id="eade" class="nr md iq bd me ns nt dn mi nu nv dp mm my nw nx mo na ny nz mq nc oa ob ms oc bi translated">结果:</h2><ul class=""><li id="1ff2" class="mu mv iq li b lj mw lm mx my mz na nb nc nd mb ne nf ng nh bi translated">解决多标签分类问题主要有两种方法:<strong class="li ir">问题转化方法</strong>和<strong class="li ir">算法自适应方法</strong>。</li><li id="4130" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">问题转换方法将多标签问题转换成一组<a class="ae od" href="https://en.wikipedia.org/wiki/Binary_classification" rel="noopener ugc nofollow" target="_blank">二元分类</a>问题，然后可以使用单类分类器来处理这些问题。</li><li id="0a41" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">而算法适应方法使算法适应直接执行多标签分类。换句话说，他们不是试图将问题转化为更简单的问题，而是试图解决问题的全部形式。</li><li id="fbcb" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">在与其他方法的广泛比较中，label-powerset 方法得分最高，其次是 one-against-all 方法。</li><li id="ecc0" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">在这个数据集上运行时，ML-KNN 和 label-powerset 都需要相当长的时间，所以实验是在训练数据的随机样本上进行的。</li></ul><h2 id="7077" class="nr md iq bd me ns nt dn mi nu nv dp mm my nw nx mo na ny nz mq nc oa ob ms oc bi translated">进一步改进:</h2><ul class=""><li id="fd99" class="mu mv iq li b lj mw lm mx my mz na nb nc nd mb ne nf ng nh bi translated">在深度学习中使用 LSTMs 可以解决同样的问题。</li><li id="fbe8" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">为了更快的速度，我们可以使用决策树，为了在速度和准确性之间进行合理的权衡，我们也可以选择集合模型。</li><li id="5672" class="mu mv iq li b lj ni lm nj my nk na nl nc nm mb ne nf ng nh bi translated">诸如 MEKA 的其他框架可以用于处理多标签分类问题。</li></ul></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h2 id="9fae" class="nr md iq bd me ns nt dn mi nu nv dp mm my nw nx mo na ny nz mq nc oa ob ms oc bi translated">GitHub 项目的<a class="ae od" href="https://github.com/nkartik94/Multi-Label-Text-Classification" rel="noopener ugc nofollow" target="_blank"> <em class="of">链接</em> </a>。LinkedIn <a class="ae od" href="https://www.linkedin.com/in/kartiknooney" rel="noopener ugc nofollow" target="_blank">简介</a>。</h2></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><blockquote class="pd"><p id="8cd6" class="pe pf iq bd pg ph pi pj pk pl pm mb dk translated">希望你喜欢这个教程。感谢阅读..！</p></blockquote></div></div>    
</body>
</html>