<html>
<head>
<title>How to train custom Word Embeddings using GPU on AWS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在AWS上使用GPU训练自定义单词嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-train-custom-word-embeddings-using-gpu-on-aws-f62727a1e3f6?source=collection_archive---------2-----------------------#2018-03-21">https://towardsdatascience.com/how-to-train-custom-word-embeddings-using-gpu-on-aws-f62727a1e3f6?source=collection_archive---------2-----------------------#2018-03-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="cd48" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">语言很重要。人类用文字交流，文字承载着意义。我们能训练机器也学习意义吗？这篇博客的主题是我们如何训练机器使用<strong class="js iu">单词嵌入</strong>来学习单词的意思。在我浏览堆栈溢出、EC2文档和博客的过程中，我将记下某人在AWS的GPU上使用TensorFlow训练单词嵌入的步骤。但是在介绍这些步骤之前，让我们先简要了解一下为什么我们需要单词嵌入。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/4000c2eaddeb98ba6e5455437ca95808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mAjkEUg6AT98KqmO11Zzow.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Language is all around us — can we make a machine learn meaning?</figcaption></figure><p id="4293" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通常，在大多数自然语言处理任务中，单词被转换成<strong class="js iu">一键向量</strong>，其中每个单词在向量中获得唯一的索引。因此，在苹果、桔子和果汁这三个单词的词汇表中，我们可以用[1 0 0]、[0 1 0]和[0 0 1]来表示这三个单词。对于小型语言任务，这些向量非常有用。然而，其中有两个主要缺点:<br/> a)对于具有<em class="le"> n </em>个字的训练数据，我们将需要<em class="le"> n </em>个维度向量。所以你可以想象1000000维向量非常大，并且非常稀疏。<br/>b)one-hot vectors中没有<strong class="js iu">上下文<em class="le"> </em> </strong>的意义——每个单词都是完全唯一的。因此，如果一个模特已经学会了“苹果汁”，它就不能<strong class="js iu">转移</strong>对填充“桔子__”的学习。单词嵌入遵循的原则是“<a class="ae lf" href="https://en.wikipedia.org/wiki/John_Rupert_Firth" rel="noopener ugc nofollow" target="_blank">你将一个单词由它所保持的公司</a>”，因此我们在单词嵌入向量中的每个维度都可以表示一个“特征”，如食物或性别或资本等等——如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lg"><img src="../Images/37337aeb59746846792d1915d889bef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mLrheV1nGz7XemDAVRcZ4A.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Visualizing Word Embeddings (from a <a class="ae lf" href="https://www.coursera.org/learn/intro-to-deep-learning/lecture/dhzl5/word-embeddings" rel="noopener ugc nofollow" target="_blank">class</a> by Andrew Ng)</figcaption></figure><p id="cb80" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">训练有素的单词嵌入可以用于翻译、文本摘要、情感分析和类比任务，例如男人&gt;女人、国王&gt;王后。<strong class="js iu"/><a class="ae lf" href="https://machinelearningmastery.com/develop-word-embeddings-python-gensim/" rel="noopener ugc nofollow" target="_blank"><strong class="js iu">word 2 vec</strong></a>和<a class="ae lf" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> Glove </strong> </a>是两个流行的预训练单词嵌入，可以作为开源软件获得——它们对于开始一般的语言任务非常强大。但是，在本教程中，我们将学习如何在特定领域为我们自己的自定义数据构建word嵌入。我正致力于为电子数据，如微处理器、电容器、ram、电源等构建字嵌入。</p><p id="906d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，让我们开始制作单词嵌入的旅程，我们将在AWS GPU上进行训练！</p><ol class=""><li id="1583" class="lh li it js b jt ju jx jy kb lj kf lk kj ll kn lm ln lo lp bi translated"><strong class="js iu">选择P2或P3实例类型</strong>T3首先，您需要选择一个EC2实例类型。最近AWS在EC2上发布了p3实例，但是它们的价格大约是每小时3美元，这是一笔不小的数目。所以我将继续使用<strong class="js iu"> p2实例</strong>，它可能没有那么快，但是更便宜(大约每小时1美元)。这篇<a class="ae lf" href="https://blog.iron.io/aws-p2-vs-p3-instances/" rel="noopener ugc nofollow" target="_blank">博客</a>进行了更详细的比较。一旦你选择了p2实例，你可能需要在你的帐户上请求一个限制增加来得到使用他们。</li><li id="cdbf" class="lh li it js b jt lq jx lr kb ls kf lt kj lu kn lm ln lo lp bi translated"><strong class="js iu">建立深度学习AMI和安全组<br/> </strong>如果您正在构建自己的GPU硬件，您需要自己设置CUDA驱动程序，但是在AWS上使用EC2实例为我们提供了使用预构建AMI或Amazon机器映像的优势。我选择了<strong class="js iu">深度学习AMI，带源代码(CUDA 8，Ubuntu)。<br/>更新(2018年6月):</strong>最新的TensorFlow库与CUDA 8不太兼容，请使用支持CUDA 9的AMI比如<a class="ae lf" href="https://aws.amazon.com/marketplace/pp/B077GCH38C" rel="noopener ugc nofollow" target="_blank">这个</a>这个。</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lv"><img src="../Images/1670c526aa0000583a899f9043af4409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TZ50qOEI6EMc0P4Q787p_Q.png"/></div></div></figure><p id="1ea3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们想在EC2实例中使用Jupyter notebook，我们需要更改安全组，以允许从端口8888访问EC2。这可以通过创建一个<strong class="js iu">新的安全组</strong>和使用<strong class="js iu">自定义TCP规则</strong>和<strong class="js iu">端口范围作为8888 </strong>和在源选择<strong class="js iu">任何地方来完成。现在您可以启动实例了。接下来，创建一个新的密钥对，并将其保存在本地计算机上。当我丢失这个的时候，我不得不再次做这些步骤。pem文件，所以请确保它在一个安全的地方！使用<code class="fe lw lx ly lz b">chmod 400 YourPEMFile.pem</code>将读写权限更改为只读。接下来，通过右键单击实例实例状态&gt; Start来启动您的GPU。</strong></p><p id="574a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.<strong class="js iu">登录并在EC2实例上设置Conda环境<br/> </strong>您可以通过<code class="fe lw lx ly lz b">ssh -i path/YourPEMFile.pem ubuntu@X.X.X.X</code>登录到您的GPU，其中<code class="fe lw lx ly lz b">X.X.X.X</code>代表您的GPU的公共IP，<code class="fe lw lx ly lz b">path/YourPEMFile.pem</code>是您在上一步中创建的<code class="fe lw lx ly lz b">.pem</code>文件的路径。一旦您登录到EC2实例，您就可以设置自己的Conda环境。你为什么需要它？我们来看看这个<a class="ae lf" href="https://stackoverflow.com/questions/34398676/does-conda-replace-the-need-for-virtualenv" rel="noopener ugc nofollow" target="_blank">的回答</a>，它讨论了康达如何优于virtualenv。键入<code class="fe lw lx ly lz b">conda -V</code>会导致一个错误，因为路径变量还没有设置好。我们需要转到<code class="fe lw lx ly lz b">.bash_profile</code>并添加路径。</p><p id="800c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面是让 <code class="fe lw lx ly lz b"><strong class="js iu">conda</strong></code> <strong class="js iu">在新EC2实例</strong> <br/>上工作的<strong class="js iu">步骤a .通过<code class="fe lw lx ly lz b">vim ~/.bash_profile</code>和<br/>创建或编辑文件b .添加<code class="fe lw lx ly lz b">export PATH=”/home/ubuntu/src/anaconda2/bin:$PATH”</code>路径指定包含可执行程序的目录，无需知道命令行上的完整路径即可启动这些程序。运行<code class="fe lw lx ly lz b">source ~/.bash_profile</code>来获取这个文件。你现在可以运行<code class="fe lw lx ly lz b">conda -V</code>并且不会看到错误。<br/> e .通过<code class="fe lw lx ly lz b">conda create -n embeddings python=3</code>创建一个新环境这将使用最新版本的Python 3创建一个新的conda环境。<br/> f .运行<code class="fe lw lx ly lz b">source activate embeddings</code>启动conda环境。</strong></p><p id="6ad9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.<strong class="js iu">安装Tensorflow-GPU、Jupyter notebook等软件包<br/> </strong>设置并激活conda环境后，现在可以安装Tensorflow等软件包了。<br/> a)我们可以使用<code class="fe lw lx ly lz b">pip install tensorflow-gpu jupyter notebook nltk scikit-learn pandas</code>来安装所有必要的包来完成word嵌入的工作。我们需要使用<code class="fe lw lx ly lz b">tensorflow-gpu</code>而不是<code class="fe lw lx ly lz b">tensorflow</code>，因为前者已经过优化，适合在GPU中使用。所有这些要求都可以用<code class="fe lw lx ly lz b">pip freeze &gt; requirements.txt</code>保存到一个文件中</p><p id="4ab4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">b)我们现在可以使用<code class="fe lw lx ly lz b">jupyter notebook --ip=0.0.0.0 --no-browser</code>转到<code class="fe lw lx ly lz b">X.X.X.X:8888/?token=&lt;TOKEN&gt;</code>来启动jupyter笔记本，其中<code class="fe lw lx ly lz b">X.X.X.X</code>是EC2机器的IP地址，<code class="fe lw lx ly lz b">&lt;TOKEN&gt;</code>是我们运行上面的命令时出现的令牌。</p><p id="47fa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">c)运行以下代码检查TensorFlow版本，并查看TensorFlow是否在GPU上正确运行。</p><pre class="kp kq kr ks gt ma lz mb mc aw md bi"><span id="1add" class="me mf it lz b gy mg mh l mi mj">import tensorflow as tf<br/>print('TensorFlow Version: {}'.format(tf.__version__)) <br/>print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))</span></pre><p id="7073" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我得到的输出是:<br/> <code class="fe lw lx ly lz b">TensorFlow Version: 1.3.0<br/>Default GPU Device: /gpu:0</code></p><p id="1cc2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了绝对确定GPU确实被使用，运行<br/> <code class="fe lw lx ly lz b">sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))</code>，你应该会看到以<code class="fe lw lx ly lz b">/gpu:0</code>结尾的东西，这也是<a class="ae lf" href="https://www.tensorflow.org/programmers_guide/using_gpu" rel="noopener ugc nofollow" target="_blank">官方文档</a>所建议的。这帮助我最近在设置实际使用GPU。</p><p id="a9f0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">5.<strong class="js iu">建立用于训练单词嵌入的文本语料库<br/> </strong>你可以使用IMDB或维基百科数据集，但由于本博客的目标是通过自定义单词嵌入，我们将建立自己的语料库。如果你有很多文本文件，那么你可以一个一个地阅读它们，并建立语料库。在这里，我假设你有PDF文件，这是有点难以处理。为此，我使用了库<code class="fe lw lx ly lz b">PyPDF2</code>。我使用的代码是:</p><pre class="kp kq kr ks gt ma lz mb mc aw md bi"><span id="3817" class="me mf it lz b gy mg mh l mi mj">import PyPDF2</span><span id="2008" class="me mf it lz b gy mk mh l mi mj">def read_pdf(fname):<br/>    pdfFileObj = open(fname, 'rb')<br/>    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)<br/>    print("TOTAL PAGES in file", pdfReader.numPages)<br/>    pageObj = pdfReader.getPage(0)<br/>    text = pageObj.extractText()<br/>    return ' '.join(text.split())</span></pre><p id="907c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我在本地存储的一堆PDF文件上运行了上面的代码。我将上述函数返回的字符串连接起来，并写入一个文本文件。我们的文本语料库准备好了！</p><p id="c0f4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 6。预处理文本并创建查找表<br/> </strong>现在，让我们预处理文本，为训练单词嵌入做好准备。在自然语言处理中，在运行分类器或算法之前，通常的做法是将所有单词小写，并进行一些停用词移除。我们将使用库<code class="fe lw lx ly lz b">re</code>(Python中的正则表达式库)来执行其中的一些任务。</p><pre class="kp kq kr ks gt ma lz mb mc aw md bi"><span id="54ff" class="me mf it lz b gy mg mh l mi mj">text = text.lower()<br/>text = text.replace('.', ' __PERIOD__ ') <br/>text = text.replace('-', ' ')    <br/># Remove ' and full stops and brackets<br/>text = re.sub(r'[{}()\']', '', text)<br/># Replace commas, dashes, colons and @ with spaces<br/>text = re.sub(r'[;,:-@#]', ' ', text) </span></pre><p id="2e16" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以根据需要做更多的预处理，比如把缩写改成全称，用<code class="fe lw lx ly lz b">__NUMBER__</code> token代替数字等等。上图中，我把句号改成了一个<code class="fe lw lx ly lz b">__PERIOD__</code>，它可以作为训练的标志。理解句子中单词上下文的动态变化是很重要的，而<code class="fe lw lx ly lz b">__PERIOD__</code>将帮助我们做到这一点。</p><p id="43fc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，让我们创建两个散列表，我们可以使用它们从索引&gt;词汇以及词汇&gt;索引中查找词汇。<br/>下面的代码可以做到这一点:</p><pre class="kp kq kr ks gt ma lz mb mc aw md bi"><span id="2756" class="me mf it lz b gy mg mh l mi mj">word_counts = Counter(words)<br/>sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)<br/>int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}<br/>vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}</span></pre><p id="c090" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">7.<strong class="js iu">执行二次采样<br/> </strong>经常出现的单词，如“the”、“is”等，对于为附近的单词提供上下文来说不是很有用。如果我们删除所有这些信息，我们就有效地删除了他们提供的任何信息。Mikolov等人提出的一种更好的方法是删除一些单词，以消除数据中的一些噪声。对于训练集中的每个单词，我们以概率丢弃它:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/8b9213d5ee88336517842c2e71d09576.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*h4xJftToHQRc_sl1ejpmfA.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Probability to discard words to reduce noise</figcaption></figure><p id="d4b4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中t是阈值参数，f(w)是单词w的频率。</p><p id="572a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">8.<strong class="js iu">创建张量流图</strong></p><p id="57f0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我总是很难想象单词嵌入和数学是如何工作的。我发现吴恩达的解释在理解数学方面非常有用，下面是我们试图用TensorFlow做的工作流程(假设一个单词向量有300个维度，词汇表有100，000个独特的单词):</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mm"><img src="../Images/438d3dd3eaa57df8b19554ee235ab8bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CvWx-sSkTKLhzrb8eMQw2g.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd mn">The 3 steps in training word embeddings — we only care about embedding matrix</strong></figcaption></figure><p id="af66" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输入的单词作为一个热点向量传递到一个隐藏的线性单元层。这些然后被连接到用于预测<strong class="js iu">上下文</strong>单词的软最大层。换句话说，给定每个单词，我们将尝试最小化预测邻居或上下文单词的损失。我们最终可以扔掉soft-max层权重，只使用我们构建的单词向量的嵌入矩阵——这些单词向量是单词的低维表示，同时保留了上下文！<br/> <a class="ae lf" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" rel="noopener ugc nofollow" target="_blank"> Chris McCormick的</a>博客是下图的灵感来源，该图是上图的另一个视图:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mo"><img src="../Images/36d44cbac8a157f5818ecc1ad5316320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aVhr_pYN4MVGMluqC-SZew.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Visualizing the neural network of word embeddings</figcaption></figure><p id="31d9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">TensorFlow使用数据流图来计算深度学习操作，我们需要首先定义图，然后在一个会话中在这个图上运行我们的数据。<br/>让我们浏览一下这段代码:</p><pre class="kp kq kr ks gt ma lz mb mc aw md bi"><span id="1075" class="me mf it lz b gy mg mh l mi mj">import tensorflow as tf<br/>train_graph = tf.Graph()  </span><span id="3288" class="me mf it lz b gy mk mh l mi mj"># Placeholders for our training data <br/>with train_graph.as_default():<br/>    inputs = tf.placeholder(tf.int32, [None], name='inputs')<br/>    labels = tf.placeholder(tf.int32, [None, None], name='labels')</span><span id="4a95" class="me mf it lz b gy mk mh l mi mj">n_vocab = len(int_to_vocab)<br/>n_embedding = 300  # Number of embedding features</span><span id="bfff" class="me mf it lz b gy mk mh l mi mj"># Embedding matrix that we care about <br/>with train_graph.as_default():<br/>    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))<br/>    word_vector = tf.nn.embedding_lookup(embedding, inputs)</span></pre><p id="4432" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先我们定义图中的<code class="fe lw lx ly lz b">placeholders</code>的<code class="fe lw lx ly lz b">inputs</code>和<code class="fe lw lx ly lz b">labels</code>。接下来，我们使用<code class="fe lw lx ly lz b">Variable</code>来定义我们想要训练的内容。这个<a class="ae lf" href="https://stackoverflow.com/questions/36693740/whats-the-difference-between-tf-placeholder-and-tf-variable" rel="noopener ugc nofollow" target="_blank">讨论</a>对他们的区别很好。我们使用一个<code class="fe lw lx ly lz b">feed_dict</code>将值输入到占位符中。我们的<code class="fe lw lx ly lz b">total vocabulary size x number of embedding dimensions</code>给出了我们需要训练的总重量。<br/>接下来，我们可以定义softmax权重和偏差:</p><pre class="kp kq kr ks gt ma lz mb mc aw md bi"><span id="6912" class="me mf it lz b gy mg mh l mi mj"># Softmax layer that we use for training but don't care about<br/>with train_graph.as_default():<br/>    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding), stddev=0.1))<br/>    softmax_b = tf.Variable(tf.zeros(n_vocab)) </span></pre><p id="4e1c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 9。负采样和损失函数<br/> </strong>我们准备了一组单词和上下文单词，它们是<strong class="js iu">正示例</strong>。但是我们也会添加一些单词和非上下文单词。这些将作为<strong class="js iu">反面</strong>的例子。所以<code class="fe lw lx ly lz b">orange juice</code>是正面例子，但<code class="fe lw lx ly lz b">orange book</code>和<code class="fe lw lx ly lz b">orange king</code>也是反面例子。这使得向量的学习更好。对于每个正面的例子，我们可以选择2到20个反面的例子。这被称为负采样，TensorFlow为此提供了一个函数<code class="fe lw lx ly lz b"><a class="ae lf" href="https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss" rel="noopener ugc nofollow" target="_blank">tf.nn.sampled_softmax_loss</a></code>！</p><p id="780f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们已经写好了整个图表，我们需要写损失函数是什么，以及如何优化权重。</p><pre class="kp kq kr ks gt ma lz mb mc aw md bi"><span id="4dd6" class="me mf it lz b gy mg mh l mi mj">with train_graph.as_default():<br/>   loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, <br/>                                      labels, word_vector,<br/>                                      n_sampled, n_vocab)<br/>    <br/>   cost = tf.reduce_mean(loss)<br/>   optimizer = tf.train.AdamOptimizer().minimize(cost)</span></pre><p id="f32b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 10。运行TensorFlow会话来训练单词嵌入<br/> </strong>写出图形和损失函数以及优化器后，我们可以编写TensorFlow会话来训练单词嵌入。</p><pre class="kp kq kr ks gt ma lz mb mc aw md bi"><span id="bbd0" class="me mf it lz b gy mg mh l mi mj"><strong class="lz iu">with</strong> tf.Session(graph=train_graph) <strong class="lz iu">as</strong> sess:<br/>    loss = 0<br/>    sess.run(tf.global_variables_initializer())<br/><br/>    <strong class="lz iu">for</strong> e <strong class="lz iu">in</strong> range(1, 11): # Run for 10 epochs<br/>        batches = get_batches(train_words, batch_size, window_size)<br/>        <strong class="lz iu">for</strong> x, y <strong class="lz iu">in</strong> batches:<br/>            <br/>            train_loss, _ = sess.run([cost, optimizer], feed_dict= {inputs: x, labels: np.array(y)[:, <strong class="lz iu">None</strong>]})<br/>            loss += train_loss</span></pre><p id="f6d0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe lw lx ly lz b">get_batches</code>此处未显示函数，但它在笔记本中(在参考文献中)——它批量返回特征和类的生成器。上述步骤在CPU上可能需要很长时间，但在p2.x实例上运行起来相当快。我能够在几分钟内运行10个纪元来训练100k单词的单词大小。我们可以使用我们学过的<code class="fe lw lx ly lz b">embedding</code>来绘制T-SNE图，以形象化地展示上下文是如何被学习的。请注意，T-SNE图将把我们的300D数据缩减到2D，因此一些上下文将会丢失，但是它保留了许多关于上下文的信息。</p><pre class="kp kq kr ks gt ma lz mb mc aw md bi"><span id="8965" class="me mf it lz b gy mg mh l mi mj">from sklearn.manifold import TSNE<br/>tsne = TSNE() <br/>embed_tsne = tsne.fit_transform(embedding[:viz_words, :]) <br/><br/>fig, ax = plt.subplots(figsize=(14, 14)) <br/><strong class="lz iu">for</strong> idx <strong class="lz iu">in</strong> range(viz_words):     <br/>     plt.scatter(*embed_tsne[idx, :], color='steelblue')            <br/>     plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)</span></pre><p id="e91d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们还可以使用主成分分析来可视化单词之间的关系，我们预计会发现如下关系:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mp"><img src="../Images/644007bd7740fd2a0dcdb913114491c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5xl5nfDKt1QYY42p4JfFVA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Visualizing word vectors using PCA from the original <a class="ae lf" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank">Mikolov paper</a></figcaption></figure><p id="0554" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这使得单词嵌入非常有趣！如果你在一家电子商务公司工作，如果你能开始识别<code class="fe lw lx ly lz b">battery size</code>和<code class="fe lw lx ly lz b">battery capacity</code>是否是同一件事，这可能会对用户产生巨大的影响，或者如果你在一个酒店预订网站，发现<code class="fe lw lx ly lz b">hotel</code>和<code class="fe lw lx ly lz b">motel</code>相似可能会产生巨大的影响！因此，在涉及语言的情况下，单词嵌入在改善用户体验方面非常有效。</p><p id="21fe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我也觉得这个话题挺有意思的。我喜欢写作和语言，也喜欢数学和机器学习。词向量在某种程度上是这两个世界之间的桥梁。我刚刚完成了一个关于文本分类的大项目，接下来我会更多地研究这些与单词向量相关的想法！</p><p id="4742" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">祝你好运~</p><p id="2c70" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我正在创建一个类似主题的新课程，名为<br/>“查询理解技术概述”。<br/>本课程将涵盖信息检索技术、文本挖掘、查询理解技巧和诀窍、提高精确度、召回率和用户点击量的方法。有兴趣的请在这里报名<a class="ae lf" href="https://sanketgupta.teachable.com/p/query-understanding-techniques/" rel="noopener ugc nofollow" target="_blank">！这种兴趣展示将允许我优先考虑和建立课程。</a></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/11a0a9419ef0c239a2529bb5b76d4b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*qM_eG38deCf8mIOJ_wwELQ.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><a class="ae lf" href="https://sanketgupta.teachable.com/p/query-understanding-techniques" rel="noopener ugc nofollow" target="_blank">My new course: Overview of Query Understanding Techniques</a></figcaption></figure><p id="be2a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你有任何问题，给我的LinkedIn个人资料<a class="ae lf" href="https://www.linkedin.com/in/sanketgupta107/" rel="noopener ugc nofollow" target="_blank">留言或者给我发电子邮件到sanket@omnilence.com。感谢阅读！</a></p><p id="1b8a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">参考文献:</strong></p><p id="6f94" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">a) <a class="ae lf" href="https://www.coursera.org/learn/intro-to-deep-learning/lecture/dhzl5/word-embeddings" rel="noopener ugc nofollow" target="_blank">吴恩达关于单词嵌入的课程</a>。斯坦福大学CS224n曼宁教授的讲座。c)<a class="ae lf" href="https://www.google.com/search?q=udacity+deep+learning&amp;oq=udacity+deep+learning&amp;aqs=chrome..69i57j0j69i60l2j69i65j69i60.4966j0j4&amp;sourceid=chrome&amp;ie=UTF-8" rel="noopener ugc nofollow" target="_blank">uda city的深度学习纳米度</a>。<br/> d)我在Github上的<a class="ae lf" href="https://github.com/sanketg10/deep-learning-repo/blob/master/embeddings/Skip-Gram_word2vec.ipynb" rel="noopener ugc nofollow" target="_blank">字矢量笔记本</a>。</p></div></div>    
</body>
</html>