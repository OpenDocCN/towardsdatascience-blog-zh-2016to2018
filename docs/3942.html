<html>
<head>
<title>Introduction to Model Trees from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始介绍模型树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-model-trees-6e396259379a?source=collection_archive---------4-----------------------#2018-07-03">https://towardsdatascience.com/introduction-to-model-trees-6e396259379a?source=collection_archive---------4-----------------------#2018-07-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/cc555ccf3f670d7a9dd09cc87c64adac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0nbUcBcXbWmfb0X1XgtJxQ.jpeg"/></div></div></figure><p id="16b1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">决策树是机器学习中一种强大的监督学习工具，用于递归地(通过特征分割)将数据分割成单独的“孤岛”，以减少您对训练集的拟合的整体加权损失。决策树分类中常用的<em class="kw"/>是基尼指数损失的模态分类器，以及决策树回归的 L2 损失均值回归。然而应该另外观察的<em class="kw">是，在树分裂过程<em class="kw">中，决策树原则上可以采用<strong class="ka ir">任何</strong> <strong class="ka ir">模型</strong>，即</em> <em class="kw">线性回归、逻辑回归、神经网络</em>。本文的目的是向您介绍这种更一般化的方法，名为模型树，它将允许您从您选择的任何模型中构建决策树(而不是依赖于标准的 CART 方法)！</em></p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi kx"><img src="../Images/ed9d66eaa7c46a503dc00d28e71d8f15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yKxIOatPakfHvYMapbYVew.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Fig 1) A schematic of using a linear regression model tree to fit a 1D training set to find segments of the training set that are well fit by a straight line.</figcaption></figure><p id="f069" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在深入探讨为什么模型树有用和重要之前，我们在我的 Github 上提供了一个从头开始的模型树 Python 代码实现:</p><blockquote class="lg lh li"><p id="1aca" class="jy jz kw ka b kb kc kd ke kf kg kh ki lj kk kl km lk ko kp kq ll ks kt ku kv ij bi translated"><a class="ae lm" href="https://github.com/ankonzoid/LearningX/tree/master/advanced_ML/model_tree" rel="noopener ugc nofollow" target="_blank">T13】https://github . com/ankonzoid/learning x/tree/master/advanced _ ML/model _ treeT15】</a></p></blockquote><p id="1963" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">关于机器学习、深度学习和强化学习的更多博客、教程和项目，请查看我的<a class="ae lm" href="https://medium.com/@ankonzoid" rel="noopener"> <strong class="ka ir">中</strong> </a> <strong class="ka ir"> </strong>和我的<a class="ae lm" href="https://github.com/ankonzoid" rel="noopener ugc nofollow" target="_blank"> <strong class="ka ir"> Github </strong> </a> <strong class="ka ir">。</strong></p><h1 id="653d" class="ln lo iq bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">模型树的目的是什么？</h1><p id="6e33" class="pw-post-body-paragraph jy jz iq ka b kb ml kd ke kf mm kh ki kj mn kl km kn mo kp kq kr mp kt ku kv ij bi translated">假设你有一些复杂的训练数据，你天真地想到了一个简单的模型来拟合这个训练集(比如线性回归或者逻辑回归)。尤其是如果您猜测的简单模型的复杂性如此之低，那么您的模型本身很有可能会不足以满足您的训练数据。然而，希望并没有在这一点上失去！模型树的目的是在你的简单模型之外建立一个决策树层次结构，试图适应你的训练集(通过特征切片创建)的几个较小部分，这样整个模型树<em class="kw">确实</em>很好地适应完整的训练集。</p><p id="2836" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了明确演示在常规决策树上构建模型树的有用性，考虑 4 阶一维多项式以及训练低深度线性回归模型树回归器(图 2.a)和默认 scikit-learn 默认决策树回归器(图 2.b)之间的差异。您将会注意到，在这个例子中，模型树轻松胜过 scikit-learn 决策树回归器。</p><p id="611b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在下面的图 2.a 中，我们绘制了线性回归模型树与数据的拟合度，并增加了树的深度，以找到深度为 5 的拟合度良好的数据。正如你所看到的，即使拟合在极低的树深度(即 0，1，2)下并不出色，但拟合具有直观的意义，因为它们都贪婪地试图通过覆盖多项式的大部分来减少损失，从远处看<em class="kw">就像</em>直线。当我们到达深度 4 和 5 时，模型树将已经很好地捕捉到数据的 x 相关性，正如从 4 阶多项式所预期的。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mq"><img src="../Images/6e31f127c94a2ae5c106806b0e6cb34c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tlllOfbIjzTfpRPLqRCHsg.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Fig 2.a) Linear regression model tree fit on a 4th-order polynomial</figcaption></figure><p id="d73d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">另一方面，在下面的图 2.b 中，我们绘制了 scikit-learn 的默认决策树回归器的拟合度，发现即使在较高的树深度，拟合度仍然很差，这主要是因为它不能有效地捕捉数据的 x 相关性，因为 scikit-learn 的决策树使用均值回归(不考虑 x 变量，只关心 y 值)！因此，解决方案(不使用集合方法)将是迫使树更深以更接近近似值。希望这是一个强有力的视觉提醒，无论何时你有一点点奢侈去理解你的训练数据的本质，模型树都可以极大地改善你的结果。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mr"><img src="../Images/bcc321f3cf6598744e386c75a83e72dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OZ4-uu0Czghua5weTT8s-A.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Fig 2.b) Decision tree regressor (scikit-learn default implementation) fit on a 4th-order polynomial</figcaption></figure><p id="5195" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后一点，请记住，模型树在概念上的构建方式与常规决策树相同，这意味着模型树也可能遭受与决策树相同的缺陷，这通常涉及容易过度拟合的问题，尤其是当您使用复杂的模型时。</p></div></div>    
</body>
</html>