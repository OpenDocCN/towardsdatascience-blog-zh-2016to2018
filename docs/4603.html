<html>
<head>
<title>Sentiment Analysis with Text Mining</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于文本挖掘的情感分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27?source=collection_archive---------2-----------------------#2018-08-25">https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27?source=collection_archive---------2-----------------------#2018-08-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a0fa" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解如何准备文本数据并运行两个不同的分类器来预测推文的情绪。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/61b08fae380fe5c8bbf93e09c55907fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FIhWkx0Ty2435Fjb"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@rvignes?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Romain Vignes</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="361d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本教程中，我将探索一些用于情感分析的文本挖掘技术。首先，我们将花一些时间准备文本数据。这将涉及清理文本数据，删除停用词和词干。为此，Kaggle 上的<a class="ae kv" href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment" rel="noopener ugc nofollow" target="_blank"> Twitter 美国航空公司情绪数据集非常适合合作。它包含 tweet 的文本和一个具有三个可能情感值的变量。</a></p><p id="1e5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了推断推文的情感，我们使用两个分类器:逻辑回归和多项式朴素贝叶斯。我们将使用网格搜索来调整两个分类器的超参数。</p><p id="f696" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将用三个指标来比较性能:精确度、召回率和 F1 分数。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="244c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们从导入包和配置一些设置开始。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="2e40" class="me mf iq ma b gy mg mh l mi mj">import numpy as np <br/>import pandas as pd <br/>pd.set_option('display.max_colwidth', -1)<br/>from time import time<br/>import re<br/>import string<br/>import os<br/>import emoji<br/>from pprint import pprint<br/>import collections</span><span id="312a" class="me mf iq ma b gy mk mh l mi mj">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>sns.set(style="darkgrid")<br/>sns.set(font_scale=1.3)</span><span id="9965" class="me mf iq ma b gy mk mh l mi mj">from sklearn.base import BaseEstimator, TransformerMixin<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.pipeline import Pipeline, FeatureUnion<br/>from sklearn.metrics import classification_report</span><span id="0e09" class="me mf iq ma b gy mk mh l mi mj">from sklearn.naive_bayes import MultinomialNB<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.externals import joblib</span><span id="9942" class="me mf iq ma b gy mk mh l mi mj">import gensim</span><span id="65d2" class="me mf iq ma b gy mk mh l mi mj">from nltk.corpus import stopwords<br/>from nltk.stem import PorterStemmer<br/>from nltk.tokenize import word_tokenize</span><span id="1024" class="me mf iq ma b gy mk mh l mi mj">import warnings<br/>warnings.filterwarnings('ignore')</span><span id="cd8f" class="me mf iq ma b gy mk mh l mi mj">np.random.seed(37)</span></pre><h1 id="052d" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">加载数据</h1><p id="aa49" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">我们读取了从 Kaggle 数据集下载的逗号分隔文件。我们打乱了数据帧，以防类被排序。对原始指数的<code class="fe nh ni nj ma b">permutation</code>应用<code class="fe nh ni nj ma b">reindex</code>方法有利于此。在这本笔记本中，我们将使用<code class="fe nh ni nj ma b">text</code>变量和<code class="fe nh ni nj ma b">airline_sentiment</code>变量。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="6805" class="me mf iq ma b gy mg mh l mi mj">df = pd.read_csv('../input/Tweets.csv')<br/>df = df.reindex(np.random.permutation(df.index))<br/>df = df[['text', 'airline_sentiment']]</span></pre><h1 id="7240" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">探索性数据分析</h1><h2 id="85ee" class="me mf iq bd mm nk nl dn mq nm nn dp mu lf no np mw lj nq nr my ln ns nt na nu bi translated">目标变量</h2><p id="0e7c" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">我们将预测三个类别标签:负面、中性或正面。</p><p id="2e33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们在下面的图表中看到的，分类标签是不平衡的。这是我们在模型训练阶段应该记住的事情。有了 seaborn 包的<code class="fe nh ni nj ma b">factorplot</code>，我们可以可视化目标变量的分布。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="93a3" class="me mf iq ma b gy mg mh l mi mj">sns.factorplot(x="airline_sentiment", data=df, kind="count", size=6, aspect=1.5, palette="PuBuGn_d")<br/>plt.show();</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/d091427d229243d724e1f4ac320ecc12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/0*v99Gfk4iL4POvy2F.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Imbalanced distribution of the target class labels</figcaption></figure><h2 id="9c86" class="me mf iq bd mm nk nl dn mq nm nn dp mu lf no np mw lj nq nr my ln ns nt na nu bi translated">输入变量</h2><p id="1510" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">为了分析<code class="fe nh ni nj ma b">text </code>变量，我们创建了一个类<code class="fe nh ni nj ma b">TextCounts</code>。在这个类中，我们计算文本变量的一些基本统计数据。</p><ul class=""><li id="7347" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated"><code class="fe nh ni nj ma b">count_words</code>:推文字数</li><li id="1106" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><code class="fe nh ni nj ma b">count_mentions</code>:其他 Twitter 账户的推荐以@开头</li><li id="8299" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><code class="fe nh ni nj ma b">count_hashtags</code>:标签字数，以#开头</li><li id="d0f7" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><code class="fe nh ni nj ma b">count_capital_words</code>:一些大写单词有时被用来“叫喊”和表达(负面)情绪</li><li id="c46a" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><code class="fe nh ni nj ma b">count_excl_quest_marks</code>:问号或感叹号的个数</li><li id="21c3" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><code class="fe nh ni nj ma b">count_urls</code>:推文中的链接数量，以 http(s)开头</li><li id="07e1" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><code class="fe nh ni nj ma b">count_emojis</code>:表情符号的数量，这可能是情绪的一个好迹象</li></ul><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="227d" class="me mf iq ma b gy mg mh l mi mj">class TextCounts(BaseEstimator, TransformerMixin):<br/>    <br/>    def count_regex(self, pattern, tweet):<br/>        return len(re.findall(pattern, tweet))<br/>    <br/>    def fit(self, X, y=None, **fit_params):<br/>        # fit method is used when specific operations need to be done on the train data, but not on the test data<br/>        return self<br/>    <br/>    def transform(self, X, **transform_params):<br/>        count_words = X.apply(lambda x: self.count_regex(r'\w+', x)) <br/>        count_mentions = X.apply(lambda x: self.count_regex(r'@\w+', x))<br/>        count_hashtags = X.apply(lambda x: self.count_regex(r'#\w+', x))<br/>        count_capital_words = X.apply(lambda x: self.count_regex(r'\b[A-Z]{2,}\b', x))<br/>        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\?', x))<br/>        count_urls = X.apply(lambda x: self.count_regex(r'http.?://[^\s]+[\s]?', x))<br/>        # We will replace the emoji symbols with a description, which makes using a regex for counting easier<br/>        # Moreover, it will result in having more words in the tweet<br/>        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&amp;]+:', x))<br/>        <br/>        df = pd.DataFrame({'count_words': count_words<br/>                           , 'count_mentions': count_mentions<br/>                           , 'count_hashtags': count_hashtags<br/>                           , 'count_capital_words': count_capital_words<br/>                           , 'count_excl_quest_marks': count_excl_quest_marks<br/>                           , 'count_urls': count_urls<br/>                           , 'count_emojis': count_emojis<br/>                          })<br/>        <br/>        return df<br/>tc = TextCounts()<br/>df_eda = tc.fit_transform(df.text)<br/>df_eda['airline_sentiment'] = df.airline_sentiment</span></pre><p id="fc92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查看 TextStats 变量与 class 变量的关系可能会很有趣。因此我们编写了一个函数<code class="fe nh ni nj ma b">show_dist</code>,它为每个目标类提供了描述性的统计数据和图表。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="ab71" class="me mf iq ma b gy mg mh l mi mj">def show_dist(df, col):<br/>    print('Descriptive stats for {}'.format(col))<br/>    print('-'*(len(col)+22))<br/>    print(df.groupby('airline_sentiment')[col].describe())<br/>    bins = np.arange(df[col].min(), df[col].max() + 1)<br/>    g = sns.FacetGrid(df, col='airline_sentiment', size=5, hue='airline_sentiment', palette="PuBuGn_d")<br/>    g = g.map(sns.distplot, col, kde=False, norm_hist=True, bins=bins)<br/>    plt.show()</span></pre><p id="2ec1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面你可以找到每个目标类别的 tweet 字数分布。为简洁起见，我们将仅限于这个变量。所有 TextCounts 变量的图表都在 Github 的<a class="ae kv" href="https://github.com/bertcarremans/TwitterUSAirlineSentiment" rel="noopener ugc nofollow" target="_blank">笔记本里。</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/4da06de8a1834b9ca420b45840cc0c6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*snmvA3GQOb_S9wV8.png"/></div></div></figure><ul class=""><li id="ac09" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated">推文中使用的字数相当低。最大的字数是 36 个，甚至有只有 2 个字的推文。所以在数据清理过程中，我们必须小心，不要删除太多的单词。但是文字处理会更快。负面推文比中性或正面推文包含更多单词。</li><li id="6bc7" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">所有推文至少有一次提及。这是基于 Twitter 数据中的提及提取推文的结果。就情感而言，提及次数似乎没有什么不同。</li><li id="3b4d" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">大多数推文不包含哈希标签。所以这个变量在模型训练期间不会被保留。同样，对于情感，散列标签的数量没有差别。</li><li id="d37e" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">大多数推文不包含大写单词，我们看不到情绪分布的差异。</li><li id="da32" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">积极的推文似乎使用了更多的感叹或问号。</li><li id="9aeb" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">大多数推文不包含网址。</li><li id="7c29" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">大多数推文不使用表情符号。</li></ul><h1 id="a380" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">文本清理</h1><p id="de93" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">在我们开始使用 tweets 的文本之前，我们需要清理它。我们将在<code class="fe nh ni nj ma b">CleanText</code> <strong class="ky ir">课上做这件事。</strong>在这个课程中，我们将执行以下操作:</p><ul class=""><li id="9ed4" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated">删除提及，因为我们也想推广到其他航空公司的推文。</li><li id="cf41" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">删除散列标签符号(#)，但不要删除实际的标签，因为这可能包含信息</li><li id="3233" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">将所有单词设为小写</li><li id="e066" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">删除所有标点符号，包括问号和感叹号</li><li id="28fd" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">删除网址，因为它们不包含有用的信息。我们没有注意到情感类别之间使用的 URL 数量的差异</li><li id="3337" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">确保将表情符号转换成一个单词。</li><li id="0303" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">删除数字</li><li id="6840" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">删除停用词</li><li id="9501" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">应用<code class="fe nh ni nj ma b">PorterStemmer</code>保留单词的词干</li></ul><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="aa45" class="me mf iq ma b gy mg mh l mi mj">class CleanText(BaseEstimator, TransformerMixin):<br/>    def remove_mentions(self, input_text):<br/>        return re.sub(r'@\w+', '', input_text)<br/>    <br/>    def remove_urls(self, input_text):<br/>        return re.sub(r'http.?://[^\s]+[\s]?', '', input_text)<br/>    <br/>    def emoji_oneword(self, input_text):<br/>        # By compressing the underscore, the emoji is kept as one word<br/>        return input_text.replace('_','')<br/>    <br/>    def remove_punctuation(self, input_text):<br/>        # Make translation table<br/>        punct = string.punctuation<br/>        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space<br/>        return input_text.translate(trantab)</span><span id="f501" class="me mf iq ma b gy mk mh l mi mj">    def remove_digits(self, input_text):<br/>        return re.sub('\d+', '', input_text)<br/>    <br/>    def to_lower(self, input_text):<br/>        return input_text.lower()<br/>    <br/>    def remove_stopwords(self, input_text):<br/>        stopwords_list = stopwords.words('english')<br/>        # Some words which might indicate a certain sentiment are kept via a whitelist<br/>        whitelist = ["n't", "not", "no"]<br/>        words = input_text.split() <br/>        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) &gt; 1] <br/>        return " ".join(clean_words) <br/>    <br/>    def stemming(self, input_text):<br/>        porter = PorterStemmer()<br/>        words = input_text.split() <br/>        stemmed_words = [porter.stem(word) for word in words]<br/>        return " ".join(stemmed_words)<br/>    <br/>    def fit(self, X, y=None, **fit_params):<br/>        return self<br/>    <br/>    def transform(self, X, **transform_params):<br/>        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)<br/>        return clean_X</span></pre><p id="413c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了展示清理后的文本变量的外观，这里有一个示例。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="922c" class="me mf iq ma b gy mg mh l mi mj">ct = CleanText()<br/>sr_clean = ct.fit_transform(df.text)<br/>sr_clean.sample(5)</span></pre><blockquote class="ol om on"><p id="67cf" class="kw kx oo ky b kz la jr lb lc ld ju le op lg lh li oq lk ll lm or lo lp lq lr ij bi translated">高兴 rt 打赌鸟愿飞南方冬天<br/>点 upc 代码检查 baggag 告诉 luggag vacat day tri 泳装<br/> vx jfk la dirti 飞机不标准<br/>告诉意味着工作需要估计时间到达请需要笔记本电脑工作感谢<br/>当然业务去 els 航空旅行姓名凯瑟琳索特罗</p></blockquote><p id="afe1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">文本清理的一个副作用是一些行的文本中没有任何单词。对于<code class="fe nh ni nj ma b">CountVectorizer</code>和<code class="fe nh ni nj ma b">TfIdfVectorizer</code>来说，这不成问题。然而，对于<code class="fe nh ni nj ma b">Word2Vec</code>算法来说，这会导致一个错误。有不同的策略来处理这些缺失的价值观。</p><ul class=""><li id="8aef" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated">删除整行，但在生产环境中这是不可取的。</li><li id="7649" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">用类似*[no_text]*的占位符文本估算缺失值</li><li id="cd99" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">当应用 Word2Vec 时:使用所有向量的平均值</li></ul><p id="d1e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们将使用占位符文本进行估算。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="1673" class="me mf iq ma b gy mg mh l mi mj">empty_clean = sr_clean == ''<br/>print('{} records have no words left after text cleaning'.format(sr_clean[empty_clean].count()))<br/>sr_clean.loc[empty_clean] = '[no_text]'</span></pre><p id="5187" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们已经清理了推文的文本，我们可以看看最常用的词是什么。下面我们将展示前 20 个单词。出现频率最高的词是“逃”。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="ee0e" class="me mf iq ma b gy mg mh l mi mj">cv = CountVectorizer()<br/>bow = cv.fit_transform(sr_clean)<br/>word_freq = dict(zip(cv.get_feature_names(), np.asarray(bow.sum(axis=0)).ravel()))<br/>word_counter = collections.Counter(word_freq)<br/>word_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])</span><span id="cedc" class="me mf iq ma b gy mk mh l mi mj">fig, ax = plt.subplots(figsize=(12, 10))<br/>sns.barplot(x="word", y="freq", data=word_counter_df, palette="PuBuGn_d", ax=ax)<br/>plt.show();</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/bc20216c86f3bcc941c7cd785f7e52b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hBvkYfey1Astmd02.png"/></div></div></figure><h1 id="f641" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">创建测试数据</h1><p id="6d42" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">为了检查模型的性能，我们需要一套测试设备。对训练数据的评估是不正确的。您不应该在用于训练模型的相同数据上进行测试。</p><p id="0ae5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们将<code class="fe nh ni nj ma b">TextCounts</code>变量与<code class="fe nh ni nj ma b">CleanText</code>变量结合起来。最初，我在<code class="fe nh ni nj ma b">GridSearchCV</code>中错误地执行了 TextCounts 和 CleanText。这花费了太长时间，因为每次运行 GridSearch 都要应用这些函数。只运行一次就足够了。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="bd7c" class="me mf iq ma b gy mg mh l mi mj">df_model = df_eda<br/>df_model['clean_text'] = sr_clean<br/>df_model.columns.tolist()</span></pre><p id="4e5e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以<code class="fe nh ni nj ma b">df_model</code>现在包含了几个变量。但是我们的矢量器(见下文)将只需要<code class="fe nh ni nj ma b">clean_text</code>变量。可以添加<code class="fe nh ni nj ma b">TextCounts</code>变量。为了选择列，我编写了下面的类<code class="fe nh ni nj ma b">ColumnExtractor</code>。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="80f5" class="me mf iq ma b gy mg mh l mi mj">class ColumnExtractor(TransformerMixin, BaseEstimator):<br/>    def __init__(self, cols):<br/>        self.cols = cols</span><span id="1764" class="me mf iq ma b gy mk mh l mi mj">    def transform(self, X, **transform_params):<br/>        return X[self.cols]</span><span id="4a95" class="me mf iq ma b gy mk mh l mi mj">    def fit(self, X, y=None, **fit_params):<br/>        return self</span><span id="3a04" class="me mf iq ma b gy mk mh l mi mj">X_train, X_test, y_train, y_test = train_test_split(df_model.drop('airline_sentiment', axis=1), df_model.airline_sentiment, test_size=0.1, random_state=37)</span></pre><h1 id="7339" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">超参数调整和交叉验证</h1><p id="e621" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">正如我们将在下面看到的，矢量器和分类器都有可配置的参数。为了选择最佳参数，我们需要在单独的验证集上进行测试。培训期间没有使用该验证集。然而，仅使用一个验证集可能不会产生可靠的验证结果。由于偶然的机会，您可能在验证集上有一个好的模型性能。如果您以其他方式分割数据，您可能会得到其他结果。为了得到更准确的估计，我们进行交叉验证。</p><p id="4e5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过交叉验证，我们可以多次将数据分成训练集和验证集。然后在不同的折叠上对评估度量进行平均。幸运的是，GridSearchCV 应用了现成的交叉验证。</p><p id="e780" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了找到矢量器和分类器的最佳参数，我们创建了一个<code class="fe nh ni nj ma b">Pipeline</code>。</p><h1 id="4a41" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">评估指标</h1><p id="a114" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">默认情况下，GridSearchCV 使用默认计分器来计算<code class="fe nh ni nj ma b">best_score_</code>。对于<code class="fe nh ni nj ma b">MultiNomialNb</code>和<code class="fe nh ni nj ma b">LogisticRegression</code>来说，这个默认的评分标准是准确性。</p><p id="412b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的函数<code class="fe nh ni nj ma b">grid_vect</code>中，我们额外生成了测试数据的<code class="fe nh ni nj ma b">classification_report</code>。这为每个目标类提供了一些有趣的度量。这在这里可能更合适。这些指标是精确度、召回率和 F1 分数<strong class="ky ir">。</strong></p><ul class=""><li id="fe0e" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated">Precision <strong class="ky ir"> : </strong>在我们预测为某个类的所有行中，我们正确预测了多少？</li><li id="a6bd" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">回想一下<strong class="ky ir"> : </strong>在某个类的所有行中，我们正确预测了多少行？</li><li id="9856" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">F1 得分<strong class="ky ir"> : </strong>精确度和召回率的调和平均值。</li></ul><p id="ddd3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">利用<a class="ae kv" href="https://en.wikipedia.org/wiki/Confusion_matrix" rel="noopener ugc nofollow" target="_blank">混淆矩阵</a>的元素，我们可以计算精确度和召回率。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="9051" class="me mf iq ma b gy mg mh l mi mj"># Based on <a class="ae kv" href="http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html" rel="noopener ugc nofollow" target="_blank">http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html</a><br/>def grid_vect(clf, parameters_clf, X_train, X_test, parameters_text=None, vect=None, is_w2v=False):<br/>    <br/>    textcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags'<br/>                      ,'count_mentions','count_urls','count_words']<br/>    <br/>    if is_w2v:<br/>        w2vcols = []<br/>        for i in range(SIZE):<br/>            w2vcols.append(i)<br/>        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))<br/>                                 , ('w2v', ColumnExtractor(cols=w2vcols))]<br/>                                , n_jobs=-1)<br/>    else:<br/>        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))<br/>                                 , ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text')), ('vect', vect)]))]<br/>                                , n_jobs=-1)</span><span id="9c19" class="me mf iq ma b gy mk mh l mi mj">    <br/>    pipeline = Pipeline([<br/>        ('features', features)<br/>        , ('clf', clf)<br/>    ])<br/>    <br/>    # Join the parameters dictionaries together<br/>    parameters = dict()<br/>    if parameters_text:<br/>        parameters.update(parameters_text)<br/>    parameters.update(parameters_clf)</span><span id="e4ba" class="me mf iq ma b gy mk mh l mi mj">    # Make sure you have scikit-learn version 0.19 or higher to use multiple scoring metrics<br/>    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5)<br/>    <br/>    print("Performing grid search...")<br/>    print("pipeline:", [name for name, _ in pipeline.steps])<br/>    print("parameters:")<br/>    pprint(parameters)</span><span id="416e" class="me mf iq ma b gy mk mh l mi mj">    t0 = time()<br/>    grid_search.fit(X_train, y_train)<br/>    print("done in %0.3fs" % (time() - t0))<br/>    print()</span><span id="9767" class="me mf iq ma b gy mk mh l mi mj">    print("Best CV score: %0.3f" % grid_search.best_score_)<br/>    print("Best parameters set:")<br/>    best_parameters = grid_search.best_estimator_.get_params()<br/>    for param_name in sorted(parameters.keys()):<br/>        print("\t%s: %r" % (param_name, best_parameters[param_name]))<br/>        <br/>    print("Test score with best_estimator_: %0.3f" % grid_search.best_estimator_.score(X_test, y_test))<br/>    print("\n")<br/>    print("Classification Report Test Data")<br/>    print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))<br/>                        <br/>    return grid_search</span></pre><h1 id="a8c2" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">GridSearchCV 的参数网格</h1><p id="07b3" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">在网格搜索中，我们将研究分类器的性能。用于测试性能的一组参数如下所示。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="7d6a" class="me mf iq ma b gy mg mh l mi mj"># Parameter grid settings for the vectorizers (Count and TFIDF)<br/>parameters_vect = {<br/>    'features__pipe__vect__max_df': (0.25, 0.5, 0.75),<br/>    'features__pipe__vect__ngram_range': ((1, 1), (1, 2)),<br/>    'features__pipe__vect__min_df': (1,2)<br/>}<br/></span><span id="bea5" class="me mf iq ma b gy mk mh l mi mj"># Parameter grid settings for MultinomialNB<br/>parameters_mnb = {<br/>    'clf__alpha': (0.25, 0.5, 0.75)<br/>}<br/></span><span id="bc65" class="me mf iq ma b gy mk mh l mi mj"># Parameter grid settings for LogisticRegression<br/>parameters_logreg = {<br/>    'clf__C': (0.25, 0.5, 1.0),<br/>    'clf__penalty': ('l1', 'l2')<br/>}</span></pre><h1 id="94d2" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">分类器</h1><p id="4d0f" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">这里我们将比较一下<code class="fe nh ni nj ma b">MultinomialNB</code>和<code class="fe nh ni nj ma b">LogisticRegression</code>的性能。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="144d" class="me mf iq ma b gy mg mh l mi mj">mnb = MultinomialNB()<br/>logreg = LogisticRegression()</span></pre><h1 id="90f9" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">计数矢量器</h1><p id="84cc" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">为了在分类器中使用单词，我们需要将单词转换成数字。Sklearn 的<code class="fe nh ni nj ma b">CountVectorizer</code>获取所有推文中的所有单词，分配一个 ID，并统计每个推文中该单词的出现频率。然后，我们使用这个单词包作为分类器的输入。这一袋单词是一个稀疏的数据集。这意味着每条记录都将有许多零，代表没有在 tweet 中出现的单词。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="4293" class="me mf iq ma b gy mg mh l mi mj">countvect = CountVectorizer()</span><span id="ee3b" class="me mf iq ma b gy mk mh l mi mj"># MultinomialNB<br/>best_mnb_countvect = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=countvect)<br/>joblib.dump(best_mnb_countvect, '../output/best_mnb_countvect.pkl')</span><span id="36f7" class="me mf iq ma b gy mk mh l mi mj"># LogisticRegression<br/>best_logreg_countvect = grid_vect(logreg, parameters_logreg, X_train, X_test, parameters_text=parameters_vect, vect=countvect)<br/>joblib.dump(best_logreg_countvect, '../output/best_logreg_countvect.pkl')</span></pre><h1 id="2e9f" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">TF-IDF 矢量器</h1><p id="9c99" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">CountVectorizer 的一个问题是可能会有频繁出现的单词。这些词可能没有歧视性信息。因此它们可以被移除。<a class="ae kv" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> TF-IDF(词频—逆文档频率)</a>可以用来对这些频繁出现的词进行降权。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="fb8f" class="me mf iq ma b gy mg mh l mi mj">tfidfvect = TfidfVectorizer()</span><span id="81cd" class="me mf iq ma b gy mk mh l mi mj"># MultinomialNB<br/>best_mnb_tfidf = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)<br/>joblib.dump(best_mnb_tfidf, '../output/best_mnb_tfidf.pkl')</span><span id="03f8" class="me mf iq ma b gy mk mh l mi mj"># LogisticRegression<br/>best_logreg_tfidf = grid_vect(logreg, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)<br/>joblib.dump(best_logreg_tfidf, '../output/best_logreg_tfidf.pkl')</span></pre><h1 id="dbb9" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">Word2Vec</h1><p id="9396" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">将单词转换成数值的另一种方法是使用<code class="fe nh ni nj ma b">Word2Vec</code>。Word2Vec 将每个单词映射到多维空间中。它通过考虑一个词在推文中出现的上下文来做到这一点。结果，相似的单词在多维空间中也彼此接近。</p><p id="7ddd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Word2Vec 算法是 gensim 包的一部分。</p><p id="6307" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Word2Vec 算法使用单词列表作为输入。为此，我们使用了<code class="fe nh ni nj ma b">nltk</code>包的<code class="fe nh ni nj ma b">word_tokenize</code>方法。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="765d" class="me mf iq ma b gy mg mh l mi mj">SIZE = 50</span><span id="10ac" class="me mf iq ma b gy mk mh l mi mj">X_train['clean_text_wordlist'] = X_train.clean_text.apply(lambda x : word_tokenize(x))<br/>X_test['clean_text_wordlist'] = X_test.clean_text.apply(lambda x : word_tokenize(x))</span><span id="581e" class="me mf iq ma b gy mk mh l mi mj">model = gensim.models.Word2Vec(X_train.clean_text_wordlist<br/>, min_count=1<br/>, size=SIZE<br/>, window=5<br/>, workers=4)</span><span id="fb55" class="me mf iq ma b gy mk mh l mi mj">model.most_similar('plane', topn=3)</span></pre><p id="08d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Word2Vec 模型提供了所有 tweets 中的词汇。对于每个单词，你也有它的向量值。向量值的数量等于所选的大小。这些是每个单词在多维空间中映射的维度。出现次数少于<code class="fe nh ni nj ma b">min_count</code>的单词不会保留在词汇表中。</p><p id="e2d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">min_count 参数的一个副作用是一些 tweets 可能没有向量值。当 tweet 中的单词在少于 min_count <em class="oo"> </em>的 tweet 中出现时，就会出现这种情况。由于 tweets 的语料库很小，在我们的案例中有发生这种情况的风险。因此，我们将 min_count 值设置为 1。</p><p id="76bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">推文可以有不同数量的向量，这取决于它包含的字数。为了使用这个输出进行建模，我们将计算每条 tweet 的所有向量的平均值。因此，我们将拥有相同数量(即大小)的输入变量。</p><p id="4624" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们用函数<code class="fe nh ni nj ma b">compute_avg_w2v_vector</code>来做这件事。在这个函数中，我们还检查 tweet 中的单词是否出现在 Word2Vec 模型的词汇表中。如果不是，则返回一个用 0.0 填充的列表。否则是单词向量的平均值。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="1359" class="me mf iq ma b gy mg mh l mi mj">def compute_avg_w2v_vector(w2v_dict, tweet):<br/>    list_of_word_vectors = [w2v_dict[w] for w in tweet if w in w2v_dict.vocab.keys()]<br/>    <br/>    if len(list_of_word_vectors) == 0:<br/>        result = [0.0]*SIZE<br/>    else:<br/>        result = np.sum(list_of_word_vectors, axis=0) / len(list_of_word_vectors)<br/>        <br/>    return result</span><span id="527c" class="me mf iq ma b gy mk mh l mi mj">X_train_w2v = X_train['clean_text_wordlist'].apply(lambda x: compute_avg_w2v_vector(model.wv, x))<br/>X_test_w2v = X_test['clean_text_wordlist'].apply(lambda x: compute_avg_w2v_vector(model.wv, x))</span></pre><p id="ce33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这给了我们一个向量维数等于<code class="fe nh ni nj ma b">SIZE</code>的序列。现在我们将分割这个向量并创建一个数据帧，每个向量值在单独的列中。这样，我们可以将 Word2Vec 变量连接到其他 TextCounts 变量。我们需要重用<code class="fe nh ni nj ma b">X_train</code>和<code class="fe nh ni nj ma b">X_test</code>的索引。否则，这将在以后的连接中产生问题(重复)。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="c8f4" class="me mf iq ma b gy mg mh l mi mj">X_train_w2v = pd.DataFrame(X_train_w2v.values.tolist(), index= X_train.index)<br/>X_test_w2v = pd.DataFrame(X_test_w2v.values.tolist(), index= X_test.index)</span><span id="b4da" class="me mf iq ma b gy mk mh l mi mj"># Concatenate with the TextCounts variables<br/>X_train_w2v = pd.concat([X_train_w2v, X_train.drop(['clean_text', 'clean_text_wordlist'], axis=1)], axis=1)<br/>X_test_w2v = pd.concat([X_test_w2v, X_test.drop(['clean_text', 'clean_text_wordlist'], axis=1)], axis=1)</span></pre><p id="c548" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们只考虑逻辑回归，因为我们在 Word2Vec 向量中有负值。多项式 lNB 假设变量具有<a class="ae kv" href="https://en.wikipedia.org/wiki/Multinomial_distribution" rel="noopener ugc nofollow" target="_blank">多项式分布</a>。因此它们不能包含负值。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="f998" class="me mf iq ma b gy mg mh l mi mj">best_logreg_w2v = grid_vect(logreg, parameters_logreg, X_train_w2v, X_test_w2v, is_w2v=True)<br/>joblib.dump(best_logreg_w2v, '../output/best_logreg_w2v.pkl')</span></pre><h1 id="52df" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">结论</h1><ul class=""><li id="8f53" class="nw nx iq ky b kz nc lc nd lf ot lj ou ln ov lr ob oc od oe bi translated">当使用计数矢量器的特性时，这两种分类器都能获得最佳结果</li><li id="da30" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">逻辑回归优于多项式朴素贝叶斯分类器</li><li id="bd1f" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">测试集上的最佳性能来自带有 CountVectorizer 特性的 LogisticRegression。</li></ul><p id="0b6d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最佳参数:</p><ul class=""><li id="6a27" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated">c 值为 1</li><li id="9b3d" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">L2 正则化</li><li id="10cc" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">max_df: 0.5 或最大文档频率 50%。</li><li id="5e92" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">min_df: 1 或者这些词需要出现在至少两条推文中</li><li id="f706" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">ngram_range: (1，2)，两个单词都作为二元语法使用</li></ul><p id="0fd4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">评估指标:</p><ul class=""><li id="707e" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated">测试准确率为 81.3%。这优于预测所有观察的多数类(这里是负面情绪)的基线性能。基线会给出 63%的准确度。</li><li id="54ad" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">这三个类别的精度都相当高。例如，在我们预测为负面的所有案例中，80%是负面的。</li><li id="ddba" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">中性类的召回率很低。在我们测试数据的所有中性案例中，我们只预测 48%是中性的。</li></ul><h1 id="ffbf" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">对新推文应用最佳模型</h1><p id="7049" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">为了好玩，我们将使用最佳模型，并将其应用于一些包含“@VirginAmerica”的新推文。我手动选择了 3 条负面和 3 条正面的推文。</p><p id="5625" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">多亏了 GridSearchCV，我们现在知道了什么是最好的超参数。因此，现在我们可以在所有训练数据上训练最佳模型，包括我们之前分离的测试数据。</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="797d" class="me mf iq ma b gy mg mh l mi mj">textcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags'<br/>,'count_mentions','count_urls','count_words']</span><span id="edd9" class="me mf iq ma b gy mk mh l mi mj">features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))<br/>, ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text'))<br/>, ('vect', CountVectorizer(max_df=0.5, min_df=1, ngram_range=(1,2)))]))]<br/>, n_jobs=-1)</span><span id="b5e7" class="me mf iq ma b gy mk mh l mi mj">pipeline = Pipeline([<br/>('features', features)<br/>, ('clf', LogisticRegression(C=1.0, penalty='l2'))<br/>])</span><span id="9b6c" class="me mf iq ma b gy mk mh l mi mj">best_model = pipeline.fit(df_model.drop('airline_sentiment', axis=1), df_model.airline_sentiment)</span><span id="123a" class="me mf iq ma b gy mk mh l mi mj"># Applying on new positive tweets<br/>new_positive_tweets = pd.Series(["Thank you @VirginAmerica for you amazing customer support team on Tuesday 11/28 at @EWRairport and returning my lost bag in less than 24h! #efficiencyiskey #virginamerica"<br/>,"Love flying with you guys ask these years. Sad that this will be the last trip 😂 @VirginAmerica #LuxuryTravel"<br/>,"Wow @VirginAmerica main cabin select is the way to fly!! This plane is nice and clean &amp; I have tons of legroom! Wahoo! NYC bound! ✈️"])</span><span id="96f2" class="me mf iq ma b gy mk mh l mi mj">df_counts_pos = tc.transform(new_positive_tweets)<br/>df_clean_pos = ct.transform(new_positive_tweets)<br/>df_model_pos = df_counts_pos<br/>df_model_pos['clean_text'] = df_clean_pos</span><span id="25c0" class="me mf iq ma b gy mk mh l mi mj">best_model.predict(df_model_pos).tolist()</span><span id="db14" class="me mf iq ma b gy mk mh l mi mj"># Applying on new negative tweets<br/>new_negative_tweets = pd.Series(["@VirginAmerica shocked my initially with the service, but then went on to shock me further with no response to what my complaint was. #unacceptable @Delta @richardbranson"<br/>,"@VirginAmerica this morning I was forced to repack a suitcase w a medical device because it was barely overweight - wasn't even given an option to pay extra. My spouses suitcase then burst at the seam with the added device and had to be taped shut. Awful experience so far!"<br/>,"Board airplane home. Computer issue. Get off plane, traverse airport to gate on opp side. Get on new plane hour later. Plane too heavy. 8 volunteers get off plane. Ohhh the adventure of travel ✈️ @VirginAmerica"])</span><span id="45ef" class="me mf iq ma b gy mk mh l mi mj">df_counts_neg = tc.transform(new_negative_tweets)<br/>df_clean_neg = ct.transform(new_negative_tweets)<br/>df_model_neg = df_counts_neg<br/>df_model_neg['clean_text'] = df_clean_neg</span><span id="b973" class="me mf iq ma b gy mk mh l mi mj">best_model.predict(df_model_neg).tolist()</span></pre><p id="fd13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型对所有推文进行了正确分类。应该使用更大的测试集来评估模型的性能。但是在这个小数据集上，它做了我们想要做的事情。</p><p id="f933" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望你喜欢读这个故事。如果你做到了，请鼓掌。</p></div></div>    
</body>
</html>