<html>
<head>
<title>Explaining Reinforcement Learning: Active vs Passive</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释强化学习:主动还是被动</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explaining-reinforcement-learning-active-vs-passive-a389f41e7195?source=collection_archive---------1-----------------------#2018-07-06">https://towardsdatascience.com/explaining-reinforcement-learning-active-vs-passive-a389f41e7195?source=collection_archive---------1-----------------------#2018-07-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/4cc6b9df9478fa82bcb0adb25c2adaf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PPxJp1Bta8UTt-r0"/></div></div></figure><p id="2a70" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这篇文章假设你熟悉强化学习(RL)和马尔可夫决策过程的基础知识，如果没有，请先参考<a class="ae kw" href="https://medium.com/@shweta_bhatt/reinforcement-learning-101-e24b50e1d292" rel="noopener">这篇</a>前一篇文章。</p><p id="eabc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇文章中，我们将探究，</p><ol class=""><li id="fecc" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka ir"> <em class="lg">解决一个 RL 问题需要哪些要素？</em> </strong></li><li id="3338" class="kx ky iq ka b kb lh kf li kj lj kn lk kr ll kv lc ld le lf bi translated">什么是被动和主动强化学习，我们如何比较这两者？T9】</li><li id="c5a8" class="kx ky iq ka b kb lh kf li kj lj kn lk kr ll kv lc ld le lf bi translated"><strong class="ka ir"> <em class="lg">常见的主动和被动 RL 技术有哪些，适用的场景有哪些？</em>T13】</strong></li></ol><h1 id="45e8" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">解决 RL 问题需要哪些要素？</h1><p id="34ab" class="pw-post-body-paragraph jy jz iq ka b kb mk kd ke kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv ij bi translated">让我们考虑一个问题，代理可以处于各种状态，并且可以从一组动作中选择一个动作。这类问题称为<strong class="ka ir"> <em class="lg">序贯决策问题</em> </strong>。一个<strong class="ka ir"/><a class="ae kw" href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="noopener ugc nofollow" target="_blank"><strong class="ka ir"/></a>的数学框架就是捕捉这样一个<strong class="ka ir">完全可观测的、非确定性的环境</strong>，具有<strong class="ka ir">马尔科夫转移模型和加性报酬</strong>的主体在其中行动。MDP 的解是一个<strong class="ka ir"> <em class="lg">最优策略</em> </strong>，它指的是<strong class="ka ir">最大化总累积报酬</strong>的每个状态的行动选择。因此，代表代理环境的<strong class="ka ir"> <em class="lg">转换模型</em> </strong>(当环境已知时)和决定代理在每个状态下需要执行什么动作的<strong class="ka ir"> <em class="lg">最优策略</em> </strong>是训练代理学习特定行为的必需元素。</p><figure class="mq mr ms mt gt jr gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/c52c672f8a534de9a5315b3a30dca9f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*ZeotbBXsHWTWBpWc"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk">Fig 1: Markov Decision Process (source: wikipedia)</figcaption></figure><h1 id="d816" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">什么是被动和主动强化学习，我们如何比较这两者？</h1><p id="d58a" class="pw-post-body-paragraph jy jz iq ka b kb mk kd ke kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv ij bi translated">主动强化学习和被动强化学习都是强化学习的类型。在被动 RL 的情况下，代理的策略是固定的，这意味着它被<strong class="ka ir"> <em class="lg">告知做什么</em> </strong>。与此相反，在主动 RL 中，代理<strong class="ka ir"> <em class="lg">需要决定做什么</em> </strong>，因为没有固定的策略可供其操作。因此，被动 RL 代理的目标是执行固定的策略(动作序列)并对其进行评估，而主动 RL 代理的目标是采取行动并学习最优策略。</p><h1 id="884e" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">有哪些常见的主动和被动 RL 技术？</h1><h1 id="b98c" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated"><strong class="ak">被动学习</strong></h1><p id="78b8" class="pw-post-body-paragraph jy jz iq ka b kb mk kd ke kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv ij bi translated">由于代理的目标是评估最优策略有多好，代理需要学习每个状态<em class="lg"> s </em>的期望效用<em class="lg"> Uπ(s) </em>。这可以通过三种方式实现。</p><h2 id="89b3" class="my ln iq bd lo mz na dn ls nb nc dp lw kj nd ne ma kn nf ng me kr nh ni mi nj bi translated"><strong class="ak"> <em class="nk">直接效用估算</em> </strong></h2><p id="9a82" class="pw-post-body-paragraph jy jz iq ka b kb mk kd ke kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv ij bi translated">在这个方法中，代理执行一个<strong class="ka ir">试验序列或运行</strong>(状态-动作转换序列，一直持续到代理到达终端状态)。每个试验给出一个样本值，代理根据样本值估计效用。可以计算为样本值的<strong class="ka ir">移动平均值。<em class="lg">主要缺点是这种方法错误地假设</em> <strong class="ka ir"> <em class="lg">状态效用是独立的</em> </strong> <em class="lg">，而实际上它们是</em> <a class="ae kw" href="https://en.wikipedia.org/wiki/Markov_property" rel="noopener ugc nofollow" target="_blank"> <strong class="ka ir"> <em class="lg">马尔科夫</em> </strong> </a> <em class="lg">。</em>还有，收敛慢。</strong></p><p id="cf12" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">假设我们有一个 4x3 的网格作为环境，代理可以在其中向左、向右、向上或向下移动(一组可用的操作)。跑步的例子</p><figure class="mq mr ms mt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nl"><img src="../Images/dd2b9e7103082d15b2875abf67c2d59d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Knr5PNn4RUxwoazJ1Hg1ZA.png"/></div></div></figure><p id="3d17" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">总奖励从<em class="lg"> (1，1) </em> = 0.72 开始</p><h2 id="125d" class="my ln iq bd lo mz na dn ls nb nc dp lw kj nd ne ma kn nf ng me kr nh ni mi nj bi translated"><strong class="ak"> <em class="nk"> 2。</em>自适应动态规划</strong></h2><p id="a4d7" class="pw-post-body-paragraph jy jz iq ka b kb mk kd ke kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv ij bi translated">ADP 是一种比直接效用估计更智能的方法，因为它通过将状态的效用估计为处于该状态的奖励和处于下一状态的预期折扣奖励之和来运行试验以学习环境模型。</p><figure class="mq mr ms mt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/9aedd2246e67eb53198104b2e9fe9b61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mDxTexPE5n--MD-Dnyjqvg.png"/></div></div></figure><p id="ad2f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<em class="lg"> R(s) </em> =处于状态的报酬<em class="lg"> s </em>，<em class="lg"> P(s'|s，π(s)) </em> =过渡模型，<em class="lg"> γ </em> =贴现因子，<em class="lg"> Uπ(s) </em> =处于状态的效用<em class="lg"> s' </em>。</p><p id="7b88" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">可以使用<strong class="ka ir">值迭代算法</strong>来解决。该算法收敛很快，但是对于大的状态空间来说，计算可能变得相当昂贵。ADP 是一种基于模型的方法，需要环境的转换模型。无模型方法是时间差异学习。</p><figure class="mq mr ms mt gt jr"><div class="bz fp l di"><div class="nn no l"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk">Fig 2: AI playing Super Mario using Deep RL</figcaption></figure><h2 id="8d7f" class="my ln iq bd lo mz na dn ls nb nc dp lw kj nd ne ma kn nf ng me kr nh ni mi nj bi translated"><strong class="ak"> <em class="nk"> 3。</em>时间差分学习(TD)</strong></h2><p id="9a45" class="pw-post-body-paragraph jy jz iq ka b kb mk kd ke kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv ij bi translated">TD 学习不要求代理学习过渡模型。更新发生在连续的状态之间，代理仅更新直接受影响的状态。</p><figure class="mq mr ms mt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi np"><img src="../Images/f6417dd21ab14a569fff1e503f5ebaee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_CvvfZcTIXzxOxyMAPb8lg.png"/></div></div></figure><p id="2a2a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<em class="lg"> α </em> =学习率，它决定了向真实效用的收敛。</p><p id="eeb7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"><em class="lg">ADP 用所有后继状态来调整 s 的效用，而 TD learning 用单个后继状态 s’的效用来调整。TD 的收敛速度较慢，但在计算方面要简单得多。</em></strong></p><h1 id="decd" class="lm ln iq bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated"><strong class="ak">主动学习</strong></h1><h2 id="5143" class="my ln iq bd lo mz na dn ls nb nc dp lw kj nd ne ma kn nf ng me kr nh ni mi nj bi translated"><strong class="ak"> <em class="nk">具有探测功能的 ADP</em></strong></h2><p id="0ec0" class="pw-post-body-paragraph jy jz iq ka b kb mk kd ke kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv ij bi translated">由于主动代理的目标是学习最优策略，代理需要学习每个状态的期望效用并更新其策略。可以使用被动 ADP 代理完成，然后使用值或策略迭代，它可以学习最佳操作。但是这种方法导致了一个贪婪的代理。<strong class="ka ir"> <em class="lg">因此，我们使用一种方法，对未探索的动作给予较高的权重，对具有较低效用的动作给予较低的权重。</em>T3】</strong></p><figure class="mq mr ms mt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nq"><img src="../Images/f028ce1f5c316b2264a4365577e136d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IvJzMBnf9KKvCO0gAS7tqw.png"/></div></div></figure><p id="233b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中<em class="lg"> f(u，n) </em>是随着期望值<em class="lg"> u </em>增加并且随着尝试次数<em class="lg"> n </em>减少的探索函数</p><figure class="mq mr ms mt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/44b41c67ebcbb4266981f8f4a8538795.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4RHFHfym9ny_HAHVW61hig.png"/></div></div></figure><p id="0de6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">R+ 是一个乐观的奖励，Ne 是我们希望代理在每个状态下被迫选择一个动作的次数。<strong class="ka ir"> <em class="lg">探索功能将被动代理转化为主动代理。</em> </strong></p><h2 id="1837" class="my ln iq bd lo mz na dn ls nb nc dp lw kj nd ne ma kn nf ng me kr nh ni mi nj bi translated"><strong class="ak">②<em class="nk">。问学</em> </strong></h2><p id="d646" class="pw-post-body-paragraph jy jz iq ka b kb mk kd ke kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv ij bi translated">Q-learning 是一种 TD 学习方法，它不需要代理学习过渡模型，而是学习 Q 值函数<em class="lg"> Q(s，a) </em>。</p><figure class="mq mr ms mt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/d57f9c40fedf018f4398ee0bf68214e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9lhkUcRMHdNtf3rW0imWWQ.png"/></div></div></figure><p id="d9bd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">q 值可以使用下面的等式来更新，</p><figure class="mq mr ms mt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/f420ccc1bd11eed4b7aa1d6d70ff5e45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*izTPOZuXYHs5c0dgW_qP3w.png"/></div></div></figure><p id="0650" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">可以使用以下策略选择下一个操作，</p><figure class="mq mr ms mt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/f44e99821737bdc9a7f1eec0ec84b209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tC9Z6QRF2-OT9kLt7-_byw.png"/></div></div></figure><p id="99f7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">同样，这比 ADP 计算简单，但速度较慢。</p><figure class="mq mr ms mt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/ac48a39692a024d6d7c347c617e584a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YUZM_D-C-UMp81PhBAwFiA.png"/></div></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk">Table 1: Comparison of active and passive RL methods</figcaption></figure><p id="0196" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我推荐以下资源来更深入地理解这些概念，</p><ol class=""><li id="8c28" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><a class="ae kw" href="http://incompleteideas.net/book/the-book-2nd.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ka ir">【强化学习——简介</strong></a><strong class="ka ir"/>——理查德·萨顿和安德鲁·巴尔托著</li><li id="0c31" class="kx ky iq ka b kb lh kf li kj lj kn lk kr ll kv lc ld le lf bi translated">人工智能:一种现代方法</li><li id="ca9b" class="kx ky iq ka b kb lh kf li kj lj kn lk kr ll kv lc ld le lf bi translated"><a class="ae kw" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ka ir">教材</strong> </a> —作者大卫·西尔弗</li></ol></div></div>    
</body>
</html>