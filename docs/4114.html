<html>
<head>
<title>Linear Regression from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-from-scratch-cd0dee067f72?source=collection_archive---------1-----------------------#2018-07-20">https://towardsdatascience.com/linear-regression-from-scratch-cd0dee067f72?source=collection_archive---------1-----------------------#2018-07-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="52a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数据科学正处于巅峰，使用机器学习模型你可以做很多事情，从预测股票价格到生成著名的蒙娜丽莎的假画(哦，这应该是一个秘密)。线性回归是最容易实现的机器学习算法之一，我们将在本文中探讨这种算法。</p><h2 id="9ae8" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">什么是线性回归？</h2><p id="4bfa" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">线性回归是一种用于定义因变量(<strong class="jp ir"> Y </strong>)和自变量(<strong class="jp ir"> X </strong>)之间关系的方法。简单地写为:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/790b20bcc62e775b82e04a5096ce6d05.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*nejpuvlh1MbMGQtve_1ztQ.png"/></div></figure><p id="2cbc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<strong class="jp ir"> y </strong>为因变量，<strong class="jp ir"> m </strong>为比例因子或系数，<strong class="jp ir"> b </strong>为偏差系数，<strong class="jp ir"> X </strong>为自变量。<a class="ae lr" href="https://stats.stackexchange.com/questions/13643/bias-an-intuitive-definition" rel="noopener ugc nofollow" target="_blank">偏差系数</a>为该模型提供了额外的自由度。目标是在<strong class="jp ir"> X </strong>和<strong class="jp ir"> Y </strong>之间画一条最佳拟合线，估计<strong class="jp ir"> X </strong>和<strong class="jp ir"> Y </strong>之间的关系。</p><p id="c5ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是我们如何找到这些系数，我们可以用不同的方法找到它们。一种是<strong class="jp ir">普通最小二乘法</strong>方法和<strong class="jp ir">梯度下降</strong>方法。我们将实现普通的最小均方方法。</p><h2 id="9774" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">普通最小均方误差</h2><p id="60ed" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">之前我们讨论了估算<strong class="jp ir"> X </strong>和<strong class="jp ir"> Y </strong>到一条直线的关系。例如，我们获得样本输入和输出，并在 2d 图上绘制这些分散点，类似于下图:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/6345bf255870513e03cbc2e11e077e62.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*2LEjQZEKKYAqKHt8niVp2Q.png"/></div></figure><p id="6a3a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图中看到的线是我们要完成的实际关系，我们希望最小化模型的误差。这条线是穿过大多数散点的最佳拟合线，也减少了误差，误差是从点到线本身的距离，如下图所示。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/3ec2d3be58be8358dfbee959b65cce6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/1*LxQraU40CdL9Qd9uSsq_xg.gif"/></div></figure><p id="5c79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">而线性模型的总误差是各点误差之和。即，</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/77fe59c78f65bdd5f49aed556cff8901.png" data-original-src="https://miro.medium.com/v2/resize:fit:154/format:webp/1*XTTjSJv7nogDk-Us9zWvRw.jpeg"/></div></figure><p id="5bbb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">r<em class="lv">I =</em>I 点与<strong class="jp ir"> i 点</strong>T34 点点之间的距离。</p><p id="85fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lv"> n = </em>总点数。</p><p id="6ee6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将每个距离平方，因为有些点会在线上，有些点会在线下。我们可以通过最小化<strong class="jp ir"> r </strong>来最小化线性模型的误差，因此我们有</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/8a034c80fe7222acf8482e91a381e31c.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*O2xXpYS7A3VlLXqT6SNMLg.png"/></div></figure><p id="0994" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<strong class="jp ir"> <em class="lv"> x </em> </strong>是输入变量<strong class="jp ir"> X </strong>的平均值，而<strong class="jp ir"> <em class="lv"> y </em> </strong>是输出变量<strong class="jp ir"> Y. </strong>的平均值</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/0ec7a9360d1ce99d20dd2e04d49a79c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*eDCn5xs-BdgSFcmaEHxH-g.jpeg"/></div></figure><p id="6006" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们用 python 实现这个方法(有趣的部分)。</p><p id="b942" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要继续下去，你需要 python 和你那令人敬畏的自我。使用<a class="ae lr" href="https://pypi.org/project/pip/" rel="noopener ugc nofollow" target="_blank"> pip </a>我们将安装以下依赖项</p><ul class=""><li id="c35f" class="ly lz iq jp b jq jr ju jv jy ma kc mb kg mc kk md me mf mg bi translated">numpy</li><li id="5da0" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">熊猫</li><li id="ed73" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">matplotlib</li></ul><p id="e76e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将使用包含不同人的头部大小和大脑重量的数据集。该数据集可在本次<a class="ae lr" href="https://github.com/FeezyHendrix/LinearRegressionfromscrath" rel="noopener ugc nofollow" target="_blank">回购</a>中获得。</p><p id="71da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们从导入数据集和依赖项开始</p><pre class="lk ll lm ln gt mm mn mo mp aw mq bi"><span id="ff0d" class="kl km iq mn b gy mr ms l mt mu">#import libraries<br/>%matplotlib inline<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span><span id="3107" class="kl km iq mn b gy mv ms l mt mu">dataset = pd.read_csv('dataset.csv')<br/>print(dataset.shape)<br/>dataset.head()</span><span id="775c" class="kl km iq mn b gy mv ms l mt mu">(237, 4)</span></pre><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/5de8b9973810ab22114331999ae29fa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*E8l-j0eR6N6IsgNhFxJf8w.png"/></div></figure><p id="2b87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们找出头部大小和大脑重量之间的关系。</p><pre class="lk ll lm ln gt mm mn mo mp aw mq bi"><span id="eede" class="kl km iq mn b gy mr ms l mt mu"># initializing our inputs and outputs<br/>X = dataset['Head Size(cm^3)'].values<br/>Y = dataset['Brain Weight(grams)'].values</span><span id="51a8" class="kl km iq mn b gy mv ms l mt mu"># mean of our inputs and outputs<br/>x_mean = np.mean(X)<br/>y_mean = np.mean(Y)</span><span id="7528" class="kl km iq mn b gy mv ms l mt mu">#total number of values<br/>n = len(X)</span><span id="9d13" class="kl km iq mn b gy mv ms l mt mu"># using the formula to calculate the b1 and b0<br/>numerator = 0<br/>denominator = 0<br/>for i in range(n):<br/>    numerator += (X[i] - x_mean) * (Y[i] - y_mean)<br/>    denominator += (X[i] - x_mean) ** 2<br/>    <br/>b1 = numerator / denominator<br/>b0 = y_mean - (b1 * x_mean)</span><span id="a76a" class="kl km iq mn b gy mv ms l mt mu">#printing the coefficient<br/>print(b1, b0)</span><span id="3271" class="kl km iq mn b gy mv ms l mt mu"># output : 0.26342933948939945 325.57342104944223</span></pre><p id="1ac4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们有了偏差系数(b)和比例因子(m)。用数学术语来说:</p><pre class="lk ll lm ln gt mm mn mo mp aw mq bi"><span id="1aba" class="kl km iq mn b gy mr ms l mt mu"><strong class="mn ir"><em class="lv">Brain weights =  325.57342104944223 + 0.26342933948939945 * Head size</em></strong></span></pre><p id="5fea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们有了一个线性模型。</p><p id="1117" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们用图表来描绘它。</p><pre class="lk ll lm ln gt mm mn mo mp aw mq bi"><span id="95d2" class="kl km iq mn b gy mr ms l mt mu">#plotting values <br/>x_max = np.max(X) + 100<br/>x_min = np.min(X) - 100</span><span id="7af7" class="kl km iq mn b gy mv ms l mt mu">#calculating line values of x and y<br/>x = np.linspace(x_min, x_max, 1000)<br/>y = b0 + b1 * x</span><span id="030f" class="kl km iq mn b gy mv ms l mt mu">#plotting line <br/>plt.plot(x, y, color='#00ff00', label='Linear Regression')</span><span id="c08d" class="kl km iq mn b gy mv ms l mt mu">#plot the data point<br/>plt.scatter(X, Y, color='#ff0000', label='Data Point')</span><span id="ff2e" class="kl km iq mn b gy mv ms l mt mu"># x-axis label<br/>plt.xlabel('Head Size (cm^3)')</span><span id="966e" class="kl km iq mn b gy mv ms l mt mu">#y-axis label<br/>plt.ylabel('Brain Weight (grams)')</span><span id="aeff" class="kl km iq mn b gy mv ms l mt mu">plt.legend()<br/>plt.show()</span></pre><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/b9f14e09923f22cc80a72b422510fd64.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*yJAWqORYMwVAtk_GKZo0Fg.png"/></div></figure><p id="77a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们需要能够衡量我们的模型有多好(准确性)。有许多方法可以实现这一点，但我们将实现<strong class="jp ir">均方根误差</strong>和<strong class="jp ir">决定系数</strong> ( <strong class="jp ir"> R 分数</strong>)。</p><p id="234b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">均方根误差是所有误差之和除以数值个数的平方根，或者从数学上讲，</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi my"><img src="../Images/be4b8319b378778fde46079977be7bdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/1*SGBsn7WytmYYbuTgDatIpw.gif"/></div></figure><p id="cc22" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里的<strong class="jp ir"><em class="lv"/></strong>是第<strong class="jp ir"> <em class="lv">与第</em> </strong>预测的输出值。现在我们会找到 RMSE。</p><pre class="lk ll lm ln gt mm mn mo mp aw mq bi"><span id="5a28" class="kl km iq mn b gy mr ms l mt mu">rmse = 0<br/>for i in range(n):<br/>    y_pred=  b0 + b1* X[i]<br/>    rmse += (Y[i] - y_pred) ** 2<br/>    <br/>rmse = np.sqrt(rmse/n)</span><span id="f35c" class="kl km iq mn b gy mv ms l mt mu">print(rmse)</span><span id="2beb" class="kl km iq mn b gy mv ms l mt mu">#output : 72.1206213783709</span></pre><p id="cb9e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们找到我们的 R 分数，以便能够从数学上衡量我们的线性模型的准确性:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mz"><img src="../Images/92c93be5423daefb7f986aacd0784ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/1*eebOVNcNlkwqM-fWjzmOLg.gif"/></div></div></figure><p id="49f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> SST </strong>是平方和的总和，<strong class="jp ir"> SSR </strong>是残差平方和的总和。</p><p id="8a66" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">R 的分数通常在 0 到 1 之间。如果模型完全错误，它也会变成负数。现在我们将找到<strong class="jp ir"> R </strong>的分数。</p><pre class="lk ll lm ln gt mm mn mo mp aw mq bi"><span id="b399" class="kl km iq mn b gy mr ms l mt mu">sumofsquares = 0<br/>sumofresiduals = 0</span><span id="2e35" class="kl km iq mn b gy mv ms l mt mu">for i in range(n) :<br/>    y_pred = b0 + b1 * X[i]<br/>    sumofsquares += (Y[i] - y_mean) ** 2<br/>    sumofresiduals += (Y[i] - y_pred) **2<br/>    <br/>score  = 1 - (sumofresiduals/sumofsquares)</span><span id="8f0b" class="kl km iq mn b gy mv ms l mt mu">print(score)</span><span id="b200" class="kl km iq mn b gy mv ms l mt mu">#output : 0.6393117199570003</span></pre><p id="d9c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">0.63 当然不差，但我们可以通过以下方式提高分数:</p><ul class=""><li id="6277" class="ly lz iq jp b jq jr ju jv jy ma kc mb kg mc kk md me mf mg bi translated">获取更多数据集</li><li id="98f4" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">改进功能</li><li id="f413" class="ly lz iq jp b jq mh ju mi jy mj kc mk kg ml kk md me mf mg bi translated">适合多种型号等</li></ul><h2 id="bdba" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">结论</h2><p id="8a93" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">线性回归是所有机器学习算法的基础，也是最容易获得的，我们已经实现了<strong class="jp ir">普通最小均方</strong>方法来根据头部大小预测大脑重量，并且还使用<strong class="jp ir">均方根误差</strong>和<strong class="jp ir">决定系数</strong> ( <strong class="jp ir"> R 分数</strong>)来测量准确度。代码可以在这个<a class="ae lr" href="https://github.com/FeezyHendrix/LinearRegressionfromscrath" rel="noopener ugc nofollow" target="_blank">回购</a>上找到。</p><p id="f068" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">信用:<a class="ae lr" href="https://mubaris.com/posts/linear-regression/" rel="noopener ugc nofollow" target="_blank">https://mubaris.com/posts/linear-regression/</a></p></div></div>    
</body>
</html>