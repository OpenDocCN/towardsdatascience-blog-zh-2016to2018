<html>
<head>
<title>Policy Networks vs Value Networks in Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习中的政策网络与价值网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/policy-networks-vs-value-networks-in-reinforcement-learning-da2776056ad2?source=collection_archive---------1-----------------------#2018-08-05">https://towardsdatascience.com/policy-networks-vs-value-networks-in-reinforcement-learning-da2776056ad2?source=collection_archive---------1-----------------------#2018-08-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="8e30" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在强化学习中，代理在他们的环境中采取随机决策，并学习从许多决策中选择正确的决策来实现他们的目标，并以超人的水平进行游戏。策略和价值网络在像<a class="ae kl" rel="noopener" target="_blank" href="/monte-carlo-tree-search-158a9">蒙特卡罗树搜索</a>这样的算法中一起使用，以执行强化学习。这两个网络都是一种叫做 MCTS 探索算法的方法的组成部分。</p><p id="a0e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它们也被称为策略迭代&amp;值迭代，因为它们被计算多次，使得它成为一个迭代过程。</p><p id="d288" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">我们来了解一下为什么它们在机器学习中如此重要，它们之间有什么区别？</strong></p><h1 id="82b9" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">什么是政策网络？</h1><p id="7c18" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">考虑世界上的任何游戏，输入🎮用户给予游戏的动作称为<strong class="jp ir">动作</strong>T0】。每一个输入(动作)都会导致不同的输出。这些输出被称为游戏的<strong class="jp ir">状态</strong> <code class="fe lp lq lr ls b"><strong class="jp ir">s</strong></code>。</p><p id="2621" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由此，我们可以制作不同的状态-动作对<code class="fe lp lq lr ls b"><strong class="jp ir">S</strong> = <strong class="jp ir">{(</strong>s0,a0<strong class="jp ir">)</strong>,s1,a1<strong class="jp ir">)</strong>,...,<strong class="jp ir">(</strong>sN,aN<strong class="jp ir">)}</strong></code>，表示哪个动作<code class="fe lp lq lr ls b">aN</code>导致哪个状态<code class="fe lp lq lr ls b">sN.</code>同样，我们可以说<strong class="jp ir"> S </strong>包含策略网络学习到的所有策略。</p><blockquote class="lt"><p id="428b" class="lu lv iq bd lw lx ly lz ma mb mc kk dk translated">通过给博弈一个特定的输入来学习给出一个确定的输出的网络被称为策略网络</p></blockquote><figure class="me mf mg mh mi mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi md"><img src="../Images/d1319b4e568c6ebd74393c4eee028475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*sDjnmi8Y8BrfE9jfmj13Tg.gif"/></div></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk"><strong class="bd mu">Policy Network (</strong>action1️, state1<strong class="bd mu">)</strong> , <strong class="bd mu">(</strong>action2, state2<strong class="bd mu">)</strong></figcaption></figure><p id="8764" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">比如:输入<code class="fe lp lq lr ls b">a1</code>给出一个状态<code class="fe lp lq lr ls b">s1</code>(上移)&amp;输入<code class="fe lp lq lr ls b">a2</code>给出一个状态<code class="fe lp lq lr ls b">s2</code>(下移)在游戏中。</p><p id="f176" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，一些行动增加了玩家的点数，导致奖励<strong class="jp ir"> r </strong>。</p><figure class="mw mx my mz gt mj gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/6f77392f769df273a439f89090a16409.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*iwpyXFpny5Z0imRSgib3uw.gif"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk"><strong class="bd mu">States Getting Rewards</strong></figcaption></figure><p id="44a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们来看看一些明显的符号:</p><figure class="mw mx my mz gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi na"><img src="../Images/c1ce4df54be252fc4a7371ccf5a00f28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ldagjoG8r7l6zIOK6d33qg.jpeg"/></div></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk"><strong class="bd mu">Usual Notations for RL environments</strong></figcaption></figure><figure class="mw mx my mz gt mj gh gi paragraph-image"><div class="ab gu cl nb"><img src="../Images/f52a3f5984d4aeb4e0b9bc0f5df666be.png" data-original-src="https://miro.medium.com/v2/format:webp/1*jnQX132sPVSnMzxhqZ3UIg.jpeg"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk"><strong class="bd mu">Optimal Policy</strong></figcaption></figure><p id="ba6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">为什么我们要使用贴现因子<em class="nc">γ</em>T24】</strong></p><blockquote class="nd ne nf"><p id="e1ac" class="jn jo nc jp b jq jr js jt ju jv jw jx ng jz ka kb nh kd ke kf ni kh ki kj kk ij bi translated">它被用作一种预防措施(通常保持在 1 以下)。它阻止报酬达到无限。</p></blockquote><p id="723f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对一项政策的无限奖励会压垮我们的代理人&amp;偏向于特定的行动，扼杀探索未知领域和游戏行动的欲望😵。</p><figure class="mw mx my mz gt mj gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/e64355a24aec3734742bd117d305b88b.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*UcVjEFeIStkbR0DBtZYvrw.png"/></div></figure><p id="2de4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是我们怎么知道你下一步该选择哪个州，最终进入最后一轮呢？</p><figure class="mw mx my mz gt mj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/5e29d1575ceefa6dd9a378b1849ca8ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*TsMgk14NLG9nNEStALqLvA.png"/></div></figure><h1 id="d1f4" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">什么是价值网络？</h1><blockquote class="nd ne nf"><p id="f464" class="jn jo nc jp b jq jr js jt ju jv jw jx ng jz ka kb nh kd ke kf ni kh ki kj kk ij bi translated">价值网络通过计算当前状态<code class="fe lp lq lr ls b"><strong class="jp ir">s</strong></code>的预期累积分数来为游戏状态分配价值/分数。每个州都要经过价值网络。获得更多奖励的州显然在网络中获得更多价值。</p></blockquote><p id="6967" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请记住，奖励是<strong class="jp ir">预期奖励，</strong>因为我们从一组状态中选择了正确的一个。</p><figure class="mw mx my mz gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi nl"><img src="../Images/e63ea5e59cacb9f54e64951d7b39c195.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*pj4AJIVs4ULpHeC2NU9rkQ.jpeg"/></div></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">Value Function</figcaption></figure></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><p id="5be2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，关键目标总是最大化回报<em class="nc">(又名马尔可夫决策过程)</em>。导致良好状态的行为显然比其他行为获得更大的回报。</p><p id="681f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为任何游戏都是通过一个接一个的动作赢得的。博弈的最优策略<code class="fe lp lq lr ls b"><strong class="jp ir"><em class="nc">π*</em></strong></code> <strong class="jp ir"> <em class="nc"> </em> </strong>由许多有助于赢得博弈的状态-行动对组成。</p><p id="89f3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">获得最大回报国家-行动对被认为是最优政策。</p><p id="66ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用<em class="nc"> arg max </em>将最优策略的等式正式写成:</p><figure class="mw mx my mz gt mj gh gi paragraph-image"><div class="ab gu cl nb"><img src="../Images/f52a3f5984d4aeb4e0b9bc0f5df666be.png" data-original-src="https://miro.medium.com/v2/format:webp/1*jnQX132sPVSnMzxhqZ3UIg.jpeg"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk"><strong class="bd mu">Optimal Policy </strong><code class="fe lp lq lr ls b"><strong class="bd mu"><em class="nt">π*</em></strong></code></figcaption></figure><p id="f563" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，最优策略告诉我们应该采取哪些行动来最大化累积折扣回报。</p><blockquote class="lt"><p id="7aca" class="lu lv iq bd lw lx ly lz ma mb mc kk dk translated">策略网络学习到的最优策略知道在当前状态下应该执行哪些动作以获得最大回报。</p></blockquote><figure class="me mf mg mh mi mj gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/aae3739105858c727e6ff76c7a3243fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/1*5DtEcGD0_p7xVEk8302-yg.gif"/></div></figure><p id="02e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您有任何疑问、疑问或需求，请在下面评论或发微博给我。</p><p id="78eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">鼓掌吧…分享一下！在<a class="ae kl" href="https://medium.com/@sagarsharma4244" rel="noopener"> <strong class="jp ir">中</strong> </a>关注我</strong>获取类似的好玩内容。</p><p id="fc31" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要获得即时通知，请在<a class="ae kl" href="https://twitter.com/SagarSharma4244" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> Twitter </strong> </a>上关注我。</p><figure class="mw mx my mz gt mj gh gi paragraph-image"><a href="https://twitter.com/SagarSharma4244"><div class="gh gi nv"><img src="../Images/ce2f13e1aad357cb162c5550d2fd4868.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*YnbtD8IipCsqVjNwkjtY8w.png"/></div></a></figure><figure class="mw mx my mz gt mj gh gi paragraph-image"><a href="https://medium.com/@sagarsharma4244"><div class="gh gi nv"><img src="../Images/ca235e42e4e86914843b9fd55288374d.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*dBbsLTjkdMA45vnfBMbkGQ.png"/></div></a></figure><p id="a3f3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">乐意帮忙。值得称赞。</p><h1 id="4fb7" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">你会喜欢的以前的故事:</h1></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><div class="mw mx my mz gt nw"><a rel="noopener follow" target="_blank" href="/monte-carlo-tree-search-158a917a8baa"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd ir gy z fp ob fr fs oc fu fw ip bi translated">蒙特卡罗树搜索</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">每个数据科学爱好者的 MCTS</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok mo nw"/></div></div></a></div><div class="ol om gp gr on nw"><a rel="noopener follow" target="_blank" href="/tensorflow-image-recognition-python-api-e35f7d412a70"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd ir gy z fp ob fr fs oc fu fw ip bi translated">TensorFlow 图像识别 Python API 教程</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">在带有 Inception-v3 的 CPU 上(以秒为单位)</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="oo l oh oi oj of ok mo nw"/></div></div></a></div><div class="ol om gp gr on nw"><a rel="noopener follow" target="_blank" href="/activation-functions-neural-networks-1cbd9f8d91d6"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd ir gy z fp ob fr fs oc fu fw ip bi translated">激活函数:神经网络</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">Sigmoid，tanh，Softmax，ReLU，Leaky ReLU 解释！！！</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="op l oh oi oj of ok mo nw"/></div></div></a></div><div class="ol om gp gr on nw"><a rel="noopener follow" target="_blank" href="/deepminds-playing-capture-the-flag-with-deep-reinforcement-learning-a9f71256442e"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd ir gy z fp ob fr fs oc fu fw ip bi translated">DeepMind 的游戏《用深度强化学习捕捉旗帜》</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">#4 研究论文解释</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="oq l oh oi oj of ok mo nw"/></div></div></a></div></div></div>    
</body>
</html>