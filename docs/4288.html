<html>
<head>
<title>Sing &amp; Song with text mining</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用文本挖掘唱歌</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sing-song-with-text-mining-cd8508ab51a5?source=collection_archive---------8-----------------------#2018-08-04">https://towardsdatascience.com/sing-song-with-text-mining-cd8508ab51a5?source=collection_archive---------8-----------------------#2018-08-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/86a47468ce0959dad86c161ddd6b3ad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nHvyizYWl_AZlRhAyfZkFg.png"/></div></div></figure><p id="9019" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">文本分析应用于许多领域，目标是发现隐藏在文本中的相关信息。自然语言处理(NLP)是一种用于文本挖掘的方法。它试图通过标记化、聚类、提取关系和使用算法识别主题来解码书面语言中的歧义。自然语言处理是人工智能的一个分支，代表着人类和计算机之间的桥梁；它可以广义地定义为通过软件对自然语言(如语音和文本)的自动操作。对于这份工作，我使用了这些参考资料:<br/><a class="ae kw" href="https://www.kaggle.com/devisangeetha/sing-a-song-lyrics-is-here/" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/devisangeetha/sing-a-song-lyrics-is-here/</a><br/><a class="ae kw" href="https://www.datacamp.com/community/tutorials/R-nlp-machine-learning" rel="noopener ugc nofollow" target="_blank">https://www . data camp . com/community/tutorials/R-NLP-machine-learning</a><br/><a class="ae kw" href="https://www.kaggle.com/rtatman/nlp-in-r-topic-modelling" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/rtatman/nlp-in-r-topic-modelling</a><br/><a class="ae kw" href="https://www.tidytextmining.com/" rel="noopener ugc nofollow" target="_blank">https://www.tidytextmining.com/</a></p><h2 id="95ff" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">探索性数据分析</h2><p id="499f" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated"><strong class="ka ir"> <em class="lv">每位艺人的歌曲数</em> </strong></p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="d077" class="kx ky iq mb b gy mf mg l mh mi">song_grp&lt;-lyrics %&gt;%group_by(artist)%&gt;%summarise(song_cnt=unique(length(song)))%&gt;%arrange(desc(song_cnt))<br/>song_grp[1:10,] %&gt;%<br/>  ungroup(artist, song_cnt) %&gt;%<br/>  mutate(song_cnt = color_bar("lightblue")(song_cnt)) %&gt;%<br/>  mutate(artist = color_tile("green","green")(artist)) %&gt;%<br/>  kable("html", escape = FALSE, align = "c", caption = "Artist With Highest song Count") %&gt;%<br/>  kable_styling(bootstrap_options = <br/>                  c("striped", "condensed", "bordered"), <br/>                full_width = FALSE)</span></pre><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/0f55d52509ab4e9ff31356353af5ced5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UmRbxbMIrFLd7nUzFESdBQ.png"/></div></div></figure><h2 id="4f6d" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">标记化</h2><p id="1e54" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">已加载 tidytext 库使用 unnest_tokens()函数开始标记化。这个函数至少需要两个参数:输出列名(即 word)和输入列名(即 text)。获取歌词数据集并将其传输到 unnest_tokens()中，然后删除停用词。它们是过于常见的词，可能不会给我们的结果增加任何意义。有不同的列表可供选择，但在这里我使用了 tidytext 包中名为 stop_words 的词典。将歌词标记成单词后，我使用 anti_join()删除了停用的单词，然后使用 distinct()删除了任何重复的记录。最后，我删除了所有少于四个字符的单词，因为在歌词中这些通常是感叹词。</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="326a" class="kx ky iq mb b gy mf mg l mh mi">lyrics_filtered &lt;- lyrics %&gt;%<br/>  unnest_tokens(word, text) %&gt;%<br/>  anti_join(stop_words) %&gt;%<br/>  distinct() %&gt;%<br/>  filter(nchar(word) &gt; 3)<br/>head(lyrics_filtered)</span><span id="ac82" class="kx ky iq mb b gy mk mg l mh mi">##   artist                  song      word<br/>## 1   ABBA Ahe's My Kind Of Girl wonderful<br/>## 2   ABBA Ahe's My Kind Of Girl     means<br/>## 3   ABBA Ahe's My Kind Of Girl   special<br/>## 4   ABBA Ahe's My Kind Of Girl    smiles<br/>## 5   ABBA Ahe's My Kind Of Girl     lucky<br/>## 6   ABBA Ahe's My Kind Of Girl    fellow</span></pre><h2 id="426e" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">字频率</h2><p id="6989" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">在音乐中，无论是重复还是罕见，单个词频都有很大的重要性。两者都会影响整首歌本身的可记忆性。歌曲作者可能想知道的一个问题是词频和热门歌曲之间是否有关联。所以我总结了每首歌的字数，为了简单评估，我还总结了所有数据集中最常用的词。</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="c8b1" class="kx ky iq mb b gy mf mg l mh mi">full_word_count &lt;- lyrics %&gt;%<br/>  unnest_tokens(word, text) %&gt;%<br/>  group_by(song) %&gt;%<br/>  summarise(num_words = n()) %&gt;%<br/>  arrange(desc(num_words)) </span><span id="73cb" class="kx ky iq mb b gy mk mg l mh mi">full_word_count[1:10,] %&gt;%<br/>  ungroup(num_words, song) %&gt;%<br/>  mutate(num_words = color_bar("lightblue")(num_words)) %&gt;%<br/>  mutate(song = color_tile("green","green")(song)) %&gt;%<br/>  kable("html", escape = FALSE, align = "c", caption = "Songs With Highest Word Count") %&gt;%<br/>  kable_styling(bootstrap_options = <br/>                  c("striped", "condensed", "bordered"), <br/>                full_width = FALSE)</span></pre><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/15c7768630ef49aa943ba3e94e006642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Nk68Rj6IOrgZ0d-SmPhQQ.png"/></div></div></figure><p id="9f54" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="lv">每首歌字数最多的艺人</em> </strong></p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="09c9" class="kx ky iq mb b gy mf mg l mh mi">lyrics_filtered %&gt;% <br/>  filter(word == "angel") %&gt;%<br/>  select(word, song, artist) %&gt;%<br/>  arrange() %&gt;%<br/>  top_n(10,song) %&gt;%<br/>  mutate(song = color_tile("lightblue","lightblue")(song)) %&gt;%<br/>  mutate(word = color_tile("green","green")(word)) %&gt;%<br/>  kable("html", escape = FALSE, align = "c", caption = "angel word per song and artist") %&gt;%<br/>  kable_styling(bootstrap_options = <br/>                  c("striped", "condensed", "bordered"), <br/>                full_width = FALSE)</span></pre><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/9b0c2960af925d3e128456b45e41b611.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rjzOAWbqZpwsWCiuCbbz1w.png"/></div></div></figure><p id="2fd2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="lv">顶字</em> </strong></p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="2cc4" class="kx ky iq mb b gy mf mg l mh mi">lyrics_filtered %&gt;%<br/>  count(word, sort = TRUE) %&gt;%<br/>  top_n(10) %&gt;%<br/>  ungroup() %&gt;%<br/>  mutate(word = reorder(word, n)) %&gt;%<br/>  ggplot() +<br/>  geom_col(aes(word, n), fill = "blue") +<br/>  theme(legend.position = "none", <br/>        plot.title = element_text(hjust = 0.5),<br/>        panel.grid.major = element_blank()) +<br/>  xlab("") + <br/>  ylab("Song Count") +<br/>  ggtitle("Most Frequently Used Words in Lyrics") +<br/>  coord_flip()</span></pre><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/8a371e95ab42930d73373a4ae2b998b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nHJbBpGWFr6m6KcfC8EwIQ.png"/></div></div></figure><p id="dab7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="lv">文字云</em> </strong></p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/93b8c677b9b40d89d8311fac354facf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pYppfG6S2w5Gk7MkGEffUQ.png"/></div></div></figure><h2 id="b5b8" class="kx ky iq bd kz la lb dn lc ld le dp lf kj lg lh li kn lj lk ll kr lm ln lo lp bi translated">相关性:找出艺术家“U2”创作的歌曲中单词之间的相关性</h2><p id="6c8d" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">对于变量之间的比较或按行分组，整齐的数据是一种有用的结构，但在行之间进行比较可能具有挑战性。大多数寻找成对计数或相关性的操作需要首先将数据转换成宽矩阵。widyr 包使得诸如计算计数和相关性之类的操作变得简单。一个有用的函数是 pairwise_count()函数。前缀 pairwise_ 意味着它将为单词变量中的每一对单词生成一行。输入中每对文档和单词对应一行，而输出中每对单词对应一行。这也是一个整齐的格式。目标是检查单词之间的相关性，这表明它们一起出现的频率相对于它们单独出现的频率。重点是 phi 系数，这是衡量二进制相关性的常用指标。这相当于皮尔逊关联，单词 X 和 Y 要么都出现，要么都不出现，而不是一个出现另一个。widyr 中的 pairwise_cor()函数让我们可以根据单词在同一部分出现的频率来找到单词之间的 phi 系数。它的语法类似于 pairwise_count()。它使用 ggraph 来可视化二元模型:可视化 widyr 包找到的词的相关性和簇。</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="5683" class="kx ky iq mb b gy mf mg l mh mi">set.seed(12345)<br/>word_corr %&gt;%<br/>  filter(correlation &gt; .75) %&gt;%<br/>  graph_from_data_frame() %&gt;%<br/>  ggraph(layout = "kk") +<br/>  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +<br/>  geom_node_point(color = "blue", size = 5) +<br/>  geom_node_text(aes(label = name), repel = TRUE) +<br/>  theme_void()</span></pre><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/065aeaedbdf6171a55a680c0c8ecb44e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HXNKZNt7i5R1HQwdjxSIRw.png"/></div></div></figure><h1 id="aa48" class="mm ky iq bd kz mn mo mp lc mq mr ms lf mt mu mv li mw mx my ll mz na nb lo nc bi translated">基于 LDA 的无监督学习</h1><p id="9474" class="pw-post-body-paragraph jy jz iq ka b kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr lu kt ku kv ij bi translated">主题建模是一种对这种文档进行无监督分类的方法，类似于对数字数据进行聚类，这种方法可以找到自然的项目组。潜在狄利克雷分配(LDA)是一种特别流行的拟合主题模型的方法。它将每个文档视为主题的混合，并将每个主题视为单词的混合。潜在狄利克雷分配是主题建模最常用的算法之一。每个文档都是主题的混合体。我们设想每个文档可能包含来自几个主题的特定比例的单词。每个话题都是单词的混合。LDA 是一种数学方法，用于同时估计这两者:找到与每个主题相关联的词的混合，同时还确定描述每个文档的主题的混合。我使用了 topicmodels 包中的 LDA()函数，设置 k = 5，创建了一个五主题 LDA 模型。该函数返回一个包含模型拟合的全部细节的对象，例如单词如何与主题相关联，以及主题如何与文档相关联。</p><p id="2277" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="lv">对数据集进行子采样</em> </strong></p><p id="ce8e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">数据集很大，处理整个数据集需要更多的计算时间，所以最好用子样本来处理它。</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="5bf1" class="kx ky iq mb b gy mf mg l mh mi">set.seed(1234) <br/>row_indexes &lt;- sample(1:nrow(lyrics), 1600, replace = F) <br/>texts_subsample &lt;-slice(lyrics, row_indexes)</span></pre><p id="838b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="lv">创建文档术语矩阵进行清洗，将术语矩阵转换成 tidytext 语料库，去除停用词并应用 LDA </em> </strong></p><p id="3d9b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在第二种方法中，我以前使用过 tm 包来应用 LDA。</p><p id="476d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">开始创建文档术语矩阵:以文档为行，术语为列，以词的频率计数为矩阵单元的矩阵。使用 tidyr 包中的 tidy 函数，我可以将文档术语矩阵转换为 tidytext 语料库，变量在列中，每个观察值在一行中，频率词在单元格中。通过这种方式，我已经准备好清除文本中的停用词，并应用 LDA 建模。</p><p id="a5f8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">LDA 输出能够给出:</p><ol class=""><li id="589d" class="nd ne iq ka b kb kc kf kg kj nf kn ng kr nh kv ni nj nk nl bi translated">估计每个主题对每个文档的贡献</li><li id="9ac2" class="nd ne iq ka b kb nm kf nn kj no kn np kr nq kv ni nj nk nl bi translated">估计每个单词对每个主题的贡献</li></ol><p id="53ec" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第二个对于大文档很有帮助。</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="747d" class="kx ky iq mb b gy mf mg l mh mi">viewsCorpus &lt;- Corpus(VectorSource(texts_subsample$text)) <br/>viewsDTM &lt;- DocumentTermMatrix(viewsCorpus)<br/>viewsDTM_tidy &lt;- tidy(viewsDTM)<br/>viewsDTM_tidy_cleaned &lt;- viewsDTM_tidy %&gt;% <br/>  anti_join(stop_words, by = c("term" = "word")) %&gt;%<br/>  filter(nchar(term) &gt; 3)<br/>top_terms_by_topic_LDA &lt;- <strong class="mb ir">function</strong>(input_text, <br/>                                   plot = T, <br/>                                   number_of_topics = 5) <br/>{    <br/>  Corpus &lt;- Corpus(VectorSource(input_text))<br/>  DTM &lt;- DocumentTermMatrix(Corpus) <br/>  unique_indexes &lt;- unique(DTM$i) <br/>  DTM &lt;- DTM[unique_indexes,] <br/>  lda &lt;- LDA(DTM, k = number_of_topics, control = list(seed = 1234))<br/>  topics &lt;- tidy(lda, matrix = "beta") <br/>  top_terms &lt;- topics  %&gt;% <br/>    group_by(topic) %&gt;% <br/>    top_n(10, beta) %&gt;%<br/>    ungroup() %&gt;% <br/>    arrange(topic, -beta) <br/>  <strong class="mb ir">if</strong>(plot == T){<br/>    top_terms %&gt;% <br/>      mutate(term = reorder(term, beta)) %&gt;% <br/>      ggplot(aes(term, beta, fill = factor(topic))) + <br/>      geom_col(show.legend = FALSE) + <br/>      facet_wrap(~ topic, scales = "free") + <br/>      labs(x = NULL, y = "Beta") + <br/>      coord_flip() <br/>  }<strong class="mb ir">else</strong>{ <br/>    <strong class="mb ir">return</strong>(top_terms)<br/>  }<br/>}<br/>top_terms_by_topic_LDA(viewsDTM_tidy_cleaned$term, number_of_topics = 5)</span></pre><p id="24f2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我选择了五个主题来列出前十个单词，可能两个就足够了，因为它们非常相似。</p><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/0646a1b1584bc5106eca65c3a10eee9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sUrAuKl48YiUelHvv64JIw.png"/></div></div></figure><p id="dd4b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="lv">词干</em> </strong></p><p id="46c6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">词干是用来去掉单词中所有的词尾变化，例如，“<em class="lv">钓鱼</em>、“<em class="lv">钓</em>、“<em class="lv">渔夫</em>”可以简化为“<em class="lv">鱼</em>”</p><pre class="lw lx ly lz gt ma mb mc md aw me bi"><span id="ead2" class="kx ky iq mb b gy mf mg l mh mi">viewsDTM_tidy_cleaned &lt;- viewsDTM_tidy_cleaned %&gt;% <br/>  mutate(stem = wordStem(term))<br/>top_terms_by_topic_LDA(viewsDTM_tidy_cleaned$stem, number_of_topics=5)</span></pre><figure class="lw lx ly lz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/9ff91800274c845ecf25dffc18ee6372.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4X-fGyBV9DhWLBitHBa82A.png"/></div></div></figure></div><div class="ab cl nr ns hu nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="ij ik il im in"><p id="50f8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="lv">仓库代码在</em><a class="ae kw" href="http://rpubs.com/claudio75/391440" rel="noopener ugc nofollow" target="_blank"><em class="lv">rpubs.com</em></a></p></div></div>    
</body>
</html>