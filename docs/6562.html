<html>
<head>
<title>Support Vector Machine: Kernel Trick; Mercer’s Theorem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机:核技巧；默瑟定理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d?source=collection_archive---------2-----------------------#2018-12-19">https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d?source=collection_archive---------2-----------------------#2018-12-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="efa6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解 SVM 系列:第二部分</h2></div><p id="2573" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">先决条件:1。关于支持向量机算法的知识，我在<a class="ae le" rel="noopener" target="_blank" href="/understanding-support-vector-machine-part-1-lagrange-multipliers-5c24a52ffc5e">的上一篇文章</a>中已经讨论过了。2.代数的一些基础知识。</p><p id="210e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本系列的第一部分中，从支持向量的数学公式中，我们发现了 SVM 的两个重要概念，它们是</p><ul class=""><li id="1802" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld lk ll lm ln bi translated">SVM 问题是一个约束最小化问题，我们已经学会了使用拉格朗日乘数法来处理这个问题。</li><li id="0915" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld lk ll lm ln bi translated">为了找到不同样本之间的最宽路径，我们只需要考虑支持向量和样本的点积。</li></ul><p id="b6f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在 SVM 的上一篇文章中，我举了一个简单的线性可分样本的例子来演示支持向量分类器。如果我们有一个如下的样本集，会发生什么？</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/b08cfc0d51e00e7e7b74b78aeb6224f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wp8tGecatxHqUgHNaVQddg.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Blue and Red samples all over the place !!!! At least it seems like (Source: Author)</figcaption></figure><p id="2915" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该图使用<code class="fe mj mk ml mm b">sklearn.</code>的内置<code class="fe mj mk ml mm b">make_circles</code> <a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html#sklearn-datasets-make-circles" rel="noopener ugc nofollow" target="_blank">数据集</a>生成</p><pre class="lu lv lw lx gt mn mm mo mp aw mq bi"><span id="ecb9" class="mr ms it mm b gy mt mu l mv mw">import numpy as np <br/>import sklearn <br/>import matplotlib.pyplot as plt<br/>from sklearn.datasets.samples_generator import make_circles</span><span id="d18c" class="mr ms it mm b gy mx mu l mv mw">X,y = make_circles(90, factor=0.2, noise=0.1) <br/>#noise = standard deviation of Gaussian noise added in data. <br/>#factor = scale factor between the two circles</span><span id="bbd3" class="mr ms it mm b gy mx mu l mv mw">plt.scatter(X[:,0],X[:,1], c=y, s=50, cmap='seismic')<br/>plt.show()</span></pre><p id="d0b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如你所知，在这个 2d 图中不可能画出一条线来区分蓝色样品和红色样品。我们还有可能应用 SVM 算法吗？</p><p id="fb2a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们给这个 2d 空间一点震动，红色样本飞离平面，看起来与蓝色样本分离，会怎么样？看一看！</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi my"><img src="../Images/a977c0bc64d486936a02daefd2451ce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*XhXJldwvZ9IpGNts41Mefw.gif"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">After ‘shaking’ the data samples now it seems there is a great possibility of classification! Find the code <a class="ae le" href="https://github.com/suvoooo/Machine_Learning/blob/master/SVMdemo.py" rel="noopener ugc nofollow" target="_blank">in my github</a> (Source: Author)</figcaption></figure><p id="775b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看来摇动数据样本确实有效，现在我们可以很容易地画一个平面(而不是我们以前使用的线)来分类这些样本。那么在摇晃过程中到底发生了什么呢？</p><p id="4f72" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们没有如上例的线性可分训练数据集时(在现实生活中，大多数数据集都相当复杂)，内核技巧就派上用场了。其思想是将非线性可分离数据集映射到一个更高维的空间，在那里我们可以找到一个可以分离样本的超平面。</p><p id="e260" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mz">因此，这一切都是为了找到将 2D 输入空间转换为 3D 输出空间的映射函数。或者是？内核技巧到底是什么？</em></p><p id="e70d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上一篇关于<a class="ae le" rel="noopener" target="_blank" href="/understanding-support-vector-machine-part-1-lagrange-multipliers-5c24a52ffc5e">支持向量的帖子中，</a>我们已经看到(请查看理解数学公式)最大化只取决于支持向量的点积，</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi na"><img src="../Images/bde75ed20a1a0e8cac6154c409178688.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vy4RBK-70-4PeEKvM_w3bQ.png"/></div></div></figure><p id="3c52" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不仅如此，由于决策规则还依赖于支持向量和新样本的点积</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/fe45fde8e9941dc14f5adffa3cca6216.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*z3Dse6PlT0EZ5uMl8nu3fg.png"/></div></figure><p id="c604" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这意味着，如果我们使用映射函数将数据映射到更高维度的空间，那么，最大化和决策规则将取决于不同样本的映射函数的点积，如下所示</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nc"><img src="../Images/759e72975af216008ff7dd23b169aef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fu34Ex4Y7in3rVHBwbE_gw.png"/></div></div></figure><p id="c6e2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">瞧啊。！如果我们有一个函数<em class="mz"> K </em>定义如下</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nd"><img src="../Images/7f4de6f96a0f8355b7f92bf30e428700.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vZtZjOKpXnTW9fInXGdVBQ.png"/></div></div></figure><p id="cb99" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么我们只需要知道<em class="mz"> K </em>而不是映射函数本身。这个函数被称为<strong class="kk iu">核函数</strong>，它降低了寻找映射函数的复杂性。所以，<strong class="kk iu">核函数在变换空间定义了内积。</strong></p><p id="a088" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看一些最常用的内核函数</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ne"><img src="../Images/8c1f3ff2841052669a79dfb8583a37cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V_jH1lc4XOUxxVCPn-13cQ.png"/></div></div></figure><p id="aeca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我已经使用径向基函数核绘制了图 2，其中从 2D 空间到 3D 空间的映射确实有助于我们分类。</p><p id="62e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">除了这个预定义的内核，<em class="mz">哪些条件决定了哪些函数可以被认为是内核？</em>这是由<strong class="kk iu">默瑟定理给出的。</strong>第一个条件相当简单，即<strong class="kk iu">内核函数必须对称。</strong>作为核函数是<em class="mz">点积(内积)</em>的映射函数我们可以写成如下——</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nf"><img src="../Images/27308a3c78b02201543ce4ab59ba81ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3wyI43_K9ulob19IE5tI6Q.png"/></div></div></figure><p id="b075" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Mercer 定理给出了函数是核函数的充要条件。理解这个定理的一种方式是—</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ng"><img src="../Images/e5ce5c3f100b4b1bc9c0919d56e3b7ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WU4T1PMct7_FxeOHw7Scpw.png"/></div></div></figure><p id="e24b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">换句话说，在有限输入空间中，如果核矩阵(也称为 Gram 矩阵)是<em class="mz">正半定的</em>那么，矩阵元素即函数 K 可以是核函数。</strong>因此 Gram matrix 将学习算法所需的所有信息、数据点和映射函数合并成内积。</p><p id="b139" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看一个从核函数中找到<strong class="kk iu">映射函数的例子，这里我们将使用高斯核函数</strong></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nh"><img src="../Images/1b9030bd100401b504675649b93c711b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dvBDsHAwe7qkGiPfsPGdoA.png"/></div></div></figure></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h2 id="5153" class="mr ms it bd np nq nr dn ns nt nu dp nv kr nw nx ny kv nz oa ob kz oc od oe of bi translated">调谐参数</h2><p id="fe77" class="pw-post-body-paragraph ki kj it kk b kl og ju kn ko oh jx kq kr oi kt ku kv oj kx ky kz ok lb lc ld im bi translated">既然我们已经讨论了非线性核，特别是高斯核(或 RBF 核)，我将以对 SVM 中的一个调整参数—<em class="mz">γ的直观理解来结束这篇文章。</em></p><p id="3bd6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">观察 RBF 核，我们看到它取决于两点之间的欧几里德距离，即如果两个向量越近，则该项越小。由于方差总是正的，这意味着对于较近的向量，RBF 核比较远的向量分布更广。当<em class="mz"> gamma </em>参数较高时，核函数值会较小，即使对于两个邻近的样本也是如此，这可能导致复杂的判定边界或引起过拟合。你可以在我的另一篇文章中读到更多。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ol"><img src="../Images/55bacb2d9fde05163b771ada541f027d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B8e0TE2rTx8gdOH1FA1rXg.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Example of over-fitting and complex decision boundary with high values of gamma. [Image Courtesy : <a class="ae le" href="https://chrisalbon.com/machine_learning/support_vector_machines/svc_parameters_using_rbf_kernel/" rel="noopener ugc nofollow" target="_blank">Chris Albon]</a></figcaption></figure></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="1771" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在结束讨论之前，让我们回顾一下到目前为止我们在本章中学到了什么</p><ol class=""><li id="5af0" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld om ll lm ln bi translated">将数据点从低维空间映射到高维空间可以使得即使对于非线性数据样本也可以应用 SVM。</li><li id="989e" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld om ll lm ln bi translated">我们不需要知道映射函数本身，只要知道核函数即可；<strong class="kk iu"> <em class="mz">内核绝招</em> </strong></li><li id="9e7a" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld om ll lm ln bi translated">函数被认为是核函数的条件；<em class="mz">半正定格拉姆矩阵。</em></li><li id="eb00" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld om ll lm ln bi translated">最常用的内核类型。</li><li id="b17f" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld om ll lm ln bi translated">调节参数γ如何导致 RBF 核的过度拟合或偏差。</li></ol><p id="d802" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">希望你喜欢这篇文章，在下一章，我们将看到一些使用 SVM 算法的机器学习例子。</p><p id="cc69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">SVM 算法背后的完整数学在这里讨论:</strong></p><div class="on oo gp gr op oq"><a rel="noopener follow" target="_blank" href="/understanding-support-vector-machine-part-1-lagrange-multipliers-5c24a52ffc5e"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">支持向量机:完整理论</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">SVM 的决策规则</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">towardsdatascience.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe md oq"/></div></div></a></div><p id="f63f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于绘制和理解支持向量机决策边界的更多信息，请查看这篇文章:</p><div class="on oo gp gr op oq"><a rel="noopener follow" target="_blank" href="/visualizing-support-vector-machine-decision-boundary-69e7591dacea"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">用 Python 实现流水线中的主成分分析和 SVM</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">管道、网格搜索和等高线图</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">towardsdatascience.com</p></div></div><div class="oz l"><div class="pf l pb pc pd oz pe md oq"/></div></div></a></div><p id="7f53" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">保持坚强！干杯！！</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="72b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="mz">如果你对更深入的基础机器学习概念感兴趣，可以考虑加盟 Medium 使用</em> </strong> <a class="ae le" href="https://saptashwa.medium.com/membership" rel="noopener"> <strong class="kk iu"> <em class="mz">我的链接</em> </strong> </a> <strong class="kk iu"> <em class="mz">。你不用额外付钱，但我会得到一点佣金。感谢大家！！</em> </strong></p><div class="on oo gp gr op oq"><a href="https://medium.com/subscribe/@saptashwa?source=publishing_settings-------------------------------------" rel="noopener follow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">每当 Saptashwa Bhattacharyya 发表文章时，收到一封电子邮件。</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">每当 Saptashwa Bhattacharyya 发表文章时，收到一封电子邮件。通过注册，您将创建一个中等帐户，如果您没有…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">medium.com</p></div></div><div class="oz l"><div class="pg l pb pc pd oz pe md oq"/></div></div></a></div></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h2 id="bc57" class="mr ms it bd np nq nr dn ns nt nu dp nv kr nw nx ny kv nz oa ob kz oc od oe of bi translated">参考资料:</h2><ol class=""><li id="0e35" class="lf lg it kk b kl og ko oh kr ph kv pi kz pj ld om ll lm ln bi translated">“统计学习的要素”；Hastie，t .等人；<a class="ae le" href="https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E" rel="noopener ugc nofollow" target="_blank">亚马逊链接</a></li><li id="e31f" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld om ll lm ln bi translated">”内核的属性”；伯克利<a class="ae le" href="https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c03_p47-84.pdf" rel="noopener ugc nofollow" target="_blank">大学；pdf </a>。</li><li id="4dae" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld om ll lm ln bi translated">“内核”；麻省理工学院<a class="ae le" href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf" rel="noopener ugc nofollow" target="_blank">讲稿</a>。</li></ol></div></div>    
</body>
</html>