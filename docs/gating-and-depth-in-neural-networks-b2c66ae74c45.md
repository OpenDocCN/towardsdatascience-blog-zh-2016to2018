# 神经网络中的门控和深度

> 原文：<https://towardsdatascience.com/gating-and-depth-in-neural-networks-b2c66ae74c45?source=collection_archive---------6----------------------->

![](img/d537da0fb60f6e83e5640a55ef3ad5b5.png)

深度是现代神经网络的关键部分。它们通过构建层次规则来实现高效的表示。到目前为止，我们都知道这一点，所以我假设我不需要说服任何人，但如果你需要复习一下，这基本上是因为我们无法有效地建模许多数据分布，这些分布在没有指数数量的神经元的情况下用单个或几个函数出现。这些分布太复杂了，无法以这样一种直接的方式建模，但尽管如此，它们确实有结构，只是碰巧是分层的。换一种方式想，想象一下如果数据生成过程是*而不是*分层的。那么生成一个复杂的分布也需要指数级的资源。毫无疑问，有些过程是这样的。也就是说，我们所知道的关于这个世界的一件事是，它是由简单的部分组成的，但是把它们放在一起有时会产生极其复杂的行为。

不幸的是，我们的网络训练算法，误差反向传播，不喜欢当事情太多递归。考虑线性神经元链的情况。如果这些神经元的(范数)权重不等于 1，误差信号将连续收缩或增长。如果收缩太快，我们看到它消失了，如果增长太快，我们说它爆炸了。对于具有非线性激活函数的神经元，情况大致相同，只是我们考虑雅可比矩阵的范数，但基本上是一样的。缩放并不酷，因为这意味着当我们从损失向后传播误差时，我们看到信号变得不正常，这意味着对权重的更新是无用的，并且我们的网络训练发散。

很好，我们都熟悉消失和爆炸渐变。我们该怎么办？基本策略是确保所有缩放不会干扰误差信号，同时仍然支持深度。我们需要保护一些信号。

在前馈网络(FFNs)中，我们可以通过跳过连接来实现这一点。最简单的情况是剩余学习，从较低层到较高层的输出被加到该较高层的输出上。然后，连续的层只需要学习增量，因为它们已经从前一层的位置“开始”了。这种剩余被认为比从零开始构建新的转换更容易学习。

这样做的另一个原因是残余连接实际上使得从给定层到输出的路径更短。你所需要做的就是穿过连续层的剩余连接，直到你到达终点，跳过中间的层。碰巧的是，深度残差网络实际上具有明显小于指定深度的有效深度，因为这正是训练中发生的情况。换句话说，我们避免了很多不利的转变。这隐含地发生在剩余网络中，但是可以通过在每一层和输出之间具有被称为密集网络的连接来明确地实现。

顺便说一下，有可能从上一层的输出开始实际上会使学习下一层的最佳变换变得更加困难。或者至少，这可能是对许多层的低效使用。因此，我们引入了门的概念，并说让我们学习一个系数来衰减相对于普通堆叠层应该使用多少身份连接。然后，剩余网络成为具有加权跳跃连接的网络的特殊情况，称为公路网络。

这非常有趣，因为在某种意义上，我们已经为 FFNs 实现了一个内存，通过门控机制，我们可以了解何时访问该内存，何时忽略它。您已经在较低层创建了一些表示，该信号可以跳过许多层，并在其他地方找到自己。其他地方层上的(学习)门告诉你是否访问该信息。

现在让我们考虑一个不同的问题:序列建模。ffn 在这方面当然很棒，序列是证明深度的好方法。如果你使用卷积 FFN(现在每个人都这样)，你实际上需要一定的深度来考虑输入中的长程相关性。这在语言建模中非常明显，因为长程相关性可能非常长。然后，跳过连接以及注意力之类的东西充当更细粒度信息的内存查找机制，以服务于更高级别的表示。这一切都很酷。

还有另一种有趣的方式来看待这个问题，那就是循环网络(RNNs)。在这种情况下，我们有一个单一的隐藏层，它应用于序列中的每个元素，然后在最后一个元素之后，您可以“及时”反向传播您的错误。所以你可以想象，当我们的梯度回到序列的开始时，它们会消失。然而，因为我们在每个序列步骤中使用相同的权重，所以在这里添加跳过连接没有意义。

相反，RNN 人所做的是用一种门控机制把其它小的记忆系统放进去。现在，您可以通过将信号写入某处的特殊状态来保护信号的某一部分，并了解何时将其读回。那么有效深度对 RNN 来说意味着什么呢？我不确定这是否真的比得上在序列方向上的深度概念，但无论如何这是一个有趣的类比。

想想我们根本没有解决渐变消失或爆炸的问题，这真的很有趣，我们只是回避了这个问题。所以现在我们可以问这是自然的还是怪异的。我碰巧觉得，我们能这么好地凭空拉出层次化的表示，还是很奇怪的。值得注意的是中间损失的有效性，特别是在 NLP 应用程序中，由于存在各种可以生成供您预测的特征(例如，词性、命名实体)的解析器，所以您可以获得这种类型的信息。

为什么这样做是很明显的，这与跳过连接有关。简单地说，你将你的表现锚定在除了最终图层输出之外的东西上。在剩余 FFN 的情况下，它是到较低层的输出，这没有什么不同，因为它也是一个额外的信号，你知道在某种程度上是一个先验的良好表示(就像你的输入！).

目前看来，渐变的消失和爆炸确实是一个基本的问题，尽管不是不可克服的。